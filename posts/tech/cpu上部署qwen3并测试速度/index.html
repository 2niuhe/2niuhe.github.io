<!DOCTYPE html>
<html lang="zh" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>CPU上部署Qwen3模型及性能测试 | Niuhe&#39;s Blog</title>
<meta name="keywords" content="LLM, Qwen3, Ollama, vLLM">
<meta name="description" content="详解如何在CPU环境下部署和运行Qwen3大语言模型">
<meta name="author" content="Niuhe">
<link rel="canonical" href="https://blog.niuhemoon.win/posts/tech/cpu%E4%B8%8A%E9%83%A8%E7%BD%B2qwen3%E5%B9%B6%E6%B5%8B%E8%AF%95%E9%80%9F%E5%BA%A6/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.3613efbd0b1772781e8f49935e973cae632a7f61471c05b17be155505ccf87b5.css" integrity="sha256-NhPvvQsXcngej0mTXpc8rmMqf2FHHAWxe&#43;FVUFzPh7U=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://blog.niuhemoon.win/base/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://blog.niuhemoon.win/base/favicon.ico">
<link rel="icon" type="image/png" sizes="32x32" href="https://blog.niuhemoon.win/base/favicon.ico">
<link rel="apple-touch-icon" href="https://blog.niuhemoon.win/base/avatar.jpeg">
<link rel="mask-icon" href="https://blog.niuhemoon.win/base/avatar.jpeg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-116933089-1', 'auto');
	
	ga('send', 'pageview');
}
</script><meta property="og:title" content="CPU上部署Qwen3模型及性能测试" />
<meta property="og:description" content="详解如何在CPU环境下部署和运行Qwen3大语言模型" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://blog.niuhemoon.win/posts/tech/cpu%E4%B8%8A%E9%83%A8%E7%BD%B2qwen3%E5%B9%B6%E6%B5%8B%E8%AF%95%E9%80%9F%E5%BA%A6/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-04-28T00:00:00+00:00" />
<meta property="article:modified_time" content="2025-04-28T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="CPU上部署Qwen3模型及性能测试"/>
<meta name="twitter:description" content="详解如何在CPU环境下部署和运行Qwen3大语言模型"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "📚文章",
      "item": "https://blog.niuhemoon.win/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "👨🏻‍💻 技术",
      "item": "https://blog.niuhemoon.win/posts/tech/"
    }, 
    {
      "@type": "ListItem",
      "position":  4 ,
      "name": "CPU上部署Qwen3模型及性能测试",
      "item": "https://blog.niuhemoon.win/posts/tech/cpu%E4%B8%8A%E9%83%A8%E7%BD%B2qwen3%E5%B9%B6%E6%B5%8B%E8%AF%95%E9%80%9F%E5%BA%A6/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "CPU上部署Qwen3模型及性能测试",
  "name": "CPU上部署Qwen3模型及性能测试",
  "description": "详解如何在CPU环境下部署和运行Qwen3大语言模型",
  "keywords": [
    "LLM", "Qwen3", "Ollama", "vLLM"
  ],
  "articleBody": "简介 Qwen3是阿里云推出的新一代大语言模型，在各项基准测试中表现出色。本文将介绍如何在普通CPU环境下部署Qwen3模型，并测试其性能表现，为没有GPU资源的用户提供参考。\nOllama方案 Ollama是一个轻量级的本地LLM运行框架，支持多种模型格式，安装非常简单：\ncurl -fsSL https://ollama.com/install.sh | sh 基本部署 使用Ollama部署Qwen3非常简单，只需一行命令即可下载并运行模型：\nollama run qwen3:30b-a3b-q4_K_M # 如果使用modelscope # ollama run modelscope.cn/lmstudio-community/Qwen3-30B-A3B-GGUF # 对于笔者使用的amd核显主机，执行rocminfo| grep gfx 结果是gfx1103 # AMD适配最近的是gfx1102，所以可以配置环境变量来让ollama适配最接近的版G # [Unit] # Description=Ollama Service # After=network-online.target # # [Service] # ExecStart=/usr/local/bin/ollama serve # User=ollama # Group=ollama # Restart=always # RestartSec=3 # Environment=\"OLLAMA_HOST=0.0.0.0\" # Environment=\"HSA_OVERRIDE_GFX_VERSION=11.0.2\" # 其它常用的环境变量 # 强制只使用cpu # Environment=\"CUDA_VISIBLE_DEVICES=-1\" # 更改模型缓存路径 # Environment=\"OLLAMA_MODELS=/path/to/models/\" 部署完成后可以通过ollama命令或者openai兼容接口进行测试\nollama ls curl http://localhost:11434/v1/models 性能测试 uv pip install llm-benchmark 编辑配置文件\nfile_name: \"custombenchmarkmodels.yml\" version: 2.0.custom models: - model: \"modelscope.cn/lmstudio-community/Qwen3-30B-A3B-GGUF\" 运行测试\nllm_benchmark run --custombenchmark=path/to/custombenchmarkmodels.yml 测试在AMD Ryzen 7 PRO 8845HS w平台平均22token/s，已经达到本地可用的速度\nvllm方案 下载cpu镜像 docker pull public.ecr.aws/q9t5s3a7/vllm-cpu-release-repo:v0.8.5 docker tag public.ecr.aws/q9t5s3a7/vllm-cpu-release-repo:v0.8.5 vllm-cpu-env:latest # 另一种方式自行编译 # git clone https://github.com/vllm-project/vllm.git # cd vllm # sudo docker build -f docker/Dockerfile.cpu -t vllm-cpu-env --shm-size=16g . 部署模型 目前CPU版本的vLLM不支持Qwen3 q8量化，只能运行非量化版本，速度较慢，约1.8token/s，基本不可用。\n# 下载模型 modelscope download --model Qwen/Qwen3-30B-A3B # 运行docker测试 docker run --rm --network=host -e TRANSFORMERS_OFFLINE=1 -e HF_DATASET_OFFLINE=1 -v /home/nh/.cache/modelscope/hub/models/Qwen/Qwen3-30B-A3B/:/model -p 8000:8000 vllm-cpu-env:latest --model /model API接口测试 部署完成后，可以通过OpenAI兼容的API接口进行测试：\ncurl http://localhost:8000/v1/chat/completions -H \"Content-Type: application/json\" -d '{ \"model\": \"/model\", \"messages\": [ {\"role\": \"user\", \"content\": \"Give me a short introduction to large language models.\"} ], \"temperature\": 0.7, \"top_p\": 0.8, \"top_k\": 20, \"max_tokens\": 8192, \"presence_penalty\": 1.5, \"chat_template_kwargs\": {\"enable_thinking\": false} }' 参考 ollama/docs/faq.md at main · ollama/ollama\nHF-Mirror\nvLLM - Qwen\nOllama加载ModelScope模型 · 文档中心\naidatatools/ollama-benchmark: LLM Benchmark for Throughput via Ollama (Local LLMs) Ollama GPU支持 - Nvidia和AMD GPU兼容性 | LlamaFactory | LlamaFactory OpenAI compatibility · Ollama Blog\n",
  "wordCount" : "738",
  "inLanguage": "zh",
  "datePublished": "2025-04-28T00:00:00Z",
  "dateModified": "2025-04-28T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Niuhe"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://blog.niuhemoon.win/posts/tech/cpu%E4%B8%8A%E9%83%A8%E7%BD%B2qwen3%E5%B9%B6%E6%B5%8B%E8%AF%95%E9%80%9F%E5%BA%A6/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Niuhe's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://blog.niuhemoon.win/base/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://blog.niuhemoon.win" accesskey="h" title="Niuhe&#39;s Blog (Alt + H)">
                <img src="https://blog.niuhemoon.win/base/avatar.jpeg" alt="" aria-label="logo"
                    height="35">Niuhe&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://blog.niuhemoon.win/search" title="🔍搜索 (Alt &#43; /)" accesskey=/>
                    <span>🔍搜索</span>
                </a>
            </li>
            <li>
                <a href="https://blog.niuhemoon.win/" title="🏠主页">
                    <span>🏠主页</span>
                </a>
            </li>
            <li>
                <a href="https://blog.niuhemoon.win/posts" title="📚文章">
                    <span>📚文章</span>
                </a>
            </li>
            <li>
                <a href="https://blog.niuhemoon.win/tags" title="🔖标签">
                    <span>🔖标签</span>
                </a>
            </li>
            <li>
                <a href="https://blog.niuhemoon.win/archives/" title="⏱时间轴">
                    <span>⏱时间轴</span>
                </a>
            </li>
            <li>
                <a href="https://blog.niuhemoon.win/about" title="🙋🏻‍♂️关于">
                    <span>🙋🏻‍♂️关于</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://blog.niuhemoon.win">🏠主页</a>&nbsp;»&nbsp;<a href="https://blog.niuhemoon.win/posts/">📚文章</a>&nbsp;»&nbsp;<a href="https://blog.niuhemoon.win/posts/tech/">👨🏻‍💻 技术</a></div>
    <h1 class="post-title">
      CPU上部署Qwen3模型及性能测试
    </h1>
    <div class="post-description">
      详解如何在CPU环境下部署和运行Qwen3大语言模型
    </div>
    <div class="post-meta"><span title='2025-04-28 00:00:00 +0000 UTC'>2025-04-28</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;738 字&nbsp;·&nbsp;Niuhe

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">文章目录</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#%e7%ae%80%e4%bb%8b" aria-label="简介">简介</a></li>
                <li>
                    <a href="#ollama%e6%96%b9%e6%a1%88" aria-label="Ollama方案">Ollama方案</a><ul>
                        
                <li>
                    <a href="#%e5%9f%ba%e6%9c%ac%e9%83%a8%e7%bd%b2" aria-label="基本部署">基本部署</a></li>
                <li>
                    <a href="#%e6%80%a7%e8%83%bd%e6%b5%8b%e8%af%95" aria-label="性能测试">性能测试</a></li></ul>
                </li>
                <li>
                    <a href="#vllm%e6%96%b9%e6%a1%88" aria-label="vllm方案">vllm方案</a><ul>
                        
                <li>
                    <a href="#%e4%b8%8b%e8%bd%bdcpu%e9%95%9c%e5%83%8f" aria-label="下载cpu镜像">下载cpu镜像</a></li>
                <li>
                    <a href="#%e9%83%a8%e7%bd%b2%e6%a8%a1%e5%9e%8b" aria-label="部署模型">部署模型</a></li>
                <li>
                    <a href="#api%e6%8e%a5%e5%8f%a3%e6%b5%8b%e8%af%95" aria-label="API接口测试">API接口测试</a></li></ul>
                </li>
                <li>
                    <a href="#%e5%8f%82%e8%80%83" aria-label="参考">参考</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h3 id="简介">简介<a hidden class="anchor" aria-hidden="true" href="#简介">#</a></h3>
<blockquote>
<p>Qwen3是阿里云推出的新一代大语言模型，在各项基准测试中表现出色。本文将介绍如何在普通CPU环境下部署Qwen3模型，并测试其性能表现，为没有GPU资源的用户提供参考。</p>
</blockquote>
<h3 id="ollama方案">Ollama方案<a hidden class="anchor" aria-hidden="true" href="#ollama方案">#</a></h3>
<p>Ollama是一个轻量级的本地LLM运行框架，支持多种模型格式，安装非常简单：</p>
<div class="highlight"><pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>curl -fsSL https://ollama.com/install.sh | sh
</span></span></code></pre></div><h4 id="基本部署">基本部署<a hidden class="anchor" aria-hidden="true" href="#基本部署">#</a></h4>
<p>使用Ollama部署Qwen3非常简单，只需一行命令即可下载并运行模型：</p>
<div class="highlight"><pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>ollama run qwen3:30b-a3b-q4_K_M
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 如果使用modelscope</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># ollama run modelscope.cn/lmstudio-community/Qwen3-30B-A3B-GGUF</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 对于笔者使用的amd核显主机，执行rocminfo| grep gfx 结果是gfx1103</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># AMD适配最近的是gfx1102，所以可以配置环境变量来让ollama适配最接近的版G</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># [Unit]</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># Description=Ollama Service</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># After=network-online.target</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># </span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># [Service]</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># ExecStart=/usr/local/bin/ollama serve</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># User=ollama</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># Group=ollama</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># Restart=always</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># RestartSec=3</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># Environment=&#34;OLLAMA_HOST=0.0.0.0&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># Environment=&#34;HSA_OVERRIDE_GFX_VERSION=11.0.2&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 其它常用的环境变量</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 强制只使用cpu</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># Environment=&#34;CUDA_VISIBLE_DEVICES=-1&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 更改模型缓存路径</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># Environment=&#34;OLLAMA_MODELS=/path/to/models/&#34;</span>
</span></span></code></pre></div><p>部署完成后可以通过ollama命令或者openai兼容接口进行测试</p>
<div class="highlight"><pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>ollama ls 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>curl http://localhost:11434/v1/models
</span></span></code></pre></div><h4 id="性能测试">性能测试<a hidden class="anchor" aria-hidden="true" href="#性能测试">#</a></h4>
<div class="highlight"><pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>uv pip install llm-benchmark
</span></span></code></pre></div><p>编辑配置文件</p>
<div class="highlight"><pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="font-weight:bold">file_name</span>: <span style="color:#0ff;font-weight:bold">&#34;custombenchmarkmodels.yml&#34;</span>
</span></span><span style="display:flex;"><span><span style="font-weight:bold">version</span>: <span style="color:#ff0;font-weight:bold">2.0</span>.custom
</span></span><span style="display:flex;"><span><span style="font-weight:bold">models</span>:
</span></span><span style="display:flex;"><span>  - <span style="font-weight:bold">model</span>: <span style="color:#0ff;font-weight:bold">&#34;modelscope.cn/lmstudio-community/Qwen3-30B-A3B-GGUF&#34;</span>
</span></span></code></pre></div><p>运行测试</p>
<div class="highlight"><pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>llm_benchmark run --custombenchmark=path/to/custombenchmarkmodels.yml
</span></span></code></pre></div><p>测试在<code>AMD Ryzen 7 PRO 8845HS w</code>平台平均22token/s，已经达到本地可用的速度</p>
<h3 id="vllm方案">vllm方案<a hidden class="anchor" aria-hidden="true" href="#vllm方案">#</a></h3>
<h4 id="下载cpu镜像">下载cpu镜像<a hidden class="anchor" aria-hidden="true" href="#下载cpu镜像">#</a></h4>
<div class="highlight"><pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>docker pull public.ecr.aws/q9t5s3a7/vllm-cpu-release-repo:v0.8.5
</span></span><span style="display:flex;"><span>docker tag public.ecr.aws/q9t5s3a7/vllm-cpu-release-repo:v0.8.5 vllm-cpu-env:latest
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 另一种方式自行编译</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># git clone https://github.com/vllm-project/vllm.git</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># cd vllm</span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># sudo docker build -f docker/Dockerfile.cpu -t vllm-cpu-env --shm-size=16g .</span>
</span></span></code></pre></div><h4 id="部署模型">部署模型<a hidden class="anchor" aria-hidden="true" href="#部署模型">#</a></h4>
<p>目前CPU版本的vLLM不支持Qwen3 q8量化，只能运行非量化版本，速度较慢，约1.8token/s，基本不可用。</p>
<div class="highlight"><pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#007f7f"># 下载模型</span>
</span></span><span style="display:flex;"><span>modelscope download --model Qwen/Qwen3-30B-A3B
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#007f7f"># 运行docker测试</span>
</span></span><span style="display:flex;"><span>docker run --rm --network=host -e TRANSFORMERS_OFFLINE=<span style="color:#ff0;font-weight:bold">1</span> -e HF_DATASET_OFFLINE=<span style="color:#ff0;font-weight:bold">1</span> -v /home/nh/.cache/modelscope/hub/models/Qwen/Qwen3-30B-A3B/:/model -p 8000:8000 vllm-cpu-env:latest --model /model
</span></span></code></pre></div><h4 id="api接口测试">API接口测试<a hidden class="anchor" aria-hidden="true" href="#api接口测试">#</a></h4>
<p>部署完成后，可以通过OpenAI兼容的API接口进行测试：</p>
<div class="highlight"><pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>curl http://localhost:8000/v1/chat/completions -H <span style="color:#0ff;font-weight:bold">&#34;Content-Type: application/json&#34;</span> -d <span style="color:#0ff;font-weight:bold">&#39;{
</span></span></span><span style="display:flex;"><span><span style="color:#0ff;font-weight:bold">  &#34;model&#34;: &#34;/model&#34;,
</span></span></span><span style="display:flex;"><span><span style="color:#0ff;font-weight:bold">  &#34;messages&#34;: [
</span></span></span><span style="display:flex;"><span><span style="color:#0ff;font-weight:bold">    {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Give me a short introduction to large language models.&#34;}
</span></span></span><span style="display:flex;"><span><span style="color:#0ff;font-weight:bold">  ],
</span></span></span><span style="display:flex;"><span><span style="color:#0ff;font-weight:bold">  &#34;temperature&#34;: 0.7,
</span></span></span><span style="display:flex;"><span><span style="color:#0ff;font-weight:bold">  &#34;top_p&#34;: 0.8,
</span></span></span><span style="display:flex;"><span><span style="color:#0ff;font-weight:bold">  &#34;top_k&#34;: 20,
</span></span></span><span style="display:flex;"><span><span style="color:#0ff;font-weight:bold">  &#34;max_tokens&#34;: 8192,
</span></span></span><span style="display:flex;"><span><span style="color:#0ff;font-weight:bold">  &#34;presence_penalty&#34;: 1.5,
</span></span></span><span style="display:flex;"><span><span style="color:#0ff;font-weight:bold">  &#34;chat_template_kwargs&#34;: {&#34;enable_thinking&#34;: false}
</span></span></span><span style="display:flex;"><span><span style="color:#0ff;font-weight:bold">}&#39;</span>
</span></span></code></pre></div><h3 id="参考">参考<a hidden class="anchor" aria-hidden="true" href="#参考">#</a></h3>
<p><a href="https://github.com/ollama/ollama/blob/main/docs/faq.md">ollama/docs/faq.md at main · ollama/ollama</a><br>
<a href="https://hf-mirror.com/">HF-Mirror</a><br>
<a href="https://qwen.readthedocs.io/en/latest/deployment/vllm.html#thinking-non-thinking-modes">vLLM - Qwen</a><br>
<a href="https://modelscope.cn/docs/models/advanced-usage/ollama-integration">Ollama加载ModelScope模型 · 文档中心</a><br>
<a href="https://github.com/aidatatools/ollama-benchmark">aidatatools/ollama-benchmark: LLM Benchmark for Throughput via Ollama (Local LLMs)</a>
<a href="https://www.llamafactory.cn/ollama-docs/gpu.html">Ollama GPU支持 - Nvidia和AMD GPU兼容性 | LlamaFactory | LlamaFactory</a>
<a href="https://ollama.com/blog/openai-compatibility">OpenAI compatibility · Ollama Blog</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://blog.niuhemoon.win/tags/llm/">LLM</a></li>
      <li><a href="https://blog.niuhemoon.win/tags/qwen3/">Qwen3</a></li>
      <li><a href="https://blog.niuhemoon.win/tags/ollama/">Ollama</a></li>
      <li><a href="https://blog.niuhemoon.win/tags/vllm/">vLLM</a></li>
    </ul>
<div class="footer-comments">
  <p>For comments, please   <a href="mailto:carlton2tang@gmail.com" class="email-button" style="font-size: inherit; padding: 5px 10px; background-color: #3f6b9a; color: white; text-decoration: none; border-radius: 3px; transition: background-color 0.3s;">
    send an email
</a> to me</p>

</div>
<nav class="paginav">
  <a class="prev" href="https://blog.niuhemoon.win/posts/tech/python-zeromq-gil/">
    <span class="title">« 上一页</span>
    <br>
    <span>Python 性能优化：使用 ZeroMQ 突破 GIL 限制</span>
  </a>
  <a class="next" href="https://blog.niuhemoon.win/posts/tech/cuda-triton-flash-attention/">
    <span class="title">下一页 »</span>
    <br>
    <span>CUDA、Triton 与 Flash Attention 学习之旅</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
      
    <span>&copy; 2025 <a href="https://blog.niuhemoon.win">Niuhe&#39;s Blog</a></span>
    <span xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/">
        Licensed under
        <a
          href="http://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1"
          target="_blank"
          rel="license noopener noreferrer"
          style="display:inline-block;"
          >CC BY-NC-SA 4.0 </a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = '📄复制';

        function copyingDone() {
            copybutton.innerHTML = '👌🏻已复制!';
            setTimeout(() => {
                copybutton.innerHTML = '📄复制';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
