[{"content":"本项目演示了如何使用JSON-RPC协议实现Go语言和Python之间的跨语言通信。项目包含两种场景的完整实现，展示了双向的RPC调用能力。\n项目概述 JSON-RPC是一种轻量级的远程过程调用协议，使用JSON作为数据交换格式。相比gRPC，JSON-RPC更加简单易用，特别适合跨语言通信场景。\n本demo项目实现了一个简单的算术运算服务，包含乘法(Multiply)和加法(Plus)两个方法，展示了以下两种场景：\n场景一：Go语言作为服务器，Python作为客户端 场景二：Python作为服务器，Go语言作为客户端 场景一：Go服务器 + Python客户端 Go服务器实现 Go服务器使用标准库中的net/rpc和net/rpc/jsonrpc包来实现JSON-RPC服务。\n文件：json_rpc_server.go\npackage main import ( \u0026#34;log\u0026#34; \u0026#34;net\u0026#34; \u0026#34;net/rpc\u0026#34; \u0026#34;net/rpc/jsonrpc\u0026#34; ) type Arith int type Args struct { A, B int } // 乘法 func (t *Arith) Mulitply(args *Args, reply *int) error { *reply = args.A * args.B return nil } func (t *Arith) Plus(args *Args, reply *int) error { *reply = args.A + args.B return nil } func main() { addr := \u0026#34;:1234\u0026#34; server := rpc.NewServer() server.Register(new(Arith)) l, e := net.Listen(\u0026#34;tcp\u0026#34;, addr) if e != nil { log.Fatalln(\u0026#34;listen error:\u0026#34;, e) } else { log.Println(\u0026#34;rpc listening \u0026#34;, addr) } defer l.Close() for { conn, err := l.Accept() if err != nil { log.Print(\u0026#34;rpc.Serve: accept:\u0026#34;, err.Error()) return } go server.ServeCodec(jsonrpc.NewServerCodec(conn)) } } Python客户端实现 Python客户端使用socket和json标准库实现JSON-RPC客户端功能。\n文件：json_rpc_client.py\nimport json import socket import itertools class RPCClient(object): def __init__(self, addr, codec=json): self._socket = socket.create_connection(addr) self._id_iter = itertools.count() self._codec = codec def _message(self, name, *params): return dict(id=next(self._id_iter), params=list(params), method=name) def call(self, name, *params): req = self._message(name, *params) id = req.get(\u0026#39;id\u0026#39;) \u0026#34;\u0026#34;\u0026#34; Golang Rpc 返回的Json格式 type serverResponse struct { Id *json.RawMessage `json:\u0026#34;id\u0026#34;` Result interface{} `json:\u0026#34;result\u0026#34;` Error interface{} `json:\u0026#34;error\u0026#34;` } \u0026#34;\u0026#34;\u0026#34; mesg = self._codec.dumps(req) mesg = mesg.encode(encoding=\u0026#39;utf-8\u0026#39;) self._socket.sendall(mesg) # This will actually have to loop if resp is bigger resp = self._socket.recv(4096) resp = resp.decode(encoding=\u0026#39;utf-8\u0026#39;) resp = self._codec.loads(resp) if resp.get(\u0026#39;id\u0026#39;) != id: raise Exception(\u0026#34;expected id=%s, received id=%s: %s\u0026#34; %(id, resp.get(\u0026#39;id\u0026#39;), resp.get(\u0026#39;error\u0026#39;))) if resp.get(\u0026#39;error\u0026#39;) is not None: raise Exception(resp.get(\u0026#39;error\u0026#39;)) return resp.get(\u0026#39;result\u0026#39;) def close(self): self._socket.close() if __name__ == \u0026#39;__main__\u0026#39;: rpc = RPCClient((\u0026#34;127.0.0.1\u0026#34;, 1234)) args = {\u0026#39;A\u0026#39;:203, \u0026#39;B\u0026#39;:3} print(rpc.call(\u0026#34;Arith.Mulitply\u0026#34;,args)) print(rpc.call(\u0026#34;Arith.Plus\u0026#34;,args)) 运行场景一 启动Go服务器： go run json_rpc_server.go 运行Python客户端： python json_rpc_client.py 输出结果：\n609 206 场景二：Python服务器 + Go客户端 Python服务器实现 Python服务器使用多线程处理并发连接，实现了完整的JSON-RPC协议。\n文件：py_server_go_client/json_rpc_server.py\nimport json import socket import threading # Define the arithmetic service class ArithService: def Multiply(self, args): return args[\u0026#39;A\u0026#39;] * args[\u0026#39;B\u0026#39;] def Plus(self, args): return args[\u0026#39;A\u0026#39;] + args[\u0026#39;B\u0026#39;] # JSON-RPC Server class JSONRPCServer: def __init__(self, host=\u0026#39;0.0.0.0\u0026#39;, port=1234): self.host = host self.port = port self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) self.socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) self.socket.bind((host, port)) self.socket.listen(5) self.running = False self.services = {} def register_service(self, name, service): self.services[name] = service def handle_client(self, client_socket): while True: try: # Read request data = client_socket.recv(4096) if not data: break # Parse request request = json.loads(data.decode(\u0026#39;utf-8\u0026#39;)) method_name = request.get(\u0026#39;method\u0026#39;) params = request.get(\u0026#39;params\u0026#39;, []) request_id = request.get(\u0026#39;id\u0026#39;) # Process request result = None error = None try: if \u0026#39;.\u0026#39; in method_name: service_name, method = method_name.split(\u0026#39;.\u0026#39;, 1) service = self.services.get(service_name) if service and hasattr(service, method): method_func = getattr(service, method) result = method_func(params[0]) else: error = f\u0026#34;Method not found: {method_name}\u0026#34; else: error = f\u0026#34;Invalid method format: {method_name}\u0026#34; except Exception as e: error = str(e) # Send response response = { \u0026#39;id\u0026#39;: request_id, \u0026#39;result\u0026#39;: result, \u0026#39;error\u0026#39;: error } client_socket.sendall(json.dumps(response).encode(\u0026#39;utf-8\u0026#39;)) except Exception as e: print(f\u0026#34;Error handling client request: {e}\u0026#34;) break client_socket.close() def start(self): self.running = True print(f\u0026#34;JSON-RPC server listening on {self.host}:{self.port}\u0026#34;) try: while self.running: client_socket, _ = self.socket.accept() client_thread = threading.Thread(target=self.handle_client, args=(client_socket,)) client_thread.daemon = True client_thread.start() except KeyboardInterrupt: self.stop() def stop(self): self.running = False self.socket.close() print(\u0026#34;Server stopped\u0026#34;) if __name__ == \u0026#39;__main__\u0026#39;: # Create server server = JSONRPCServer() # Register services server.register_service(\u0026#39;Arith\u0026#39;, ArithService()) # Start server try: server.start() except KeyboardInterrupt: server.stop() Go客户端实现 Go客户端使用标准库的net/rpc/jsonrpc包连接Python服务器。\n文件：py_server_go_client/json_rpc_client.go\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/rpc/jsonrpc\u0026#34; ) // Args represents the arguments for arithmetic operations type Args struct { A int `json:\u0026#34;A\u0026#34;` B int `json:\u0026#34;B\u0026#34;` } func main() { // Connect to the Python JSON-RPC server client, err := jsonrpc.Dial(\u0026#34;tcp\u0026#34;, \u0026#34;127.0.0.1:1234\u0026#34;) if err != nil { log.Fatal(\u0026#34;dialing:\u0026#34;, err) } defer client.Close() // Prepare arguments args := \u0026amp;Args{A: 203, B: 3} // Call Multiply method var multiplyResult int err = client.Call(\u0026#34;Arith.Multiply\u0026#34;, args, \u0026amp;multiplyResult) if err != nil { log.Fatal(\u0026#34;Multiply error:\u0026#34;, err) } fmt.Printf(\u0026#34;Multiply: %d * %d = %d\\n\u0026#34;, args.A, args.B, multiplyResult) // Call Plus method var plusResult int err = client.Call(\u0026#34;Arith.Plus\u0026#34;, args, \u0026amp;plusResult) if err != nil { log.Fatal(\u0026#34;Plus error:\u0026#34;, err) } fmt.Printf(\u0026#34;Plus: %d + %d = %d\\n\u0026#34;, args.A, args.B, plusResult) } 运行场景二 启动Python服务器： cd py_server_go_client python json_rpc_server.py 运行Go客户端： cd py_server_go_client go run json_rpc_client.go 输出结果：\nMultiply: 203 * 3 = 609 Plus: 203 + 3 = 206 技术要点 JSON-RPC协议格式 请求格式：\n{ \u0026#34;id\u0026#34;: 1, \u0026#34;method\u0026#34;: \u0026#34;Arith.Multiply\u0026#34;, \u0026#34;params\u0026#34;: [{\u0026#34;A\u0026#34;: 203, \u0026#34;B\u0026#34;: 3}] } 响应格式：\n{ \u0026#34;id\u0026#34;: 1, \u0026#34;result\u0026#34;: 609, \u0026#34;error\u0026#34;: null } 实现特点 Go服务器：使用官方RPC库，支持方法注册和自动路由 Python服务器：手动实现JSON-RPC协议，支持服务注册和多线程处理 跨语言兼容：两种语言的客户端和服务器可以互相通信 错误处理：包含完整的错误处理机制 并发支持：服务器支持多客户端并发访问 总结 本项目展示了JSON-RPC在跨语言通信中的实用性。相比gRPC，JSON-RPC具有以下优势：\n简单易用：协议简单，易于理解和实现 跨语言友好：基于JSON，几乎所有语言都有很好的支持 调试方便：可读的JSON格式便于调试 轻量级：协议开销小，适合简单的RPC场景 通过本demo，可以快速上手跨语言RPC开发，为分布式系统的不同语言组件提供通信基础。\n","permalink":"https://blog.niuhemoon.win/posts/tech/cross-language-json-rpc-go-python/","summary":"\u003cp\u003e本项目演示了如何使用JSON-RPC协议实现Go语言和Python之间的跨语言通信。项目包含两种场景的完整实现，展示了双向的RPC调用能力。\u003c/p\u003e","title":"跨语言JSON-RPC通信实现：Go与Python互操作Demo"},{"content":" Python Fire 是一个由 Google 开发的开源库，用于从任何 Python 对象自动生成命令行界面（CLI）。它的核心优势是：简单、自动化、几乎零样板代码。\n如果你厌倦了为写一个简单的脚本而不得不编写 argparse 的大量模板代码，那么 Fire 绝对是你的得力助手。\n为什么选择 Fire 而不是 Argparse？ argparse 是 Python 的标准库，功能强大，但不够简洁。Fire 通过自动化解决了这个问题。\n场景：创建一个简单的问候脚本\n使用 argparse 的传统方式：\n你需要定义解析器、添加参数、解析参数，然后才能调用你的函数。\n# greet_argparse.py import argparse def greet(name, greeting=\u0026#34;Hello\u0026#34;): \u0026#34;\u0026#34;\u0026#34;生成一个问候语\u0026#34;\u0026#34;\u0026#34; print(f\u0026#34;{greeting}, {name}!\u0026#34;) if __name__ == \u0026#39;__main__\u0026#39;: parser = argparse.ArgumentParser(description=\u0026#34;Greets someone.\u0026#34;) parser.add_argument(\u0026#34;name\u0026#34;, type=str, help=\u0026#34;The name to greet.\u0026#34;) parser.add_argument(\u0026#34;--greeting\u0026#34;, type=str, default=\u0026#34;Hello\u0026#34;, help=\u0026#34;The greeting to use.\u0026#34;) args = parser.parse_args() greet(args.name, args.greeting) 使用 fire 的现代方式：\n你只需要关注你的核心逻辑，然后把函数交给 fire.Fire()。\n# greet_fire.py import fire def greet(name, greeting=\u0026#34;Hello\u0026#34;): \u0026#34;\u0026#34;\u0026#34;生成一个问候语\u0026#34;\u0026#34;\u0026#34; print(f\u0026#34;{greeting}, {name}!\u0026#34;) if __name__ == \u0026#39;__main__\u0026#39;: fire.Fire(greet) Fire 会自动检查函数签名，将 name 识别为必需的位置参数，将 greeting 识别为可选的 --greeting 标志。命令行调用方式完全一样：\n# 两种方式调用起来完全相同 $ python greet_fire.py World Hello, World! $ python greet_fire.py World --greeting \u0026#34;Good morning\u0026#34; Good morning, World! 不仅仅是函数 Fire 的强大之处在于它可以处理任何 Python 对象，比如类。它会自动将类的方法映射为子命令。\n# calculator.py import fire import time class Calculator: \u0026#34;\u0026#34;\u0026#34;一个简单的计算器\u0026#34;\u0026#34;\u0026#34; def add(self, a, b): \u0026#34;\u0026#34;\u0026#34;计算两数之和\u0026#34;\u0026#34;\u0026#34; return a + b def subtract(self, a, b): \u0026#34;\u0026#34;\u0026#34;计算两数之差\u0026#34;\u0026#34;\u0026#34; time.sleep(2) # 模拟耗时操作 return a - b if __name__ == \u0026#39;__main__\u0026#39;: fire.Fire(Calculator) 执行子命令：\n$ python calculator.py add 2 3 5 $ python calculator.py subtract 10 4 6 Fire 还会自动根据你的 docstrings 生成帮助文档。\n$ python calculator.py -- --help 安装 pip install fire 总结 使用 Python Fire: 当你需要为脚本、内部工具快速创建 CLI，或者想在开发调试时探索代码。 使用 argparse: 当你构建需要分发给用户的正式应用，且需要对参数进行精细化控制时。 对于绝大多数日常脚本和内部工具来说，Fire 都能极大地提升你的开发效率。\n参考 Google Fire - GitHub Repository\n","permalink":"https://blog.niuhemoon.win/posts/tech/python-fire-cli-intro/","summary":"\u003cblockquote\u003e\n\u003cp\u003ePython Fire 是一个由 Google 开发的开源库，用于从任何 Python 对象自动生成命令行界面（CLI）。它的核心优势是：\u003cstrong\u003e简单、自动化、几乎零样板代码\u003c/strong\u003e。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e如果你厌倦了为写一个简单的脚本而不得不编写 \u003ccode\u003eargparse\u003c/code\u003e 的大量模板代码，那么 \u003ccode\u003eFire\u003c/code\u003e 绝对是你的得力助手。\u003c/p\u003e","title":"[AI]Python Fire: 告别繁琐，一键生成命令行界面"},{"content":"MCP技术概述 MCP (Model Context Protocol) 是一个用于连接AI模型与外部数据源和工具的开放标准协议。它允许AI助手安全地访问本地和远程资源，实现更强大的功能扩展。\nMCP核心特性：\n标准化的客户端-服务器通信协议 支持多种传输方式（STDIO、HTTP、SSE） 基于JSON-RPC的消息格式 工具调用和资源访问能力 协议架构 ┌─────────────────┐ ┌─────────────────┐ │ MCP Client │ │ MCP Server │ │ (AI模型) │◄──►│ (工具提供者) │ └─────────────────┘ └─────────────────┘ │ │ └───────── MCP ──────────┘ (JSON-RPC over Transport) 服务器实现 计算器MCP服务器 以下是一个完整的MCP服务器实现，支持三种传输方式：\n#!/usr/bin/env python3 \u0026#34;\u0026#34;\u0026#34; Calculator MCP服务器 使用FastMCP实现标准MCP协议 \u0026#34;\u0026#34;\u0026#34; import logging from mcp.server.fastmcp import FastMCP # 配置日志 logging.basicConfig(level=logging.INFO) logger = logging.getLogger(\u0026#34;calculator-mcp-server\u0026#34;) # 初始化FastMCP服务器 mcp = FastMCP(\u0026#34;Calculator MCP Server\u0026#34;) @mcp.tool() def add(a: float, b: float) -\u0026gt; float: \u0026#34;\u0026#34;\u0026#34;Add two numbers(两个数字相加) Parameters: a (float): First number to add b (float): Second number to add Returns: float: The sum of a and b. \u0026#34;\u0026#34;\u0026#34; try: result = a + b logger.info(f\u0026#34;Addition: {a} + {b} = {result}\u0026#34;) return result except Exception as e: logger.error(f\u0026#34;Failed to add numbers: {e}\u0026#34;) raise RuntimeError(f\u0026#34;Failed to add numbers: {str(e)}\u0026#34;) def main_stdio(): \u0026#34;\u0026#34;\u0026#34;STDIO传输模式入口点\u0026#34;\u0026#34;\u0026#34; logger.info(\u0026#34;启动Calculator MCP服务器 (STDIO传输模式)\u0026#34;) mcp.run(transport=\u0026#34;stdio\u0026#34;) def main_remote(host: str = \u0026#34;127.0.0.1\u0026#34;, port: int = 8008, transport: str = \u0026#34;http\u0026#34;): \u0026#34;\u0026#34;\u0026#34;HTTP传输模式入口点\u0026#34;\u0026#34;\u0026#34; import uvicorn logger.info(f\u0026#34;启动Calculator MCP服务器 ({transport.upper()}传输模式) - {host}:{port}\u0026#34;) if transport == \u0026#34;sse\u0026#34;: app = mcp.sse_app() else: app = mcp.streamable_http_app() uvicorn.run(app, host=host, port=port) def main_http_with_args(): \u0026#34;\u0026#34;\u0026#34;带命令行参数解析的HTTP服务器启动器\u0026#34;\u0026#34;\u0026#34; import argparse import sys # 如果从主脚本调用，需要过滤掉 --http 参数 argv = sys.argv[1:] if argv and argv[0] == \u0026#34;--http\u0026#34;: argv = argv[1:] parser = argparse.ArgumentParser(description=\u0026#34;Calculator MCP服务器 - HTTP传输模式\u0026#34;) parser.add_argument(\u0026#34;--host\u0026#34;, default=\u0026#34;127.0.0.1\u0026#34;, help=\u0026#34;绑定的主机地址\u0026#34;) parser.add_argument(\u0026#34;--port\u0026#34;, type=int, default=8008, help=\u0026#34;绑定的端口号\u0026#34;) args = parser.parse_args(argv) main_remote(args.host, args.port) def main_sse_with_args(): \u0026#34;\u0026#34;\u0026#34;带命令行参数解析的SSE服务器启动器\u0026#34;\u0026#34;\u0026#34; import argparse import sys # 如果从主脚本调用，需要过滤掉 --sse 参数 argv = sys.argv[1:] if argv and argv[0] == \u0026#34;--sse\u0026#34;: argv = argv[1:] parser = argparse.ArgumentParser(description=\u0026#34;Calculator MCP服务器 - SSE传输模式\u0026#34;) parser.add_argument(\u0026#34;--host\u0026#34;, default=\u0026#34;127.0.0.1\u0026#34;, help=\u0026#34;绑定的主机地址\u0026#34;) parser.add_argument(\u0026#34;--port\u0026#34;, type=int, default=8008, help=\u0026#34;绑定的端口号\u0026#34;) args = parser.parse_args(argv) main_remote(args.host, args.port, transport=\u0026#34;sse\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: import sys if len(sys.argv) \u0026gt; 1 and sys.argv[1] == \u0026#34;--http\u0026#34;: # HTTP模式：python calculator.py --http [--host HOST] [--port PORT] main_http_with_args() elif len(sys.argv) \u0026gt; 1 and sys.argv[1] == \u0026#34;--sse\u0026#34;: # SSE模式：python calculator.py --sse [--host HOST] [--port PORT] main_sse_with_args() else: # 默认使用STDIO模式 main_stdio() 服务器启动方式 STDIO传输模式（默认）：\npython calculator.py HTTP传输模式：\npython calculator.py --http # 或指定主机和端口 python calculator.py --http --host 0.0.0.0 --port 8080 SSE传输模式：\npython calculator.py --sse # 或指定主机和端口 python calculator.py --sse --host 0.0.0.0 --port 8080 客户端实现 STDIO客户端 以下是一个完整的STDIO客户端实现，用于连接MCP服务器：\n#!/usr/bin/env python3 \u0026#34;\u0026#34;\u0026#34; MCP Client Demo A simplified MCP client that connects to MCP servers and demonstrates basic tool listing and execution functionality. \u0026#34;\u0026#34;\u0026#34; import asyncio import json import logging import os import shutil from contextlib import AsyncExitStack from typing import Any, Dict, List, Optional from mcp import ClientSession, StdioServerParameters from mcp.client.stdio import stdio_client # Configure logging logging.basicConfig( level=logging.INFO, format=\u0026#34;%(asctime)s - %(levelname)s - %(message)s\u0026#34; ) logger = logging.getLogger(__name__) class MCPServer: \u0026#34;\u0026#34;\u0026#34;Manages connection to an MCP server and tool execution.\u0026#34;\u0026#34;\u0026#34; def __init__(self, name: str, config: Dict[str, Any]) -\u0026gt; None: self.name = name self.config = config self.session: Optional[ClientSession] = None self.exit_stack = AsyncExitStack() self.tools: List[Dict[str, Any]] = [] async def connect(self) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34;Connect to the MCP server.\u0026#34;\u0026#34;\u0026#34; command = ( shutil.which(\u0026#34;npx\u0026#34;) if self.config[\u0026#34;command\u0026#34;] == \u0026#34;npx\u0026#34; else self.config[\u0026#34;command\u0026#34;] ) if command is None: raise ValueError(f\u0026#34;Invalid command for server {self.name}: {self.config[\u0026#39;command\u0026#39;]}\u0026#34;) logger.info(f\u0026#34;Connecting to {self.name} server...\u0026#34;) server_params = StdioServerParameters( command=command, args=self.config[\u0026#34;args\u0026#34;], env={**os.environ, **self.config.get(\u0026#34;env\u0026#34;, {})} ) try: stdio_transport = await self.exit_stack.enter_async_context( stdio_client(server_params) ) read, write = stdio_transport session = await self.exit_stack.enter_async_context( ClientSession(read, write) ) await session.initialize() self.session = session logger.info(f\u0026#34;Successfully connected to {self.name} server\u0026#34;) except Exception as e: logger.error(f\u0026#34;Error connecting to server {self.name}: {e}\u0026#34;) await self.disconnect() raise async def get_tools(self) -\u0026gt; List[Dict[str, Any]]: \u0026#34;\u0026#34;\u0026#34;Get available tools from the server.\u0026#34;\u0026#34;\u0026#34; if not self.session: raise RuntimeError(f\u0026#34;Server {self.name} not connected\u0026#34;) logger.info(f\u0026#34;Getting tools from {self.name} server...\u0026#34;) tools_response = await self.session.list_tools() tools = [] for item in tools_response: if isinstance(item, tuple) and item[0] == \u0026#34;tools\u0026#34;: for tool in item[1]: tools.append({ \u0026#34;name\u0026#34;: tool.name, \u0026#34;description\u0026#34;: tool.description, \u0026#34;input_schema\u0026#34;: tool.inputSchema }) self.tools = tools return tools async def execute_tool(self, tool_name: str, arguments: Dict[str, Any]) -\u0026gt; Any: \u0026#34;\u0026#34;\u0026#34;Execute a tool on the server.\u0026#34;\u0026#34;\u0026#34; if not self.session: raise RuntimeError(f\u0026#34;Server {self.name} not connected\u0026#34;) logger.info(f\u0026#34;Executing tool {tool_name} on {self.name} server...\u0026#34;) try: result = await self.session.call_tool(tool_name, arguments) logger.info(f\u0026#34;Tool execution successful\u0026#34;) return result except Exception as e: logger.error(f\u0026#34;Error executing tool {tool_name}: {e}\u0026#34;) raise async def disconnect(self) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34;Disconnect from the server.\u0026#34;\u0026#34;\u0026#34; try: await self.exit_stack.aclose() self.session = None logger.info(f\u0026#34;Disconnected from {self.name} server\u0026#34;) except Exception as e: logger.error(f\u0026#34;Error disconnecting from server {self.name}: {e}\u0026#34;) SSE客户端 以下是一个SSE客户端实现，用于测试SSE传输：\nimport asyncio import os import sys import traceback from typing import Dict, Any from unittest import result import httpx from mcp import ClientSession from mcp.client.sse import sse_client TOOL_TESTS = { \u0026#34;add\u0026#34;: {\u0026#34;a\u0026#34;: 5, \u0026#34;b\u0026#34;: 3}, \u0026#34;subtract\u0026#34;: {\u0026#34;a\u0026#34;: 10, \u0026#34;b\u0026#34;: 4}, \u0026#34;multiply\u0026#34;: {\u0026#34;a\u0026#34;: 6, \u0026#34;b\u0026#34;: 7}, \u0026#34;divide\u0026#34;: {\u0026#34;a\u0026#34;: 20, \u0026#34;b\u0026#34;: 5}, \u0026#34;factorial\u0026#34;: {\u0026#34;a\u0026#34;: 5}, \u0026#34;log\u0026#34;: {\u0026#34;a\u0026#34;: 100, \u0026#34;base\u0026#34;: 10}, \u0026#34;remainder\u0026#34;: {\u0026#34;a\u0026#34;: 17, \u0026#34;b\u0026#34;: 5}, \u0026#34;sin\u0026#34;: {\u0026#34;angle\u0026#34;: 30, \u0026#34;degrees\u0026#34;: True}, \u0026#34;cos\u0026#34;: {\u0026#34;angle\u0026#34;: 60, \u0026#34;degrees\u0026#34;: True}, \u0026#34;tan\u0026#34;: {\u0026#34;angle\u0026#34;: 45, \u0026#34;degrees\u0026#34;: True}, \u0026#34;power\u0026#34;: {\u0026#34;base\u0026#34;: 2, \u0026#34;exponent\u0026#34;: 8}, \u0026#34;sqrt\u0026#34;: {\u0026#34;a\u0026#34;: 16} } class MCPClient: def __init__(self): self.session = None self.available_tools = [] async def connect(self, server_url: str): print(f\u0026#34;Connecting to server {server_url}\u0026#34;) try: print(\u0026#34;Create SSE Client...\u0026#34;) # Store the context managers but don\u0026#39;t enter them yet self._streams_context = sse_client(url=server_url) streams = await self._streams_context.__aenter__() print(\u0026#34;Create MCP Session...\u0026#34;) self._session_context = ClientSession(*streams) self.session = await self._session_context.__aenter__() print(\u0026#34;Init Session...\u0026#34;) await self.session.initialize() print(\u0026#34;Get Tool List...\u0026#34;) response = await self.session.list_tools() self.available_tools = response.tools tool_names = [tool.name for tool in self.available_tools] print(f\u0026#39;Connect Successfully! Available Tools: {tool_names}\u0026#39;) return True except Exception as e: print(f\u0026#39;Connect Failed: {e}\u0026#39;) print(traceback.format_exc()) # Clean up any resources that might have been created await self.cleanup() return False async def call_tool(self, tool_name: str, parameters: Dict[str, Any]) -\u0026gt; str: if not self.session: print(\u0026#39;Error: Not Connect to MCP server\u0026#39;) return \u0026#34;Not Connect to MCP server\u0026#34; try: print(f\u0026#39;Call Tool: {tool_name}\u0026#39;) print(f\u0026#39;Params: {parameters}\u0026#39;) result = await self.session.call_tool(tool_name, parameters) print(result) if hasattr(result, \u0026#39;content\u0026#39;): content_str = \u0026#34;\u0026#34; for item in result.content: if hasattr(item, \u0026#39;text\u0026#39;): content_str += item.text + \u0026#34;, \u0026#34; content_str = content_str.rstrip(\u0026#39;, \u0026#39;) else: content_str = str(result) output = content_str or \u0026#34;No Output\u0026#34; print(f\u0026#34;Tool Execute Result: {output}\u0026#34;) return output except Exception as e: error_msg = f\u0026#34;Tool Execute Failed: {e}\u0026#34; print(error_msg) print(traceback.format_exc()) return error_msg async def main(): if len(sys.argv) \u0026lt; 2: print(\u0026#34;Usage: python test_mcp_client.py \u0026lt;MCP Server URL\u0026gt; (Such as http://localhost:8000/sse)\u0026#34;) sys.exit(1) server_url = sys.argv[1] client = MCPClient() try: if await client.connect(server_url): print(\u0026#39;Connect Success\u0026#39;) # Test tool execution await client.call_tool(\u0026#34;add\u0026#34;, {\u0026#34;a\u0026#34;: 5, \u0026#34;b\u0026#34;: 3}) print(\u0026#34;\\nTests completed, exiting...\u0026#34;) else: print(\u0026#34;Connect Failed\u0026#34;) sys.exit(1) except KeyboardInterrupt: print(\u0026#34;\\nKeyboard interrupt received, shutting down...\u0026#34;) except Exception as e: print(\u0026#39;Execute Failed\u0026#39;) print(traceback.format_exc()) sys.exit(1) finally: print(\u0026#34;\\nCleaning up resources...\u0026#34;) await client.cleanup() if __name__ == \u0026#34;__main__\u0026#34;: asyncio.run(main()) 使用示例 配置文件 客户端配置文件 servers_config.json：\n{ \u0026#34;mcpServers\u0026#34;: { \u0026#34;calculator\u0026#34;: { \u0026#34;command\u0026#34;: \u0026#34;python\u0026#34;, \u0026#34;args\u0026#34;: [\u0026#34;../server/calculator.py\u0026#34;], \u0026#34;env\u0026#34;: {} } } } 运行示例 启动服务器：\n# STDIO模式 python calculator.py # HTTP模式 python calculator.py --http --host 127.0.0.1 --port 8008 # SSE模式 python calculator.py --sse --host 127.0.0.1 --port 8008 运行客户端：\n# STDIO客户端 python mcp_client_demo.py # SSE客户端 python test_mcp_sse_client.py http://localhost:8008/sse 参考资料 Model Context Protocol 官方文档\nFastMCP Python库文档\nMCP协议规范\n","permalink":"https://blog.niuhemoon.win/posts/tech/mcp-development-guide/","summary":"MCP技术概述 MCP (Model Context Protocol) 是一个用于连接AI模型与外部数据源和工具的开放标准协议。它允许AI助手安全地访问本地和远程资源，实现更强大的功能扩展。 MCP核心特性： 标准化的客户端-服务器通信协议 支持多种传输方式（STDIO、HTTP、SSE） 基于JSON-RPC的消息格式 工具调用和资源访","title":"MCP技术开发与实践"},{"content":"本文介绍如何搭建 vLLM 的开发环境，包括 CPU 和 GPU 两种环境配置方法。vLLM 是一个高效的大语言模型推理和服务框架，支持各种主流的开源模型。\n准备工作 在开始搭建环境前，我们需要先克隆 vLLM 的代码库并安装一些基本依赖。\nInfo\n以下步骤适用于 CPU 和 GPU 环境的通用部分\n克隆代码库 git clone https://github.com/vllm-project/vllm.git cd vllm 安装基础依赖 sudo apt-get install ccache numactl xz-utils ninja-build 创建虚拟环境 uv venv .venv source .venv/bin/activate uv pip install pip uv pip install sccache CPU 开发环境 Note\nCPU 环境适合没有 GPU 的开发者或者只需要进行功能开发和测试的场景\n方法一：使用 Docker 使用 Docker 是最简单的方式，可以避免环境配置问题：\nsudo docker build -f docker/Dockerfile.cpu --target vllm-dev -t vllm-cpu-dev --shm-size=16g . 方法二：本地配置 参考 docker/Dockerfile.cpu 文件，我们可以在本地配置开发环境：\nCPU 环境详细配置步骤 # 安装 vLLM 依赖 uv pip install -r requirements/cpu.txt --index-strategy unsafe-best-match uv pip install -r requirements/build.txt # 构建和安装 vLLM VLLM_TARGT_DEVICE=cpu python3 setup.py bdist_wheel VLLM_TARGET_DEVICE=cpu python3 setup.py develop # 安装测试和开发依赖 uv pip compile requirements/test.in -o requirements/test.txt --index-strategy unsafe-best-match --torch-backend cpu uv pip install -r requirements/dev.txt # 安装 pre-commit 钩子 pre-commit install --hook-type pre-commit --hook-type commit-msg # 检查 vLLM 版本 pip list | grep vllm GPU 开发环境 Warning\nGPU 环境需要 NVIDIA GPU 硬件和相应的 CUDA 驱动支持\n参考 docker/Dockerfile 文件，我们可以配置 GPU 开发环境。以下步骤主要针对只开发 Python 部分，不需要编译 C++ 代码的情况：\nGPU 环境详细配置步骤 # 安装 vLLM 依赖 uv pip install -r requirements/cuda.txt uv pip install -r requirements/build.txt # 使用预编译的二进制文件安装 vLLM VLLM_USE_PRECOMPILED=1 pip install --editable . # 安装开发依赖 uv pip install --extra-index-url https://download.pytorch.org/whl/cu128 --index-strategy unsafe-best-match -r requirements/dev.txt # 安装 pre-commit 钩子 pre-commit install --hook-type pre-commit --hook-type commit-msg # 检查 vLLM 版本 pip list | grep vllm 测试 vLLM 功能测试 安装完成后，可以使用以下代码测试 vLLM 是否正常工作：\nfrom vllm import LLM, SamplingParams # Define a list of input prompts prompts = [ \u0026#34;Hello, my name is\u0026#34;, \u0026#34;The capital of France is\u0026#34;, \u0026#34;The largest ocean is\u0026#34;, ] if __name__ == \u0026#34;__main__\u0026#34;: # Define sampling parameters sampling_params = SamplingParams(temperature=0.8, top_p=0.95) # Initialize the LLM engine with the OPT-125M model llm = LLM(model=\u0026#34;facebook/opt-125m\u0026#34;) # Generate outputs for the input prompts outputs = llm.generate(prompts, sampling_params) # Print the generated outputs for output in outputs: prompt = output.prompt generated_text = output.outputs[0].text print(f\u0026#34;Prompt: {prompt!r}, Generated text: {generated_text!r}\u0026#34;) Tip\n测试代码使用了 facebook/opt-125m 模型，这是一个较小的模型，适合快速测试。在实际应用中，您可以替换为其他支持的模型。\n单元测试 Tip\n单元测试对huggingface有依赖，需要先 export HUGGINGFACE_TOKEN=\u0026lt;your_token\u0026gt;\npytest tests/test_logger.py -v 总结 通过以上步骤，我们成功搭建了 vLLM 的开发环境，无论是 CPU 还是 GPU 环境。vLLM 作为一个高效的大语言模型推理框架，可以帮助我们更好地部署和使用各种大语言模型。\nNote\n对于生产环境，建议使用官方提供的预构建 Docker 镜像或按照官方文档进行更详细的配置。\n","permalink":"https://blog.niuhemoon.win/posts/tech/vllm-dev-environment/","summary":"\u003cp\u003e本文介绍如何搭建 vLLM 的开发环境，包括 CPU 和 GPU 两种环境配置方法。vLLM 是一个高效的大语言模型推理和服务框架，支持各种主流的开源模型。\u003c/p\u003e","title":"vLLM 开发环境搭建"},{"content":"本文介绍如何使用FRP实现内网穿透，并结合NoMachine实现高效的远程桌面控制方案。\nFRP内网穿透 什么是FRP Info\nFRP (Fast Reverse Proxy) 是一个可用于内网穿透的高性能反向代理应用，支持TCP、UDP、HTTP、HTTPS等多种协议。使用FRP可以将内网服务暴露到公网，实现远程访问内网资源。\n部署架构 服务端配置 (frps) 在具有公网IP的服务器上部署frps服务端：\n下载并解压frp wget https://github.com/fatedier/frp/releases/download/v0.61.2/frp_0.61.2_linux_amd64.tar.gz tar -zxvf frp_0.61.2_linux_amd64.tar.gz cd frp_0.61.2_linux_amd64 编辑/etc/frp/frps.toml配置文件 # frps.toml bindPort = 7000 kcpBindPort = 7001 quicBindPort = 7002 auth.token = \u0026#34;hello_frps\u0026#34; # 仪表板配置 webServer.addr = \u0026#34;0.0.0.0\u0026#34; webServer.port = 7500 webServer.user = \u0026#34;admin\u0026#34; webServer.password = \u0026#34;yourfrpspassword\u0026#34; # 虚拟主机配置 vhostHTTPPort = 8080 vhostHTTPSPort = 8081 # 日志配置 log.to = \u0026#34;/var/log/frp/frps.log\u0026#34; log.level = \u0026#34;info\u0026#34; log.maxDays = 3 启动frps服务 Tip\n建议将frps配置为系统服务，以便开机自启动。可以使用systemd创建服务文件。\n编辑 /etc/systemd/system/frps.service\n[Unit] Description=frp server After=network.target [Service] Type=simple ExecStart=/usr/local/bin/frps -c /etc/frp/frps.toml Restart=on-failure RestartSec=5s LimitNOFILE=65535 [Install] WantedBy=multi-user.target systemctl enable frps systemctl start frps systemctl status frps 客户端配置 (frpc) 在内网设备上配置frpc客户端：\n下载并解压frp（与服务端相同） curl -s https://api.github.com/repos/fatedier/frp/releases/latest | grep -oP \u0026#39;\u0026#34;tag_name\u0026#34;: \u0026#34;\\K(.*)(?=\u0026#34;)\u0026#39; mkdir -p ~/frpc_install \u0026amp;\u0026amp; cd ~/frpc_install \u0026amp;\u0026amp; wget https://github.com/fatedier/frp/releases/download/v0.62.0/frp_0.62.0_linux_amd64.tar.gz tar -xzf ~/frpc_install/frp_0.62.0_linux_amd64.tar.gz -C ~/frpc_install sudo mkdir -p /usr/local/bin \u0026amp;\u0026amp; sudo cp ~/frpc_install/frp_0.62.0_linux_amd64/frpc /usr/local/bin/ \u0026amp;\u0026amp; sudo chmod +x /usr/local/bin/frpc frpc --version 编辑frpc.toml配置文件 frpc配置 loginFailExit = false udpPacketSize = 1500 serverAddr = \u0026#34;frps_server_ip\u0026#34; serverPort = 7000 user = \u0026#34;\u0026#34; [auth] method = \u0026#34;token\u0026#34; token = \u0026#34;hello_frps\u0026#34; [log] to = \u0026#34;/var/log/frp/frpc.log\u0026#34; level = \u0026#34;info\u0026#34; maxDays = 3 disablePrintColor = false [transport] dialServerTimeout = 10 dialServerKeepalive = 7200 poolCount = 0 tcpMux = true tcpMuxKeepaliveInterval = 30 protocol = \u0026#34;tcp\u0026#34; connectServerLocalIP = \u0026#34;\u0026#34; proxyURL = \u0026#34;\u0026#34; heartbeatInterval = 30 heartbeatTimeout = 90 [transport.tls] enable = true certFile = \u0026#34;\u0026#34; keyFile = \u0026#34;\u0026#34; trustedCaFile = \u0026#34;\u0026#34; serverName = \u0026#34;\u0026#34; disableCustomTLSFirstByte = true [metadatas] token = \u0026#34;\u0026#34; [webServer] addr = \u0026#34;127.0.0.1\u0026#34; port = 57400 user = \u0026#34;\u0026#34; password = \u0026#34;\u0026#34; pprofEnable = false [[proxies]] name = \u0026#34;vscode\u0026#34; type = \u0026#34;tcp\u0026#34; localIP = \u0026#34;192.168.5.3\u0026#34; localPort = 8088 remotePort = 8088 [proxies.transport] useEncryption = false useCompression = false [[proxies]] name = \u0026#34;nomachine\u0026#34; type = \u0026#34;tcp\u0026#34; localIP = \u0026#34;192.168.5.3\u0026#34; localPort = 4000 remotePort = 4000 [proxies.transport] useEncryption = false useCompression = false 启动frpc客户端 [Unit] Description=frpc service After=network.target [Service] Type=simple User=nh ExecStart=/usr/local/bin/frpc -c /etc/frp/frpc.toml Restart=on-failure RestartSec=5s [Install] WantedBy=multi-user.target NoMachine远程桌面 NoMachine简介 Info\nNoMachine是一款高性能的远程桌面软件，支持Windows、macOS、Linux等多种操作系统。相比传统的VNC或RDP，NoMachine提供更流畅的图形界面体验和更低的延迟。\nTip\n对于分辨率调整，例如VGA-1，很可能无法调整到1080p分辨率\n调整分辨率 /user/NX/etc/server.cfg\n调整 CreateDisplay=1的相关配置，使用x11 Virtual DisplayServer\nCreateDisplay 1 # # When \u0026#39;CreateDisplay\u0026#39; is enabled, specify the display owner and let # NoMachine create the new display without querying the user. If the # server supports only one concurrent connection, the connecting user # must be the display owner set in this key. # #DisplayOwner \u0026#34;\u0026#34; DisplayOwner \u0026#34;your_name\u0026#34; # # When \u0026#39;CreateDisplay\u0026#39; is enabled, specify the resolution of the new # desktop in the WxH format. Default is 1280x800. # #DisplayGeometry 1280x800 DisplayGeometry 1920x1080 分辨率更多样\n参考 FRP官方文档 NoMachine官方网站 Linux系统服务配置指南 ","permalink":"https://blog.niuhemoon.win/posts/tech/frp-nomachine-guide/","summary":"\u003cp\u003e本文介绍如何使用FRP实现内网穿透，并结合NoMachine实现高效的远程桌面控制方案。\u003c/p\u003e","title":"FRP内网穿透与NoMachine远程桌面使用指南"},{"content":"在 Python 编程中，全局解释器锁（Global Interpreter Lock，简称 GIL）一直是制约 CPU 密集型任务性能的主要瓶颈。本文将介绍如何使用 ZeroMQ 这一高性能分布式消息队列库来突破 GIL 限制，通过将单一进程拆分为多个通过消息通信的进程，从而充分利用多核 CPU 资源，显著提升性能。\n环境配置与安装 首先，我们需要创建一个虚拟环境并安装必要的依赖包。这里我们使用 uv 命令来创建和管理虚拟环境\nuv venv .venv source .venv/bin/activate \u0026amp;\u0026amp; uv pip install pyzmq numpy matplotlib tqdm ZeroMQ 核心概念 ZeroMQ (也写作 ØMQ, 0MQ 或 zmq) 是一个高性能的异步消息传递库，旨在用于分布式或并发应用程序。它提供了一个消息队列，但与传统的消息队列不同，它可以在没有专门的消息代理（broker）的情况下运行。\nInfo\nZeroMQ 提供了一种轻量级的消息传递机制，非常适合用于构建分布式系统和并行计算应用。\nZeroMQ 的主要特点 无代理设计：不需要中央消息服务器 异步 I/O 模型：非阻塞操作，提高并发性能 多种通信模式：支持请求-回复、发布-订阅、推送-拉取等模式 多种传输协议：支持 TCP、IPC、进程内通信等 跨语言支持：提供多种编程语言的绑定 ZeroMQ 的常用通信模式 请求-回复 (REQ-REP)：客户端发送请求，服务器回复 发布-订阅 (PUB-SUB)：发布者发送消息，订阅者接收 推送-拉取 (PUSH-PULL)：任务分发和结果收集，适合并行处理 路由-经销商 (ROUTER-DEALER)：高级异步通信模式 基本的 ZeroMQ 示例 以下是一个简单的请求-回复模式示例，展示了 ZeroMQ 的基本用法：\n基本的 ZeroMQ REQ-REP 模式示例 #!/usr/bin/env python # 基本的 ZeroMQ REQ-REP 模式示例 import zmq import time import sys import threading def server(): # 创建上下文和 socket context = zmq.Context() socket = context.socket(zmq.REP) socket.bind(\u0026#34;tcp://*:5555\u0026#34;) print(\u0026#34;服务器已启动，等待请求...\u0026#34;) # 服务循环 while True: # 等待客户端请求 message = socket.recv() print(f\u0026#34;收到请求: {message.decode()}\u0026#34;) # 模拟工作 time.sleep(1) # 发送回复 socket.send(b\u0026#34;Request processed\u0026#34;) def client(): # 创建上下文和 socket context = zmq.Context() socket = context.socket(zmq.REQ) socket.connect(\u0026#34;tcp://localhost:5555\u0026#34;) # 发送请求 for i in range(5): print(f\u0026#34;发送请求 {i}...\u0026#34;) socket.send(f\u0026#34;请求 #{i}\u0026#34;.encode()) # 获取回复 message = socket.recv() print(f\u0026#34;收到回复: {message.decode()}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: if len(sys.argv) \u0026lt; 2: print(\u0026#34;用法: python zmq_basic.py [server|client]\u0026#34;) sys.exit(1) if sys.argv[1] == \u0026#34;server\u0026#34;: server() elif sys.argv[1] == \u0026#34;client\u0026#34;: client() elif sys.argv[1] == \u0026#34;both\u0026#34;: # 在单独的线程中启动服务器 server_thread = threading.Thread(target=server) server_thread.daemon = True server_thread.start() # 给服务器一点时间启动 time.sleep(1) # 运行客户端 client() else: print(\u0026#34;用法: python zmq_basic.py [server|client|both]\u0026#34;) 这个简单示例展示了 ZeroMQ 的基本通信模式。服务器创建一个 REP 套接字并绑定到特定端口，客户端创建一个 REQ 套接字并连接到服务器。客户端发送请求，服务器处理后回复。\nGIL 限制下的性能问题 在深入 ZeroMQ 解决方案之前，让我们先了解 Python 中的 GIL 问题以及它如何影响 CPU 密集型任务的性能。\nGIL（全局解释器锁）是 CPython 解释器中的一个互斥锁，它确保同一时刻只有一个线程可以执行 Python 字节码。这意味着即使在多核处理器上，标准的 Python 线程也无法实现真正的并行计算。\nWarning\n由于 GIL 的存在，Python 多线程在 CPU 密集型任务上通常无法提供性能提升，有时甲至会因为线程切换开销而导致性能下降。\n演示 GIL 限制的示例程序 下面是一个 CPU 密集型任务的示例，它模拟了图像处理中的模糊滤镜操作：\nCPU密集型任务示例 - 受GIL限制的模糊滤镜操作 #!/usr/bin/env python # filename: cpu_bound_demo.py # CPU密集型任务示例 - 受GIL限制的性能问题 import numpy as np import time import matplotlib.pyplot as plt from matplotlib.figure import Figure from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas def generate_data(size=1000): \u0026#34;\u0026#34;\u0026#34;生成随机数据矩阵\u0026#34;\u0026#34;\u0026#34; return np.random.random((size, size)) def process_data(data, kernel_size=5): \u0026#34;\u0026#34;\u0026#34;对数据应用简单的模糊滤镜 (CPU密集型操作)\u0026#34;\u0026#34;\u0026#34; result = np.zeros_like(data) rows, cols = data.shape # 简单的滑动窗口平均 (模拟模糊操作) for i in range(kernel_size//2, rows - kernel_size//2): for j in range(kernel_size//2, cols - kernel_size//2): # 提取窗口 window = data[i-kernel_size//2:i+kernel_size//2+1, j-kernel_size//2:j+kernel_size//2+1] # 计算平均值 result[i, j] = np.mean(window) return result def visualize_results(original, processed, execution_time, title): \u0026#34;\u0026#34;\u0026#34;可视化原始数据和处理后的数据\u0026#34;\u0026#34;\u0026#34; fig = Figure(figsize=(10, 5)) canvas = FigureCanvas(fig) ax1 = fig.add_subplot(121) ax1.imshow(original, cmap=\u0026#39;viridis\u0026#39;) ax1.set_title(\u0026#39;Original Data\u0026#39;) ax1.axis(\u0026#39;off\u0026#39;) ax2 = fig.add_subplot(122) ax2.imshow(processed, cmap=\u0026#39;viridis\u0026#39;) ax2.set_title(f\u0026#39;Processed Data\\nExecution Time: {execution_time:.2f} seconds\u0026#39;) ax2.axis(\u0026#39;off\u0026#39;) fig.suptitle(title) fig.tight_layout() # 保存图像 fig.savefig(f\u0026#34;{title.replace(\u0026#39; \u0026#39;, \u0026#39;_\u0026#39;).lower()}.png\u0026#34;) print(f\u0026#34;结果已保存为 {title.replace(\u0026#39; \u0026#39;, \u0026#39;_\u0026#39;).lower()}.png\u0026#34;) def run_single_process(data_size=500, kernel_size=5): \u0026#34;\u0026#34;\u0026#34;在单个进程中运行数据处理\u0026#34;\u0026#34;\u0026#34; print(f\u0026#34;生成 {data_size}x{data_size} 的数据矩阵...\u0026#34;) data = generate_data(data_size) print(\u0026#34;开始处理数据...\u0026#34;) start_time = time.time() result = process_data(data, kernel_size) end_time = time.time() execution_time = end_time - start_time print(f\u0026#34;处理完成，耗时: {execution_time:.2f} 秒\u0026#34;) # 可视化结果 visualize_results(data, result, execution_time, \u0026#34;Single Process\u0026#34;) return execution_time if __name__ == \u0026#34;__main__\u0026#34;: # 运行单进程版本 execution_time = run_single_process(data_size=500, kernel_size=5) print(f\u0026#34;单进程执行时间: {execution_time:.2f} 秒\u0026#34;) 这个程序生成一个随机数据矩阵，然后对其应用模糊滤镜操作。由于操作是 CPU 密集型的，且在单个进程中运行，因此受到 GIL 的限制，无法充分利用多核处理器的优势。\n使用 ZeroMQ 拆分进程\n为了突破 GIL 限制，我们可以将任务拆分成多个独立的进程，每个进程处理数据的一部分，然后使用 ZeroMQ 进行进程间通信。\nInfo\nZeroMQ 允许我们创建多个独立进程，每个进程可以充分利用一个 CPU 核心，从而绕过 GIL 限制，实现真正的并行计算。\n我们将采用以下架构：\n主进程：负责数据分割、任务分发和结果收集 工作进程：接收数据块，进行处理，并返回结果 通信模式：使用 PUSH-PULL 模式进行任务分发和结果收集 实现分布式处理 下面是使用 ZeroMQ 实现分布式处理的代码：\nZeroMQ 分布式处理实现 #!/usr/bin/env python # filename: zmq_distributed_demo.py # 使用不同并行处理方法对比CPU密集型任务的性能 import numpy as np import time import zmq import pickle import multiprocessing from multiprocessing import Pool, Process, Queue, cpu_count import os import matplotlib.pyplot as plt from matplotlib.figure import Figure from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas import hashlib # 导入单进程版本中的函数 from cpu_bound_demo import generate_data, process_data, visualize_results def worker(worker_id): \u0026#34;\u0026#34;\u0026#34;工作进程 - 接收数据块并处理\u0026#34;\u0026#34;\u0026#34; context = zmq.Context() # 设置接收任务的PULL socket receiver = context.socket(zmq.PULL) receiver.connect(\u0026#34;tcp://localhost:5557\u0026#34;) # 设置发送结果的PUSH socket sender = context.socket(zmq.PUSH) sender.connect(\u0026#34;tcp://localhost:5558\u0026#34;) print(f\u0026#34;工作进程 {worker_id} 已启动\u0026#34;) # 处理循环 while True: try: # 接收任务 task = receiver.recv_pyobj() # 检查是否是终止信号 if task == \u0026#34;DONE\u0026#34;: print(f\u0026#34;工作进程 {worker_id} 收到终止信号\u0026#34;) break # 解包任务数据 chunk_id, data_chunk, kernel_size = task # 处理数据 result_chunk = process_data(data_chunk, kernel_size) # 发送结果 sender.send_pyobj((chunk_id, result_chunk)) except Exception as e: print(f\u0026#34;工作进程 {worker_id} 错误: {e}\u0026#34;) # 关闭连接 receiver.close() sender.close() context.term() print(f\u0026#34;工作进程 {worker_id} 已终止\u0026#34;) def split_data(data, num_chunks, kernel_size=5, overlap=True): \u0026#34;\u0026#34;\u0026#34;将数据分割成多个块，可选添加重叠区域以解决边界问题\u0026#34;\u0026#34;\u0026#34; rows, cols = data.shape chunk_size = rows // num_chunks chunks = [] chunk_bounds = [] # 记录每个块的有效边界（不包括重叠区域） # 计算重叠大小，至少需要kernel_size//2的重叠 overlap_size = kernel_size // 2 if overlap else 0 for i in range(num_chunks): # 计算块的起始和结束行，包括重叠区域 start_row = max(0, i * chunk_size - overlap_size) end_row = min(rows, (i + 1) * chunk_size + overlap_size if i \u0026lt; num_chunks - 1 else rows) # 计算有效边界（不包括重叠区域） valid_start = i * chunk_size valid_end = min(rows, (i + 1) * chunk_size) # 存储块及其有效边界 chunks.append(data[start_row:end_row, :].copy()) # 使用copy()确保数据独立 chunk_bounds.append((valid_start - start_row, valid_end - start_row)) return chunks, chunk_bounds def run_distributed_process(data_size=500, kernel_size=5, num_workers=None, input_data=None): \u0026#34;\u0026#34;\u0026#34;使用ZeroMQ分布式运行数据处理\u0026#34;\u0026#34;\u0026#34; # 如果没有指定工作进程数，使用CPU核心数 if num_workers is None: num_workers = multiprocessing.cpu_count() if input_data is None: print(f\u0026#34;生成 {data_size}x{data_size} 的数据矩阵...\u0026#34;) data = generate_data(data_size) else: print(f\u0026#34;使用提供的输入数据...\u0026#34;) data = input_data # 分割数据，并添加重叠区域 print(f\u0026#34;将数据分割成 {num_workers} 块，并添加重叠区域...\u0026#34;) data_chunks, chunk_bounds = split_data(data, num_workers, kernel_size) # 创建ZeroMQ上下文 context = zmq.Context() # 设置任务分发的PUSH socket task_sender = context.socket(zmq.PUSH) task_sender.bind(\u0026#34;tcp://*:5557\u0026#34;) # 设置结果收集的PULL socket result_receiver = context.socket(zmq.PULL) result_receiver.bind(\u0026#34;tcp://*:5558\u0026#34;) # 启动工作进程 processes = [] for i in range(num_workers): p = multiprocessing.Process(target=worker, args=(i,)) p.daemon = True p.start() processes.append(p) # 给工作进程一点时间启动 time.sleep(0.5) # 开始计时 print(\u0026#34;开始分布式处理数据...\u0026#34;) start_time = time.time() # 发送任务到工作进程 for i, chunk in enumerate(data_chunks): task_sender.send_pyobj((i, chunk, kernel_size)) # 收集结果 results = [None] * len(data_chunks) for _ in range(len(data_chunks)): chunk_id, result_chunk = result_receiver.recv_pyobj() results[chunk_id] = result_chunk # 合并结果，只保留每个块的有效区域 final_results = [] for i, result_chunk in enumerate(results): valid_start, valid_end = chunk_bounds[i] final_results.append(result_chunk[valid_start:valid_end, :]) # 合并有效区域 result = np.vstack(final_results) # 结束计时 end_time = time.time() execution_time = end_time - start_time print(f\u0026#34;处理完成，耗时: {execution_time:.2f} 秒\u0026#34;) # 发送终止信号给工作进程 for _ in range(num_workers): task_sender.send_pyobj(\u0026#34;DONE\u0026#34;) # 等待工作进程终止 for p in processes: p.join(timeout=1) # 关闭ZeroMQ连接 task_sender.close() result_receiver.close() context.term() # 可视化结果 visualize_results(data, result, execution_time, \u0026#34;ZeroMQ Distributed\u0026#34;) return execution_time, result # 新增的直接使用多进程的实现 def mp_worker(data_chunk, kernel_size, result_queue, chunk_id): \u0026#34;\u0026#34;\u0026#34;多进程工作函数 - 处理数据块并将结果放入队列\u0026#34;\u0026#34;\u0026#34; try: # 处理数据 result_chunk = process_data(data_chunk, kernel_size) # 将结果放入队列 result_queue.put((chunk_id, result_chunk)) except Exception as e: print(f\u0026#34;多进程工作函数错误: {e}\u0026#34;) def run_multiprocessing(data_size=500, kernel_size=5, num_workers=None, input_data=None): \u0026#34;\u0026#34;\u0026#34;使用Python原生多进程运行数据处理\u0026#34;\u0026#34;\u0026#34; # 如果没有指定工作进程数，使用CPU核心数 if num_workers is None: num_workers = cpu_count() if input_data is None: print(f\u0026#34;生成 {data_size}x{data_size} 的数据矩阵...\u0026#34;) data = generate_data(data_size) else: print(f\u0026#34;使用提供的输入数据...\u0026#34;) data = input_data # 分割数据，并添加重叠区域 print(f\u0026#34;将数据分割成 {num_workers} 块，并添加重叠区域...\u0026#34;) data_chunks, chunk_bounds = split_data(data, num_workers, kernel_size) # 创建结果队列 result_queue = Queue() # 创建进程 processes = [] # 开始计时 print(\u0026#34;开始多进程处理数据...\u0026#34;) start_time = time.time() # 启动工作进程 for i, chunk in enumerate(data_chunks): p = Process(target=mp_worker, args=(chunk, kernel_size, result_queue, i)) processes.append(p) p.start() # 收集结果 results = [None] * len(data_chunks) for _ in range(len(data_chunks)): chunk_id, result_chunk = result_queue.get() results[chunk_id] = result_chunk # 等待所有进程完成 for p in processes: p.join() # 合并结果，只保留每个块的有效区域 final_results = [] for i, result_chunk in enumerate(results): valid_start, valid_end = chunk_bounds[i] final_results.append(result_chunk[valid_start:valid_end, :]) # 合并有效区域 result = np.vstack(final_results) # 结束计时 end_time = time.time() execution_time = end_time - start_time print(f\u0026#34;处理完成，耗时: {execution_time:.2f} 秒\u0026#34;) # 可视化结果 visualize_results(data, result, execution_time, \u0026#34;Multiprocessing\u0026#34;) return execution_time, result # 用于验证结果一致性的函数 def calculate_result_hash(result): \u0026#34;\u0026#34;\u0026#34;计算结果数组的哈希值以验证一致性\u0026#34;\u0026#34;\u0026#34; # 将numpy数组转换为字节序列 # 先四舍五入到固定小数位数，避免浮点数误差引起的不一致 rounded_result = np.round(result, 6) result_bytes = rounded_result.tobytes() # 计算SHA-256哈希 return hashlib.sha256(result_bytes).hexdigest() def compare_performance(): \u0026#34;\u0026#34;\u0026#34;比较三种实现的性能并验证结果一致性\u0026#34;\u0026#34;\u0026#34; print(\u0026#34;\\n===== 性能对比 =====\u0026#34;) data_size = 2000 kernel_size = 5 # 设置随机种子以确保可重现性 np.random.seed(42) # 运行单进程版本 print(\u0026#34;\\n运行单进程版本...\u0026#34;) single_data = generate_data(data_size) # 保证所有实现使用相同的输入数据 # 为了确保结果一致性，我们将使用与分布式实现相同的处理方式 # 将数据分割，处理，然后再合并 num_workers = cpu_count() data_chunks, chunk_bounds = split_data(single_data, num_workers, kernel_size) single_start = time.time() # 处理每个块 result_chunks = [] for chunk in data_chunks: result_chunk = process_data(chunk, kernel_size) result_chunks.append(result_chunk) # 合并结果，只保留每个块的有效区域 final_results = [] for i, result_chunk in enumerate(result_chunks): valid_start, valid_end = chunk_bounds[i] final_results.append(result_chunk[valid_start:valid_end, :]) # 合并有效区域 single_result = np.vstack(final_results) single_time = time.time() - single_start print(f\u0026#34;处理完成，耗时: {single_time:.2f} 秒\u0026#34;) visualize_results(single_data, single_result, single_time, \u0026#34;Single Process\u0026#34;) # 运行分布式版本 print(\u0026#34;\\n运行ZeroMQ分布式版本...\u0026#34;) zmq_time, zmq_result = run_distributed_process(data_size=data_size, kernel_size=kernel_size, input_data=single_data) # 运行原生多进程版本 print(\u0026#34;\\n运行原生多进程版本...\u0026#34;) mp_time, mp_result = run_multiprocessing(data_size=data_size, kernel_size=kernel_size, input_data=single_data) # 验证结果一致性 single_hash = calculate_result_hash(single_result) zmq_hash = calculate_result_hash(zmq_result) mp_hash = calculate_result_hash(mp_result) print(f\u0026#34;\\n哈希值检查:\u0026#34;) print(f\u0026#34; 单进程结果哈希: {single_hash[:10]}...\u0026#34;) print(f\u0026#34; ZeroMQ结果哈希: {zmq_hash[:10]}...\u0026#34;) print(f\u0026#34; 多进程结果哈希: {mp_hash[:10]}...\u0026#34;) # 检查结果是否相同 if single_hash == zmq_hash and single_hash == mp_hash: print(\u0026#34; 结果一致性检查: 通过 \\u2705\u0026#34;) else: print(\u0026#34; 结果一致性检查: 失败 \\u274c\u0026#34;) if single_hash != zmq_hash: print(\u0026#34; - 单进程与ZeroMQ结果不一致\u0026#34;) if single_hash != mp_hash: print(\u0026#34; - 单进程与多进程结果不一致\u0026#34;) if zmq_hash != mp_hash: print(\u0026#34; - ZeroMQ与多进程结果不一致\u0026#34;) # 检查结果是否相同 zmq_match = (single_hash == zmq_hash) mp_match = (single_hash == mp_hash) results_match = zmq_match and mp_match # 计算加速比 zmq_speedup = single_time / zmq_time mp_speedup = single_time / mp_time print(\u0026#34;\\n===== 结果对比 =====\u0026#34;) print(f\u0026#34;单进程执行时间: {single_time:.2f} 秒\u0026#34;) print(f\u0026#34;ZeroMQ分布式执行时间: {zmq_time:.2f} 秒 (加速比: {zmq_speedup:.2f}x)\u0026#34;) print(f\u0026#34;原生多进程执行时间: {mp_time:.2f} 秒 (加速比: {mp_speedup:.2f}x)\u0026#34;) print(f\u0026#34;结果一致性检查: {\u0026#39;通过\u0026#39; if results_match else \u0026#39;失败\u0026#39;}\u0026#34;) # 绘制性能对比图 fig = Figure(figsize=(10, 6)) canvas = FigureCanvas(fig) ax = fig.add_subplot(111) methods = [\u0026#39;Single Process\u0026#39;, \u0026#39;ZeroMQ Distributed\u0026#39;, \u0026#39;Python Multiprocessing\u0026#39;] times = [single_time, zmq_time, mp_time] colors = [\u0026#39;blue\u0026#39;, \u0026#39;green\u0026#39;, \u0026#39;orange\u0026#39;] ax.bar(methods, times, color=colors) ax.set_ylabel(\u0026#39;Execution Time (seconds)\u0026#39;) ax.set_title(\u0026#39;Performance Comparison\u0026#39;) # 添加数值标签 for i, v in enumerate(times): ax.text(i, v + 0.1, f\u0026#34;{v:.2f}s\u0026#34;, ha=\u0026#39;center\u0026#39;) # 添加加速比 ax.text(1, times[1] * 0.5, f\u0026#34;Speedup: {zmq_speedup:.2f}x\u0026#34;, ha=\u0026#39;center\u0026#39;, fontsize=10, bbox=dict(facecolor=\u0026#39;white\u0026#39;, alpha=0.8)) ax.text(2, times[2] * 0.5, f\u0026#34;Speedup: {mp_speedup:.2f}x\u0026#34;, ha=\u0026#39;center\u0026#39;, fontsize=10, bbox=dict(facecolor=\u0026#39;white\u0026#39;, alpha=0.8)) fig.tight_layout() fig.savefig(\u0026#34;performance_comparison_all.png\u0026#34;) print(\u0026#34;性能对比图已保存为 performance_comparison_all.png\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: # 比较性能 compare_performance() 性能对比分析 为了验证 ZeroMQ 分布式处理的效果，我们将其与单进程版本和 Python 原生多进程版本进行对比。\n我们使用相同的输入数据，分别用三种方法处理，并记录执行时间：\n单进程版本：所有计算在一个进程中完成 ZeroMQ 分布式版本：使用 ZeroMQ 进行进程间通信 Python 原生多进程版本：使用 Python 的 multiprocessing 模块 性能测试结果\n===== 结果对比 ===== 单进程执行时间: 9.75 秒 ZeroMQ分布式执行时间: 1.28 秒 (加速比: 7.62x) 原生多进程执行时间: 1.27 秒 (加速比: 7.66x) 结果一致性检查: 通过 vLLM 架构模拟：CPU-GPU 并行优化 除了解决 CPU 密集型任务的 GIL 限制，ZeroMQ 还可以用于优化 CPU 和 GPU 之间的协作。这里我们模拟了类似 vLLM（一种高效的大语言模型推理框架）的架构，通过 ZeroMQ 实现 CPU 和 GPU 任务的并行处理。\n传统顺序处理的问题\n在传统的深度学习推理中，处理流程通常是顺序的：\nCPU 进行预处理 等待 GPU 完成计算 CPU 进行后处理 这种方式导致 GPU 在 CPU 处理期间处于空闲状态，无法充分利用计算资源。\n使用 ZeroMQ 实现 CPU-GPU 并行 通过 ZeroMQ，我们可以实现 CPU 和 GPU 的并行工作：\n模拟vllm拆分cpu和gpu工作负载 #!/usr/bin/env python # 模拟vLLM架构的简化版本，使用ZeroMQ分离GPU和CPU工作负载 import numpy as np import time import zmq import multiprocessing import threading import queue import json import argparse from tqdm import tqdm import matplotlib.pyplot as plt from matplotlib.figure import Figure from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas # 模拟GPU计算的函数 def simulate_gpu_computation(input_data, computation_time=0.1): \u0026#34;\u0026#34;\u0026#34;模拟GPU上的矩阵乘法计算\u0026#34;\u0026#34;\u0026#34; # 实际上是在CPU上运行，但我们用sleep来模拟GPU计算时间 time.sleep(computation_time) # 模拟矩阵乘法 result = np.dot(input_data, input_data.T) return result # 模拟CPU处理的函数 def simulate_cpu_preprocessing(request_id, size=100, processing_time=0.05): \u0026#34;\u0026#34;\u0026#34;模拟CPU上的预处理操作\u0026#34;\u0026#34;\u0026#34; # 模拟预处理耗时 time.sleep(processing_time) # 生成随机输入数据 input_data = np.random.random((size, size)) return input_data def simulate_cpu_postprocessing(request_id, result, processing_time=0.05): \u0026#34;\u0026#34;\u0026#34;模拟CPU上的后处理操作\u0026#34;\u0026#34;\u0026#34; # 模拟后处理耗时 time.sleep(processing_time) # 简单处理结果 processed_result = np.mean(result) return processed_result # 传统方式：单进程中顺序执行CPU和GPU操作 def traditional_approach(num_requests=10, matrix_size=100, preprocess_time=0.05, gpu_time=0.1, postprocess_time=0.05): \u0026#34;\u0026#34;\u0026#34;传统方式：在单一进程中顺序执行CPU和GPU操作\u0026#34;\u0026#34;\u0026#34; print(\u0026#34;\\n运行传统方式（单进程顺序执行）...\u0026#34;) start_time = time.time() gpu_active_time = 0 for i in tqdm(range(num_requests), desc=\u0026#34;处理请求\u0026#34;): request_id = f\u0026#34;req_{i}\u0026#34; # CPU预处理 preprocess_start = time.time() input_data = simulate_cpu_preprocessing(request_id, matrix_size, preprocess_time) preprocess_end = time.time() # GPU计算 gpu_start = time.time() result = simulate_gpu_computation(input_data, gpu_time) gpu_end = time.time() gpu_active_time += (gpu_end - gpu_start) # CPU后处理 postprocess_start = time.time() final_result = simulate_cpu_postprocessing(request_id, result, postprocess_time) postprocess_end = time.time() end_time = time.time() total_time = end_time - start_time gpu_utilization = gpu_active_time / total_time * 100 print(f\u0026#34;传统方式完成时间: {total_time:.2f} 秒\u0026#34;) print(f\u0026#34;GPU活跃时间: {gpu_active_time:.2f} 秒\u0026#34;) print(f\u0026#34;GPU利用率: {gpu_utilization:.2f}%\u0026#34;) return total_time, gpu_utilization # GPU进程：接收输入数据，执行GPU计算，返回结果 def gpu_worker(port_recv=5555, port_send=5556): \u0026#34;\u0026#34;\u0026#34;GPU工作进程，接收输入数据，执行GPU计算，发送结果\u0026#34;\u0026#34;\u0026#34; context = zmq.Context() # 设置接收输入数据的PULL socket receiver = context.socket(zmq.PULL) receiver.bind(f\u0026#34;tcp://*:{port_recv}\u0026#34;) # 设置发送结果的PUSH socket sender = context.socket(zmq.PUSH) sender.bind(f\u0026#34;tcp://*:{port_send}\u0026#34;) print(\u0026#34;GPU工作进程已启动\u0026#34;) gpu_active_time = 0 processed_count = 0 start_time = time.time() # 记录每次GPU活动的开始和结束时间 gpu_activity_periods = [] try: while True: # 接收任务 message = receiver.recv_pyobj() # 检查是否是终止信号 if message == \u0026#34;DONE\u0026#34;: print(\u0026#34;GPU工作进程收到终止信号\u0026#34;) # 发送GPU利用率信息 total_time = time.time() - start_time gpu_utilization = gpu_active_time / total_time * 100 if total_time \u0026gt; 0 else 0 sender.send_pyobj({ \u0026#34;type\u0026#34;: \u0026#34;STATS\u0026#34;, \u0026#34;gpu_active_time\u0026#34;: gpu_active_time, \u0026#34;total_time\u0026#34;: total_time, \u0026#34;gpu_utilization\u0026#34;: gpu_utilization, \u0026#34;processed_count\u0026#34;: processed_count, \u0026#34;gpu_activity_periods\u0026#34;: gpu_activity_periods }) break # 解包任务数据 request_id, input_data, gpu_time = message # 执行GPU计算 gpu_start = time.time() result = simulate_gpu_computation(input_data, gpu_time) gpu_end = time.time() # 记录GPU活动时间段 gpu_activity_periods.append((gpu_start, gpu_end)) # 更新GPU活跃时间 gpu_active_time += (gpu_end - gpu_start) processed_count += 1 # 发送结果 sender.send_pyobj((request_id, result)) except Exception as e: print(f\u0026#34;GPU工作进程错误: {e}\u0026#34;) finally: # 关闭连接 receiver.close() sender.close() context.term() print(\u0026#34;GPU工作进程已终止\u0026#34;) # CPU进程：生成请求，预处理，发送到GPU，接收结果，后处理 def zmq_approach(num_requests=10, matrix_size=100, preprocess_time=0.05, gpu_time=0.1, postprocess_time=0.05): \u0026#34;\u0026#34;\u0026#34;使用ZeroMQ分离CPU和GPU操作\u0026#34;\u0026#34;\u0026#34; print(\u0026#34;\\n运行ZeroMQ方式（分离CPU和GPU操作）...\u0026#34;) # 启动GPU工作进程 gpu_process = multiprocessing.Process(target=gpu_worker) gpu_process.daemon = True gpu_process.start() # 给GPU进程一点时间启动 time.sleep(0.5) # 创建ZeroMQ上下文 context = zmq.Context() # 设置发送输入数据的PUSH socket sender = context.socket(zmq.PUSH) sender.connect(\u0026#34;tcp://localhost:5555\u0026#34;) # 设置接收结果的PULL socket receiver = context.socket(zmq.PULL) receiver.connect(\u0026#34;tcp://localhost:5556\u0026#34;) # 开始计时 start_time = time.time() # 创建结果字典 results = {} # 记录CPU活动时间段 cpu_activity_periods = [] # 启动预处理和发送线程 def preprocess_and_send(): for i in range(num_requests): request_id = f\u0026#34;req_{i}\u0026#34; # CPU预处理开始 cpu_start = time.time() # CPU预处理 input_data = simulate_cpu_preprocessing(request_id, matrix_size, preprocess_time) # CPU预处理结束 cpu_end = time.time() # 记录CPU预处理时间段 cpu_activity_periods.append((\u0026#34;preprocess\u0026#34;, cpu_start, cpu_end)) # 发送到GPU进程 sender.send_pyobj((request_id, input_data, gpu_time)) # 简单的进度显示 if (i + 1) % 10 == 0 or i == num_requests - 1: print(f\u0026#34;已发送 {i + 1}/{num_requests} 个请求\u0026#34;) send_thread = threading.Thread(target=preprocess_and_send) send_thread.daemon = True send_thread.start() # 接收结果和后处理 for _ in tqdm(range(num_requests), desc=\u0026#34;接收和处理结果\u0026#34;): # 接收结果 message = receiver.recv_pyobj() # 处理结果 if isinstance(message, dict) and message.get(\u0026#34;type\u0026#34;) == \u0026#34;STATS\u0026#34;: # 这是GPU进程发送的统计信息 gpu_stats = message continue request_id, result = message # CPU后处理开始 cpu_start = time.time() # CPU后处理 final_result = simulate_cpu_postprocessing(request_id, result, postprocess_time) # CPU后处理结束 cpu_end = time.time() # 记录CPU后处理时间段 cpu_activity_periods.append((\u0026#34;postprocess\u0026#34;, cpu_start, cpu_end)) # 存储结果 results[request_id] = final_result # 发送终止信号给GPU进程 sender.send_pyobj(\u0026#34;DONE\u0026#34;) # 接收GPU统计信息 gpu_stats = receiver.recv_pyobj() # 等待GPU进程终止 gpu_process.join(timeout=1) # 结束计时 end_time = time.time() total_time = end_time - start_time # 计算CPU-GPU重叠时间 gpu_periods = gpu_stats[\u0026#39;gpu_activity_periods\u0026#39;] overlap_time = calculate_overlap(cpu_activity_periods, gpu_periods) overlap_percentage = (overlap_time / total_time) * 100 # 关闭ZeroMQ连接 sender.close() receiver.close() context.term() print(f\u0026#34;ZeroMQ方式完成时间: {total_time:.2f} 秒\u0026#34;) print(f\u0026#34;GPU活跃时间: {gpu_stats[\u0026#39;gpu_active_time\u0026#39;]:.2f} 秒\u0026#34;) print(f\u0026#34;CPU活跃时间: {sum([end-start for _, start, end in cpu_activity_periods]):.2f} 秒\u0026#34;) print(f\u0026#34;CPU-GPU重叠时间: {overlap_time:.2f} 秒 ({overlap_percentage:.2f}%)\u0026#34;) print(f\u0026#34;GPU利用率: {gpu_stats[\u0026#39;gpu_utilization\u0026#39;]:.2f}%\u0026#34;) return total_time, gpu_stats[\u0026#39;gpu_utilization\u0026#39;], overlap_percentage def calculate_overlap(cpu_periods, gpu_periods): \u0026#34;\u0026#34;\u0026#34;计算CPU和GPU活动时间的重叠部分\u0026#34;\u0026#34;\u0026#34; # 将CPU预处理和后处理时间段合并为单一列表 cpu_time_ranges = [(start, end) for _, start, end in cpu_periods] # 初始化重叠时间 total_overlap = 0.0 # 对每个GPU时间段，计算与所有CPU时间段的重叠 for gpu_start, gpu_end in gpu_periods: for cpu_start, cpu_end in cpu_time_ranges: # 计算重叠部分 overlap_start = max(gpu_start, cpu_start) overlap_end = min(gpu_end, cpu_end) # 如果有重叠，累加重叠时间 if overlap_end \u0026gt; overlap_start: total_overlap += (overlap_end - overlap_start) return total_overlap def compare_performance(num_requests=50): \u0026#34;\u0026#34;\u0026#34;比较传统方式和ZeroMQ方式的性能\u0026#34;\u0026#34;\u0026#34; # 设置参数 matrix_size = 100 preprocess_time = 0.05 # CPU预处理时间 gpu_time = 0.1 # GPU计算时间 postprocess_time = 0.05 # CPU后处理时间 # 运行传统方式 trad_time, trad_gpu_util = traditional_approach( num_requests, matrix_size, preprocess_time, gpu_time, postprocess_time ) # 运行ZeroMQ方式 zmq_time, zmq_gpu_util, overlap_percentage = zmq_approach( num_requests, matrix_size, preprocess_time, gpu_time, postprocess_time ) # 计算加速比 speedup = trad_time / zmq_time print(\u0026#34;\\n===== 性能对比 =====\u0026#34;) print(f\u0026#34;传统方式执行时间: {trad_time:.2f} 秒, GPU利用率: {trad_gpu_util:.2f}%\u0026#34;) print(f\u0026#34;ZeroMQ方式执行时间: {zmq_time:.2f} 秒, GPU利用率: {zmq_gpu_util:.2f}%\u0026#34;) print(f\u0026#34;CPU-GPU重叠比例: {overlap_percentage:.2f}%\u0026#34;) print(f\u0026#34;加速比: {speedup:.2f}x\u0026#34;) print(f\u0026#34;GPU利用率提升: {zmq_gpu_util - trad_gpu_util:.2f}%\u0026#34;) # 绘制性能对比图 fig = Figure(figsize=(15, 5)) canvas = FigureCanvas(fig) # 执行时间对比 ax1 = fig.add_subplot(131) methods = [\u0026#39;Traditional\u0026#39;, \u0026#39;ZeroMQ\u0026#39;] times = [trad_time, zmq_time] ax1.bar(methods, times, color=[\u0026#39;blue\u0026#39;, \u0026#39;green\u0026#39;]) ax1.set_ylabel(\u0026#39;Execution Time (seconds)\u0026#39;) ax1.set_title(\u0026#39;Execution Time Comparison\u0026#39;) # 添加数值标签 for i, v in enumerate(times): ax1.text(i, v + 0.1, f\u0026#34;{v:.2f}s\u0026#34;, ha=\u0026#39;center\u0026#39;) # 添加加速比 ax1.text(0.5, max(times) * 0.5, f\u0026#34;Speedup: {speedup:.2f}x\u0026#34;, ha=\u0026#39;center\u0026#39;, fontsize=12, bbox=dict(facecolor=\u0026#39;white\u0026#39;, alpha=0.8)) # GPU利用率对比 ax2 = fig.add_subplot(132) utils = [trad_gpu_util, zmq_gpu_util] ax2.bar(methods, utils, color=[\u0026#39;blue\u0026#39;, \u0026#39;green\u0026#39;]) ax2.set_ylabel(\u0026#39;GPU Utilization (%)\u0026#39;) ax2.set_title(\u0026#39;GPU Utilization Comparison\u0026#39;) ax2.set_ylim(0, 100) # 添加数值标签 for i, v in enumerate(utils): ax2.text(i, v + 1, f\u0026#34;{v:.2f}%\u0026#34;, ha=\u0026#39;center\u0026#39;) # 添加利用率提升 ax2.text(0.5, 50, f\u0026#34;Improvement: {zmq_gpu_util - trad_gpu_util:.2f}%\u0026#34;, ha=\u0026#39;center\u0026#39;, fontsize=12, bbox=dict(facecolor=\u0026#39;white\u0026#39;, alpha=0.8)) # CPU-GPU重叠比例 ax3 = fig.add_subplot(133) ax3.pie([overlap_percentage, 100-overlap_percentage], labels=[\u0026#39;Overlap\u0026#39;, \u0026#39;Non-overlap\u0026#39;], colors=[\u0026#39;green\u0026#39;, \u0026#39;lightgray\u0026#39;], autopct=\u0026#39;%1.1f%%\u0026#39;, startangle=90) ax3.set_title(\u0026#39;CPU-GPU Overlap Percentage\u0026#39;) ax3.axis(\u0026#39;equal\u0026#39;) # Equal aspect ratio ensures that pie is drawn as a circle fig.tight_layout() fig.savefig(\u0026#34;vllm_simulation_comparison.png\u0026#34;) print(\u0026#34;性能对比图已保存为 vllm_simulation_comparison.png\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#39;模拟vLLM架构的简化版本\u0026#39;) parser.add_argument(\u0026#39;--requests\u0026#39;, type=int, default=50, help=\u0026#39;请求数量\u0026#39;) args = parser.parse_args() # 比较性能 compare_performance(args.requests) 性能对比 我们比较了传统顺序处理和 ZeroMQ 并行处理的性能差异：\n运行传统方式（单进程顺序执行）... 处理请求: 100%|█████████████████████████████████████████████████████████████| 50/50 [00:10\u0026lt;00:00, 4.94it/s] 传统方式完成时间: 10.13 秒 GPU活跃时间: 5.04 秒 GPU利用率: 49.73% 运行ZeroMQ方式（分离CPU和GPU操作）... GPU工作进程已启动 接收和处理结果: 8%|████▍ | 4/50 [00:00\u0026lt;00:05, 8.69it/s]已发送 10/50 个请求 接收和处理结果: 18%|██████████ | 9/50 [00:01\u0026lt;00:04, 9.73it/s]已发送 20/50 个请求 接收和处理结果: 28%|███████████████▍ | 14/50 [00:01\u0026lt;00:03, 9.85it/s]已发送 30/50 个请求 接收和处理结果: 38%|████████████████████▉ | 19/50 [00:02\u0026lt;00:03, 9.89it/s]已发送 40/50 个请求 接收和处理结果: 46%|█████████████████████████▎ | 23/50 [00:02\u0026lt;00:02, 9.86it/s]已发送 50/50 个请求 接收和处理结果: 100%|███████████████████████████████████████████████████████| 50/50 [00:05\u0026lt;00:00, 9.69it/s] GPU工作进程收到终止信号 GPU工作进程已终止 ZeroMQ方式完成时间: 5.16 秒 GPU活跃时间: 5.04 秒 CPU活跃时间: 5.07 秒 CPU-GPU重叠时间: 4.96 秒 (96.09%) GPU利用率: 89.08% ===== 性能对比 ===== 传统方式执行时间: 10.13 秒, GPU利用率: 49.73% ZeroMQ方式执行时间: 5.16 秒, GPU利用率: 89.08% CPU-GPU重叠比例: 96.09% 加速比: 1.96x GPU利用率提升: 39.35% 通过 ZeroMQ 实现的 CPU-GPU 并行处理，我们获得了以下优势：\n更高的 GPU 利用率 更短的总执行时间 CPU 和 GPU 更好的工作重叠 总结与最佳实践 通过本教程，我们展示了如何使用 ZeroMQ 突破 Python GIL 限制，显著提升 CPU 密集型任务的性能，以及如何优化 CPU-GPU 协作。以下是一些最佳实践：\n何时使用 ZeroMQ 进行并行处理 CPU 密集型任务：计算密集的操作，如图像处理、数值计算等 可拆分的任务：能够被分割成独立子任务的问题 需要灵活通信模式的场景：超出简单多进程模型的复杂通信需求 CPU-GPU 协作优化：在深度学习推理等场景中优化资源利用 ZeroMQ vs Python 原生多进程 ZeroMQ 优势：更灵活的通信模式，更好的扩展性（可跨网络），更精细的控制 原生多进程优势：使用更简单，适合不需要复杂通信的场景 注意事项 进程间通信开销：分布式处理虽然能突破 GIL 限制，但也引入了通信开销 数据序列化：在进程间传递数据需要序列化和反序列化，对于大型数据可能成为瓶颈 任务粒度：太小的任务会使通信开销超过并行处理的收益，太大的任务会影响负载均衡 资源管理：在 CPU-GPU 并行场景中，需要合理管理内存和计算资源 通过合理使用 ZeroMQ 进行分布式处理，我们可以充分发挥多核处理器和 GPU 的性能，显著提升 Python 程序的执行效率，特别是对于计算密集型任务和深度学习推理场景。\n","permalink":"https://blog.niuhemoon.win/posts/tech/python-zeromq-gil/","summary":"\u003cp\u003e在 Python 编程中，全局解释器锁（Global Interpreter Lock，简称 GIL）一直是制约 CPU 密集型任务性能的主要瓶颈。本文将介绍如何使用 ZeroMQ 这一高性能分布式消息队列库来突破 GIL 限制，通过将单一进程拆分为多个通过消息通信的进程，从而充分利用多核 CPU 资源，显著提升性能。\u003c/p\u003e","title":"Python 性能优化：使用 ZeroMQ 突破 GIL 限制"},{"content":"简介 Qwen3是阿里云推出的新一代大语言模型，在各项基准测试中表现出色。本文将介绍如何在普通CPU环境下部署Qwen3模型，并测试其性能表现，为没有GPU资源的用户提供参考。\nOllama方案 Ollama是一个轻量级的本地LLM运行框架，支持多种模型格式，安装非常简单：\ncurl -fsSL https://ollama.com/install.sh | sh 基本部署 使用Ollama部署Qwen3非常简单，只需一行命令即可下载并运行模型：\nollama run qwen3:30b-a3b-q4_K_M # 如果使用modelscope # ollama run modelscope.cn/lmstudio-community/Qwen3-30B-A3B-GGUF 性能测试 uv pip install llm-benchmark 编辑配置文件\nfile_name: \u0026#34;custombenchmarkmodels.yml\u0026#34; version: 2.0.custom models: - model: \u0026#34;modelscope.cn/lmstudio-community/Qwen3-30B-A3B-GGUF\u0026#34; 运行测试\nllm_benchmark run --custombenchmark=path/to/custombenchmarkmodels.yml 测试在AMD Ryzen 7 PRO 8845HS w平台平均22token/s，已经达到本地可用的速度\nvllm方案 下载cpu镜像 docker pull public.ecr.aws/q9t5s3a7/vllm-cpu-release-repo:v0.8.5 docker tag public.ecr.aws/q9t5s3a7/vllm-cpu-release-repo:v0.8.5 vllm-cpu-env:latest # 另一种方式自行编译 # git clone https://github.com/vllm-project/vllm.git # cd vllm # sudo docker build -f docker/Dockerfile.cpu -t vllm-cpu-env --shm-size=16g . 部署模型 目前CPU版本的vLLM不支持Qwen3 q8量化，只能运行非量化版本，速度较慢，约1.8token/s，基本不可用。\n# 下载模型 modelscope download --model Qwen/Qwen3-30B-A3B # 运行docker测试 docker run --rm --network=host -e TRANSFORMERS_OFFLINE=1 -e HF_DATASET_OFFLINE=1 -v /home/nh/.cache/modelscope/hub/models/Qwen/Qwen3-30B-A3B/:/model -p 8000:8000 vllm-cpu-env:latest --model /model API接口测试 部署完成后，可以通过OpenAI兼容的API接口进行测试：\ncurl http://localhost:8000/v1/chat/completions -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;/model\u0026#34;, \u0026#34;messages\u0026#34;: [ {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Give me a short introduction to large language models.\u0026#34;} ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;top_p\u0026#34;: 0.8, \u0026#34;top_k\u0026#34;: 20, \u0026#34;max_tokens\u0026#34;: 8192, \u0026#34;presence_penalty\u0026#34;: 1.5, \u0026#34;chat_template_kwargs\u0026#34;: {\u0026#34;enable_thinking\u0026#34;: false} }\u0026#39; 参考 ollama/docs/faq.md at main · ollama/ollama\nHF-Mirror\nvLLM - Qwen\nOllama加载ModelScope模型 · 文档中心\naidatatools/ollama-benchmark: LLM Benchmark for Throughput via Ollama (Local LLMs)\n","permalink":"https://blog.niuhemoon.win/posts/tech/cpu%E4%B8%8A%E9%83%A8%E7%BD%B2qwen3%E5%B9%B6%E6%B5%8B%E8%AF%95%E9%80%9F%E5%BA%A6/","summary":"简介 Qwen3是阿里云推出的新一代大语言模型，在各项基准测试中表现出色。本文将介绍如何在普通CPU环境下部署Qwen3模型，并测试其性能表现，为没有GPU资源的用户提供参考。 Ollama方案 Ollama是一个轻量级的本地LLM运行框架，支持多种模型格式，安装非常简单： curl -fsSL https://ollama.com/install.sh | sh 基","title":"CPU上部署Qwen3模型及性能测试"},{"content":"引言 Info\n在深度学习和高性能计算领域，GPU 加速技术已成为提升模型训练和推理速度的关键。CUDA、Triton 和 Flash Attention 作为这一领域的重要技术，对于理解和优化大型语言模型尤为重要。本文将客观地探讨这些技术的学习路径、核心概念以及学习过程中的关键策略。\n学习方法论的演变 传统的学习理念常常强调\u0026quot;授人以鱼不如授人以渔\u0026quot;，即教授具体技能比直接提供解决方案更有价值。然而，在技术快速迭代的今天，这一理念需要进一步发展。现代学习方法论更强调\u0026quot;教人学习新技能\u0026quot;的重要性：\n授人以鱼，仅解一日之饥； 授人以渔，可解一生之需； 教人学习新技能，则能建造渔船，养活整村。\n这一理念尤为重要，因为在毕业后，人们往往发现自己身处\u0026quot;丛林\u0026quot;而非\u0026quot;海洋\u0026quot;，传统教育中习得的\u0026quot;钓鱼技能\u0026quot;可能无法直接应用。掌握学习新技能的能力，才是应对快速变化的技术环境的关键。\nCUDA、Triton 与 Flash Attention 概述 Flash Attention Flash Attention 是一种针对 Transformer 模型的优化 Attention 机制实现，其核心目标是最小化 GPU 上的内存拷贝（IO aware）。在大型语言模型中，Attention 机制是计算密集型操作，而 Flash Attention 通过优化内存访问模式，显著提升了性能。\nCUDA CUDA 是英伟达提供的软件栈，允许开发者编写在英伟达 GPU 上运行的 GPU kernels。它提供了直接访问 GPU 计算资源的能力，是高性能计算的基础工具。\nTriton Triton 是 OpenAI 的项目，旨在提供一种更标准化、硬件无关的方式来编写 GPU kernels。它允许使用 Python 编写代码并编译到 CUDA 或 ROCm 等后端，弥合了机器学习从业者（通常熟悉 Python）和 GPU kernel 开发者之间的差距。\nGPU 内存层级与性能优化 理解 GPU 内存层级结构是掌握性能优化的关键。GPU 内存主要分为两类：\nDRAM/HBM：容量大但访问速度相对较慢 SRAM：容量小但访问速度快 在 GPU 编程中，频繁的内存拷贝是性能瓶颈的主要来源。Kernel Fusion 是 Flash Attention 使用的关键优化技术之一，它通过合并多个操作到一个 kernel 中，减少内存访问次数，从而提升性能。\n学习驱动力：解决实际问题 学习新技术的动力应源于解决实际问题，而非盲目追逐技术热点。对于 CUDA 和 Triton 的学习，一个常见的驱动力是希望能够理解、调试和优化深度学习架构中的底层 GPU kernels，从而突破现有代码的性能限制。\n利用现代工具辅助学习 在学习复杂技术时，现代工具可以提供巨大帮助：\nTip\nAI 辅助工具：ChatGPT 等 AI 工具可以作为学习过程中的\u0026quot;最佳助手\u0026quot;，随时提供帮助，解锁学习过程中的障碍 个性化学习路径：不必过分依赖预设的学习路线图，可以通过不断提问和探索，基于自身知识构建个性化的学习路径 实践导向的学习策略 Note\n动手实践的重要性\n学习不仅仅是理论研究，更重要的是动手实践。对于 Triton 和 CUDA 的学习，直接开始官方教程是一个良好的起点。学习过程中，应该\u0026quot;边学边做\u0026quot;，通过实践加深理解。\n设定明确的学习目标\n学习任何新技术时，设定明确的目标至关重要。这些目标应该是具体的、可衡量的，并且与实际问题相关。例如，\u0026ldquo;实现一个基本的 CUDA kernel\u0026quot;或\u0026quot;优化特定模型中的 Attention 机制\u0026quot;都是良好的学习目标。\n实用学习策略示例 Vector Addition 学习示例 即使是像 Vector Addition 这样基础的教程，对于没有 GPU 编程经验的人来说也可能存在理解障碍。有效的学习策略包括：\n首先让代码能够运行起来 通过调试工具（如 Triton Interpreter）理解代码执行过程 遇到知识盲区时，有针对性地查找相关资源，但要带着目标去学习，不要偏离主线 攻克 Flash Attention 理解 Flash Attention 这类复杂算法的有效方法：\n首先通读相关论文，获取整体理解，并标记不理解的概念 不要立即停下来深入研究每一个不理解的点，而是先读完一遍，然后再针对性地查阅资料 对于重要的算法和证明，进行\u0026quot;主动学习\u0026quot;，例如尝试编写代码实现或手动推导证明过程 Flash Attention 的关键知识点 Online Softmax Flash Attention 为了在分块计算 Attention 时正确计算 Softmax 的归一化因子，使用了在线计算 Softmax 的方法。理解这一算法需要掌握相关数学原理，并通过编码实现和手动推导来加深理解。\nBlock Matrix Multiplication Flash Attention 将查询（Queries）、键（Keys）和值（Values）分块进行计算，因此理解分块矩阵乘法是掌握该算法的关键。\nTensor Shapes and Strides 理解 tensor 在内存中的存储方式（形状和步长）对于编写高效的 GPU kernels 至关重要，因为 GPU kernel 直接操作内存地址。\nBack Propagation Flash Attention 的反向传播过程也需要在 GPU kernel 中实现，因此需要理解反向传播的原理和梯度计算。PyTorch 通过链式法则计算梯度，并在矩阵乘法中避免显式计算 Jacobian 矩阵，这些优化方法对于理解和实现高效的反向传播至关重要。\n持续学习的建议 Warning\n追随好奇心而非炒作\n不要盲目追逐热门技术，而是应该基于自己的兴趣和目标进行学习。技术热点可能转瞬即逝，但解决实际问题的能力将长期有效。\n持续学习的关键要素 注重持续性 学习是一个长期的过程，持续的努力比一时的冲动更重要。即使起点不高，只要坚持不懈，最终也能取得进步。\n避免噪音 市场和社交媒体上有很多噪音，要专注于自己的学习路径和长期目标。可以了解新技术动态（\u0026ldquo;双流学习\u0026rdquo;），但不要轻易改变学习方向。\n长期投入才能精通 精通任何技能都需要时间的积累，学习是一个循序渐进的过程。\u0026ldquo;10000小时定律\u0026quot;在技术学习中依然适用，尽管现代工具可以在一定程度上加速学习过程。\n学习成功的关键：自信与行动 学习成功的关键在于学习者自身的态度和行动。建立自信心至关重要，而自信来源于完成困难的任务。不要仅仅停留在观看教程的被动学习阶段，要积极主动地挑战自己，尝试改进已有的工作。\n参与社区活动和挑战（如排行榜竞赛）是提升技能的有效方式，即使一开始成绩不佳也没关系，\u0026ldquo;唯一的失败是不参与\u0026rdquo;。参与本身就是一种挑战和进步。\n实用学习资源 对于没有 GPU 的学习者，可以利用以下资源进行学习和实践：\nInfo\nGoogle Colab 云平台上的 GPU 实例 Triton Interpreter 结语 CUDA、Triton 和 Flash Attention 的学习是一个循序渐进的过程，需要理论与实践相结合，持续投入时间和精力。通过设定明确目标、积极实践、利用现代工具和保持持续学习的态度，任何人都能够逐步掌握这些复杂的技术，并将其应用于解决实际问题。\n在技术快速发展的今天，掌握\u0026quot;学习如何学习\u0026quot;的能力比掌握特定技术更为重要，这将使学习者能够不断适应新的技术环境，并在技术变革中保持竞争力。\n参考资源 NOTEBOOKLM音频 Youtube ","permalink":"https://blog.niuhemoon.win/posts/tech/cuda-triton-flash-attention/","summary":"引言 Info 在深度学习和高性能计算领域，GPU 加速技术已成为提升模型训练和推理速度的关键。CUDA、Triton 和 Flash Attention 作为这一领域的重要技术，对于理解和优化大型语言模型尤为重要。本文将客观地探讨这些技术的学习路径、核心概念以及学习过程中的关键策略。 学习方法论的演变 传统的学习理念常常强调","title":"CUDA、Triton 与 Flash Attention 学习之旅"},{"content":"Docker Compose 的两个版本 目前 Docker Compose 有两个主要版本：\nDocker Compose V1：传统版本，命令为 docker-compose Docker Compose V2：新版本，命令为 docker compose（无连字符） # 检查版本 docker-compose --version # V1 docker compose version # V2 一个简单的 Web 应用示例 让我们从一个简单的 Web 应用开始，它包含一个 Web 服务和一个数据库服务。\n创建项目结构 my-web-app/ ├── docker-compose.yaml ├── web/ │ ├── Dockerfile │ ├── app.py │ └── requirements.txt 编写 Web 应用 (app.py) from flask import Flask, jsonify import os import pymysql app = Flask(__name__) @app.route(\u0026#39;/\u0026#39;) def hello(): return jsonify(message=\u0026#34;Hello from Flask!\u0026#34;) @app.route(\u0026#39;/db\u0026#39;) def db_connection(): connection = pymysql.connect( host=os.environ.get(\u0026#39;DB_HOST\u0026#39;, \u0026#39;db\u0026#39;), user=os.environ.get(\u0026#39;DB_USER\u0026#39;, \u0026#39;user\u0026#39;), password=os.environ.get(\u0026#39;DB_PASSWORD\u0026#39;, \u0026#39;password\u0026#39;), database=os.environ.get(\u0026#39;DB_NAME\u0026#39;, \u0026#39;mydb\u0026#39;), cursorclass=pymysql.cursors.DictCursor ) try: with connection.cursor() as cursor: cursor.execute(\u0026#34;SELECT 1 as result\u0026#34;) result = cursor.fetchone() return jsonify(db_connection=\u0026#34;成功\u0026#34;, result=result) except Exception as e: return jsonify(db_connection=\u0026#34;失败\u0026#34;, error=str(e)) finally: connection.close() if __name__ == \u0026#39;__main__\u0026#39;: app.run(host=\u0026#39;0.0.0.0\u0026#39;, port=5000) 依赖文件 (requirements.txt) flask==2.0.1 pymysql==1.0.2 Dockerfile FROM python:3.9-slim WORKDIR /app COPY requirements.txt . RUN pip install --no-cache-dir -r requirements.txt COPY app.py . CMD [\u0026#34;python\u0026#34;, \u0026#34;app.py\u0026#34;] Docker Compose 文件 (docker-compose.yaml) version: \u0026#39;3\u0026#39; services: web: build: ./web ports: - \u0026#34;5000:5000\u0026#34; environment: - DB_HOST=db - DB_USER=user - DB_PASSWORD=password - DB_NAME=mydb depends_on: - db restart: always db: image: mysql:5.7 environment: - MYSQL_ROOT_PASSWORD=rootpassword - MYSQL_DATABASE=mydb - MYSQL_USER=user - MYSQL_PASSWORD=password volumes: - db_data:/var/lib/mysql restart: always volumes: db_data: Docker Compose 核心概念 1. 服务 (Services) 服务是应用的组成部分，在上面的例子中，我们定义了两个服务：\nweb：一个 Flask 应用，从 Dockerfile 构建 db：一个 MySQL 数据库，使用官方镜像 每个服务都可以配置：\n镜像或构建上下文 端口映射 环境变量 依赖关系 重启策略等 2. 网络 (Networks) Docker Compose 会自动创建一个网络，所有服务都连接到这个网络。服务可以通过服务名称相互访问，如 web 服务可以通过 db 主机名访问数据库。\n如果需要自定义网络，可以这样配置：\nnetworks: frontend: driver: bridge backend: driver: bridge services: web: networks: - frontend - backend db: networks: - backend 3. 数据卷 (Volumes) 数据卷用于持久化数据或在容器间共享数据：\nvolumes: db_data: # 命名卷，数据会在容器重启后保留 也可以使用绑定挂载，将主机目录挂载到容器：\nvolumes: - ./config:/app/config # 绑定挂载 常用 Docker Compose 命令 启动服务 # 构建并启动所有服务（前台） docker compose up # 构建并启动所有服务（后台） docker compose up -d # 仅构建镜像，不启动 docker compose build 停止服务 # 停止服务但不删除容器 docker compose stop # 停止服务并删除容器 docker compose down # 停止服务并删除容器、网络、数据卷 docker compose down -v 查看状态 # 查看服务状态 docker compose ps # 查看日志 docker compose logs # 实时查看日志 docker compose logs -f 执行命令 # 在 web 服务中执行命令 docker compose exec web bash # 在 web 服务中运行一次性命令 docker compose run --rm web python -c \u0026#34;print(\u0026#39;Hello\u0026#39;)\u0026#34; 实用示例：WordPress 博客 下面是一个更实用的示例，部署 WordPress 博客：\nversion: \u0026#39;3\u0026#39; services: wordpress: image: wordpress:latest ports: - \u0026#34;8080:80\u0026#34; environment: - WORDPRESS_DB_HOST=db - WORDPRESS_DB_USER=wordpress - WORDPRESS_DB_PASSWORD=wordpress - WORDPRESS_DB_NAME=wordpress depends_on: - db restart: always db: image: mysql:5.7 environment: - MYSQL_ROOT_PASSWORD=rootpassword - MYSQL_DATABASE=wordpress - MYSQL_USER=wordpress - MYSQL_PASSWORD=wordpress volumes: - db_data:/var/lib/mysql restart: always volumes: db_data: 使用 docker compose up -d 启动后，访问 http://localhost:8080 即可看到 WordPress 安装界面。\n高级功能 1. 环境变量 可以使用 .env 文件存储环境变量：\n# .env 文件 DB_PASSWORD=mysecretpassword WORDPRESS_PORT=8080 然后在 docker-compose.yaml 中引用：\nservices: wordpress: ports: - \u0026#34;${WORDPRESS_PORT}:80\u0026#34; 2. 健康检查 services: web: healthcheck: test: [\u0026#34;CMD\u0026#34;, \u0026#34;curl\u0026#34;, \u0026#34;-f\u0026#34;, \u0026#34;http://localhost:5000\u0026#34;] interval: 30s timeout: 10s retries: 3 3. 扩展服务 # 启动 3 个 web 服务实例 docker compose up -d --scale web=3 最佳实践 使用版本控制：将 docker-compose.yaml 纳入版本控制系统 环境变量分离：使用 .env 文件存储敏感信息 合理命名：为服务、网络和卷使用有意义的名称 限制资源使用：设置内存和 CPU 限制 使用健康检查：确保服务正常运行 结论 Docker Compose 极大地简化了多容器应用的部署和管理。通过简单的 YAML 配置，你可以定义完整的应用栈，包括服务、网络和数据卷。\n无论是开发环境、测试环境还是小型生产环境，Docker Compose 都是一个强大且易用的工具。对于更大规模的部署，可以考虑使用 Kubernetes 或 Docker Swarm 等容器编排平台。\n希望这篇指南能帮助你快速上手 Docker Compose！\n","permalink":"https://blog.niuhemoon.win/posts/tech/docker-compose-guide/","summary":"Docker Compose 的两个版本 目前 Docker Compose 有两个主要版本： Docker Compose V1：传统版本，命令为 docker-compose Docker Compose V2：新版本，命令为 docker compose（无连字符） # 检查版本 docker-compose --version # V1 docker compose version # V2 一个简单的 Web 应用示例 让我们从一个简单的 Web 应用开始，它包含一个 Web 服务和一个数据库服务。 创建项目结构 my-web-app/ ├── docker-compose.yaml ├── web/ │ ├── Dockerfile │ ├── app.py","title":"Docker Compose 实用指南：从入门到实践"},{"content":"在人工智能快速发展的今天，大语言模型(LLM)已成为许多应用的核心技术。本文将详细介绍如何使用多种编程语言接入大语言模型API，以DeepSeek API为例，展示不同语言的实现方式。\n介绍 大语言模型API通常提供两种响应模式：\n非流式响应(Non-streaming): 等待模型生成完整回答后一次性返回 流式响应(Streaming): 实时返回模型生成的内容，提供更好的用户体验 本文将展示如何使用Python、curl和Golang实现这两种模式的API调用，并在折叠部分提供其他语言的实现方式。\nPython实现 Python是接入大语言模型API最常用的语言之一，主要通过官方提供的OpenAI库实现。\n标准实现 以下是使用Python OpenAI库的标准实现，支持流式和非流式响应：\n# 请先安装OpenAI SDK: `pip3 install openai` from openai import OpenAI # 全局配置 STREAM = True # 设置为True启用流式模式 # 使用DeepSeek API密钥和基础URL初始化客户端 client = OpenAI( api_key=\u0026#34;sk-your_deepseek_apikey\u0026#34;, base_url=\u0026#34;https://api.deepseek.com\u0026#34; ) # 创建聊天完成请求 response = client.chat.completions.create( model=\u0026#34;deepseek-chat\u0026#34;, messages=[ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a helpful assistant\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Hello\u0026#34;}, ], stream=STREAM ) # 根据流式模式处理响应 if STREAM: # 流式模式需要迭代处理每个数据块 full_response = \u0026#34;\u0026#34; for chunk in response: if chunk.choices[0].delta.content is not None: content = chunk.choices[0].delta.content full_response += content print(content, end=\u0026#34;\u0026#34;, flush=True) print() # 在结尾添加换行 else: # 非流式模式直接打印响应 print(response.choices[0].message.content) 异步实现 对于需要高并发处理的应用，可以使用Python的异步实现：\n# 请先安装OpenAI SDK: `pip3 install openai` import asyncio from openai import AsyncOpenAI # 全局配置 STREAM = True # 设置为True启用流式模式 # 使用DeepSeek API密钥和基础URL初始化异步客户端 client = AsyncOpenAI( api_key=\u0026#34;sk-your_deepseek_apikey\u0026#34;, base_url=\u0026#34;https://api.deepseek.com\u0026#34; ) async def get_completion(): # 创建聊天完成请求 response = await client.chat.completions.create( model=\u0026#34;deepseek-chat\u0026#34;, messages=[ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a helpful assistant\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me about quantum computing\u0026#34;}, ], stream=STREAM ) # 根据流式模式处理响应 if STREAM: # 流式模式需要异步迭代处理每个数据块 full_response = \u0026#34;\u0026#34; async for chunk in response: if chunk.choices[0].delta.content is not None: content = chunk.choices[0].delta.content full_response += content print(content, end=\u0026#34;\u0026#34;, flush=True) print() # 在结尾添加换行 return full_response else: # 非流式模式直接返回响应 print(response.choices[0].message.content) return response.choices[0].message.content async def main(): await get_completion() # 运行异步函数 if __name__ == \u0026#34;__main__\u0026#34;: asyncio.run(main()) curl实现 curl是一个命令行工具，可以直接在终端中调用API，非常适合快速测试和调试：\n#!/bin/bash # DeepSeek API 使用 curl 的示例 echo \u0026#34;使用 curl 发送请求到 DeepSeek API...\u0026#34; curl \u0026#34;https://api.deepseek.com/v1/chat/completions\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -H \u0026#34;Authorization: Bearer sk-your_deepseek_apikey\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;deepseek-chat\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Write me a haiku about mountains\u0026#34; } ] }\u0026#39; echo -e \u0026#34;\\n\\n请求完成！\u0026#34; 要实现流式响应，可以添加stream=true参数：\n#!/bin/bash # DeepSeek API 使用 curl 的流式响应示例 echo \u0026#34;使用 curl 发送流式请求到 DeepSeek API...\u0026#34; curl \u0026#34;https://api.deepseek.com/v1/chat/completions\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -H \u0026#34;Authorization: Bearer sk-your_deepseek_apikey\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;deepseek-chat\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Write me a haiku about mountains\u0026#34; } ], \u0026#34;stream\u0026#34;: true }\u0026#39; echo -e \u0026#34;\\n\\n请求完成！\u0026#34; Golang实现 Golang提供了高性能的API调用实现，使用openai-go库可以轻松实现流式和非流式响应。\n非流式实现 package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;github.com/openai/openai-go\u0026#34; \u0026#34;github.com/openai/openai-go/option\u0026#34; ) func main() { // 创建DeepSeek API客户端 client := openai.NewClient( option.WithAPIKey(\u0026#34;sk-your_deepseek_apikey\u0026#34;), option.WithBaseURL(\u0026#34;https://api.deepseek.com\u0026#34;), ) ctx := context.Background() question := \u0026#34;Write me a haiku\u0026#34; fmt.Print(\u0026#34;\u0026gt; \u0026#34;) fmt.Println(question) fmt.Println() params := openai.ChatCompletionNewParams{ Messages: []openai.ChatCompletionMessageParamUnion{ openai.UserMessage(question), }, Model: \u0026#34;deepseek-chat\u0026#34;, } completion, err := client.Chat.Completions.New(ctx, params) if err != nil { panic(err) } fmt.Println(completion.Choices[0].Message.Content) } 流式实现 package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;github.com/openai/openai-go\u0026#34; \u0026#34;github.com/openai/openai-go/option\u0026#34; ) func main() { // 创建DeepSeek API客户端 client := openai.NewClient( option.WithAPIKey(\u0026#34;sk-your_deepseek_apikey\u0026#34;), option.WithBaseURL(\u0026#34;https://api.deepseek.com\u0026#34;), ) ctx := context.Background() question := \u0026#34;Write me a haiku\u0026#34; fmt.Print(\u0026#34;\u0026gt; \u0026#34;) fmt.Println(question) fmt.Println() stream := client.Chat.Completions.NewStreaming(ctx, openai.ChatCompletionNewParams{ Messages: []openai.ChatCompletionMessageParamUnion{ openai.UserMessage(question), }, Model: \u0026#34;deepseek-chat\u0026#34;, }) for stream.Next() { evt := stream.Current() if len(evt.Choices) \u0026gt; 0 { fmt.Print(evt.Choices[0].Delta.Content) } } fmt.Println() if err := stream.Err(); err != nil { panic(err.Error()) } } 其他语言实现 Node.js实现 Node.js可以使用内置的fetch API或第三方库调用大语言模型API：\n// 使用 Node.js 的 fetch API 发送请求到 DeepSeek API async function getCompletion() { console.log(\u0026#34;使用 Node.js fetch API 发送请求到 DeepSeek API...\u0026#34;); const apiKey = \u0026#34;sk-your_deepseek_apikey\u0026#34;; const url = \u0026#34;https://api.deepseek.com/v1/chat/completions\u0026#34;; const response = await fetch(url, { method: \u0026#34;POST\u0026#34;, headers: { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Authorization\u0026#34;: `Bearer ${apiKey}` }, body: JSON.stringify({ model: \u0026#34;deepseek-chat\u0026#34;, messages: [ { role: \u0026#34;user\u0026#34;, content: \u0026#34;Write me a haiku about oceans\u0026#34; } ] }) }); const data = await response.json(); // 打印完整的JSON响应 console.log(\u0026#34;\\n完整响应:\u0026#34;); console.log(JSON.stringify(data, null, 2)); // 只打印生成的内容 console.log(\u0026#34;\\n生成的内容:\u0026#34;); console.log(data.choices[0].message.content); return data; } // 运行异步函数 getCompletion() .then(() =\u0026gt; console.log(\u0026#34;\\n请求完成！\u0026#34;)) .catch(error =\u0026gt; console.error(\u0026#34;发生错误:\u0026#34;, error)); Rust实现 Rust可以使用reqwest库实现API调用：\nuse reqwest; use serde::{Deserialize, Serialize}; use serde_json::{json, Value}; #[derive(Serialize, Deserialize, Debug)] struct Message { role: String, content: String, } #[derive(Serialize, Deserialize, Debug)] struct RequestBody { model: String, messages: Vec\u0026lt;Message\u0026gt;, } #[tokio::main] async fn main() -\u0026gt; Result\u0026lt;(), Box\u0026lt;dyn std::error::Error\u0026gt;\u0026gt; { let client = reqwest::Client::new(); let request_body = RequestBody { model: \u0026#34;deepseek-chat\u0026#34;.to_string(), messages: vec![ Message { role: \u0026#34;user\u0026#34;.to_string(), content: \u0026#34;Write me a haiku about mountains\u0026#34;.to_string(), }, ], }; let response = client .post(\u0026#34;https://api.deepseek.com/v1/chat/completions\u0026#34;) .header(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) .header(\u0026#34;Authorization\u0026#34;, \u0026#34;Bearer sk-your_deepseek_apikey\u0026#34;) .json(\u0026amp;request_body) .send() .await?; let json: Value = response.json().await?; println!(\u0026#34;{}\u0026#34;, json[\u0026#34;choices\u0026#34;][0][\u0026#34;message\u0026#34;][\u0026#34;content\u0026#34;]); Ok(()) } 总结 本文介绍了使用Python、curl和Golang接入大语言模型API的多种方式，包括流式和非流式响应处理。无论您选择哪种编程语言，都可以轻松地将大语言模型集成到您的应用中。\n选择合适的实现方式取决于您的具体需求：\n快速原型开发和测试：使用Python或curl 高性能服务：考虑Golang或异步Python 前端应用：可以使用JavaScript/Node.js 希望本文对您接入大语言模型API有所帮助！\n参考资料 OpenAI Python库文档 DeepSeek API文档 Golang openai-go库 ","permalink":"https://blog.niuhemoon.win/posts/tech/llm-api-integration-guide/","summary":"\u003cp\u003e在人工智能快速发展的今天，大语言模型(LLM)已成为许多应用的核心技术。本文将详细介绍如何使用多种编程语言接入大语言模型API，以DeepSeek API为例，展示不同语言的实现方式。\u003c/p\u003e","title":"多种编程语言接入大语言模型API指南"},{"content":"UV 相比 pip 的主要优势 速度更快\n使用 Rust 编写，性能优化更好 并行下载依赖 更智能的依赖解析算法 更好的依赖解析\n更准确的依赖树计算 更好地处理版本冲突 支持 lockfile，确保环境的可重现性 现代化特性\n原生支持虚拟环境管理 与 pip 完全兼容 支持从多种源安装包（PyPI、Git、本地等） 安装方法 在 macOS 上安装 UV：\nbrew install uv 基础使用 虚拟环境管理 # 创建虚拟环境 uv venv \u0026lt;env_name\u0026gt; # 激活虚拟环境 source \u0026lt;env_name\u0026gt;/bin/activate 包管理基础命令（pip适配） # 安装单个包 uv pip install package_name # 从 requirements.txt 安装依赖 uv pip install -r requirements.txt # 卸载包 uv pip uninstall package_name # 列出当前环境中已安装的所有包 uv pip list # 显示特定包的详细信息（版本、依赖等） uv pip show package_name # 导出当前环境的依赖到 requirements.txt uv pip freeze \u0026gt; requirements.txt # 以开发模式安装当前目录下的项目 # 适用于开发自己的包时，修改代码后无需重新安装 uv pip install -e . 依赖管理最佳实践 requirements.in vs requirements.txt requirements.in（源依赖文件）\nfastapi\u0026gt;=0.100.0 uvicorn[standard] python-dotenv requests 手动维护的直接依赖声明 使用灵活的版本限制（\u0026gt;=, ~=, ^） 只包含项目直接需要的包 requirements.txt（锁定文件）\n# This file was autogenerated by uv via the following command: # uv pip compile requirements.in -o requirements.txt annotated-types==0.7.0 # via pydantic anyio==4.9.0 # via starlette, watchfiles ... 由 uv pip compile 自动生成 包含所有依赖的精确版本 确保环境的可重现性 依赖管理工作流 # 创建 requirements.in 文件声明直接依赖 # 编译依赖生成 requirements.txt uv pip compile requirements.in -o requirements.txt # 安装依赖 uv pip sync requirements.txt 多环境依赖管理 # 开发环境依赖 uv pip compile requirements.in requirements-dev.in -o requirements-dev.txt # 生产环境依赖 uv pip compile requirements.in -o requirements.txt # 测试环境依赖 uv pip compile requirements.in requirements-test.in -o requirements-test.txt 依赖更新与安全 # 更新单个包 uv pip compile --upgrade-package fastapi # 更新所有包 uv pip compile --upgrade # 更新到最新的兼容版本 uv pip compile --upgrade-compatible # 生成带哈希值的依赖文件以确保安全性 uv pip compile --generate-hashes requirements.in -o requirements-hashes.txt 依赖约束 # 使用约束文件限制版本 uv pip compile --constraint constraints.txt requirements.in 项目实践示例 1. 项目初始化 # 创建项目目录 mkdir uv-demo cd uv-demo # 创建并激活虚拟环境 uv venv .venv source .venv/bin/activate 2. 依赖管理实践 创建 requirements.in 文件：\nfastapi\u0026gt;=0.100.0 uvicorn[standard] python-dotenv requests 编译并安装依赖：\nuv pip compile requirements.in -o requirements.txt uv pip sync requirements.txt 3. 示例应用 创建 main.py：\nfrom fastapi import FastAPI app = FastAPI() @app.get(\u0026#34;/\u0026#34;) async def root(): return {\u0026#34;message\u0026#34;: \u0026#34;Hello from UV managed environment!\u0026#34;} 运行应用：\nuvicorn main:app --reload UVX 工具 UVX 是 UV 的扩展工具，提供了一种简单的方式来运行 Python 包提供的命令。\nUVX 基本用法 # 使用特定包提供的命令 uvx --from \u0026lt;package_name\u0026gt; \u0026lt;command\u0026gt; # 使用临时安装的包运行命令 uvx --with \u0026lt;package_name\u0026gt; \u0026lt;command\u0026gt; # 使用 requirements.txt 中的依赖运行命令 uvx --with-requirements requirements.txt \u0026lt;command\u0026gt; 常见用例 # 运行 pytest 测试 uvx --with pytest pytest # 使用 black 格式化代码 uvx --with black black . # 在 CI 环境中运行测试 uvx --with-requirements requirements-dev.txt pytest 快速尝试新包：itry 模式 Simon Willison 提出了一种使用 uvx 快速尝试新 Python 包的简单模式，无需预先安装或创建虚拟环境：\n# 使用 uvx 启动带有指定包的 ipython 环境 uvx --with llm --with sqlite-utils ipython 这个命令会：\n创建一个专用的临时虚拟环境 安装指定的包（这里是 llm 和 sqlite-utils）以及 ipython 启动 ipython REPL，让你可以立即开始使用这些包 你还可以通过 --python 选项指定 Python 版本：\nuvx --python 3.13 --with llm --with sqlite-utils ipython 可以将这个模式封装成一个简单的 shell 脚本，方便日常使用：\n#!/bin/sh # itry - 用于通过 uvx 启动带有指定包的 ipython 的脚本 # 显示帮助信息 [ \u0026#34;$1\u0026#34; = \u0026#34;--help\u0026#34; ] \u0026amp;\u0026amp; { echo \u0026#34;用法: itry [包名...]\u0026#34; echo \u0026#34;示例: itry llm sqlite-utils datasette\u0026#34; exit 0 } # 初始化空字符串用于存储包 PACKAGES=\u0026#34;\u0026#34; # 处理所有参数，在每个参数前添加 --with for arg in \u0026#34;$@\u0026#34;; do PACKAGES=\u0026#34;$PACKAGES --with $arg\u0026#34; done # 如果存在，移除开头的空格 PACKAGES=\u0026#34;${PACKAGES# }\u0026#34; # 执行 uvx 命令 exec uvx $PACKAGES --python 3.13 ipython 推荐的项目结构 project/ ├── pyproject.toml # 项目配置 ├── requirements.in # 依赖声明 ├── requirements.txt # 锁定的依赖 └── src/ # 源代码目录 配置源 UV 支持通过配置文件指定包源，默认会查找以下位置的配置文件：\n~/.config/uv/uv.toml (用户级配置) /etc/uv/uv.toml (系统级配置) 示例配置（使用清华镜像源）：\n[[index]] url = \u0026#34;https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple/\u0026#34; default = true 更多配置选项请参考 UV 官方配置文档\n版本控制与 CI/CD 集成 版本控制策略\n将 requirements.txt 和 requirements-*.txt 加入版本控制 将虚拟环境（.venv）添加到 .gitignore CI/CD 集成\n# CI/CD 环境中使用 uv pip sync requirements.txt # 开发环境使用 uv pip sync requirements-dev.txt 开发工作流\n# 1. 创建虚拟环境 uv venv .venv source .venv/bin/activate # 2. 安装开发依赖 uv pip sync requirements-dev.txt # 3. 开发模式安装项目 uv pip install -e . 总结 UV 是一个现代化的 Python 包管理器，它在保持与 pip 兼容的同时，提供了更好的性能和依赖管理能力。它特别适合：\n需要快速依赖安装的大型项目 需要严格依赖版本控制的生产环境 需要可重现构建环境的团队协作项目 参考文档 UV 官方文档 UV CLI 参考 UV GitHub 仓库 ","permalink":"https://blog.niuhemoon.win/posts/tech/uv-guide/","summary":"UV 相比 pip 的主要优势 速度更快 使用 Rust 编写，性能优化更好 并行下载依赖 更智能的依赖解析算法 更好的依赖解析 更准确的依赖树计算 更好地处理版本冲突 支持 lockfile，确保环境的可重现性 现代化特性 原生支持虚拟环境管理 与 pip 完全兼容 支持从多种源安装包（PyPI、Git、本地等） 安装方法 在 macOS 上安装 U","title":"UV 包管理器使用指南"},{"content":"快速回忆C语言怎么写\n示例代码 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;string.h\u0026gt; // 定义枚举类型 enum Grade {A, B, C, D, F}; // 定义结构体 struct Student { char name[50]; int age; enum Grade grade; }; // 函数声明 void inputStudent(struct Student *s); void displayStudent(const struct Student *s); void saveToFile(const struct Student *s, const char *filename); int main() { struct Student students[100]; int count = 0; char filename[] = \u0026#34;students.txt\u0026#34;; // 输入学生信息 for (int i = 0; i \u0026lt; 3; i++) { // 假设输入3个学生 printf(\u0026#34;Enter details for student %d:\\n\u0026#34;, i + 1); inputStudent(\u0026amp;students[count]); count++; } // 显示学生信息 printf(\u0026#34;\\nStudent Details:\\n\u0026#34;); for (int i = 0; i \u0026lt; count; i++) { displayStudent(\u0026amp;students[i]); } // 保存到文件 for (int i = 0; i \u0026lt; count; i++) { saveToFile(\u0026amp;students[i], filename); } printf(\u0026#34;\\nStudent details saved to %s\\n\u0026#34;, filename); return 0; } // 输入学生信息 void inputStudent(struct Student *s) { printf(\u0026#34;Name: \u0026#34;); scanf(\u0026#34;%s\u0026#34;, s-\u0026gt;name); printf(\u0026#34;Age: \u0026#34;); scanf(\u0026#34;%d\u0026#34;, \u0026amp;s-\u0026gt;age); printf(\u0026#34;Grade (0=A, 1=B, 2=C, 3=D, 4=F): \u0026#34;); int grade; scanf(\u0026#34;%d\u0026#34;, \u0026amp;grade); s-\u0026gt;grade = (enum Grade)grade; } // 显示学生信息 void displayStudent(const struct Student *s) { printf(\u0026#34;Name: %s, Age: %d, Grade: %d\\n\u0026#34;, s-\u0026gt;name, s-\u0026gt;age, s-\u0026gt;grade); } // 保存学生信息到文件 void saveToFile(const struct Student *s, const char *filename) { FILE *fp = fopen(filename, \u0026#34;a\u0026#34;); if (fp == NULL) { perror(\u0026#34;Error opening file\u0026#34;); return; } fprintf(fp, \u0026#34;Name: %s, Age: %d, Grade: %d\\n\u0026#34;, s-\u0026gt;name, s-\u0026gt;age, s-\u0026gt;grade); fclose(fp); } CheetSheet C Cheat Sheet \u0026amp; Quick Reference\n其它资源 cplusplus.com/reference/ Learn C in Y Minutes ","permalink":"https://blog.niuhemoon.win/posts/tech/c-language-cheetsheet/","summary":"\u003cp\u003e快速回忆C语言怎么写\u003c/p\u003e","title":"C Language Cheetsheet"},{"content":"背景介绍 最近我一直在研究如何让LLM更好地理解和生成特定领域的内容。我选择了自己喜欢的小说《逍遥小散仙》作为实验数据，希望通过SFT（Supervised Fine-Tuning）让模型学习小说的内容、人物关系和写作风格，最终能够进行相关的分析和续写。这篇文章详细记录了我的数据集创建过程，以及最终的微调结果。\n数据集创建过程 数据预处理 首先将小说用calibre从epub格式转成txt格式，为后续的处理做准备。\n方法一：使用LLM提取问答数据 第一种方法是使用LLM从小说文本中提取问答对。这种方法的核心思想是将小说分成多个段落，然后让LLM为每个段落生成相关的问题和答案。\n1. 设置LLM接口 首先，我创建了llm.py文件，用于与LLM API进行交互：\nfrom openai import OpenAI from typing import List, Dict import json import re BaseURL = \u0026#39;https://api.aiproxy.io/v1\u0026#39; APIKEY = \u0026#39;sk-xxxx\u0026#39; # 已隐藏真实API密钥 Model = \u0026#39;gpt-4o-mini-2024-07-18\u0026#39; # 也可以使用DeepSeek API # BaseURL=\u0026#39;https://api.deepseek.com/v1\u0026#39; # APIKEY=\u0026#39;sk-xxxx\u0026#39; # Model=\u0026#39;deepseek-chat\u0026#39; client = OpenAI(api_key=APIKEY, base_url=BaseURL) def get_qa(raw_content, stream=False) -\u0026gt; str: response = client.chat.completions.create( model=Model, messages=[ {\u0026#39;role\u0026#39;: \u0026#39;system\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;你是一位小说分析师，负责从以下文本中提取信息并生成问答对。输出格式要求:\\n```json\\n[{\u0026#34;question\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;answer\u0026#34;: \u0026#34;*\u0026#34;}]```\u0026#39;}, {\u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: f\u0026#39;请仔细阅读文本,并根据文本内容生成问题和答案,问答对要包含文本所有有效信息, 问答对互相独立,看不到其它问题。确保问题涵盖角色、情节、情感和背景等多方面。答案应直接复制原文内容。直接用JSON格式回答。文本:\\n[{raw_content}]\\n请生成JSON格式(列表)问答对:\\n\u0026#39;} ], temperature=0.7, top_p=1.0, max_tokens=4096, stream=stream ) if stream: for chunk in response: if chunk.choices[0].delta.content is not None: print(chunk.choices[0].delta.content, end=\u0026#39;\u0026#39;) else: r_content = response.choices[0].message.content json_match = re.findall(r\u0026#39;```(?:json)?\\n(.*?)\\n```\u0026#39;, r_content, re.DOTALL) return json_match[0] if json_match else None 2. 加载和处理数据 接下来，创建main.py文件来加载小说文本并分块处理：\nfrom datasets import load_dataset from llm import get_qa import tqdm import json import time from typing import List # 加载整个TXT文件作为训练集 dataset = load_dataset(\u0026#34;text\u0026#34;, data_files={\u0026#34;train\u0026#34;: \u0026#34;xiaoyao.txt\u0026#34;}, sample_by=\u0026#34;paragraph\u0026#34;) # 输出数据集信息 print(dataset) chunk_size = 10 output_file = \u0026#39;results.jsonl\u0026#39; def convert_to_json(s) -\u0026gt; List: try: return json.loads(s) except Exception: return False total_chunks = (len(dataset[\u0026#39;train\u0026#39;]) + chunk_size - 1) // chunk_size pbar = tqdm.tqdm(total=len(dataset[\u0026#39;train\u0026#39;])) for idx in tqdm.tqdm(range(0, len(dataset[\u0026#39;train\u0026#39;]), chunk_size), total=total_chunks): chunk = \u0026#39;\\n\u0026#39;.join(dataset[\u0026#39;train\u0026#39;][idx: idx+chunk_size][\u0026#39;text\u0026#39;]) # 获取 QA 结果 response = get_qa(chunk) # 处理响应并保存 try: j_list = convert_to_json(response) if isinstance(j_list, list): with open(output_file, \u0026#39;a\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: for item in j_list: if not item: continue json.dump(item, f, ensure_ascii=False) f.write(\u0026#39;\\n\u0026#39;) else: # 如果是单个对象，直接写入 with open(output_file, \u0026#39;a\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: json.dump(response, f, ensure_ascii=False) f.write(\u0026#39;\\n\u0026#39;) except Exception as e: print(f\u0026#34;Error processing response: {e}\u0026#34;) 这段代码的工作流程是：\n使用datasets库加载小说文本，按段落进行分割 每次取10个段落组成一个chunk 将chunk发送给LLM进行问答对生成 解析LLM返回的JSON格式问答对 将问答对保存到JSONL文件中 3. 数据转换为SFT格式 生成问答对后，需要将其转换为SFT训练所需的格式。创建convert.py文件：\nimport json # 初始化一个空列表来存储转换后的数据 data = [] with open(\u0026#39;results.jsonl\u0026#39;, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as file: for line in file: # 解析每一行的 JSON 数据 json_object = json.loads(line) new_format = { \u0026#34;instruction\u0026#34;: json_object[\u0026#34;question\u0026#34;], \u0026#34;input\u0026#34;: \u0026#34;\u0026#34;, # 如果需要，可以根据需要填充 \u0026#34;output\u0026#34;: json_object[\u0026#34;answer\u0026#34;] } # 将新格式字典添加到列表中 data.append(new_format) with open(\u0026#39;output.json\u0026#39;, \u0026#39;w\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as outfile: json.dump(data, outfile, ensure_ascii=False, indent=2) 这段代码将问答对转换为SFT训练常用的指令格式：\ninstruction：问题 input：输入（在这个场景中为空） output：答案 4. 生成的数据示例 以下是生成的问答对数据示例：\n{ \u0026#34;instruction\u0026#34;: \u0026#34;小玄对自己的处境有何自我安慰的想法？\u0026#34;, \u0026#34;input\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;output\u0026#34;: \u0026#34;忽尔厚颜无耻地思道：男子汉大丈夫，自古以来就三妻四妾，她们老爹不就娶了五房夫人嘛，岳父可以放火，女婿就不能点灯么，姐妹俩总不能一点情理也不讲吧......\u0026#34; }, { \u0026#34;instruction\u0026#34;: \u0026#34;水若对小玄的反应是什么？\u0026#34;, \u0026#34;input\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;output\u0026#34;: \u0026#34;水若突地一呆，用手抵住了他的胸口，怔怔盯了他片刻，猛然地将其推开，挣扎坐起，恼恨交加道：滚开！以后，再不许你碰我！\u0026#34; } 方法二：提取对话数据 第二种方法是提取小说中的对话内容。我使用了extract-dialogue工具来完成这项工作，该工具可以从小说文本中自动识别并提取对话内容，包括说话人和对话内容。\n1. 安装和配置工具 首先，克隆工具仓库并安装依赖：\ngit clone https://github.com/KMnO4-zx/extract-dialogue.git cd extract-dialogue \u0026amp;\u0026amp; pip install -r requirements.txt 2. 配置API 创建.env文件，配置DeepSeek API：\n# 创建.env文件 DEEPSEEK_BASE_URL=https://api.deepseek.com DEEPSEEK_API=sk-xxxx 3. 创建主程序 创建main.py文件，用于处理小说文本并提取对话：\nfrom extract import system_prompt from schema import novel_schema from LLM import DeepseekChat from utils import ReadFiles from tqdm import tqdm import json # 指定小说文件路径 file_path = \u0026#39;./xiaoyao.txt\u0026#39; # 读取文件内容并分块，每块最大token数为500 docs = ReadFiles(file_path).get_content(max_token_len=500, cover_content=0) # 获取系统提示词，用于指导LLM提取对话 sys_prompt = system_prompt(novel_schema) # 初始化DeepSeek模型 model = DeepseekChat() # 获取文件名（不含扩展名）用于输出文件命名 file_name = file_path.split(\u0026#39;/\u0026#39;)[-1].split(\u0026#39;.\u0026#39;)[0] # 遍历所有文本块，提取对话 for i in tqdm(range(len(docs))): response = model.chat(sys_prompt, docs[i]) try: # 解析JSON响应 response = json.loads(response) # 将每条对话写入JSONL文件 for item in response: with open(f\u0026#39;{file_name}.jsonl\u0026#39;, \u0026#39;a\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: json.dump(item, f, ensure_ascii=False) f.write(\u0026#39;\\n\u0026#39;) except Exception as e: print(e) 4. 提取结果示例 运行程序后，会得到包含角色和对话内容的JSONL文件，每行是一个JSON对象：\n{\u0026#34;role\u0026#34;: \u0026#34;小玄\u0026#34;, \u0026#34;dialogue\u0026#34;: \u0026#34;办不到。\u0026#34;} {\u0026#34;role\u0026#34;: \u0026#34;方少麟\u0026#34;, \u0026#34;dialogue\u0026#34;: \u0026#34;办不到？你是不想？还是办不到？\u0026#34;} {\u0026#34;role\u0026#34;: \u0026#34;小玄\u0026#34;, \u0026#34;dialogue\u0026#34;: \u0026#34;换个条件。\u0026#34;} {\u0026#34;role\u0026#34;: \u0026#34;方少麟\u0026#34;, \u0026#34;dialogue\u0026#34;: \u0026#34;我想不出别的。如果这两个都做不到，那么我无法相信你之前说的话。\u0026#34;} {\u0026#34;role\u0026#34;: \u0026#34;方少麟\u0026#34;, \u0026#34;dialogue\u0026#34;: \u0026#34;行，你告诉我，除此之外，你还能做到什么？\u0026#34;} {\u0026#34;role\u0026#34;: \u0026#34;小玄\u0026#34;, \u0026#34;dialogue\u0026#34;: \u0026#34;我能让皇朝军退兵。如果你也退回泽阳，就可避免两败俱伤，令万千生灵涂炭！\u0026#34;} {\u0026#34;role\u0026#34;: \u0026#34;方少麟\u0026#34;, \u0026#34;dialogue\u0026#34;: \u0026#34;我不能。朝廷失政日久，如今天下荒荒，皆要推倒昏君，如果你无法证明昏君已经不在，凭我是说服不了别人的。\u0026#34;} 失败原因与经验总结 后续使用了LammaFactory工具进行SFT微调，测试效果都比较差，模型只能简单记忆问答对，而不能进行更复杂的对话。我认为一方面是数据集质量比较差，并且在微调时候产生了过拟合，模型遗忘了原有的能力。但更主要问题在于直接进行SFT可能不是最佳选择，应该先进行领域适应性的预训练。此外，数据集的规模和质量也有待提高。\n","permalink":"https://blog.niuhemoon.win/posts/tech/failed-novel-sft-dataset-creation/","summary":"","title":"记录一次失败的小说SFT数据集创建"},{"content":"Intro 利用whisper模型转录播客文本\n安装whisper.cpp并下载模型（Mac平台） 爬取播客mp3并转成wav 执行转录 Usage Install Whisper git clone https://github.com/ggerganov/whisper.cpp.git cd whisper.cpp bash ./models/download-ggml-model.sh large-v3-turbo make large-v3-turbo Transcribe 使用项目jerinphilip/pytorch-dev-podcasts-transcribe: Scripts to transcribe https://pytorch-dev-podcast.simplecast.com/episodes\ndownload.py import requests from lxml import etree import os def download(url, fpath): if os.path.exists(fpath): print(f\u0026#34;文件 {fpath} 已存在，跳过下载。\u0026#34;) return get_response = requests.get(url, stream=True) with open(fpath, \u0026#34;wb\u0026#34;) as f: for chunk in get_response.iter_content(chunk_size=1024): if chunk: # filter out keep-alive new chunks f.write(chunk) if __name__ == \u0026#34;__main__\u0026#34;: URL = \u0026#34;https://feeds.simplecast.com/OB5FkIl8\u0026#34; page = requests.get(URL) tree = etree.fromstring(page.content) nodes = tree.xpath(\u0026#34;//item\u0026#34;) print(len(nodes)) nodes.reverse() for idx, node in enumerate(nodes): enclosure = node.xpath(\u0026#34;./enclosure\u0026#34;) if enclosure: mp3_url = enclosure[0].get(\u0026#34;url\u0026#34;) title = node.xpath(\u0026#34;./title/text()\u0026#34;)[0] print(title) title = title.replace(\u0026#34; \u0026#34;, \u0026#34;-\u0026#34;) + \u0026#34;.mp3\u0026#34; title = str(idx) + \u0026#34;:\u0026#34; + title download(mp3_url, title) to-wave.sh #!/bin/bash mkdir -p wavs/ MP3s=$(ls *.mp3) for mp3 in ${MP3s[@]}; do PREFIX=$(echo $mp3 | sed \u0026#39;s/.mp3//g\u0026#39;) echo $PREFIX rm wavs/$PREFIX.wav ffmpeg -i $PREFIX.mp3 -acodec pcm_s16le -ar 16000 -ac 1 wavs/$PREFIX.wav done transcribe.sh #/bin/bash WHISPER_SRC=$HOME/Workshop/whisper.cpp WAVS=$(find wavs/*.wav) for wav in ${WAVS[@]}; do wav_path=$(realpath $wav) set -x # $WHISPER_SRC/build/bin/main -m $WHISPER_SRC/models/ggml-base.en.bin -f $wav_path --output-txt $WHISPER_SRC/build/bin/whisper-cli -m $WHISPER_SRC/models/ggml-large-v3-turbo.bin -f $wav_path --output-txt -t 8 -p 1 done pytorch podcast transcript EP1 Binding-C++-objects-to-Python Binding-C++-objects-to-Python Hello, my name is Edward, and this is episode one of my podcast about PyTorch things. I\u0026rsquo;m not really sure how this is going to work out or where I\u0026rsquo;m going to go with this, but for now, the idea behind this podcast is just to, you know, be a casual form for me to talk about, you know, various aspects of the PyTorch project. No particular organization. Today, I want to talk a little bit about how we bind Python to PyTorch. That is to say, you know, the whole point of PyTorch is to provide an object called a tensor that people can use. And, you know, to make this tensor object available from Python, we have to do bindings for it. And these bindings are actually quite intricate in some sense. And I want to just explain why it\u0026rsquo;s not as easy as it seems and talk a little bit about like how we actually solve this in the project, and some of the work that I\u0026rsquo;ve been working on recently. So what are Python bindings? Well, let\u0026rsquo;s imagine that you\u0026rsquo;re trying to design any sort of, you know, high performance computing library that has bindings available from a dynamically scripted language. So if you were just writing a data structure in the language itself, you would probably just define a class for the object in question in the language itself. And that would give you something very reasonable. Now, the problem is, you know, when you\u0026rsquo;re writing in interpreted languages like Python, all of the objects need to have a very regular layout. And it means that, you know, when you want to do something that actually needs to be very efficient, that needs to actually have some sort of packed layout, typically, the language itself won\u0026rsquo;t give you enough facilities to actually define the exact data layout you need. It\u0026rsquo;s going to be something that, you know, you have to go to a lower level language, like C or C++ to do. So the typical situation for anyone who\u0026rsquo;s writing a language, sorry, a library in this situation is you\u0026rsquo;ll have some sort of data structure, in our case, let\u0026rsquo;s call this data structure a tensor. And in and then you want to somehow make it possible for people to access this data structure from Python. So you\u0026rsquo;ve got two objects in hand, right? You\u0026rsquo;ve got this concept of an object in C++ land or in C land, a struct that knows nothing about Python, per se, because maybe you also wanted this library to be usable by other people who don\u0026rsquo;t have Python. And then you also need to somehow give a representation, a Python representation, that regular Python programs can understand. And sort of this split, this split where you want it to work both in a Python agnostic context and a Python context is where some of the complexity of binding objects in this way comes from. Now wait, Edward, you might be thinking, hey, you know, I can bind objects to Python. There\u0026rsquo;s this cool library called pybind11. And all I need to do is just take my object, you know, and wrap it up in this magic class underscore template. And then pybind11 goes through all the work somehow of, you know, making it possible to actually, you know, turn this object into a Python object. And I don\u0026rsquo;t know what it really does. Well, but you know, something happens. And so I want to talk a little bit about what happens in this case. And actually, when we talk about a type like tensor, we don\u0026rsquo;t actually use pybind11 to bind it, because pybind11 does something very interesting, uses a hash map, and we don\u0026rsquo;t want to pay the cost for that. So let\u0026rsquo;s talk about what it means to make a type actually available in Python. So we\u0026rsquo;ve got some C++ type, we\u0026rsquo;ve got some C struct, and we want to make it available to Python. So when we\u0026rsquo;re writing some Python bindings, we need to define a Python layout data structure that represents the Python object in question. So remember, Python is an interpreted language, all of the objects have a very regular form, Python is ref counted. So one of the things that every Python object needs to have is a header saying what kind of object it is, and what its reference count is. So if you like go and look up your CPython, you know, API notes about how to define a new define a new object, it\u0026rsquo;ll tell you, hey, you know, first to find this header, then you can put in your fields. And then there\u0026rsquo;s a description of the data type you have to do to actually say what the object in question is. Okay, that\u0026rsquo;s cool. So you can like copy paste some code and get this working. And then you have a problem, which is that you\u0026rsquo;ve got this Python object, and it\u0026rsquo;s not the same thing as your C struct. So what do you do? Well, you could do something like, okay, a Python object is simply a object that contains the C++ object in question. But this usually isn\u0026rsquo;t really quite what you want. Because let\u0026rsquo;s say that you have a pre existing C++ object, and you want to pass it to Python, right? like say I allocated a tensor from C++, and I want to return it from my program, and actually have, you know, someone in Python make use of it. If you just put the tensor in the Python object struct directly, well, you need to somehow, you know, move the data over into this new struct layout that\u0026rsquo;s got this header that, you know, Python expects your stuff to have, and you probably don\u0026rsquo;t want to actually move all of the data in question. So you know, the obvious thing to do in this situation is do an indirection, right? So instead of having the entire, you know, contents of the object stored, you\u0026rsquo;ll just have a pointer, right, maybe a shared pointer to the representation in question. Okay, so that, you know, lets you construct a Python object. But something very strange will happen if you actually try to run the code in this case. What will happen is, um, you pass your object to Python, um, you construct one of these Python objects, you wrap it up, uh, you set the pointer to point to the C++ object in question, and you got this Python object. Then, the next time you decide you want to return this Python object, well, okay, um, I need to go wrap up my, uh, pointer into one of these Python objects and return that. Notice something has happened. I\u0026rsquo;ve actually returned a new object in this situation so that, you know, even though both of these Python objects point to the same underlying C++ object, um, they\u0026rsquo;re two different Python objects. And if I do something like, you know, A is B, you know, the test for object identity, uh, in Python, uh, Python will just happily tell me, no, they\u0026rsquo;re not the same thing, even though the C++ type is actually the same thing. So usually when we bind, um, objects that have this notion of, you know, object identity, you know, usually objects you can mutate like tensors, for example. Um, we want to also preserve this notion of object identity when we bind them to Python. And so Pybind 11 lets you bind arbitrary objects to Python, and it also preserves object identity. And the way it does this is it maintains a giant hash map of all the C++ objects you\u0026rsquo;ve sent through it so that the next time you send the same C++ pointer through it, it can look it up in the hash table and say, oh, this is the Python object that I used last time. So let me just return that again. And this is how everything bound with Pybind 11 is going to work. Okay. Is this setting off performance alarm bells for you? Because it is for me. And it, this is kind of not actually, you know, this is not that fast. And if you, um, really care about making things fast, you don\u0026rsquo;t actually want to bind your objects this way, you want something cheaper to actually implement on this. You want, for example, to just be able to dereference a field on your object to get the Python object in question. And so this is what we did for tensor. So for tensors, we don\u0026rsquo;t maintain a hash map mapping and a given tensor to its Python object. Instead, we have a field on the tensor object. And this field simply points to the Python object in question that we want to return. So if I want to pass a tensor from C++ to Python, I just read out this field. If it\u0026rsquo;s not null, then I, there\u0026rsquo;s a Python object and I\u0026rsquo;ll just return that directly. If it is null, that means it\u0026rsquo;s the first time I\u0026rsquo;m actually sending this tensor to Python. So I can just go ahead and allocate one of these Python objects as I would have done before. And then I actually, you know, get this object in Python in this situation. So that, you know, works okay. And remember that even though, you know, allocating a new object and then setting it to the tensor seems very thread unsafe, all of our Python interactions are protected by the global interpreter lock. So actually, you know, Python takes care of all the synchronization for us. So this works decently well. And it\u0026rsquo;s what we do. One thing that you have to be careful about is this pointer that the tensor object has to the Python object is non-owning. Because remember, the Python object needs to keep the tensor C++ tensor live, right? So it has a strong reference from Python to C++. If the C++ object also had a strong reference to the Python object, you\u0026rsquo;d have a reference loop. And that\u0026rsquo;s bad because when you have a reference cycle in a ref kind of language, the result will never actually ever get deallocated. So strong reference from Python to C++ because, you know, if you\u0026rsquo;ve got a Python object, you better have a C++ tensor backing it. And C++ tensor to Python is a weak reference. Those of you who are thinking ahead might realize that there is a problem. And the problem is this. Because the reference to the Python object is weak, if I only have strong references to the C++ object and I have no more references to the Python object, then the Python object will actually be dead and it will get garbage collected by the CPython interpreter. So that\u0026rsquo;s not so great. And, you know, you kind of are wondering, well, what about this stale PyObject pointer in this case? Well, fortunately, we can actually define what the destructor for Python tensor object should be. So we just say, oh, clear out the PyObject field from the tensor when this happens. But this does mean that something very strange can happen in this situation. Namely, if you have a tensor and you send it to Python and then at some point all the Python references are dead, the next time you send it to Python, you will get a completely distinct object. Now, granted, it\u0026rsquo;s kind of difficult to notice when this has happened because, well, the old object isn\u0026rsquo;t around because you promised that you weren\u0026rsquo;t going to have any references to it. But, you know, if you, like, for example, took the ID of the object, the ID would be different between the two versions. And more importantly, and one of the reasons why I\u0026rsquo;ve recently been working on a patch to change this behavior, if you actually had some Python data stored on the tensor, for example, all objects in Python, you know, you can add arbitrary attributes to them after the fact using the underscore underscore dict attribute. Well, if you went ahead and added a bunch of these things to the tensor and then expected once you saved it in C++, for example, if you were saving it for backwards, one of the most common cases when we\u0026rsquo;ll save a tensor in C++ and it will outlive its Python equivalent, you won\u0026rsquo;t get that information when it pops back out into Python. And we have a bug tracking this issue and people don\u0026rsquo;t really like it, although it\u0026rsquo;s, you know, it\u0026rsquo;s kind of hard to solve a problem like this. So next time, I want to talk a little bit about how we are going to solve this. And it\u0026rsquo;s actually pretty nifty. It\u0026rsquo;s using a trick that Sam Gross, one of the original PyTorch developers, came up with. And I\u0026rsquo;m eager to share it with you next time. See ya.\nEP2 History-and-constraints-of-the-dispatcher History-and-constraints-of-the-dispatcher Hi, my name is Edward, and welcome to the PyTorch Dev Podcast. Today, I want to talk a little bit about the history and motivations behind one of the sort of more intricate pieces of PyTorch Core, the dispatcher. Now, what exactly is the dispatcher? Well, the dispatcher is basically the code that when you call a function, like when you call at::add or you call a method on a tensor, it figures out where you actually want to call it. I\u0026rsquo;ve done a few talks about the dispatcher in the past, and I also have a blog post talking about how the dispatcher works. And today, I want to do something a little different. So if you want to learn more about those aspects of the dispatcher, I recommend you go check out those posts. Instead, what I want to do is I want to do a little historical story about how the dispatcher came to be and what the various constraints and features we needed played out over time to make it into the system that it is today. So to talk about the dispatcher, we first need to talk about the time before the dispatcher. So before the dispatcher existed, and before A10 existed, PyTorch was built off of this library called TH. And TH itself wasn\u0026rsquo;t written when PyTorch was written, instead it itself came from a further back library called LuaTorch, which was basically the torch libraries like TH and THC bound to the Lua programming language. So when Adam Paschka and Coe wrote the first version of PyTorch, what they did was they just took all of the old school TH and THC libraries and wrote bindings for them for Python. And they also wrote an autograd system and data parallel support. But binding these torch libraries, which previously could only be called from Lua to Python, was sort of the first step on the journey here. So to understand how these bindings worked back in the day, it\u0026rsquo;s important to understand a little bit about how TH used to be constructed. As you know is the case today, a tensor library involves a lot of different operations, and each of these operations needs to be implemented for every D-type you want to support. So if you talk about an operation like add, it needs to be implemented for floats, and doubles, and integers, and 32-bit integers, and 8-bit integers, and so forth and so forth. TH was written in C. And if you\u0026rsquo;ve ever written any C before, you may know that C doesn\u0026rsquo;t really have any facilities for actually parameterizing over different D-types. So the way that they solved this problem was they were like, \u0026ldquo;Okay, we\u0026rsquo;re going to define a file. We are not going to talk about a float or a double. We\u0026rsquo;re just going to talk about some abstract type.\u0026rdquo; And then we will just include this file eight times with different settings of various macros to stamp out each version of the file. So if you talk about a function like add, we would have a TH_float_tensor_add and a TH_double_tensor_add and so forth and so forth. So there\u0026rsquo;d be like eight functions, and you know, at the Python binding level, what they did was they wrote some generated code, which basically was like, \u0026ldquo;Hey, you know, what\u0026rsquo;s the input tensor?\u0026rdquo; \u0026ldquo;Oh, it\u0026rsquo;s a floating-point tensor. Okay, I\u0026rsquo;m going to call TH_float_add in this case.\u0026rdquo; So it would just be the switch statement of all the various different dispatch types, and that\u0026rsquo;s how things were for a pretty long time. And about the time I joined Facebook, we were sort of trying to figure out what to do about the internals of PyTorch. And one of the things that was happening was that, you know, we had just bound the Torch library, and everything else was written in Python. And it turned out that PyTorch was kind of slow. And Sam Gross did some measurements and found that, you know, the reason why PyTorch was slow was because too much of it was written in Python. And so what we wanted to do was we wanted to port everything into C. But not actually C, because writing this TH code with its, you know, macros being stamped out eight times was actually pretty horrible. So what we actually wanted to do was write some C++. And during this time, Zachary DeVito came up with this idea. \u0026ldquo;Oh, all we want is a simple tensor library that gives us a tensor type in C++, just like the tensor type you would have in Python, with all the stuff you want. And then it\u0026rsquo;ll be easy to port all this stuff from Python to C++, because we\u0026rsquo;ll just use this tensor type and write the stuff we want in this case.\u0026rdquo; So Zach sort of, it\u0026rsquo;s really funny, like the way A10 got written was I think Zach locked himself in a room for two weeks. And at the end of two weeks, A10 was created. And Zach went through a bunch of different designs. He actually, I remember we were chatting about this and he was like, you know, I\u0026rsquo;ve gotten to this point and I don\u0026rsquo;t know if I should implement multiple dispatch or not. And we like talked about some of the pros and cons. And in the end, he didn\u0026rsquo;t decide to do that. And so what Zach did was in order to figure out which implementation of a particular D type you wanted to go to, instead of having one of these if statements, we were going to have a virtual object because this is C++ and C++ is all about objects and it\u0026rsquo;s all about virtual methods. So the concept was every tensor had a type object associated with it. By the way, the term type still shows up in various parts of the code base, even though these type objects no longer exist. But what the type object was, was it had virtual methods for every single operation you could imagine doing on a tensor. Adding, subtracting, sigmoid, whatever, you name it, it was there. And so every tensor would have a pointer to a type object that implemented all of the things you wanted for the object in question. And so to actually call an add on a tensor, you would instead go and the implementation of the method on the tensor object would instead go call the add on the type object attached to the tensor. And that would do a virtual call to actually get to the real implementation in question. Why did Zach do it this way? Because, you know, if you have done any object oriented programming, a really normal way to design an object hierarchy in a situation is, oh, I got a tensor super class, and I\u0026rsquo;m going to inherit a float tensor from it and an int tensor from it, and so forth and so forth. So there are a few reasons for this. So one is that Zach really wanted tensor to be what we think of as a pointer type. So let\u0026rsquo;s think about in, in Python, if I have a tensor and I say y equals, I have a tensor named x, and they say y equals x, then I want y to actually refer to the same memory and the same tensor really as x, right? I don\u0026rsquo;t like make a copy in this situation. We don\u0026rsquo;t pass objects by value in Python, they get passed by reference. Although some PL people would take offense to me calling it that way. But in any case, you know, assignment and passing things to parameters, they preserve object identity. You don\u0026rsquo;t create new versions of the object every time you do that. In C++, you have to actually, you know, say what you want your object to do. So if you just define a tensor class with a bunch of fields for sizes and strides and so forth, then if you pass that class by value to somewhere else, you will in fact copy all those fields when you get there. And that\u0026rsquo;s not at all what the Python semantics are. So tensor has to be some sort of class, which doesn\u0026rsquo;t do this. And so we need to not, we wanted tensor to actually work like the Python semantics. And so you can\u0026rsquo;t actually just subclass from tensor directly, because that just doesn\u0026rsquo;t work at all. Like, that\u0026rsquo;s not how C++ classes work. So another reason why Zach wanted a virtual dispatch rather than an if statement was because of the fact that CUDA support was this like sort of separate thing that was optional, you didn\u0026rsquo;t have to, you know, have a version of PyTorch with CUDA, you could instead link against the dynamic library that provided CUDA support. And then that would actually let you, you know, get all the CUDA functionality, but you could also not link against that library, and you\u0026rsquo;d only get the CPU support. So you had these libraries living in two different dynamic libraries. And if you\u0026rsquo;ve ever tried to write some code with multiple libraries, you might know that you can\u0026rsquo;t actually call a function in another library, unless you depend on that library. And the way things were set up is the CUDA library depended on the CPU library, but not vice versa. So if you\u0026rsquo;re in some CPU code, and you call this function, and actually the tensor turns out to be a CUDA tensor, you need to figure out how to actually get to the CUDA library. And the only way you really can do that is via via a virtual call. The types provide the virtual call, they worked pretty well, it was pretty fast. And we were happy for a while, until the next thing came along. So the next thing that came along was that, you know, we had this pretty cool A10 concept, there are all these operators, they all lived on the type object, and someone came up to us and they were like, \u0026ldquo;Hey, I want to define my own operator on top of the tensor class.\u0026rdquo; And I\u0026rsquo;d like, you know, like, I\u0026rsquo;d like to define tons and tons of custom operators, actually, because I\u0026rsquo;m Facebook, and I\u0026rsquo;ve got, you know, various very specialized use cases that I don\u0026rsquo;t have a general purpose operator for, but I still want to implement. And this type class, right, with all these virtual methods on it, there\u0026rsquo;s a problem. You can\u0026rsquo;t retroactively add more virtual methods to a class. Okay, sure, you can inherit from the class, but you can\u0026rsquo;t actually, um, but like, you have to, like, inherit each time you do it and make sure you inherit from the thing you inherited from previously. And this clearly is untenable if you\u0026rsquo;ve got, you know, 20 different people saying, \u0026ldquo;Hey, I want to add my own extra operator in this situation.\u0026rdquo; And it was actually kind of important to make sure that people register directly inside the type object, because remember, we also have this feature in PyTorch called autograd. And so actually, when you call a type object, you\u0026rsquo;re not necessarily calling into the CPU recruiter implementation. In some situations, you might call to the autograd implementation that has something different, and then eventually you\u0026rsquo;ll call into the CPU type afterwards. So this need for open registration meant that it wasn\u0026rsquo;t really tenable to keep using virtual tables. Virtual tables are a sort of marvel of C++ design, but one of the reasons why they can be implemented the way they are implemented is because you\u0026rsquo;re not allowed to add more methods onto them after the fact. And we wanted to be able to load up extra libraries, add new methods to them, and do that. And this is when the dispatcher sort of in its modern incarnation came into being, right? So the idea behind the dispatcher is, okay, we are not going to, um, we\u0026rsquo;re not going to let C++ handle the V table layout for us. Instead, we\u0026rsquo;re going to re-implement the V table ourselves. And furthermore, instead of having all of the virtual methods for all operations laid out into a single table, in which case, like, it\u0026rsquo;s not at all clear, like, um, how to add more things to the table, we\u0026rsquo;re just going to maintain separate tables per operator. So that when you call an operator, you know, you call add, you\u0026rsquo;re like, okay, um, uh, I\u0026rsquo;m going to go look at the add dispatch table. And, uh, it\u0026rsquo;s going to tell me how to go to CPU or CUDA because we want a lot of operators, open registration of operators, but for different backends like CPU and CUDA, those get added way less frequently. And, um, yeah, and that sort of brings the dispatcher into sort of, it\u0026rsquo;s, um, you know, a relatively modern form. There\u0026rsquo;s some things we added after the fact. Um, for example, uh, we wanted the ability to do multiple dispatch. So the, the request for multiple dispatch, um, came from a few places. So one case where sort of, we\u0026rsquo;d always known this was a bit of a problem is, um, we have support for sparse tensors in PyTorch. And so you have this interesting problem, which is that, uh, if you\u0026rsquo;ve got a dense tensor and a sparse tensor and you add them together, you want to send this to the sparse kernel because the sparse kernel is what is going to actually know how to deal with the sparse tensor. But in the initial implementation of the, um, type objects that did dispatching, um, we always looked at the type of the first object to figure out where to go. And since the first object\u0026rsquo;s a dense tensor, we go to the dense implementation and then you have to do some extra tests to see if things are sparse and route them to the right, right direction. Multiple dispatch would let you change the behavior of dispatch for, um, uh, to respect the, um, arguments of multiple, uh, to, to respect the types of multiple tensor arguments. So if you had a dense and a sparse, okay, actually that means I should go do something else, not just, you know, blindly look at the first argument. And, uh, Zach and I were talking about, um, how to like implement multiple dispatch quickly, um, during the fair offsite in Montreal. That\u0026rsquo;s like a few years ago. And Zach was like, Hey, you know, here\u0026rsquo;s how you could do it, right? You could maintain a, a set of keys, a bit set of keys representing all of the things represented by a tensor under some ordering saying which one you wanted to go to. And then if you want it to do multiple dispatch, all you needed to do was bitwise or all of these fields together, and then just pick out what the leftmost bit on the resulting, uh, key was to like get the, you know, highest priority dispatch key you want to dispatch in this case. You didn\u0026rsquo;t have to like do any like, okay, looping over the arguments, looking for the right one. It\u0026rsquo;s just do this bitwise or extract out the first bit and, and you\u0026rsquo;re done. And this basically served as the basis for the multiple dispatch implementation that is in PyTorch today, where you have a bunch of dispatch keys. They have a priority and we always dispatch to the highest priority key. These semantics came out because, you know, we had an idea about how to implement them efficiently. Uh, similarly, um, the work on automatic boxing came out of this problem, which is that, okay, uh, you know, we have all this, we have all these operators, we made operators extensible, and then we suddenly had a problem, which is that we couldn\u0026rsquo;t easily write code that was generic over all operators. Previously, the way we did this was we had a code generation phase, which, you know, knew about all the operators in PyTorch and was able to just write, you know, specialized C++ code for each one. But once we like open the gates up to let people register whatever operators they wanted, there were all these operators leaving outside of our repository, which the code generation knew nothing about and which, you know, we then couldn\u0026rsquo;t really generically program in any reasonable way. And so if the code gen doesn\u0026rsquo;t know about it, well, C++ doesn\u0026rsquo;t know what about the kernels in question. And so, uh, Sebastian Messmer, um, did this sort of years long project of sort of making sure that all objects, uh, all functions, even if they were registered outside of the dispatcher could via templating magic actually be generically programmed over. And so the, the technology of back and fallback, which sort of only recently went to stable is based on this. So the dispatcher today is pretty complicated. There\u0026rsquo;s a lot of features that it supports, but, um, you know, if you sort of look through the history, you can see, you know, there were various design constraints that got us where we were today. The design constraint of letting, you know, CPU and CUDA live in different dynamic libraries, the design constraint of open registration, and even, you know, the design constraints of allowing for multiple dispatch or automatic boxing. So these days, you know, the dispatcher has a lot of features. You can do a lot of things with it. And it\u0026rsquo;s also a little slow. Unfortunately, we\u0026rsquo;ve tried to make it faster, but it\u0026rsquo;s certainly a lot faster than if you were doing all of this in Python. And, um, I don\u0026rsquo;t know, um, the next time you have some project and you\u0026rsquo;re wondering, oh, why is the dispatcher this way? Just think about the constraints. It\u0026rsquo;s a really useful way to reason about things. Thank you all for listening. See you all later.\nEP3 Dynamic-library-structure Dynamic-library-structure Hi, my name is Edward, and welcome to today\u0026rsquo;s episode of the PyTorch Dev Podcast. Today, I want to talk a little bit about someone\u0026rsquo;s, or perhaps anyone who is a software architect\u0026rsquo;s favorite subject, the library structure in PyTorch. Now, what do I mean by the library structure in PyTorch? Isn\u0026rsquo;t PyTorch just one library that everyone uses? Well, that\u0026rsquo;s true in one sense, in that, you know, we distribute a single PyTorch wheel that people use and think of as one unit, but internally in our library, PyTorch is actually split into multiple separate dynamic libraries, at least in open source, but this is also true inside our internal build system. It\u0026rsquo;s split into multiple different libraries, you know, naming, ranging from C10, A10 Core, A10, Torch, Torch Python, and, you know, each of these libraries is, you know, a proper unit of encapsulation and means that you can\u0026rsquo;t, for example, willy-nilly depend on something from Torch Python from C10. If you\u0026rsquo;re not very familiar with, you know, what people are using these libraries for, you might think that this is just a whole waste of time, right? Like, you try to write some code, you put it in some folder, and then you have to decide which folder you\u0026rsquo;re going to put it in, and then it turns out you put it in the wrong folder, and you\u0026rsquo;ve got to, like, move some stuff around to make everything work out. It can really feel like a waste of time for no good reason. And some of the library structure in PyTorch is vestigial, and, you know, really shouldn\u0026rsquo;t be there, and we should, you know, reconsider how it\u0026rsquo;s actually set up. But a lot of the libraries in PyTorch exist for some good reasons, and in today\u0026rsquo;s podcast, I want to explain what the reasons behind the library split in PyTorch are. And hopefully, that will help you also think about how to better structure your code so that you don\u0026rsquo;t accidentally, you know, violate one of these abstraction boundaries. So, principle one that I would say about dynamic library, you know, structuring in general, like just how you decide to set up libraries, is that for any major dependency you might have, it\u0026rsquo;s usually a good idea to give it a separate library. So a good example of this is CUDA. CUDA is a really honking big dependency, right? Like, you\u0026rsquo;ve got to actually have NVIDIA\u0026rsquo;s CUDA runtime libraries, and then there\u0026rsquo;s, you know, actually a whole bunch of code in PyTorch that only really makes sense when you\u0026rsquo;re running on a system that has a GPU. We offer CPU-only builds of PyTorch, which don\u0026rsquo;t have any CUDA bits for people who don\u0026rsquo;t have GPUs. And the point of this is that, you know, many people don\u0026rsquo;t want CUDA, and so there should be a way to use PyTorch without having to actually drag in all a CUDA. And if you had PyTorch as one single giant library in the situation, that wouldn\u0026rsquo;t work. You\u0026rsquo;d, you know, have to always get in the CUDA dependency. Well, you might say, hey, Edward, you know, isn\u0026rsquo;t the normal thing in open source to give you a bunch of configure flags, and you just ask for which features you want? And the answer is yes, that\u0026rsquo;s true. Like, if you\u0026rsquo;ve ever built Python from source, for example, there\u0026rsquo;s a whole bunch of flags you can toggle on and off. But if you\u0026rsquo;re actually working in, say, a Linux distribution, or you\u0026rsquo;re working inside FB code, typically, it\u0026rsquo;s frowned upon to recompile the same piece of software multiple times with different flag settings. Because, well, you know, how are you going to distinguish between all these different versions? So when you\u0026rsquo;re in a situation where you can only ever build some piece of code once, well, you had better not, you know, you\u0026rsquo;d better find some other way besides if-deafing to split things out. And so in PyTorch, we have a A10 CPU library that has all of our CPU kernels, and we have an A10 CUDA library that contains all of our CUDA kernels. And so if you\u0026rsquo;re, say, in Buck, and you want to depend on a library, but you don\u0026rsquo;t want any of the CUDA functionality, there is actually a dependency you can depend on, the CPU-only dependency, that will prevent you from bringing in all your CUDA code. So if you look at another really important library, TorchPython, this one is also split off from LibTorch. And why is it split off? Well, because LibTorchPython has a dependency on the CPython API. And there\u0026rsquo;s plenty of situations when you are, you know, doing a C++-only application, and you don\u0026rsquo;t actually want to have the dependency on Python. So that\u0026rsquo;s principle one. Whenever there is a major dependency, there is probably a library split lurking nearby. Principle two is sort of related, but more of an internal concern, which is that you want to split so that you can use what you need. So what do I mean by that? Well, in many situations, binary size is at a premium, and you don\u0026rsquo;t want to actually ship code that you don\u0026rsquo;t actually use. So, you know, honestly, principle one is sort of the extreme version of this where the, you know, thing you\u0026rsquo;re not using is a giant, you know, honking blob of code that is from someone else. But, you know, PyTorch is also big in and of itself. And we don\u0026rsquo;t want to necessarily use code in PyTorch, if you know, we don\u0026rsquo;t need it, we don\u0026rsquo;t want to actually put things in, if you don\u0026rsquo;t need it. And so similarly, parts of PyTorch are split in this way, so that we can actually distribute these things without all of the functionality in question. So one good example of this in PyTorch is the split between A10 core and A10. Although this split is a little historical, because mobile is deciding to ship more and more stuff. In the beginning of the project, there was only a very limited subset of functionality that needed to be shipped on mobile. And so when you know, we, when we wanted to actually put PyTorch into production, we wanted to actually merge the Cafe2 and PyTorch code bases, we needed to find a way to like, put in the code that we wanted on mobile in one place, and all the code that you know, wasn\u0026rsquo;t relevant to mobile in some other place. And that\u0026rsquo;s why A10 is split into A10 core and A10. A10 core is the stuff that\u0026rsquo;s relevant for mobile, and A10 is everything else that you know, you might not be so interested in. I say the split is a little historical, because as time has gone on, and mobile has gotten more and more features, it turns out that A10 does provide a bunch of stuff that mobile wants. But in the beginning, it didn\u0026rsquo;t. And A10 core is this sort of minimal version that, you know, is generally applicable and takes up less binary space than all of A10. Another good example of this is the torch and A10 split. So A10 is short for a tensor library, and originally it was conceived of as just a way to do PyTorch code. Like, you know, you want to do an add, okay, A10 will tell you how to add two tensors together. Whereas torch is the lag, the library that actually gives you all of the sort of neural network functionality. So it knows how to do automatic differentiation, it knows about NN modules, all that good stuff. And so once again, if you\u0026rsquo;re in a situation where you don\u0026rsquo;t actually care about doing AD, you don\u0026rsquo;t care about doing neural networks, you just need a way to do some tensor computations, well, the split between A10 and torch means that you can just use A10 in that situation. So that\u0026rsquo;s principle two, which is split on what you need, a more, you know, sort of internal version of split on dependencies. And principle three is kind of a cop out, but it\u0026rsquo;s really important, which is we split our libraries for technical reasons. That is to say, sometimes there is no way to actually ship PyTorch unless we actually have things split in some particular way. Let me explain one particular example. So a very, um, a sort of rite of passage for any new developer on PyTorch is writing a new function and forgetting to slap a torch_api macro on it. You\u0026rsquo;ll get a very obscure linker error saying, hey, you know, I have no idea what this symbol is, even though, you know, like it compiled fine and the symbol is there, what the heck\u0026rsquo;s going on? So why does this macro exist in the first place? Well, this macro exists because of something very interesting. So I have to take a brief detour to explain. So when we write dynamic libraries, we have to specify what symbols we actually expose as opposed to private symbols, which aren\u0026rsquo;t available to external users. That kind of makes sense. And if you\u0026rsquo;re writing a, you know, standard Linux library, you usually just expose everything. Like you don\u0026rsquo;t really care about a very much hygiene in this case. But on Windows, there\u0026rsquo;s actually a problem, which is the Windows DLL format only allows for about 65,000 exported public symbols. Now 65,000 would be a lot of cookies to eat, but as far as symbols go, it\u0026rsquo;s nothing. And any, you know, self-respecting project is going to quickly hit this limit. So on Windows, because of this limitation, people tend to be a lot more careful about what actual symbols they put in their libraries. So you have to actually say, you know, what symbols you want. And if you, you know, if there\u0026rsquo;s a symbol that you don\u0026rsquo;t want, you just don\u0026rsquo;t make it public. So on Windows, we have hidden visibility by default, and you must explicitly export a symbol you want to. And guess what macro does that? Well, that\u0026rsquo;s the torch API macro. Okay, that\u0026rsquo;s cool. But what does that mean? Well, remember, the symbol limit still applies. Just using the torch API macro doesn\u0026rsquo;t mean that, you know, you\u0026rsquo;re not continuously adding more and more symbols. And it turns out that the consolidated PyTorch A10 and A10 CUDA libraries goes over the Windows symbol limit if you put them together. So no, we cannot ship PyTorch unless these libraries are separate, so that we are under the public symbol limit. Another example of a technical reason requiring us to actually keep the libraries split is for mobile. So mobile, mobile started off, you know, just having a small dependency on A10. But eventually, they actually needed operators. But there\u0026rsquo;s a problem, right, which is that A10 has a ton of operators, and mobile doesn\u0026rsquo;t really want most of them. Like, there\u0026rsquo;s only a few operators that are actually used by models in practice, and they\u0026rsquo;d much rather prefer to only ship those operators. So mobile has some very complicated system for recompiling PyTorch so that, you know, only the operators they care about are compiled for any given library. Okay, that\u0026rsquo;s cool. What do I recompile in this case? Well, library split comes to rescue. Because we have all of our CPU kernels in a separate library, A10 CPU, that\u0026rsquo;s the only library that needs to get recompiled on a per-app basis for mobile. A10 itself, which just contains, you know, common code that\u0026rsquo;s used everywhere, doesn\u0026rsquo;t need to get recompiled in this situation. So, you know, having the library split in this way made it easier for mobile to do selective build. And if you ever propose merging these things together, well, you\u0026rsquo;d better have an answer for what you\u0026rsquo;re going to do on the mobile side. So, what are the principles behind PyTorch\u0026rsquo;s library split? Well, you know, whenever there\u0026rsquo;s a major dependency, that usually means there\u0026rsquo;s going to be a library split. We split because that lets us, you know, let people use code that, you know, use what you need. You know, we don\u0026rsquo;t go to the, you know, extreme with this because it\u0026rsquo;s very hard to deal with lots and lots of itty-bitty libraries. But like for major partitions of functionality, there will be a library split usually in that situation. And finally, there are a bunch of weird-ass technical reasons like, you know, Windows and mobile that also require us to split things in this way. Okay, so that\u0026rsquo;s why we have so many libraries in PyTorch. Some of the libraries probably can get merged together, like A10 core and A10 probably can be merged together. C10 probably could be moved into A10, except there\u0026rsquo;s this funny business with our AMD Rock\u0026rsquo;em support where hipification works differently in one case or another. Yeah, it\u0026rsquo;s complicated. There\u0026rsquo;s a lot of things that sort of have created over time. But, you know, usually if you\u0026rsquo;re running into a library problem, the best fix is not to actually like rage against the library structure in PyTorch. It\u0026rsquo;s just to do a few simple things to, you know, sort of unblock yourself. So what are those things? So one thing you can always do is sometimes some code is put in the wrong place. And so you just need to put the code in the right place, right? Just move a file around. Yeah, I know it\u0026rsquo;s annoying. You can always put a little stub in the old location so that you don\u0026rsquo;t have to update all the includes. But, you know, oftentimes just moving a file to the appropriate place because, you know, whoever put it there originally didn\u0026rsquo;t think too hard about it. That often will solve a problem you have. Of course, sometimes you do need to break layering, right? Like sometimes you need to be able to call into some code in, say, Torch when you\u0026rsquo;re inside C10. And there no amount of moving files around will save you. And so there\u0026rsquo;s another trick that\u0026rsquo;s, you know, sort of used very commonly in the code base, namely making a virtual interface and that you can call into the, you know, higher level library layer from a lower level library layer. So one really good example of this is device guard. Device guard works by having a device guard interface for every implementation of the device guard. And so if you\u0026rsquo;re in a situation where you don\u0026rsquo;t necessarily know if you have access, direct access to the library in question, you can use device guard and it will do a virtual jump to the actual implementation, which might be CUDA to actually get the functionality that you want. Of course, if you\u0026rsquo;re actually in the CUDA library, you don\u0026rsquo;t have to do this virtual jump. And so there\u0026rsquo;s actually a specialized version of device guard called CUDA guard, which lets you do exactly this when you don\u0026rsquo;t need to violate the layering. So that\u0026rsquo;s all I wanted to say say about library structure today. Thanks for listening. See you next time.\nEP4 Vectorization Vectorization Hello, and welcome to the PyTorch Dev Podcast. My name is Edward, and today I want to talk about vectorization. Vectorization is a very important component of any self-respecting deep learning, or really any numeric computing library that lives on CPU. But sometimes it has a bit of a reputation for being this very mysterious, very magical thing. You know, numerical codes go into compiler, vectorized instructions come out, and, you know, you\u0026rsquo;re not really meant to know how exactly the sausage is made. Well, actually, you know, vectorization isn\u0026rsquo;t that magic, and today I want to talk a little bit about how we make use of vector instructions in PyTorch, on what vectorization is, and some of the sort of tips and pitfalls associated with vectorization in the code base. So what is vectorization? Well, imagine that you\u0026rsquo;re doing some computation on your CPU. Normally, the way a CPU works, and what you learned in your architecture class, is you have a bunch of instructions. You feed the instructions into the CPU, and the CPU goes ahead and does the things that you ask it to do. So, for example, if you, you know, want to do an add, you tell the CPU, hey, I want to add this number and this number together from these two registers, and the CPU will go ahead and do that for that single instruction. Now, as you might imagine, when we\u0026rsquo;re doing numeric computing, we don\u0026rsquo;t have just one number. We have a lot of numbers, and we want to do the same thing to all of these numbers. And that\u0026rsquo;s where vector instructions come in. Vector instructions are a form of what we call SIMD parallelism. That\u0026rsquo;s SIMD, single instruction, multiple data, where instead of giving your CPU an instruction to do an operation on a single piece of data, you can give your CPU an instruction to work on multiple pieces of data. That\u0026rsquo;s why it\u0026rsquo;s called vectorization, because you\u0026rsquo;re working on a vector of numbers rather than one number. So, when you want to write some vectorized code, you have a bunch of these vector registers, which are larger registers than you\u0026rsquo;d normally be able to use to do various computations, the idea being you, like, fit in multiple numbers into these registers, and then you have a whole pile of new instructions to do things like add, but not just add one number, but add all of the numbers in your vector registers. And the vector instructions are actually pretty simple, And so if you wanted to, you know, go and learn how to, like, you know, write some vectorized code by hand, all you\u0026rsquo;d have to do is really pull up the Intel manual or, you know, whatever, you know, manual for whatever processor you wanted to do, and, like, just look and find which instructions you wanted to do. Or you could use a library like Sleaf, which already provides pre-vectorized instructions for you. Or you could even just, you know, write some code and hope that your compiler\u0026rsquo;s auto-vectorizer handles it for you. You just, you need to pass the flag, like MAVX, and it will try its best to vectorize your code for you. So on Intel CPUs, which are the CPUs that most people are using, the vector instructions are called AVX, stand for Advanced Vector Extension. And there\u0026rsquo;s a bunch of different versions of AVX, basically, because over the years, Intel was like, Ah, you know, we only really want to do vector operations on two pieces of data. So here, have an extension that does that. Actually, that was called SSE. And then over time, they gave more instructions, bigger vector registers, and more and more features. And so as time went on, you know, they released AVX, then AVX2, then AVX512. And so just, you know, over time, there\u0026rsquo;s more and more functionality. But remember, and this is going to be very important later in this podcast, that, you know, you need a CPU that actually has the silicon for doing whatever it is you want to do. So if you\u0026rsquo;ve got, like, a, you know, CPU from, like, 2015, chances are it doesn\u0026rsquo;t actually have AVX512, it only has AVX2. You can actually find out what vector extensions are supported by your CPU on Linux by catting out the contents of PROC CPU info. That\u0026rsquo;s a magic file that the Linux kernel provides that tells you all about your CPUs, and tell you the model, and it\u0026rsquo;ll also tell you all the extensions that it supports. And then you can look and see, you know, which AVX is on there. Okay, so AVX is a bunch of vector instructions. I\u0026rsquo;m not really here to teach you, like, how to write AVX code. I actually have no idea how to write AVX code by hand. Instead, in PyTorch, we have a bunch of abstractions to make it easy for us to manually vectorize our code. Because often, we don\u0026rsquo;t really trust the compiler to do a good job in vectorization. So, we just want to, you know, actually tell, hey, here are the exact instructions I want you to use, so that there\u0026rsquo;s no possibility for the compiler to mess it up. And then the set of header files which help us do vectorization in PyTorch are called the VEC, aptly named VEC headers. And so, currently in PyTorch, we don\u0026rsquo;t have support for AVX 512. We just have support for AVX 2, a.k.a. AVX 256, so-called because the registers are 256 bits wide. And so we have a class called VEC 256, which just represents a bunch of vector data stored in the vector registers and then has a bunch of operations like add, sub, you know, sign, and so forth for doing vector operations on this vector piece of data. So, if you want to write some vectorized code, chances are, you know, you might just be able to, like, get VEC 256 and then get your data into VEC 256. And we actually have a bunch of wrapper functions like CPU kernel, which help, you know, handle all the fiddly, you know, edge conditions. Because remember, vector instructions always work on, you know, four pieces of data. So, what if you\u0026rsquo;ve got seven pieces of data? Well, you have to do the vectorized instruction on the four, but then you need a manual loop to finish the last three. So, you, like, get your vectorized thing and then you just tell, you know, exactly what vector instructions you want to do by just calling these methods on VEC 256. And if you want to, like, actually implement some new and interesting functionality using the raw intrinsics, the intrinsics being various special functions your compiler provides that lets you just directly call various vector instructions, you can do that, too. And typically, you just go into the VEC 256 class and you write in exactly what instructions you want it to use in this situation. So, it\u0026rsquo;s a pretty fun exercise to, you know, add vectorization support for something. And, like, if you\u0026rsquo;re sort of in the mood for just, like, you know, cracking open the Intel manual and, like, reading some papers and trying to figure out how to vectorize something, you know, a pretty fun task is, you know, hey, I need to do something fast. And right now, we only have these crappy, you know, single instruction implementation for it in PyTorch. Maybe I can vectorize it. Some things are easy to do, like, if you\u0026rsquo;re just doing some point-wise operation, you just need to figure out the right sequence of vector instructions to get the computation you want to do. Some things are harder to do. I remember a U-man wizard way back in the day actually implemented a vectorized sort for PyTorch. We never merged it because it was too complicated. But, you know, like, that\u0026rsquo;s the kind of thing, like, there\u0026rsquo;s a ton of things you can accelerate using vector instructions. And actually, they will run a lot faster on CPU if you do that. So it\u0026rsquo;s often worth doing it this way. So that\u0026rsquo;s it for what is vectorization and how people do vectorization in PyTorch. And that\u0026rsquo;s nearly it. But I want to tell you a little bit more about some of the weird things that we do in the codebase to actually make this all tick. So remember this thing that I said, right? I said that not all CPUs support all vector instructions. Depending on if your CPU is from 2010 or 2015 or from 2020, you know, you\u0026rsquo;re going to have different support for vectorized instructions. And no one really wants to, you know, try to run their PyTorch program and get a SIG illegal instruction because, you know, you tried to feed the CPU some instruction it didn\u0026rsquo;t understand. And this is actually a bit of a problem for us because when you compile your code, that\u0026rsquo;s when the compiler makes the decision to make use the various vector instructions that it has available. But the compiler doesn\u0026rsquo;t know where you\u0026rsquo;re actually going to run the code later. It\u0026rsquo;s not like, you know, you\u0026rsquo;re compiling some code and you\u0026rsquo;re trying to test if, you know, you have LibXML on your system. And if you do have it, then you compile the support for LibXML. Otherwise, you don\u0026rsquo;t compile with support for it. It\u0026rsquo;s not like that because you actually have no idea where your end user is going to run your code. And so, you know, you have no idea what vector instructions are going to be available. And so, you know, if you don\u0026rsquo;t do anything special, you really can only ship your software for the lowest common denominator of CPU you want to support. And typically that\u0026rsquo;s just, you know, no vector instructions at all because, you know, old CPUs have been around for a really long time. So the way we work around this problem is, you know, we just say, OK, fine, some CPUs support vector instructions, some don\u0026rsquo;t. So let\u0026rsquo;s just compile our instructions multiple times for each level of CPU support we want to support. And then just, you know, query the CPU processor at runtime and use that to pick the particular compiled version of our code that actually does the vector instructions. So we have a system that does this. It\u0026rsquo;s called Dispatch Stub. Dispatch Stub sounds very complicated. And in fact, you can also use it to dispatch to CUDA versus CPU. But really, it has one goal in life. And its goal in life is to let you get to the appropriately vectorized version of your code depending on what CPU capability you have. So there\u0026rsquo;s a bunch of macros and if you like sort of cargo called the code, you can, you know, usually figure out how to make this work. But the basic concept is in the native slash CPU folder, any file you put in there will get compiled multiple times, once per vectorization level that PyTorch supports. And then each of these compilation units will register its kernel to Dispatch Stub saying, hey, I\u0026rsquo;m the AVX 256 version. Hey, I\u0026rsquo;m the AVX version. And hey, I\u0026rsquo;m the non-vectorized version. And then Dispatch Stub will just, you know, query what CPU capabilities you have and then dispatch to the correct one. And there\u0026rsquo;s a bunch of sort of magic that has to happen to actually make this all work out. For example, when you actually compile this code multiple times, you have to be really, really careful not to accidentally compile any other code that you don\u0026rsquo;t actually want. And this is important because when you compile C++, normally you would imagine you just compile the functions that you define in your C++ file. But that\u0026rsquo;s not entirely true. When you do, for example, template specializations, C++ will blat out another bunch of code and then sort of rely on the linker to duplicate this code later. And so if you happen to blat out some code that in fact uses some vector instructions and then that copy of the code overrides the regular version of the code that you compiled with no vector instructions. Because remember, we don\u0026rsquo;t want to assume that everyone supports vector instructions. Then you can end up with normal code like vector resize using AVX2 instructions and then your binary packagers will be very unhappy because they\u0026rsquo;ll like package the binaries. It\u0026rsquo;ll work all fine because all of our test machines are AVX2 and then like some user is going to report to us that, hey, when I import Torch, I get a SIG illegal instruction. What\u0026rsquo;s up with that? Actually, we do have a test for this now in CI, so you don\u0026rsquo;t have to worry about silently breaking this. There\u0026rsquo;s two more things I want to say. One is that if you want to, you know, sort of, if you\u0026rsquo;ve got a very featureful CPU, you can actually manually change what vector instruction you want to do. There\u0026rsquo;s an environment variable that lets you do this. It escapes my mind at the moment, but you can look it up. It\u0026rsquo;s got capability in its name, in all caps, and that you can just use it to, you know, switch between versions. And it\u0026rsquo;s actually a pretty nice way to see like how much extra benefit you\u0026rsquo;re getting at a higher level of vectorization. One last thing. So very, very recently, okay, not that recently at this point, but fairly recently, Intel\u0026rsquo;s released support for the new AVX512 extension. And so we\u0026rsquo;ve sort of been using it on and off, but we actually don\u0026rsquo;t support it in the library proper. And the reason we don\u0026rsquo;t support it is because of this funny thing that happens to Intel CPUs when you start running AVX512 instructions. They downclock. Somehow, for some reason, when they design the CPUs, they like put too much silicon on it. And if you like actually use the AVX512 silicon, it overheats the chips, so they can\u0026rsquo;t actually use all of the chips at this point in time. So they downclock the processor to make sure their heat output isn\u0026rsquo;t too big. And that means that if you are switching in and out of AVX512 instructions and regular instructions, the downclocking will actually kill your overall performance. So we\u0026rsquo;ve kind of been, like, kind of loathed to actually add support for AVX512. But there\u0026rsquo;s some very enthusiastic open source contributors who have been trying to add support for this at the framework level. So go with them. They\u0026rsquo;re working on it. If you\u0026rsquo;re really interested, check out their PR, which I\u0026rsquo;m going to post as a link in the rest of this podcast. So that\u0026rsquo;s everything I have to say about vectorization. Vectorization. It\u0026rsquo;s not magic. Well, okay, when we recompile your code multiple times, that\u0026rsquo;s maybe a little magic. Hopefully this explains some reasons why you have to put some code in CPU, some code in not in CPU. Some of it is vectorized, some of it isn\u0026rsquo;t. And hopefully it also tells you why you can\u0026rsquo;t just, you know, use random templates inside the CPU folder because of symbol problems. So that\u0026rsquo;s all for today. See you all next time.\nEP5 Inference-mode Inference-mode Hi, my name is Edward, and welcome to today\u0026rsquo;s edition of the PyTorch Dev Podcast. Today, I want to talk about a new feature that recently landed in head in Master PyTorch called Inference Mode that was spearheaded by Ailing Zhang, but also had a lot of contributions from the rest of the folks in composability. What is Inference Mode? Well, Inference Mode is a thing that you can do when you are writing some PyTorch code and you are guaranteed that you\u0026rsquo;re only going to run inference on it. And Inference Mode basically makes your code run faster in this situation. It\u0026rsquo;s fast enough to get something like 5% to 10% wins when we have used it inside production at Facebook. And today, I just want to talk a little bit about where this feature comes from, why it\u0026rsquo;s necessary, and a little bit about how we implemented it. Okay, so first off, why does Inference Mode exist in the first place? And, you know, you might be thinking, hey, Edward, you know, if I just have some code in PyTorch and I don\u0026rsquo;t, you know, require grad on any of my inputs, so there\u0026rsquo;s no parameters, I\u0026rsquo;m not training, I don\u0026rsquo;t call backwards on it, shouldn\u0026rsquo;t this code, you know, just be as good as, you know, running some plain old tensor operations without, you know, having any separate for autograd. Like, that seems like it should be just as fast. And, you know, if I\u0026rsquo;m a little worried about accidentally setting some requires grad equals true, well, there\u0026rsquo;s this no grad mode, this no grad context manager, which I can already use in PyTorch to just say, hey, whatever the requires grad fields on my tensors are, ignore that and just don\u0026rsquo;t require gradients. So why is there an opportunity to make things go faster? And so it turns out that there are two things that we do in PyTorch to support automatic differentiation that can\u0026rsquo;t be turned off. They must be done because it may be possible at some point in the future that you will attempt to use these tensors for AD, and if we don\u0026rsquo;t do these things ahead of time, we\u0026rsquo;re just screwed. Whether or not this is the right trade-off or not, this is historically where PyTorch has been, where, you know, you can always write your code and then try to use it with autograd later, and this will work out. And so inference mode changes some of these assumptions. It says, hey, no, actually, I guarantee that I\u0026rsquo;m not going to use these tensors to do autograd later, and as a result, we can do things a little faster. So there are two things that slow, like, ostensibly inference-only mode code down in PyTorch that inference mode targets. So the first thing that happens is whenever you do any sort of mutation to a tensor in PyTorch, and really, whenever you, like, just allocate any tensor at all, we have some safety tracking for mutation called a version counter. So what is a version counter in PyTorch? Well, a version counter solves the problem that is pretty common, which is, let\u0026rsquo;s say you have a tensor, and you need to save its value for later. Well, tensors are large, and so we don\u0026rsquo;t want to make copies of them. So we just save that tensor directly. What if someone, along the time when you saved it for, say, backwards, that\u0026rsquo;s the most common case version counters are used for, and when you actually use it, when you do the backwards commutation, someone goes ahead and modifies the tensor under you? Well, that\u0026rsquo;s great. It turns out all of your, you know, automatic differentiation isn\u0026rsquo;t going to work. You\u0026rsquo;re just going to get wrong gradients in this situation because someone, you know, monkeyed about this value, and you were expecting the old value prior to the mutation to be the one that you were going to use for your backwards formula. So because this can, you know, basically result in silently incorrect results, like you have no idea that things have gone wrong, but things have gone wrong, we have a mechanism called version counters, which help us detect when mutations have happened. The mechanism is pretty simple. Basically, we associate every tensor with a version. When you mutate the tensor, we update the version. And whenever we save a tensor for backwards, we look at what the current version was and say, okay, whatever this version is, when we look at it again later in the backwards, you have to, you know, have the same version that you had when you saved it. So if there was a different version, we want to just raise an error and say, hey, someone mutated this saved tensor for backwards. Uh-oh. All right. So that means that we have to do a bunch of, you know, work, right? So we have to allocate these version counters. We can\u0026rsquo;t actually store them directly on the tensor because remember, mutating a tensor or mutating a view of a tensor, hey, these, you know, are the same thing. So we need to make sure you get updated in both of these cases. So it\u0026rsquo;s not something you can store on the tensor directly. And it also isn\u0026rsquo;t something you can store in the storage, if you know what that is, um, for very complicated reasons involving detach. So these are actually like separate heap allocated counters that we keep around and you have to allocate them. And you also have to do the reference count bumps on them. And these, these version counter bumps, sorry, not reference count bumps, version counter bumps. And we have to do these bumps atomically because there might be a mutation from separate threads. So that also leads to cost, right? It leads to having to do all these extra operations. So can we get rid of this when there\u0026rsquo;s no requires grad true anywhere in your program? The answer is no, because you don\u0026rsquo;t know if in the future, someone is going to use this tensor to actually save it for backwards, because it\u0026rsquo;s going to be used with some other requires grad true thing. So we need to know ahead of time that, you know, hey, this is going to be a tensor that is never, ever going to alias with a tensor that is going to be saved for backwards. The second thing that we have to do ahead of time is something called view tracking. So what is view tracking? Well, let\u0026rsquo;s just think about how views work in PyTorch. So if you\u0026rsquo;ve read my blog post about, you know, basic concepts in PyTorch, you may know that PyTorch tensors are strided. And so if I want to take a view on a tensor, I can just, you know, allocate another tensor, share the data and just, you know, record, you know, what the offset should be and, you know, whether or not I\u0026rsquo;m going to like, you know, inflate my strides or anything like that. And this is pretty cool. And ordinarily, you would think that when I do a view on an operation, that\u0026rsquo;s the only thing I need to do. Well, unfortunately, in the presence of automatic differentiation, that\u0026rsquo;s not enough. And the case that causes problems is what if you take a view from a tensor and then you mutate the view with another tensor that requires gradients. Let me say that again, because it\u0026rsquo;s a little bit of a complicated example. You have a tensor, take a view of it. You mutate the view with a requires grad true tensor. So something very interesting happens in this situation, which is that if you then go back to the base tensor and you use it as part of some computation, that base tensor now requires grad equals true. The requires grad trueness of the, you know, input mutation on the view infects the base tensor. And if you think about why this might be the case, it makes sense because, hey, you know, I have this thing and I need to keep track of all uses of it because, you know, I want to differentiate on it. And, you know, if I mutate it into the view, it is going to like implicitly show up in the base. And so if I make uses of the base that end up contributing to my loss, well, those also count as, you know, uses that I have to, you know, count towards, you know, when I do automatic differentiation in this case. And so just recording, you know, the storage and the strides and the offset in the tensor when we do views isn\u0026rsquo;t enough. We actually need to record some extra view metadata so that we can make this situation work. So I\u0026rsquo;ve covered the two situations where we need to do this extra work. So one is in-place updates to do version counter bumps. And the second is view metadata tracking. And if you were thinking back to the original motivation for inference mode, well, hey, you know, these are very obscure situations. And if I\u0026rsquo;m just running inference on my tensors, you know, I don\u0026rsquo;t expect any of these things to actually matter. So inference mode is the way for the user to tell, hey, I am going to guarantee you that I am not going to do any of these naughty things. And then I can just skip doing version counters. So I just won\u0026rsquo;t allocate the version counters at all. I won\u0026rsquo;t do version counter bumps on my tensors. And I\u0026rsquo;m just not going to do any of the view metadata tracking. I\u0026rsquo;m just going to, you know, leave it all alone. And then, you know, my code will run faster as long as I\u0026rsquo;m not using it for AD. So that doesn\u0026rsquo;t sound too hard, right? Just put in a bunch of if statements and, you know, or, you know, like, because we\u0026rsquo;ve talked about the dispatcher, right? Oh, do some fancy dispatcher stuff. Just make these things not get run in those cases. But there\u0026rsquo;s a problem. The problem is we don\u0026rsquo;t actually want to have our users pinky promise us that they\u0026rsquo;re going to handle everything correctly. Because we don\u0026rsquo;t actually trust our users to do things correctly. You shouldn\u0026rsquo;t either. I wouldn\u0026rsquo;t trust myself to get these things right. I\u0026rsquo;m worried that I\u0026rsquo;m going to accidentally use one of these tensors in Autograd later and everything\u0026rsquo;s going to blow up and, like, I\u0026rsquo;m going to be sad. So the sort of magic sauce and what sort of took us a long time to sort of get inference mode working was how do we do this safely? Let us say, how can we let the user say, I promise not to use these things for Autograd and then actually hold the user to this promise so that if they actually do use it for inference modes later, if they use an inference mode tensor in automatic information, we actually give a proper error message in this case. And so I\u0026rsquo;m just going to describe a little bit about how we do this. And, you know, if you want to actually see the details, we\u0026rsquo;ve got a very nice RFC co-authored by Eiling and me. And you can read that for all the sort of nitty-gritty details of how everything works. But there\u0026rsquo;s two basic things that we need to do. So the first thing that we need to do is we want to get rid of version counters, right? We want to get rid of the need to track when mutations happen. And so in order to verify that, you know, this never actually causes problems for automatic differentiation, we need to enforce some sort of invariant that says, oh, yeah, you know, one of these tensors that doesn\u0026rsquo;t record version counters, you\u0026rsquo;re not allowed to ever actually try to use the version counter to enforce safety. Because that\u0026rsquo;s a place where the system could go wrong. So in other words, we have a no aliasing requirement. The no aliasing requirement says that any tensor that doesn\u0026rsquo;t have the version counter, and we\u0026rsquo;re actually going to just refer to these as inference tensors, because they\u0026rsquo;re just tensors that happen when you do inference mode, right? You just don\u0026rsquo;t allocate version counters for them. Any inference tensor must not alias with any tensor that is saved for backwards. So how do we actually do this? Well, you know, we take an inference tensor, we say, okay, there\u0026rsquo;s no version counter on it. Whenever we make aliases to this tensor, we also need to make sure these are also inference tensors, because, you know, hey, it\u0026rsquo;s an aliasing requirement, right? Like, you know, just because you take a view of a tensor doesn\u0026rsquo;t mean you can save that, because if you mutate that, well, you know, it still affects the view of the tensor. And then we just say, okay, any inference tensor is not allowed to be saved for backwards. And so there\u0026rsquo;s one place we have to write this check, which is namely when we save variables for tensors. So the no aliasing invariant involves basically setting up this dynamic alias analysis that just says, hey, this is a class of tensors, these inference tensors, which are guaranteed not to alias with AD. And we only have to check one place to make sure this actually happens. And so that\u0026rsquo;s very nice and not too hard to implement. Second part is view tracking, right? So what do we do if we, you know, don\u0026rsquo;t track the view metadata in a situation? And this one\u0026rsquo;s actually not so hard. We basically just say, okay, we don\u0026rsquo;t record the view metadata for these tensors. And now we need to, sorry, I said this one\u0026rsquo;s not so hard, but this one\u0026rsquo;s also tricky in its own way. So naively, what you\u0026rsquo;d expect you to be able to do in this situation, you say, okay, I\u0026rsquo;m just not going to record the view metadata. And then if I ever do something to a tensor that, you know, might require the view metadata, I just raise an error. Does that work? Almost, but there\u0026rsquo;s one problem. And the problem is, if you have a base tensor, and you mutate it with something that requires grad equals true, ordinarily, your views also become requires grad equals true, right? The flow goes both ways, right? Like, if I put in some data that I need to track gradients for, then all the views also need to track gradients as well. And in the case of the base tensor, I don\u0026rsquo;t actually know if I\u0026rsquo;ve recorded the view metadata or not in the situation. So what we do is we just say, okay, well, these inference tensor things, you know, the tensors that were allocated in inference mode, you\u0026rsquo;re not allowed to mutate them outside of inference mode. And that just sort of, you know, with a very heavy hammer prevents this sort of situation from causing a problem. So that\u0026rsquo;s what inference mode does in a nutshell. It says, okay, when you\u0026rsquo;re inside inference mode, you know, we allocate these inference tensors, these inference tensors do less work. They don\u0026rsquo;t track versions, and they don\u0026rsquo;t track view metadata. And once you have this situation, you just have a bunch of extra checks, a bunch of, like, sort of restrictions on how you can use these tensors outside of inference mode that sort of guarantees that you can\u0026rsquo;t actually observe that you fail to record all this information. You\u0026rsquo;ll just error in those cases. So we\u0026rsquo;ve been deploying this to a bunch of places. There\u0026rsquo;s this old RAII guard called auto non-variable type mode. It didn\u0026rsquo;t make any sense. It just happened to make people\u0026rsquo;s code run faster, but it didn\u0026rsquo;t do any error checking. And we\u0026rsquo;ve been moving people over to use inference mode in this situation. Actually, that\u0026rsquo;s all Eiling stuff. She\u0026rsquo;s been very like a trooper moving all of our mobile stuff over. It\u0026rsquo;s been quite an adventure because there\u0026rsquo;s a ton of places that only do inference. Like, ever try to debug a PyTorch problem on Oculus? Yeah, me neither. Good work. So that\u0026rsquo;s everything I had to say about inference mode today. Right now, it\u0026rsquo;s only available from C++, but we\u0026rsquo;ll be adding a Python API for it very soon. So that\u0026rsquo;s all I wanted to say for today. See you next time.\nEP6 Just-enough-CUDA-to-be-dangerous Just-enough-CUDA-to-be-dangerous Hi, my name is Edward, and welcome to today\u0026rsquo;s edition of the PyTorch Developer Podcast. Today, I want to do a very whirlwind intro to CUDA programming. Now, disclaimer, I am by no means a CUDA programming expert. I\u0026rsquo;ve ridden a CUDA kernel or two in my time, but most of the time I defer to such experts as, say, Natalia Gimelschein to actually do the heavy CUDA lifting. But having worked on PyTorch a while, I have picked up a thing or two about CUDA. And so today\u0026rsquo;s episode, I just want to, like, talk about really, really, really fast, you know, here\u0026rsquo;s just a big pile of stuff that is important to know about CUDA programming, about programming GPUs in general, just enough so that you can be a little dangerous, even if you\u0026rsquo;re not, like, actually writing CUDA kernels. Because it\u0026rsquo;s really helpful to know a little bit about the programming model, what happens on GPU, because, well, you know, PyTorch is a GPU-accelerated deep learning library. And so if you add some functionality to PyTorch, we expect you to be able to also run it with GPU acceleration. All right. So where to get started? Part one. What is CUDA? So to answer what is CUDA, we actually have to answer a different question first, which is, what is a GPU? So the GPU is a piece of hardware in your computer that sort of made the deep learning revolution possible. And its name is short for Graphics Processing Unit, because historically, that\u0026rsquo;s what we actually use them for. We use them to actually, you know, render graphics scenes on your computers if you\u0026rsquo;re playing a video game or, you know, doing some sort of photo or video application. And it just turns out that the types of things GPUs are good at doing are also good at doing deep learning models. Why is that the case? Well, the way a GPU works is that instead of having, so remember when we talked about vectorization and I said a CPU, you feed it a bunch of instructions and it does the instructions one by one. And, you know, basically that\u0026rsquo;s it. And you can, like, put a bunch of cores in your CPU. And if you have a really beefy machine, maybe you have 32 or 64 cores. But, you know, there\u0026rsquo;s only so many cores you actually put in your CPU. And that\u0026rsquo;s basically it. Like, that\u0026rsquo;s the level of parallelism you\u0026rsquo;re going to get. You have to, you know, spawn threads and, you know, use them to actually make your CPUs go. Well, a GPU has tons and tons of really, really simple cores. And the way they operate is they just say, okay, well, I\u0026rsquo;m going to run the same computation on every core so I don\u0026rsquo;t have to worry about, you know, all the cores doing different things. And I\u0026rsquo;m just going to have so many cores doing the same operations that if I have a ton of data, like, say, in a image or in a deep learning tensor, because I have such massive parallelism, I can actually just, you know, do things very quickly. Because even if there\u0026rsquo;s a million things to do, well, you know, I have a lot of cores, and so they can make quick work of it. So the basic idea behind GPUs is instead of having, you know, these big, beefy CPUs, but not that many of them, we have these many, many cores, and we massively parallelize our algorithms. And that\u0026rsquo;s how we\u0026rsquo;re actually going to do things really fast. And so CUDA is the programming language slash compiler stack slash software ecosystem that NVIDIA developed for programming their GPUs for, you know, sort of general purpose programming. Because back in the day, like, when you had a GPU, you used it to do graphics processing. So you\u0026rsquo;d write shaders, you\u0026rsquo;d write, you know, those sorts of things. And no one was really thinking about doing, you know, actual mathematical general purpose computation, except for, you know, a weird branch of researchers who are looking into so-called GP GPUs, general purpose GPUs. And they would, like, go through lots of tricks to try to, you know, you know, get the shader to do exactly just the thing that they wanted them to do for whatever computation they want to do. And NVIDIA built this software stack called CUDA, and so we can use CUDA to do general purpose programming on GPUs. And in PyTorch, what we do this for is so that we can do deep learning neural network computations on them. So what is the CUDA programming model? So, you know, the GPU is not your CPU, right? Like, on the CPU, if you want to do some stuff, you just send some instructions to the processor, and, you know, it just does the stuff. You don\u0026rsquo;t have to think about it, right? That\u0026rsquo;s normal operating. But your GPU is typically living on a separate, you know, device in your CPU, and, like, it\u0026rsquo;s got its own memory, and it\u0026rsquo;s, you know, not, like, anything at all like your CPU. So there\u0026rsquo;s actually a little bit of a difference when you want to program a GPU in this situation. And so the sort of very, very short version of, like, what you should think of as a CUDA programming model is there is CUDA memory. That memory is memory that lives on the GPU. That\u0026rsquo;s the memory that, you know, programs on the GPU can actually run. If you want to compute some data on the GPU, you have to first move it to the GPU so that it\u0026rsquo;s accessible. Then you can write various kernels, and these kernels are, you know, sort of written because CUDA is a programming language built on top of C++. They\u0026rsquo;re written in C++, but they\u0026rsquo;re different than normal C++ because, you know, unlike a regular CPU where, you know, you have a single processor and you just feed it instructions, these programs need to work as, uh, this, uh, running on, you know, all the little itty-bitty processors that are on your GPU. And so these special kernels, you, you know, you want to go write a specific, them in a subset of C++ that, you know, your CUDA compiler actually understands. And in general, like when you write a program or like in PyTorch, there are going to be like, you know, dozens or really hundreds of CUDA kernels each for some particular task that you want to do. I\u0026rsquo;m not going to talk about how you actually write a CUDA kernel today, but say you have a bunch of these kernels. What you need to do is after you put the data on the GPU, you need to ask the CUDA driver, Hey, can you please run this kernel? And the CUDA kernel will, CUDA driver will go ahead and say, okay, um, I\u0026rsquo;m going to go tell that GPU, the actual device to go ahead and run this computation to do the thing that I want it to do. And here is sort of one of the most important things about the CUDA programming model. The, the, the most important thing, if you\u0026rsquo;ve never written CUDA before, and there\u0026rsquo;s one takeaway I want you to get from this podcast, it is this process is asynchronous. I\u0026rsquo;ll repeat it again. This process is asynchronous. So you tell the driver, Hey, please do this computation. The driver\u0026rsquo;s like, okay, I\u0026rsquo;m going to go do this computation. And the kernel call you made is going to immediately return, even though the GPU is off, you know, chundering away on the data that you asked it to process. This is a good thing because it means that your CPU host program that\u0026rsquo;s responsible for figuring out what kernel calls to do can run ahead while the GPU computation is happening and figure out what the next thing you want it to do is. And so you can say, Hey, after you\u0026rsquo;re done doing this previous computation, please do this next computation. And you can cue it ahead and the GPU can be ready to go right when the previous computation finishes. By the way, how does it know that it wants that kernel to run after the previous kernel you ran? Because if it\u0026rsquo;s asynchronous, couldn\u0026rsquo;t these just like run in any order? Couldn\u0026rsquo;t it just start running it when you ask it for it? Well, there\u0026rsquo;s this thing called streams. Streams imply sequential execution. So you put CUDA kernels on streams, and every kernel on a stream is guaranteed to finish before the next kernel on that stream happens. Normally, when people just write GPU-accelled programs, there\u0026rsquo;s just one stream. It\u0026rsquo;s the default stream. Everything goes on that. Everything is sequentialized. But if you\u0026rsquo;re doing like fancy tricks, you might have multiple streams. And one of the things PyTorch needs to do is although most people don\u0026rsquo;t use streams, we do want it to be possible to use streams with our software. So we have to write all of our code in a stream generic way. One last thing that\u0026rsquo;s useful to know about the CUDA programming model is it has a notion of a current device. So you know, when you do a kernel launch, well, you might have multiple GPUs in your machine, right? And each of these GPUs has its own memory. And so you can\u0026rsquo;t just say, oh, well, you know, GPU two, please operate on the memory and GPU zero. Technically, this will work if you have, you know, device to device transfer, but it\u0026rsquo;ll be kind of slow, right? So most of the time, we don\u0026rsquo;t allow it. And you know, you have to actually make sure the memory is in the same place. And so the current CUDA device, which is a CUDA concept, is something that you have to say, okay, I am now setting my current device to be GPU two, so that all my kernels actually operate on GPU two, because the kernels don\u0026rsquo;t actually take in what device they want to run on explicitly. PyTorch also has a notion of a current stream. This is not a CUDA concept. This is something that PyTorch built on top of CUDA. And this is so that we don\u0026rsquo;t have to also constantly say which stream we want to run on. CUDA kernels explicitly take which stream you want, or zero for the default stream. Okay, so that\u0026rsquo;s the basics of the CUDA programming model. So what are the implications of this model when we are doing PyTorch programming? So remember, I said the most important thing about CUDA programming is it is asynchronous. So what happens if something bad happens in your CUDA kernel? Because bad things can happen in your CUDA kernel. They\u0026rsquo;re basically C++, right? You can do an out-of-bounds pointer dereference. You can have an assert failure. You can, you know, trigger a compiler bug. Lots of things can go wrong, right? So what happens when something goes wrong? Well, first off, when you launch the kernel that actually is going to do something bad, it\u0026rsquo;s not going to raise an error, right? It\u0026rsquo;s just going to return and say, hey, everything\u0026rsquo;s okay. But that\u0026rsquo;s not actually necessarily the case, right? Because at some later point in time, when the drivers finally got in ahead to getting to figure out, hey, you know, there\u0026rsquo;s something wrong because I\u0026rsquo;ve just run this kernel and get it, you might be somewhere way else later in your CPU host side program, at which point the CUDA driver, you\u0026rsquo;ll be doing some random call into the CUDA API, like trying to malk something or like trying to launch a different kernel and say, oh, no, no, no, no, no. Something bad has happened. An internal assert failed. I don\u0026rsquo;t know. And well, crap, because, you know, you\u0026rsquo;ve got this code and it has nothing to do with the error that just got raised because the error was actually caused by some kernel launch, you know, miles and miles away in your code. So this is like the most, like, you know, anyone who like just sort of like signs up for PyTorch and doesn\u0026rsquo;t know any CUDA and like has to debug a GPU problem. This is probably the first thing you\u0026rsquo;re going to run into. And you\u0026rsquo;re like, oh my God, what the heck is going on? And the answer is, remember, it\u0026rsquo;s async. You\u0026rsquo;re getting the results way later after you actually queue the kernel. What can you do in this situation? Well, there\u0026rsquo;s a bunch of things you can do, but the easiest and, you know, simplest way to like solve a problem like this is to use this environment variable called CUDA launch blocking, which says, hey, you know, wait until the previous kernels have all finished before actually executing my kernel. And in this case, because we\u0026rsquo;re waiting, we can actually make sure that we, you know, have gotten all the errors before we move on and try to do the next operation. So that will cause the errors to move to the right place. Your programs will run really slow because remember asynchronous execution is a good thing. It lets us make sure we keep the pipeline of GPU computation going. Whereas, you know, with blocking, you\u0026rsquo;re going to wait and then the GPU is going to idle while your, you know, very slow CPU host tries to figure out what the next thing to execute is until you get to the next thing. And then it\u0026rsquo;s going to run again, right? So your utilization is going to be crap, but at least you know where the errors are going. Let\u0026rsquo;s talk about, um, this asynchronous thing again, right? So we said that, you know, CUDA programming, uh, has to, uh, you know, run ahead so that, you know, we can, uh, make up for costs of, you know, uh, launching overhead and, you know, waiting for a CPU to figure out what the things to do. Well, there\u0026rsquo;s another consequence to this, right? Which is anytime you ask for some memory that\u0026rsquo;s in CUDA in your GPU, and you want to actually like look at it on the CPU, like you want to say, Oh, is it a two? Is it a three? Can I do something with this? You have to wait, right? You have to wait for all of the asynchronously queued kernels to finish executing so that you can actually see what the data in that memory is. And then you have to copy it back to CPU and then you can actually go look at it. So syncs are really, really, really expensive. And whenever we write code in PyTorch, we really want to try to avoid doing synchronizations that are unnecessary. And sometimes this is not so easy to do because there are a lot of innocuous sounding methods that can cause synchronizations. For example, if you ask for a torch dot non-zero on a CUDA tensor, that will cause a sync. Why does that cause a sync? Well, it causes a sync because non-zero gives you a tensor whose size is the number of non-zero entries in the original tensor. How do you know what the non-zero entries are? Well, you have to look at the data sync. Another example is dot item, which, you know, takes some elements somewhere in a tensor and then gives you what its value is. And you look at this and you\u0026rsquo;re like, Oh, well, I got this thing from CUDA memory. So that means I had to wait for all the computation to finish to get that thing from CUDA memory. So try really, really, really hard not to do syncs. Sometimes this is impossible, right? Like maybe you\u0026rsquo;re doing some iterative algorithm and you\u0026rsquo;re like, you know, repeatedly running some kernel and waiting for some value to converge before you do thing before you stop and go do something else. Well, yeah, you\u0026rsquo;re kind of out of luck, right? You\u0026rsquo;re going to have to actually sync when you do that. But there\u0026rsquo;s often some way to set things up so that you don\u0026rsquo;t need to do the sync. Or maybe there\u0026rsquo;s like a different version, right? Like there\u0026rsquo;s a fast version that doesn\u0026rsquo;t sync and then the slower version that does sync. And you want to think about actually providing both of these things. Speaking of asserts and syncs, remember what I said about, you know, like your errors showing up in way random places, right? So in PyTorch, we actually have this philosophy, which is that we are willing to pay a performance cost in our CUDA kernels so that we get good error reporting. Let me give an example of this. Say you\u0026rsquo;re writing some sort of embedding. And so what is it embedding? It\u0026rsquo;s just a glorified hash table lookup, right? So you got some index, you want to go look at the element at that index, right? What if the index is out of bounds? Well, we could say, oh, you know, we really care about performance. So we want to, we don\u0026rsquo;t want to bounds check. We\u0026rsquo;re just going to do the dereference. And if there\u0026rsquo;s, you know, if it\u0026rsquo;s out of bounds, well, too bad for the user, right? Like you asked for it, it\u0026rsquo;s up to you to make sure things are in bounds. We do not make this assumption. We will bounds check these axes. For one, it\u0026rsquo;s not that expensive to do because, you know, you\u0026rsquo;re this massively parallel CUDA, you know, GPU device, and you know, you\u0026rsquo;re going to be spending lots of time usually being memory bound. So like, you know, extra computation usually isn\u0026rsquo;t that expensive. But two is that if you do a, you know, invalid memory axis, you\u0026rsquo;re just going to get an invalid memory axis, and you have no idea what could have caused this problem. If you do a bounds check, and you do an assert, you will get that assert when things fail later. And so you can, for example, grab the PyTorch code base, and they\u0026rsquo;ll tell you, hey, this is what caused the assert. And then you can have some clue, oh, it was this operator without having to run CUDA non-blocking. So I have told you a little bit about what GPUs are. I\u0026rsquo;ve told you about the CUDA programming module. And then I started harping over and over and over about syncs, async, all that stuff, because really, the asynchronous nature of CUDA is what really, really trips people up. In fact, like even in advanced usages, like this, these streams, we have multiple streams, like making sure all of the, you know, synchronizations between streams happen correctly, and happen correctly with, say, our CUDA caching allocator. Oh, yeah, we have a caching allocator, because CUDA malloc is really slow. So, you know, we get a bunch of memory from CUDA, and then we, you know, reuse it for our own stuff. But making sure this all gets synced up so that like async stuff doesn\u0026rsquo;t messes up. Yeah, that\u0026rsquo;s like probably the hardest thing about working in CUDA. So if you can remember, async is cool, but it is very complicated. And make sure to remember that when you\u0026rsquo;re working on CUDA, you go a long way, even if you don\u0026rsquo;t know anything about how to write CUDA algorithms like me. All right, so that\u0026rsquo;s all I wanted to say today. Thanks for listening. See you next time.\nEP7 Functionalization Functionalization Hi, my name is Edward, and welcome to today\u0026rsquo;s edition of the PyTorch Dev Podcast. Today, I want to talk about a process called functionalization, which is used in multiple parts in the PyTorch codebase. What do I mean by functionalization? Well, I don\u0026rsquo;t necessarily mean the conversion of things into functions, but what I actually mean is the removal of mutation from operations that you do in PyTorch. And it turns out that being able to remove mutation, being able to transform an otherwise mutable program or trace into a purely functional form is a very useful transformation and one that we use in several places in PyTorch. So I just want to talk a little bit about why this is useful and then tell you about how we do it. Okay, so why is functionalization important? Well, a long, long time ago, in our pre-PyTorch 0.4 days, we didn\u0026rsquo;t actually support doing autograd and mutation at the same time. And there was a reason why this was the case. It\u0026rsquo;s because, you know, when you have a program and you just, you know, write a bunch of pure function calls, you can easily just, you know, create a autograd graph that represents the calls you just made and then replay that graph when you go backwards in time. But if mutation is allowed in the mix, if you\u0026rsquo;re allowed to sort of modify something in place when you are working on the forward path of your function, not only do you have to somehow deal with a mutation, but you also have to somehow modify all of the other aliases, all the other views on the object in that situation. And this is kind of complicated and difficult to think about how to actually do this. And so the way that, you know, we actually implement this in PyTorch is morally inside PyTorch\u0026rsquo;s internals, we convert your program into a functional form, one where the mutations are removed. And so the autograd trace is not, you know, recording, hey, you know, this is the mutation that happened, but actually here is the purely functional version of the program that would actually give you the, you know, same computation that you would have gotten if you had done the mutation in question. So why is functionalization important? It\u0026rsquo;s important because we can use it to implement automatic differentiation in the presence of mutation. You know, you don\u0026rsquo;t have to do this, but one of the things people really like about using PyTorch is you can just sort of do all the thing that Python normally lets you do. And one of those things is mutate tensors. So it\u0026rsquo;s kind of nice that, you know, autograd works with this. Another thing that this is useful for and got repurposed after the fact is PyTorch has an integration with XLA. XLA is the backend for TensorFlow. You know, it\u0026rsquo;s a very nice backend, generates good code. And there\u0026rsquo;s something very important about it, which is it is purely functional. It doesn\u0026rsquo;t support mutation. And so when we have a PyTorch program that has a bunch of mutations in it, when we translate it into XLA, HLO, IR, we need to figure out a way to get rid of all of those mutations. And so, in fact, the Torch XLA extension developed by David LeBenzé and co actually does, you know, the same kind of functionalization that our autograd pass does when mutations happen in our program. So it\u0026rsquo;s useful in a bunch of places. In the past, we\u0026rsquo;ve sort of, like, you know, re-implemented this trick as needed. But we\u0026rsquo;re going to eventually work on adding functionalization as a proper pass to PyTorch so anyone can opt into it if it\u0026rsquo;s something you need for your backend. Okay, so I\u0026rsquo;ve talked a little bit about functionalization and why it\u0026rsquo;s important. But, like, you know, why is this a hard thing to do? Because if you, you know, ask a, you know, diehard functional programmer, well, they\u0026rsquo;ll just tell you, hey, you know, getting rid of mutation is not too hard. You know, instead of, like, adding two to a variable, you directly to a variable, you just say, okay, well, you know, X plus two equals Y. And then anywhere you previously referred to X, you just refer to Y instead. So what\u0026rsquo;s the big deal? And the big problem is aliases. So let\u0026rsquo;s say that I have a tensor and I, you know, take out a bunch of views on it and then I fill the tensor with ones. The modification that I did is not just, you know, take this tensor and replace it with an entire tensor full of ones. It\u0026rsquo;s also all the views that I\u0026rsquo;ve taken of this tensor, all those views also need to get filled with ones. And this poses a very hard implementation challenge for us because when I am, you know, writing a runtime system for PyTorch, we\u0026rsquo;re doing a reference counted implementation. We want things to get promptly disposed of. And so this object that I\u0026rsquo;m filling all with ones doesn\u0026rsquo;t actually know where all the views are. So imagine that, like, for any given tensor, you knew all the aliases to that tensor. Then you could still functionalize in, you know, a little bit complicated, but not too bad of a way. And the way you would do it is you would say, hey, here\u0026rsquo;s my tensor. Here are all the aliases. I do some mutation to the tensor and then I look up all of the aliases and then I replay that same mutation on each of the aliases. Well, okay. I had to, like, narrow the scope of the mutation, right? Because the view is only looking at a part of the tensor. And so I just only need to apply the mutation from that part. But then I just go ahead and apply the mutation to each of them. And in the same way, you know, let y equal x plus 2 and then, you know, all previous references to x are now y. I can just update all of these one by one and then I have a new updated functional graph that doesn\u0026rsquo;t have any reference to mutation. But I can\u0026rsquo;t do this. I can\u0026rsquo;t actually maintain this list of aliases because if I did maintain that list of aliases, well, one is we\u0026rsquo;re ref counted. So if they were strong references, then you\u0026rsquo;d keep all of your views live even if, you know, no one was actually using them, right? Like if someone takes out a view to your tensor and then you mutate that tensor and that view never gets used, you need that view to go dead in that situation. And if you made these all weak references, well, that still causes problems because you have to, you know, do a pile of bookkeeping on the tensor in order to keep track of all these views. You no longer have a fixed size representation for a tensor. The set of aliases to it may grow unboundedly. I actually remember a long time ago when Sam Gross was initially implementing our C++ Autograd system and he was trying to get mutation to work in this situation. He came to my desk and he asked me, hey, Edward, so I\u0026rsquo;m trying to figure out how to, you know, deal with these mutable aliases. And, you know, I was thinking, you know, could I just store the aliases for all the tensors and update them? And I was like, Sam, that\u0026rsquo;s not a good idea. Don\u0026rsquo;t do it that way. And so Sam went away and he thought about the problem and he came up with a better solution. And I want to tell you how that solution works today. So just to recap the situation, right? So we want to do a mutation to a tensor and we want to somehow get all of the aliases, all of the views on that tensor to see the change in question. But we don\u0026rsquo;t know what those views are. So how can we make sure we actually get this mutation, get the knowledge of this change to all of those sites? Well, the answer is, you know, if you can\u0026rsquo;t do it now, do it later. So when we do a mutation to some base tensor, we say, okay, here\u0026rsquo;s the mutation that has happened. And we also flip some bit, flip some version saying, hey, everyone else, all of y\u0026rsquo;all, y\u0026rsquo;all views. If someone else tries to ask you what the functional computation graph corresponding to you are. And it turns out the base has changed under you. In the meantime, from the last time you came and looked at us, you need to stop and recompute what your new value is subject to the mutations that happened. So let\u0026rsquo;s just go through an example to see what happens here. So let\u0026rsquo;s say I\u0026rsquo;ve got my tensor A. I have a view V on the tensor A and I add two to every element in A. So I go ahead and do that. I update the version on A. So it says, hey, I\u0026rsquo;m out of date. V, you know, when it was taken out from A, recorded the old version. So I got version zero, version zero recorded in V. I update the version on A to go to one. And so A records, hey, this is the mutation that I made in the situation. And so the next time I access V, the first thing that I need to do is I say, hey, V, are you up to date? And V goes and looks at its base, which is A. And it says, hmm, last time I looked at A, I was version zero. But now A is version one. So I need to do an update. So V then goes and looks at what the changes that were made to A in the meantime were, reapplies them to V, and then says, OK, here\u0026rsquo;s the up to date, representation in purely functional form of the contents of V. Now, in Autograd, we don\u0026rsquo;t quite replay things because when we have the, when we have the computation represented by A, we actually don\u0026rsquo;t have to, like, you know, replay the mutation on V. We can just say, OK, just take whatever the current state of A is, take whatever the actual contents of A are, you know, and the functional trace that created it, and then just reapply that V operation to actually get the contents of V, right? So it\u0026rsquo;s sort of not, not, if you imagine, like, two tracks, right, running, and V is one track and A is one track, it\u0026rsquo;s not like A is making changes and those changes get merged into V one by one, but actually that every time you make a change to A and then relook at V, a new branch branches off A and you just sort of forget all about the old branch that you had before. In fact, Autograd even does a further optimization, which is we don\u0026rsquo;t even have to remember what the views are because every view is related to its base tensor simply by the strides that are recorded in the view. If you have read my blog post, which is an introduction to PyTorch, I explain what strides are, and, you know, any view operation boils down to a re-striding at some offset on the tensor. So we just have this as-strided backwards that just gets applied in this situation. Of course, XLA doesn\u0026rsquo;t actually support striding, so for XLA, we actually just replay the view operations, and that\u0026rsquo;s how it goes about doing these updates. So that\u0026rsquo;s basically how functionalization works. So we don\u0026rsquo;t eagerly update all the aliases when we do a mutation. Instead, we lazily update them when they get accessed. This preserves the ref counting properties we want, where we only ever have references from, you know, subsidiary things to the computation graphs that preceded them, and we don\u0026rsquo;t need to maintain lists of tensors that tell us what the aliases of a base tensor are. So another pretty interesting property about this scheme is it\u0026rsquo;s actually quite a bit better than static analysis. So, like, let\u0026rsquo;s imagine that your LLVM or some sort of compiler or, like, the TorchScript compiler, and you want to, you have a program, and it\u0026rsquo;s got some mutation in it, and you want to remove that mutation because maybe you\u0026rsquo;ve got some functional optimizations that work better in this situation. Well, when you\u0026rsquo;re in the compiler setting, it\u0026rsquo;s actually kind of difficult to remove all of the mutations because you just don\u0026rsquo;t know what the aliasing properties of your imports are. This is why, actually, like, when you\u0026rsquo;re writing functions, sometimes putting a restrict qualifier, which says, hey, this pointer input is guaranteed not to alias with this other pointer input. The restrict qualifier is so important because the fact that you can prove that they\u0026rsquo;re not alias because you told the compiler that then enables a bunch of optimizations that the compiler can do. But in general, the compiler has to be very conservative, and it has to, like, sort of, you know, you know, if it doesn\u0026rsquo;t know, it has to assume, oh, this could alias with something else, and that just impedes a huge number of optimizations you might do. Whereas PyTorch, which is sort of just running this functionalization as we run our program eagerly, always has absolutely precise alias information about what exactly alias was with something else. And so we can absolutely perfectly remove mutation in this situation without any loss of fidelity. Of course, you know, like, this is only for a single trace, whereas, you know, your optimizer might be working under, you know, very different situations where some things may alias sometimes and some things don\u0026rsquo;t, right? So it\u0026rsquo;s the price of generality. When we specialize, specialize, specialize to the specific case, we can do something really good in this situation. So that\u0026rsquo;s it for functionalization in PyTorch. It is how we, you know, sometimes I like to tell people, hey, you know, PyTorch kind of wore a hair shirt where we were like, hey, we care about mutation, we care about supporting in-place operations, and then we had to do a whole bunch of, you know, complexity. Like, we actually have to work pretty hard to make sure mutation works for our users. But at the end of the day, like, how do we do this? We map the mutable operations into the functional universe, and then we do the things that, you know, automatic differentiation, all that good stuff. So it\u0026rsquo;s actually pretty nicely factored in this way. And this is one of the, like, really joyful things about working about PyTorch. All right, that\u0026rsquo;s all I have to say today. Talk to you all next time.\nEP8 The-road-to-structured-kernels The-road-to-structured-kernels Hi, my name is Edward, and welcome to today\u0026rsquo;s edition of the PyTorch Dev Podcast. Today, I want to talk a little bit about structured kernels and metatensors, a project that I\u0026rsquo;ve been working on for the better part of a year, maybe more than that at this point. Structured kernels are basically a new way of writing kernels in PyTorch, where you can, instead of writing a kernel from whole cloth that does all of the computation, all of the determining whether or not the inputs are right, and all of the output shape size computation, for example, it allows you to factor your kernels into a structured form, where you write a meta function, which says, you know, what the input checks need to be, and what the output sizes are going to be, and then an actual implementation function, which you can then do a separate implementation for CPU and CUDA, and they reuse the meta function to do all the, you know, shape checking, but then the actual implementation bits can be different in both cases. And then metatensors are a sort of easy extension on top of this, which is that, well, once you have this meta function, that all it does is check the input d types and figures out what the output shape needs to be, you can actually then do a third tensor type, not CPU or CUDA, but meta, which simply says, okay, that\u0026rsquo;s cool, you\u0026rsquo;ve figured out what the output shape needs to be, I\u0026rsquo;m done, I\u0026rsquo;m just going to give you back that tensor without actually having done any of the computation at all. So metatensors are just tensors that don\u0026rsquo;t have any storage associated with themselves. They just, you know, like, they\u0026rsquo;re just sort of like a abstract interpretation of the tensor, just without the data in question. So these are two new sort of features slash endeavors slash projects that have been going on in PyTorch. Not every kernel is structured. There\u0026rsquo;s a bunch of kernels that you can port to structured if you want. And I\u0026rsquo;ve got a very detailed RFC on the topic in the PyTorch RFC\u0026rsquo;s repository. And that\u0026rsquo;s not really what I want to talk about today. I\u0026rsquo;m not going to tell you really about how structured kernels work. So I just want to talk a little bit about the history behind structured kernels. And in particular, and the reason why I\u0026rsquo;m doing this episode, Anjali Chordia asked me, hey, Edward, you know, why did it take so long for us to do structured kernels? They seem like a pretty simple idea. This is not her words, but I\u0026rsquo;m elaborating. They seem like a simple idea. Like, you know, of course you don\u0026rsquo;t want to write the shape checks multiple times in your CPU and CUDA kernels. How come, you know, it wasn\u0026rsquo;t done this way from the beginning? How come we didn\u0026rsquo;t do it earlier? And this is actually a pretty good question because for me, I was, you know, originally when I decided that I was going to work on this, I thought to myself, oh, you know, I\u0026rsquo;ll be able to wrap this up in a half. I\u0026rsquo;ll be able to port, you know, 80% of all operators. Life will be great. You know, what could possibly go wrong? Well, a lot of things. So let\u0026rsquo;s talk about that. Before I dive into when we started working on structured kernels, it\u0026rsquo;s useful to think about sort of what problems were showing up for us in PyTorch development that sort of led to the idea that we actually need to invest some time on this. And there are two like very distant causes that sort have caused us some consternation and we didn\u0026rsquo;t really act on them. And then a more immediate cause. And I want to talk about the distant causes first. So distant cause one was, um, we were writing, um, we were writing compiler passes for the JIT and they needed to do shape propagation. And there was a problem, right? Which is that like, Hey, uh, you know, you\u0026rsquo;ve got some input shapes and, uh, you know, you\u0026rsquo;re running an ad on them and you don\u0026rsquo;t know what the output shape is. How do you actually compute it? And so remember like, you know, PyTorch as it is written mostly today and historically the way it\u0026rsquo;s written, um, all the shape checks, all the output computation, they\u0026rsquo;re all sort of interleaved with the actual kernel computation that does the honest to goodness work. So if someone came to you and they said, Hey, you know, I want to know what the output shape of this, the ad on these two tensors of these sizes are, but like, I don\u0026rsquo;t want you to actually do any compute. I\u0026rsquo;d actually not have a good answer for you because there wouldn\u0026rsquo;t actually be any way to call this code in the situation. So what did people do? Well, you know, we could have done something like structured kernels, but we sort of routed around the problem by just being like, okay, we\u0026rsquo;re just going to build a, we\u0026rsquo;re just going to write the formulas ourselves. Cause like a lot of these operators, the shape calculations are really simple and you know, what could possibly go wrong? So we wrote a bunch of, um, shape, you know, transfer functions that like, you know, said abstractly what various operators did. And these promptly fell out of date and no one uses them because like the coverage is really bad. And a lot of them are wrong. And they\u0026rsquo;re wrong for really interesting reasons, because it turns out that computing the output size of like an ad is actually really complicated in PyTorch. There\u0026rsquo;s a lot of things that go into it because it\u0026rsquo;s not just, Oh yeah, if the two sizes are the same, then I give you output, that\u0026rsquo;s the same size because Hey, like there is, uh, you know, broadcasting to worry about there is tight promotion to worry about if, if you were cared about D types, which you often do in compiler passes, there\u0026rsquo;s strides to care about. If you\u0026rsquo;re like doing memory loud, actually the stride handling for like, you know, uh, point wise operations is really, really complicated because we need to answer questions. Like if I add an NCHW and an NHG, HWC tensor together, what is the output layout? And like, these are questions that are all resolved in the actual kernel today. And if you\u0026rsquo;re just like someone like, you know, like who, who you don\u0026rsquo;t really care about these shape functions, you\u0026rsquo;re just trying to do some other work, right. That actually uses these shape functions. You\u0026rsquo;re not going to spend the time thinking about all of the exhaustive error cases that go into this problem. So, okay. So we needed some sort of, um, shape pass for JIT and we wrote a kind of crappy one and now no one uses it actually like when people really need like accurate shape information, what typically happens is they just trace through a honest for goodness, real execution of the Python high torch kernels running through the actual kernels in question. And then that gives you super accurate, you know, sizes and shapes and D types and layouts for everything that happened. And then you can like, just use that information directly. Right. So like, you just worked around the fact that you didn\u0026rsquo;t actually have a function that you could have just called to find out what the shapes computed to be. So this is like, kind of like, you know, ah, this kind of sucks, but sounds like refactoring everything in PyTorch to like put the shape computation separately. Seems like a lot of work. So, you know, I\u0026rsquo;m just a compiler developer. I\u0026rsquo;m not going to work on it. And so things stay like that for a while. The second inkling we had that there would be a need for structured kernels was this like very old proposal called async CPU. So what is async CPU? Well, you know, when we look at normal PyTorch programs, there\u0026rsquo;s two devices that everyone uses, CPU and CUDA, right? CPU is synchronous. You like say, okay, I want to do an ad and it goes ahead and does the ad. And then once the ad\u0026rsquo;s done, you get a new CPU tensor with the result of having done the ad. CUDA is asynchronous. I talked a little bit about this in my previous podcast about, you know, just enough to be dangerous in CUDA, right? When you run a CUDA kernel, we actually run ahead and return to you immediately while the CUDA kernel is still processing. And eventually we, um, uh, we can keep queuing more kernels. And only when we do a synchronized, we actually observe the result. Well, there\u0026rsquo;s nothing special about being asynchronous that requires it to only happen on CUDA. And so if we are CPU, we can also just do, um, a version of CPU that\u0026rsquo;s asynchronous, right? So you like cue some work onto some thread pool and then the thread pool goes off and starts doing the CPU work. And then, you know, you actually return immediately. And so if your CPU computations are very beefy, uh, then, you know, you might actually profitably reduce latency this way because you can keep, you know, running your control thread along while, you know, you\u0026rsquo;re chugging out the actual, uh, CPU computations. So this was kind of cool. And, you know, we were talking about this, um, during the time and there was a problem. And the problem was like, we really wanted to reuse all the existing CPU kernels. We didn\u0026rsquo;t want to write an entirely new backend for async CPU. That would be silly, right? Because we got these perfectly good regular CPU kernels. We just need to make them async. But there was a problem. If you want to return immediately after running, uh, you know, queuing up the pool of work, you need to return a tensor. And that tensor you return needs to actually have all of the, like, you know, metadata, the sizes, the D types, the layouts, all that stuff, because we have a ton of code that assumes that I can, you know, without inducing async, you know, uh, access this information. And in CUDA, this isn\u0026rsquo;t really a problem because we like already did the copy paste, uh, from CPU kernels to CUDA kernels. So like the CUDA kernels knew how to compute all the shapes while also asynchronously firing off the kernels, because that\u0026rsquo;s what the CUDA runtime dealt with. But like, if we were going to do this entirely new async CPU backend, it would be really silly if we like copy pasted every single CPU kernel and then like async-ified it. Like that would just be a terrible maintenance problem. And so we couldn\u0026rsquo;t implement async CPU because once again, there was no way to run computations without, um, without doing, uh, a huge refactor of PyTorch. And there weren\u0026rsquo;t really that many compelling use cases for async CPU at the time. So we just let that lie. And, you know, it was just like, okay, well, we can\u0026rsquo;t do this, but maybe it doesn\u0026rsquo;t really matter. And so there were, there was always other stuff to work on at the time. The thing that actually convinced me that we needed to actually spend some time doing this refactoring work was, um, when I was working with Bram Wasti on this project called lazy tensor, um, lazy tensors are this concept that like keeps coming back again and again. Um, and it\u0026rsquo;s just, you know, instead of, uh, eagerly executing computations, when you ask for them in your eager mode API, we wait, we say, okay, we\u0026rsquo;re not going to actually run these computations because maybe we will notice, uh, that, uh, there\u0026rsquo;s a sequence of operations that happen and they can be fused together. And then now I can actually, um, you know, use some fuse kernel in this case and run a lot faster in this situation. Um, lazy is different from tracing because with tracing, you just like run the entire computation through you, you capture whatever the, um, control flow was at that time. And then you like compile the entire trace laziness is sort of trying to be this more controlled, uh, controlled situation where you, uh, can run your code repeatedly and like, you know, we\u0026rsquo;ll keep lazily evaluating and then like doing the optimizations every time. So actually in theory, anything you could run in eager mode, you could also run with a lazy tensor, but you could actually pass it to some graph backend that does optimization. It\u0026rsquo;s, it\u0026rsquo;s very similar to tracing, but the difference is you do expect to run the eager code every time. Um, and like, you know, if the trace is the same, then you reuse it. Otherwise, you know, you recompile XLA, by the way, in PyTorch is an example of a lazy tensor in PyTorch. Okay. So Bram and I were working on this prototype. Well, really Bram was like doing all the work and I was like, you know, advising as like someone who was working in core PyTorch. And, um, besides like all the design problems that like lazy tensors entail and which would be a great story for another day on this podcast, um, something became clear, which is that, Hey, when you do a lazy tensor, you need to return a tensor and that tensor needs to have valid sizes and strides and D types, but you didn\u0026rsquo;t actually run your computation. That was like, Oh my God, this is terrible. This is exactly the same problem. We\u0026rsquo;ve run into, um, you know, third times a charm. Let\u0026rsquo;s actually do this. And so I pitched structured kernels as this project and thus embarked on this year long journey to like actually bring structured kernels into PyTorch. Why did it take so long to do structured kernels? Well, there\u0026rsquo;s, you know, a really difficult problem whenever you want to do any development in PyTorch, which is we have too many goddamn operators. Like we\u0026rsquo;ve got like, uh, so one of the things that I did before embarking on the structured kernels project was to like, try to taxonomize every operator in PyTorch. And I actually like have a spreadsheet of all our operators. I like went through them one by one and try to classify what kind of thing they did, what kind of shape computation they were. And it was only like 1700 operators. This is slightly inflated because like when there was a in place and out of place and out variant, I counted these all separately, but still 1700 offers. That\u0026rsquo;s a lot of operators that you actually have to do. And we keep adding new operators every, you know, release. And so this number just keeps going up. So, oh my God, like how the heck are we going to actually refactor all of this code? And it\u0026rsquo;s even worse because, uh, remember like PyTorch came from LuaTorch, which came from Torch 7. And so there\u0026rsquo;s like this legacy C TH implementation. And actually like we had already started a project for porting these crufty TH kernels written in C, written in this bastardized macro system and getting them into a more shiny modern C++. And even to this day, we are still not done getting rid of all the TH kernels. So like that\u0026rsquo;s a lot of work and structured kernels, like refactoring kernels in this way would have been a lot more work. So like the first thing that I like had to grapple with was like, how the heck am I actually going to like stage this change in a reasonable way so that we can like start partially migrating things while not having problems. The second big problem that I ran into was tensor iterator. So for those of you who don\u0026rsquo;t know, tensor iterator is the class in PyTorch, which was responsible for implementing all of our unary, binary, and basically all of our like, you know, kernels that like, you know, basically know how to operate on strided tensors. Tensor iterator is pretty cool. It does a lot of interesting stuff. It\u0026rsquo;s also really, really, really, really complicated. And like, you know, if, so remember when I was like, how do you do add, well, there\u0026rsquo;s type promotion, and there\u0026rsquo;s, you know, layout propagation, and there\u0026rsquo;s all that stuff. Well, a lot of stuff is in tensor iterator. And it\u0026rsquo;s like this big ball of code that like, no one really knows how to refactor. And I needed to somehow like, not duplicate this code, because like, it\u0026rsquo;s really complicated code, I don\u0026rsquo;t want two copies of it. And at the same time, like, make it possible to use without, you know, running the computation, even though it\u0026rsquo;s like embedded into this giant monolithic tensor iterator class that like, I have no idea how to do. That like took, I don\u0026rsquo;t know, I think it like took two months to figure out a reasonable design for structured kernels that could actually deal with this involving like, basically, I added a virtual method to tensor iterator that got invoked once it had actually figured out what the sizes and the shapes and the d types were, and then overwrote it to call into the structured kernel machinery. The technical details are important, but like basically big blob of legacy code. And originally, I was like, I\u0026rsquo;m just not going to solve this problem. Because, you know, tensor is too complicated, someone should just rewrite it, but like, add and sub and all these really important operators are tensor iterators. So I needed to, in fact, figure out some way to actually solve this problem. So yeah, so that all took a while. And we\u0026rsquo;re still not done. There\u0026rsquo;s still a lot of kernels that need to be ported to structured, but we\u0026rsquo;re in a much better spot right now. There\u0026rsquo;s a lot of work going on porting kernels to structured in PyTorch. We\u0026rsquo;re getting better and better coverage. We\u0026rsquo;re hoping to hit covering all the operators that XLA supports. That\u0026rsquo;s a really decent chunk of operators. And I don\u0026rsquo;t know, I\u0026rsquo;m pretty optimistic about the project, even though, you know, it\u0026rsquo;s like sort of sucked up my time and energy for a year at this point. That\u0026rsquo;s all I really wanted to say about structured kernels and metatensors. Metatensors, by the way, really simple, right? But how are you going to test them? And like getting testing to work on them was also a project, but, but I\u0026rsquo;m out of time. I\u0026rsquo;m going to leave you all here. Thanks all for listening. See y\u0026rsquo;all next time.\nEP9 Backend-extensibility Backend-extensibility Hello, everyone, and welcome to the PyTorch Dev Podcast. Today, I want to take an ambling journey through what we like to call in PyTorch as backend extensibility. What do I mean by backend extensibility? Well, PyTorch, you know, as a project has a number of things that it\u0026rsquo;s supposed to do. And one of the things that it\u0026rsquo;s supposed to give is GPU acceleration, right? Because if we didn\u0026rsquo;t have GPU acceleration, you could just use NumPy and do a lot of things you wanted to do there. But GPU acceleration means that you can take your same PyTorch program and run it not only on CPU, but also on CUDA at the same time. And so we call these things backends. At the very beginning of the PyTorch project, CPU and CUDA were the only backends that were available. If you go back to the LuaTorch days, or also further back to the Torch 7 days, right, there was the TH library, and there\u0026rsquo;s the THC library. And those were really the only backends in town. So everyone sort of used one or the other. And when PyTorch was initially released, that was still the same deal. We had a CPU, and we had a CUDA, and that\u0026rsquo;s all that happened. Then over time, people, you know, came to us and they\u0026rsquo;re like, oh, I\u0026rsquo;ve got some hardware, or I\u0026rsquo;ve got some other backend. And I\u0026rsquo;ve also liked to use it with PyTorch in the framework. And we started working on making it possible to add more and more backends to PyTorch. So that\u0026rsquo;s what I mean by backend extensibility. And so where is PyTorch when it comes to backend extensibility? Well, let\u0026rsquo;s dig into it. So the first thing to really know about PyTorch is that from the beginning, it was designed for CPU and CUDA. So if you have something that looks a lot like CUDA as the backend you want to do, things are going to work out okay for you. And so a really good example of something that\u0026rsquo;s really like CUDA, in fact, so like CUDA that like, it\u0026rsquo;s actually just frigging transpiling CUDA kernels into their own kernel language is the AMD HIP Rockum project. So, you know, CUDA is an NVIDIA invention, and it\u0026rsquo;s only targeted at NVIDIA GPUs. AMD also produces GPUs. And for the longest time, they didn\u0026rsquo;t have any general purpose programming capabilities. Well, Rockum is AMD stack for doing, uh, doing general purpose programming on their GPUs. And the way they set things up was they were like, okay, well, CUDA has a, you know, decade long head start in, you know, building a software stack, really one of the like key advantages of, you know, being on NVIDIA hardware, let\u0026rsquo;s just try to make use of as much of it as possible. And so the way Rockum works is that they have a language for kernels that is basically the same as CUDA. So they like copy pasted as much of the language as possible. And so when you want to write a, um, kernel, uh, in PyTorch, you write a CUDA kernel. And then in our AMD HIP Rockum build, we actually transpile this kernel via just doing a bunch of regular expression. It replaces really, we call it hippification, but like, it really is just a bunch of, you know, search and replace, this is on strings. We hippify it into a HIP kernel and we just directly go ahead and compile that. And that\u0026rsquo;s what, you know, you actually run if you\u0026rsquo;re running your, um, PyTorch on an AMD GPU. This actually works by the way. Um, like you can get one of the few, there aren\u0026rsquo;t that many GPUs that AMD releases that support Rockum, but if you have one of those GPUs, you can run your, uh, PyTorch programs on it. We did make some weird choices with, um, Rockum because this was one of the first, uh, first external backends that we added to PyTorch. And one of the weird choices we made was that, um, so remember PyTorch is only CPU and CUDA. And so while Rockum is like this thing, but no one is writing code so that it works with Rockum. So what they just did, they were like, okay, we\u0026rsquo;re just gonna, you know, not rename any of the user facing interface. So if you want to put your, you know, tensor into Rockum memory, just call CUDA on it with our special AMD build of PyTorch, which is mutually exclusive from the regular Nvidia builds of PyTorch. And then that\u0026rsquo;ll put it on the Rockum GPU. Kind of goofy, worked out okay for them. It was easy for them to reuse all of our tests, right? Cause all our tests were just written, uh, assuming that CUDA was the thing and, you know, not really a big deal. Kind of annoying from my perspective. I really hate that, um, it\u0026rsquo;s CUDA masquerading as it\u0026rsquo;s, sorry, it\u0026rsquo;s hit masquerading as CUDA. And I\u0026rsquo;d really like them to fix it someday, but I haven\u0026rsquo;t been able to get them to fix it, but so that\u0026rsquo;s how they do it. And because Rockum is so, so similar to CUDA, like literally like most things that CUDA provides, like streams and, you know, current device and, uh, you know, like, uh, CUDA and N, they all translate into the hip world. So, um, it wasn\u0026rsquo;t too hard. It\u0026rsquo;s not too hard, right. When like someone is, uh, doing the API exactly the same way. The sort of next, um, example of device extensibility that sort of lives in PyTorch\u0026rsquo;s history is our XLA integration. So XLA, if you\u0026rsquo;re not aware is Google\u0026rsquo;s underlying, um, compiler for TensorFlow. So like TensorFlow is the front end that you can write your neural networks in, but then XLA is this compiler that can take in, um, a high level, uh, IR and then compile it. And so for example, JAX, the new darling of, you know, Google\u0026rsquo;s, uh, research researchers, um, JAX also targets, um, XLA and, you know, it doesn\u0026rsquo;t have to share any TensorFlow code, but it just, you know, uses the underlying compiler. So XLA is pretty cool. If you want to run your code on TPUs, Google\u0026rsquo;s hardware for deep learning, XLA is the only game in town. And so we wanted to also make it possible to run PyTorch programs on TPUs. And as a result, um, we invested in an XLA integration. Now XLA is a lot different from, uh, Cuda and Rockham, right? So Cuda, uh, if you remember from my podcast about, um, every, just, you know, enough to know enough about Cuda to be dangerous. Um, Cuda is an asynchronous programming model, right? So you like have a bunch of kernels, you call into Cuda and, uh, Cuda goes and says, okay, well, these are the things I\u0026rsquo;m going to run as you told me to, well, XLA is nothing like that, right? XLA is a graph mode compiler. It expects to be given a graph of high-level IR, and then, uh, it will actually optimize it and run it for you. So we had to do something completely different for, uh, XLA. And, um, this is what we did. What was like, we added a new XLA type because, um, similarly to HIP, um, we, you know, wanted the main code, sorry, not similar to HIP. Um, XLA has a bunch of integration that needs to work with, um, the XLA code base. And so we wanted to let live, have let that live out of trees. So what we did was we put in a bunch of hooks in PyTorch core to sort of make this all work. And one of the things we did is there\u0026rsquo;s an XLA device type, similar to how there\u0026rsquo;s a CPU and CUDA device type. So you have to go and send a PR to PyTorch core and say, Hey, you know, can I have this device type? So we put in this device type, and then, um, we have this dispatcher thing, which I also have talked about in a previous podcast. We have this dispatcher thing that is a, um, entry point where for any of the operators that are defined in native functions.yaml, you can define your own, uh, implementation of it, register it at runtime. So like literally you have a dynamic library and has a bunch of static initializers and those static initializers are registrations. There\u0026rsquo;s a very user-friendly API modeled off to PyBind 11 that you can use to do this. So you register those things. And then whenever there\u0026rsquo;s a tensor that is an XLA tensor, uh, when you call any code in PyTorch, it will, um, it will hit the dispatcher. You\u0026rsquo;ll see that, Oh, this is an XLA tensor. And then it will route to the dynamically loaded XLA implementation that does whatever you want. And then XLA itself, because, you know, it\u0026rsquo;s a graph mode thing actually doesn\u0026rsquo;t do any computation. It just goes ahead and builds a computation graph. And so at the front end, there\u0026rsquo;s some stuff you have to do differently. Like there\u0026rsquo;s a special set of optimizers, which you know how to deal with the fact that XLA computation is lazy and not eager, but like XLA was sort of the first, like actually usable external backend that we developed for. And, um, we did a only so-so job in supporting them in their endeavor. So actually, uh, you know, it turns out there\u0026rsquo;s a lot of boilerplate you have to write when you want to add support for an external backend. And also XLA doesn\u0026rsquo;t support all operators in PyTorch, right? PyTorch has a lot of operators and, um, XLA, you know, well, XLA is cool. And there\u0026rsquo;s a lot of operators that it just doesn\u0026rsquo;t support or, you know, like they just didn\u0026rsquo;t have time to add support for. So XLA also has this mechanism for allowing for a fallback to CPU where it\u0026rsquo;s like, okay, you\u0026rsquo;re running your PyTorch program on XLA, and then you get to some operator that XLA doesn\u0026rsquo;t know how to do. So XLA was going to go ahead and, uh, compute the actual output for you, given the XLA graph that was at hand. And then with a CPU tensor fallback to calling the regular PyTorch CPU kernel, and then, you know, doing it back to XLA. It\u0026rsquo;s kind of a question whether or not this is a good idea to do by default or not, because these are like terrible performance cliffs, but it\u0026rsquo;s really useful if you just want to figure out if your program is going to run or not, right? Just like being able to move in between these ways. So because there\u0026rsquo;s kind of a lot of like infrastructure you have to write, XLA actually went ahead and, um, built a mini code generator. Like it\u0026rsquo;s this Python script that gets run during the, um, build process of XLA that actually generates all the code that registers to PyTorch and XLA like predates our sort of nice registration API. We had a not so nice registration API before, and it was very hard to use. And so XLA has this code and it does all this stuff and it\u0026rsquo;s not so nice. So actually Brian Hirsch, um, one of the members of the composability team has recently been working on sort of revamping XLA\u0026rsquo;s code generation and letting it live in PyTorch as a thing that external backgrounds can use, uh, when they want to like, you know, plug into our system and get all the niceties, like, you know, fall back to CPU in this situation. So rewinding a sec. So what is adding a new, uh, back into PyTorch look like in the XLA universe? Um, well, you need a, you know, first send a PR to PyTorch main repository being like, Hey, Hey, Hey, I\u0026rsquo;ve got this new device called XLA. Um, you know, I need PyTorch to know about, you also have to tell PyTorch about this dispatch key thing. The dispatch key is the thing that actually like, um, you know, we do the virtual call based on it\u0026rsquo;s different from the device type because not all device types have dispatch keys. And also we have a bunch of concepts that aren\u0026rsquo;t device types like V mapping and meta tensors actually meta tensors count as device. Um, and like autograd and these things aren\u0026rsquo;t really devices in their own right. So dispatch key is this like generalized idea of all the things you might go to. You have to go ask for a dispatch key to be added. And then there\u0026rsquo;s a little bit of Python binding code, which we never like wrote in generic way. So you\u0026rsquo;ve got to go edit those parts. But once you do all those things, you basically, um, don\u0026rsquo;t need to do anything else in PyTorch core, right? Because there\u0026rsquo;s this virtual table that you can manually program in using the torch library macros. And this is what XLA does. And it\u0026rsquo;s, it\u0026rsquo;s a little not so nice to do this directly. And so people have resorted to code generation to actually manage these things. So this is like basically the current state of the art in external backends. Um, I guess something that\u0026rsquo;s a little good to talk about is like, what are some of the challenges of doing an external backend in this way? Because we\u0026rsquo;ve actually, um, had a bunch of people try to actually go onto this, uh, treadmill. Uh, and you\u0026rsquo;ll see why I call it a treadmill in a moment. Uh, for example, um, ML compute from Apple and Intel as well. So some of the reasons why, uh, this is a little difficult. So one is that, um, you know, PyTorch cares a lot about backwards compatibility, compatibility, but only for our end users and not our backend extenders. So here\u0026rsquo;s an example of a backwards compatible chain you can make, um, which is, uh, say there\u0026rsquo;s some function and we want to add some new functionality to it. We could add a new argument to that function and give it a default value. So, you know, from the perspective of someone using PyTorch from Python, this is perfectly backwards compatible because, Hey, uh, you know, like if I am not passing this argument, it\u0026rsquo;ll get defaulted. And then I will ostensibly get whatever old behavior I had. Well, that\u0026rsquo;s not the case when you\u0026rsquo;re doing, uh, a backend because well, uh, when you, you know, have to register one of these functions that knows how to process this operator, you have a problem, right? Like this operator is now trying to give you extra arguments and you\u0026rsquo;re like, Oh, well, my old operator implementation only knows how to handle three arguments, not four arguments. What do in principle, this isn\u0026rsquo;t actually a BC breaking change, right? We could like somehow detect that the user gave us the defaulted argument and then, you know, call your, uh, old function without that argument in that case. And if they give a non-default value for the new argument, then we error in that case, but it\u0026rsquo;s kind of hard to do this in C plus plus only. So, you know, this is not something that like we\u0026rsquo;ve worked. And the upshot of this is that like, if you want to like do a backend, you\u0026rsquo;re actually going to have to do a lot of work, keeping all of your, you know, understood functions up to date with, um, with the, uh, changes to all the operators. Cause we keep adding new operators. We keep adding new knobs on operators. And so it\u0026rsquo;s, it\u0026rsquo;s kind of a treadmill keeping up to date, by the way, XLA can keep up to date because we actually have it included as part part of the build system in PyTorch. So like whenever you\u0026rsquo;re working on a new, uh, operator in PyTorch, um, XLA will tell you if you broke XLA and then like only through the heroic efforts of, you know, Jack, Sal and the rest of the XLA team, does this actually work okay? Because like you accidentally break the XLA test. You\u0026rsquo;re an average PyTorch developer. You know nothing about XLA. You can just sort of send up the bat signal and they will make the compatibility patch to like get XLA going. Uh, what I\u0026rsquo;ve heard, um, some other people have done when they\u0026rsquo;re, uh, extending the backend in this way is they like, don\u0026rsquo;t bother. And so every release they like try to catch up and there\u0026rsquo;s like a ton of stuff and this is not so great. I\u0026rsquo;m hopeful that Brian Hirsch\u0026rsquo;s work on a code gen for external backends can make this easier because there\u0026rsquo;s some things that we just technically can\u0026rsquo;t do in pure C++ but are easier for us to do in Python. But, um, this, this code is still be in the process of being landed for XLA. It\u0026rsquo;s really close. We, we actually tried landing it a few days ago and it got reverted because it broke something, but not, not for a good reason. Like it\u0026rsquo;s been passing, passing half of the CIs for a good while now. So I\u0026rsquo;m out of time. Um, there\u0026rsquo;s probably more things about backend extensibility that I should talk about, but, um, I\u0026rsquo;ll save them for another podcast. See you next time.\nEP10 The-life-and-death-of-Variable The-life-and-death-of-Variable Hello, everyone, and welcome to the PyTorch Dev Podcast. Today, I want to talk about a topic that I\u0026rsquo;ve received two requests for when soliciting topics to talk about in the podcast, and this topic is variable. Actually, it\u0026rsquo;s a kind of strange topic to be talking about, because if you look at PyTorch today in the Python frontend, there actually is no variable anymore, and that\u0026rsquo;s because we got rid of it. It was a banner feature in PyTorch 0.4, and then a bit later, we actually got rid of it in the C++ code, although there\u0026rsquo;s still a bunch of places where we still talk about variable. That\u0026rsquo;s just because we\u0026rsquo;ve been too lazy to rename all the type names in the code base. But it\u0026rsquo;s still really useful to know the history behind variable, because there are a lot of strange APIs that still exist because of the fact that tensors were structured in a different way. And it\u0026rsquo;s also kind of informative just to look at how the format of tensors has evolved over time, and also where they might be going in the future, because I would definitely not be the person who would say that we are in a perfect state. So where does our story begin? Our story begins a long, long time ago, even before the existence of C++ Autograd and PyTorch in LuaTorch. So in LuaTorch, tensor was represented as a C struct. And remember this thing, right, how the TH library in LuaTorch has a bunch of C code that\u0026rsquo;s munged about with a preprocessor? Well, that\u0026rsquo;s true for the data type as well. So when we wrote TH code, we had a C struct, and there was a separate C struct for every D type we supported. So there was a TH float tensor, a TH double tensor, a TH int tensor, and so forth and so forth. This made life really hard if you wanted to write code polymorphically over different tensor types, but it didn\u0026rsquo;t matter because we were just, you know, rewriting all of our code every time when you, like, wrote code in TH. We just, you know, redefined the macros and then stamped out different versions of the code. So along comes PyTorch, and we\u0026rsquo;re still using the good old-fashioned TH tensors. And Zach comes along, right, and he wants to build this A10 C++ library. And one of the things that he needs in the C++ library is he wants to be able to write code polymorphically over device types without templating them. Because you see, in C++, if you write templated code, you don\u0026rsquo;t actually get to type check the contents of your template, right? Like, the way C++ works, well, until, you know, C++ 20, whatever, concepts come along, the way C++ works is that when you write a templated function, C++ only checks the stuff that isn\u0026rsquo;t related to the template. Anything related to the template is deferred until you actually instantiate the template in question. So, you know, C++ templates are famously a source of really bad error messages. And so, you know, we had a bunch of people who were previously writing all of our operations in Python, and we were going to try to write them in C++. And so, like, forcing them to template all their code on D type would have been a really, really bad idea. So, like, if there was one good idea in the A10 library, it was this, don\u0026rsquo;t parametrize your tensor type on D type. Okay, so we had a single tensor type, and we put it all together. And we said, okay, there\u0026rsquo;s going to be a single tensor impl that represents all the D types in question. And that\u0026rsquo;s going to be pretty cool. But remember that the TH library and Zach\u0026rsquo;s A10 library didn\u0026rsquo;t know anything about automatic differentiation. And at the time, AD was implemented entirely in Python. So there was, like, no concept of this in C++. And this was true in LuaTorch as well. AD was a thing that was implemented in Lua, not inside the libraries themselves. And so when Sam came along, and he was like, oh, my God, you know, autograd is too slow, we need to make it faster. And we\u0026rsquo;re going to do it by putting it to C++. He was in the position of needing to write an implementation of autograd in C++ rather than in Python. And so the most obvious way to do this was to preserve the abstraction barrier that was enforced upon us when autograd was written in Python, namely that the tensor subsystem knows nothing about automatic differentiation. So let\u0026rsquo;s think about it, right? Like, say you have some library that gives you a tensor object and lets you do various basic operations on them. Well, what if you want to augment this with some notion of history and a notion of an autograd tape that you record graph operations to do later when you want to autograd on them? Well, if you have this strong abstraction barrier between the tensor and the AD system, you can actually modify your tensors to like add the new metadata you need. So what are you going to do, you\u0026rsquo;re going to wrap them in a variable. So variables were just this wrapper around tensors that, you know, gave all the extra metadata that you needed to get yourself working in the situation. And so it started off as a requirement, right? Because autograd was written in Python. And then when we moved everything to C++, well, the most easy thing to do was to preserve this abstraction barrier. So, you know, we had everything in C++, but you know, it was still like implemented as there is a variable wrapper and it is on top of the Aten library. In fact, they even lived in separate dynamic libraries, if you remember the dynamic library podcast. So, okay, so we\u0026rsquo;ve got this variable concept and, you know, it\u0026rsquo;s like 0.3 in PyTorch days. And, you know, we\u0026rsquo;ve got tons of people using PyTorch and they love it. And we keep getting all these questions about when should I wrap my tensors and variables? What\u0026rsquo;s the difference between a variable and a tensor? When do I use dot data to get a tensor out? And what we discovered is that it was actually really, really confusing for people to have to manage both variables and tensors. Now, it is a really like easy way to organize the code when we were implementing it. But the problem from the user experience perspective is there\u0026rsquo;s too much expressivity, there\u0026rsquo;s too much freedom in this representation. Namely, you can have a tensor, you can have a variable that doesn\u0026rsquo;t require grad, and you can have another variable that does require grad. And the problem is that, you know, each of these three states, the tensor state and the variable doesn\u0026rsquo;t require grad state, these states are basically the same. Like, semantically, they do exactly the same thing. The only problem is, well, you know, while you\u0026rsquo;ve got this variable thing, you\u0026rsquo;ve got this tensor thing. So people have to, you know, worry about, you know, switching between these two modes, even though like, you know, if they\u0026rsquo;re just thinking about like, what is it they want to do, right? Like, what they really want to do is they want some tensors to record gradient gradients and some to not. And, you know, having to deal with this extra distinction that doesn\u0026rsquo;t do anything useful. Well, that\u0026rsquo;s pretty confusing, and they don\u0026rsquo;t like that. So we were like, okay, in 0.4, we want to get rid of variable, right? And we want to just make it so that when you\u0026rsquo;re writing PyTorch code, you don\u0026rsquo;t have to deal with, you know, remembering if you wrapped something in a variable or not. So we got rid of variable. How did we do it? Well, we cheated. The way we cheated was we just said, okay, well, we\u0026rsquo;ve got this big C++ implementation with variable to tensor. And like, oh, you know, it\u0026rsquo;s a ton of code to refactor, we don\u0026rsquo;t really feel like refactoring it. Also, we didn\u0026rsquo;t actually know how to do this refactor. So here\u0026rsquo;s what we\u0026rsquo;re going to do. In PyTorch, we\u0026rsquo;re only going to provide you variables. So like this thing that we call tensor, secretly, it\u0026rsquo;s a variable. And you know, that means that, you know, we\u0026rsquo;ve eliminated this illegal state, when you don\u0026rsquo;t actually get to, you know, look at the, the illegal state is now a bare tensor, right, because you all you have are variables or variables with requires grad. And that worked pretty well for a while. So we had this problem, though, which is that like, in the Python API, there\u0026rsquo;s only tensor. But if you like, dive down to C++, and you\u0026rsquo;re like a C++ writer, there\u0026rsquo;s actually still this variable concept. And so one of the things that like, we really wanted to do was, you know, hey, like, maybe we want the Python and the C++ APIs to look the same. Like, maybe that\u0026rsquo;s a good idea. And we can do it. But there\u0026rsquo;s a problem. And here\u0026rsquo;s the problem. The problem is that the way we implemented autograd is via this unwrapping operation on variables. So the idea is that like, you have a bunch of variables floating around, you do some operation on them. And when you do the operation, well, you know, you\u0026rsquo;ve got a variable, so you go over to the variable implementation. And let\u0026rsquo;s say you\u0026rsquo;re doing the implementation of add. So we\u0026rsquo;re going to set up some autograd graph, right to like, you know, record, and then we want to actually run the original, the original code that actually implements the add kernel. So how do we do that? Well, inside every variable, it\u0026rsquo;s there\u0026rsquo;s a tensor that you can unwrap from it. So we just unwrap the tensors from the variables, and then we call add on those. And those are just tensors, not variables. And so we can actually get to the actual kernel question. So how do we do this? For if there\u0026rsquo;s no separation between variables and tensors, if every tensor is a variable, how do we actually do this? And you think to yourself, Oh, yeah, you know, Ed, what you should just do in this situation is you should like make a super call, right? Like, you, you\u0026rsquo;ve got your autograd code, and then you just want to call super colon colon add, and that\u0026rsquo;ll bounce you over to, you know, whatever the, you know, the parent implementation is ostensibly doing the actual addition. But we have a lot of operators in PyTorch. And many of these operators actually call other operators in their implementations. And when they call those other operators, you don\u0026rsquo;t actually want them to hit autograd. In the situation, you want them to go and you want them to go and go straight to the, you know, non autograd actual kernel computation, right? Because it\u0026rsquo;s sort of like, you know, once you do an autograd call, you\u0026rsquo;ve actually you\u0026rsquo;re done. There\u0026rsquo;s no like internal autograd bookkeeping you need to do like, it\u0026rsquo;s a single atomic unit in the situation. So you want to bypass everything underneath. Those of you who have read my dispatcher talk know how we solve this problem. So Wilfeng implemented the C++ tensor variable merge. And the way we solve the problem was we introduced some thread local state. So what we said was, okay, what we\u0026rsquo;re going to do is we\u0026rsquo;re going to have these variables, and we\u0026rsquo;re going to, you know, do our autograd stuff on them. And then we\u0026rsquo;re going to set some thread local state that says, don\u0026rsquo;t do any more autograd stuff. That\u0026rsquo;s actually what auto non variable type mode used to do. We\u0026rsquo;ve killed that now. Check out the inference mode podcast for more details on that. So we set this TLS. And now whenever we do function calls, we just check is, you know, the autograd skip TLS bit set. And if it is set, then, you know, we go and go to the actual kernel instead. The actual implementation is more complicated than that. But if you\u0026rsquo;re just thinking about autograd, this is all you need to know. And so in that way, we didn\u0026rsquo;t actually have to do any unwrapping step to actually, you know, make it so that we stopped running the autograd code and started running the tensor code. Now, there are a few other complications. So one of the things that was supported in the variable API is this data attribute. So what does the data attribute look like? Well, you know, if I have a tensor x, then I can say x dot data, and I\u0026rsquo;ll get out, well, who knows what it does today. But in the old days, right, if you had a variable, well, you know, x was the variable, and then x dot data was the tensor on the inside. And so if x was a thing that requires gradient, well, x dot data is a plain old tensor, obviously doesn\u0026rsquo;t require gradient. So we had to like figure out like what exactly these things should do in the new world order, because we\u0026rsquo;re not wrapping variables anymore. So there aren\u0026rsquo;t any, there\u0026rsquo;s no tensor inside waiting to, you know, burst out. Sorry, the tensor was not inside you all along. So what are you going to do, right? Well, we just looked at those semantics, and we\u0026rsquo;re like, okay, well, you know, what is this x dot data? Well, it aliases the same storage as the original tensor. So it\u0026rsquo;s kind of like a alias call. But you know, it doesn\u0026rsquo;t require gradient, even if the variable required gradient. So it\u0026rsquo;s kind of like a detached call. So, you know, and you know, what about the version counters? Well, version counters are a concept on variable originally, and then we put them on tensor. And so what are version counters? Well, that\u0026rsquo;s a long story for another time. But if you know what version counters are, we stored version counters on variables, when we put them on tensors. If you took out the data, the inside tensor of the variable, you would actually disconnect from the original version counter. So we also simulated that behavior. So basically, we like look, and we\u0026rsquo;re like, what is all the observable behavior you could see when you did a dot data, and then try to figure out what that would look like in a universe where, you know, there are no variables, right? Everything\u0026rsquo;s just a tensor. So that was done. And it sort of worked for a while, we were in this weird nether state where we had collapsed the representation. So there was only one, there was only one tensor representation, rather than a variable wrapping a tensor, but we hadn\u0026rsquo;t actually expunged all the variable classes from the code base. And then later, I actually went and finished off the job and got rid of all those wrappers. And then that\u0026rsquo;s sort of where we are today, right? So we have tensors, it\u0026rsquo;s a single struct, but the struct has a few fields, really one field dedicated for letting you slot in autograd metadata if you actually want it in the future. This data is not actually defined in tensor, it still lives in a separate dynamic library, the in the autograd folder in CSERC, and it contains a bunch of extra data. And so if you don\u0026rsquo;t actually require autograd, we don\u0026rsquo;t bother allocating all this data, and you can save a bunch of time. By the way, one of the reasons why, you know, inference mode and, you know, no grad mode is faster than, you know, if you\u0026rsquo;re recording autograd. And so that\u0026rsquo;s like basically the state of tensor today. So where could we be going in the future with this? Well, one of the things that people have been looking into recently is how to make it so that you can nest automatic differentiation repeatedly in a style that is not the same old style that we normally support double backwards in PyTorch. Namely, you know, you retain graph and then you back prop through the graph again. So more like a Jack style, like, you know, repeatedly differentiate a piece of code ahead of time. So how can we do that? Well, we\u0026rsquo;ve got a prototype that knows how to do this. And actually it\u0026rsquo;s done by, well, who would guess, wrapping the tensor into multiple levels of gradient tracking to make it work out. So I don\u0026rsquo;t know. Revenge of the wrappers, I suppose. So that\u0026rsquo;s all I wanted to say about a variable today. See you all next time.\nEP11 How-new-operators-are-authored How-new-operators-are-authored Hello, everyone, and welcome to the PyTorch Dev Podcast. Today, I\u0026rsquo;d like to give a short intro slash primer about the general developer experience that happens when you want to add a new operator to PyTorch. Despite, you know, PyTorch being a library for doing numeric computing, and so, you know, hey, you know, what are we all about? Well, we\u0026rsquo;re all about a big pile of operator implementations for all the things you might want to do. Actually, it\u0026rsquo;s not that common that we go about and add a new operator to PyTorch. It\u0026rsquo;s actually pretty rare because we kind of have a lot of operators in PyTorch, and, you know, most of the time when you want to do something interesting and new, usually you just, you know, put a bunch of operators together to do whatever it is that you are interested in doing. And that\u0026rsquo;s like, you know, that\u0026rsquo;s basically what people are doing when they write deep learning models, right? They\u0026rsquo;re just putting operators together into bigger and better operations. So you only really need to write a custom operator when there is something that you need to do that, like, sort of can\u0026rsquo;t be done efficiently by putting everything separately. So, like, kind of classic example, which applies to PyTorch and is sort of ameliorated if you\u0026rsquo;ve got a fusing compiler, is if you, say, want to write a new pointwise op that consists of a bunch of pointwise operations, and you don\u0026rsquo;t actually want to, you know, run them separately one by one, loop by loop, right? Because that takes up a lot of memory traffic. Well, then writing an operator for that case is quite a big benefit because once you fuse them together, things can run substantially faster. But, okay, so let\u0026rsquo;s say that, you know, you\u0026rsquo;re actually doing some sort of really fancy linear algebra or you need a new pointwise fused op or, you know, any sort of situation where you, you know, need the performance that you can only get from writing a kernel. What does it look like when you want to add a new operator? Well, there\u0026rsquo;s sort of two main modes that people write new operators in PyTorch. One is adding a new operator to the library proper. So this is, you know, this is core PyTorch. The next release of PyTorch, the operator is going to show up and, you know, you can make use of it. It, you know, is something that you put in native functions.yaml, a file we\u0026rsquo;ll be talking about more later in this podcast. And it\u0026rsquo;s just something that we consider in core. But there\u0026rsquo;s another way to write a new operator in PyTorch, and that\u0026rsquo;s using the operator extension mechanism. So using the Torch library header and macro, you can actually define operators completely externally from PyTorch. And then you can just, you know, you register them via a PyBind 11-like registration system. And then these operators become available for you to use via the Torch.ops namespace. So, you know, there\u0026rsquo;s a difference between these two things, right? If you add a new operator to core PyTorch, the thing you need to do is you need to make sure everything works, right? So you need a CPU implementation, you need a CUDA implementation, you need working derivatives for it, you need, you know, comprehensive tests like autograd checking, all that stuff. And sometimes, oh, and not only that, but, you know, your operator needs to handle all of the different kinds of tensors that, you know, a PyTorch programmer might throw at you, including tensors with strange strides or different layouts or very different D types. Now, if you\u0026rsquo;re just someone, you know, who like just needs a little code that works on floating points just for this particular case on CPU, often you don\u0026rsquo;t actually want to go through all that rigmarole. And also maybe your operator is like not very well defined, right? It\u0026rsquo;s not, doesn\u0026rsquo;t mathematically make sense. It\u0026rsquo;s not really something of general use. It\u0026rsquo;s just something very specific for your problem. Well, writing a custom operator is great for this use case, right? Because you just write out your operator and you do the thing for exactly the use case you need. And no one else is really bothered by the fact that you wrote a custom operator like this. So the use cases in these two situations are kind of different. But let\u0026rsquo;s talk a little bit about what happens when you add a core operator to PyTorch. So what exactly does this entail? So the first thing you need to do is you need to define what the API for this operation is going to be. And the reason for this is PyTorch is not just a Python library. It is actually, you know, also a C++ library that you can use directly from C++. And it\u0026rsquo;s also a compiler and interpreter that, you know, you can interpret PyTorch programs on. And so, you know, you don\u0026rsquo;t just write an operator by writing a new Python signature. We need to write a API declaration for the operator, which is generic across all of the different modes of use. Interpreter, C++, Python, and other situations that can work in all those cases. And what we call this, you know, specification is a JIT schema string. So if you\u0026rsquo;re in PyTorch core itself, there\u0026rsquo;s this file called native functions.yaml. And what it has is it has all of the JIT schema strings for all of the operations that you might be interested in. And JIT schema strings are like some sort of mashup of the Python type system and the C++ type system. So, you know, you can say, OK, well, my first argument is a tensor. My second argument is maybe a integer list because I need to like provide what the padding is. The schema also knows about aliasing. So like what if I have a function, does the input alias with the output? And it also knows about like mutation, like is my function purely functional, which is most functions in PyTorch, or does it, you know, mutate one of its inputs? And you have to tell it that too. You don\u0026rsquo;t need this information if you\u0026rsquo;re just writing Python code, but you do need this information to say if you\u0026rsquo;re a compiler and you\u0026rsquo;re trying to understand whether or not it\u0026rsquo;s safe to, you know, do a code movement optimization or not. OK, so that\u0026rsquo;s cool. So you write this entry in native functions.yaml and what this does is it triggers off a very long sequence of code generation pipeline, which actually goes ahead and generates the Python bindings for your program, the C++ bindings for your code, et cetera, et cetera. And so all you need to do after you define one of these native functions.yaml entries is you just need to provide an actual CPU and CUDA kernel. And so, you know, in the YAML file, it\u0026rsquo;s not, it\u0026rsquo;s not, I\u0026rsquo;m not really here to like tell you exactly how to do this. If you want to like look at the actual code, you should look at some of the further reading links after this podcast. But what, you know, what you\u0026rsquo;re going to do, right, is you\u0026rsquo;re going to write a CPU implementation. You can say, OK, my CPU implementation is going to be say softmax underscore CPU. And one of the things the code gen does is it generates a header stub. And what this header stub says is, hey, here is the C++ function I expect you to have written. And once you write it, then I will, you know, do all the necessary plumbing to make it possible to, you know, call into your kernel. Now, the way you do this is a little different depending on if the kernel is structured or not. See my previous podcast about structured kernels. But the same, the general concept is the same. It\u0026rsquo;s just we generate slightly different stubs in the two situations. So there\u0026rsquo;s different code you have to write. You have to write more code if you\u0026rsquo;re doing the old-fashioned way because you have to also define the out variant and the in-place variant directly. And the structured kernel version takes care of all of that for you. But as a result, you have to, like, structure your code a little differently. But it\u0026rsquo;s very, very similar. OK, so you\u0026rsquo;ve gotten to this point and you\u0026rsquo;ve got all the scaffolding that you need to actually call your operator. How do you actually implement your operator? Well, as I said, in PyTorch, we expect you whenever you add a new operator to give a CPU and CUDA implementation. So what does a CPU implementation typically look like? Well, normally, if you\u0026rsquo;re doing some CPU code, it depends on how complicated it is. But pretty common situations are, for example, there\u0026rsquo;s some external library that\u0026rsquo;s already written efficient CPU kernels and you just go ahead and use those directly, right? So in that case, all you\u0026rsquo;re doing in the kernel is, you know, you got some tensors, you figure out, you know, what their data pointers are, you make sure that, you know, all the invariants that the library expects are upheld, like that the inputs are contiguous. Most libraries don\u0026rsquo;t handle discontiguous inputs. It\u0026rsquo;s pretty uncommon. And then, you know, you just call another function in question and maybe you have to go and allocate the output tensor for it to write into. But if you\u0026rsquo;re actually writing a operation yourself, well, there\u0026rsquo;s a few facilities for writing very common styles of operations. In particular, if you\u0026rsquo;re doing a point-wise operation or a reduction, we have this really useful class called tensor iterator, which takes care of all the sort of gnarly details of, you know, like, if I have a tensor in a different layout, how do I, you know, restride it so that I iterate over all the different strides without, you know, processing memory that\u0026rsquo;s not necessary, blah, blah, blah, blah, blah, do all of those things and then all you have to write is a little lambda that says how to actually do the point-wise operation in question. So, you know, and then all the other infrastructure taken care of for you. This only works if you\u0026rsquo;re doing one of these very simple, you know, like point-wise cell operations or there\u0026rsquo;s a few other cases like, you know, tensor iterator can also handle reductions in some sense. If you don\u0026rsquo;t have a CPU kernel that falls into this, these categories, then you might actually have to, you know, oh, goodness me, write some efficient CPU code that actually does the thing you want. Sometimes, you know, it\u0026rsquo;s simple, it\u0026rsquo;s easy enough to just write a plain old for loop in C++ because maybe you don\u0026rsquo;t need it to be that fast. It\u0026rsquo;s just that doing a for loop in Python is too slow. And there\u0026rsquo;s also like lots of other libraries that try to build off of this, right? Like Numba and Cython. All these ideas are like, oh, yeah, you know, like Python\u0026rsquo;s really slow, but like maybe you want to write numeric loops in Python and then they compile the C++. Well, it\u0026rsquo;s not too hard to write these loops in C++ as well. And so in PyTorch, people usually do that. We provide a bunch of facilities for, you know, doing common optimizations. For example, if your algorithm is parallelizable, you can use the parallel for loop construct that we provide to, you know, farm out your computation onto different threads so that, you know, you can take advantage of multi-threading. and, you know, if your kernel is running slow, well, typically kernels are pretty short. So you can like easily run it under perf and then take a look and see, okay, you know, am I missing cache a lot? Am I spending a lot of time on instructions? You just do normal techniques for optimizing performance in this case. Optimizing numeric code is different, right? Like people always like to say, oh, yeah, you know, matrix multiply, how do you implement that on CPU? Well, you know, you really need to know about cache and so it\u0026rsquo;s very different from optimizing other types of code. But there\u0026rsquo;s also a sense in which optimizing CPU code is very easy because, sorry, optimizing CPU kernels is very easy because there\u0026rsquo;s just not that much code. So you can actually come up with a pretty good mental model of what you\u0026rsquo;re supposed to do in this situation. Okay, so that\u0026rsquo;s it for CPU. What about a CUDA kernel? Well, CUDA kernels are pretty similar, right? Like we need to do all the same things except instead of writing CPU code, there\u0026rsquo;s this CUDA programming model and we need to know things about how the device actually works but then you\u0026rsquo;re still writing CUDA and many of the things that, you know, you like expect to see in CPU, you know, the CUDA ecosystem is well developed enough so that alternatives to these things also exist in those situations. So for example, if you need to debug your CUDA kernel is crashing, well, there\u0026rsquo;s a tool CUDA memcheck which will tell you about, you know, what is causing the crash. You can also, in a pinch, use CUDA GDB which actually lets you step in problems. you can also add asserts to your kernels, good old-fashioned printf debugging and, you know, if all else fails, well, you know, once again, your CUDA kernels are usually pretty small so you can like maybe bisect your way to figure out what the error is. Really, the hard part about writing a CUDA kernel is actually understanding the device model enough so that you can actually write concurring code and so if you ever like look at a presentation about how to write CUDA programming, like what they\u0026rsquo;re actually going to do is they\u0026rsquo;re going to spend a lot of time talking about how these processors actually work, you know, what the like actual physical details of the hardware are because this actually really matters if you want to write efficient code. Of course, if you\u0026rsquo;re doing something simple like a pointwise op, well, it turns out tensor iterator also works in that situation so you can just, you know, use our, you know, scaffolding in that case but it\u0026rsquo;s actually kind of challenging to write a good CUDA kernel. an example that I\u0026rsquo;m thinking of recently is we were working on some linear algebra code and the algorithm that like, so a lot of the times, right, there will be a well-known CPU implementation and we want to add it to PyTorch and we need to somehow figure out how to GPU accelerate it and so this CPU implementation in question had a problem which is that it needed to do a little bit of computation at first to figure out how many iterations of approximation it was going to do. Well, basically we were doing like these Taylor expansions for the computation in question and we needed to like look at the conditioning of the matrix to figure out like how many Taylor expansions we needed to do and so I remember reviewing this, the CUDA implementation for this PR and us arguing about well, you know, we can\u0026rsquo;t actually on CUDA make a decision based on the data what to compute on without doing a synchronization because remember CUDA is async and so if we need to like look at the data in question we have to wait for whatever prior kernels we\u0026rsquo;re running to finish running to give us the data and then we need to run our actually operation and then get it to CPU so we like talked over and like, you know, benchmarked a bunch of different options and it turned out it was still it was still better to synchronize so that we could pick a good Taylor approximation for this case but like there\u0026rsquo;s going to be a lot of problems like this where like, you know, it\u0026rsquo;s not easy to program a GPU and so you\u0026rsquo;re going to have to actually understand like there\u0026rsquo;s actually like non-trivial technical content and like recasting an algorithm so it works on CUDA but let\u0026rsquo;s say you do that, right? So you\u0026rsquo;ve got your CPU kernel and then you\u0026rsquo;ve got your CUDA kernel and you\u0026rsquo;ve plugged it all in via the native functions dot YAML system well, then you\u0026rsquo;re basically done. That\u0026rsquo;s it. You\u0026rsquo;ve got some more stuff to do, right? You\u0026rsquo;ve got to test your operator and we have a bunch of facilities for testing in PyTorch but they all involve you know, just like running the kernel in question and you know, well, you\u0026rsquo;ve already got the bindings provided for you so it\u0026rsquo;s pretty easy to get that hooked up. We have a bunch of stuff like for example, Mike Ruberry\u0026rsquo;s been working on a new op info abstraction which lets you describe some properties about an operator and then we can automatically run tests based on what kind of properties the operators on hold. Unfortunately, these kind of mostly are for like unary and binary ops so very simple types that are very regular and there are simple things we can check but you know, there are also some like very generalizable checks we do. For example, there\u0026rsquo;s a check in our test suite called GradCheck. What does GradCheck do? Well, remember that when you\u0026rsquo;re writing an operator in PyTorch you also have to say how to differentiate it so we typically have symbolic derivatives for all of our operations usually cast in terms of other functions that you might have to implement. Well, what GradCheck will do is GradCheck will use your analytic, sorry, not analytic, not symbolic. It\u0026rsquo;ll use your analytic derivative formula and it\u0026rsquo;ll also numerically compute what the derivative is based on your forward implementation and then it\u0026rsquo;ll just compare the two and figure out whether or not they agree or not and if they don\u0026rsquo;t agree, GradCheck will fail and this will work for any differential function you have. You don\u0026rsquo;t have to write a separate test for each of them. But yeah, so you add some tests and you have to write your docs for the new operator and you\u0026rsquo;ve got your kernels and then that\u0026rsquo;s great and usually you submit the PR and you give some benchmarks. It\u0026rsquo;s very easy to benchmark kernels once again because they\u0026rsquo;re very regular and you can just try them a bunch of different input sizes. And then you\u0026rsquo;re off to the races. Really the hardest part is convincing PyTorch that we actually do want to take your operator. But that\u0026rsquo;s a story for another time. That\u0026rsquo;s all I wanted to say today. Talk to you next\nEP12 History-and-constraints-of-Tensor History-and-constraints-of-Tensor Hi, my name is Edward, and welcome to today\u0026rsquo;s edition of the PyTorch Dev Podcast. Today I want to talk about a topic which was requested also multiple times by several people, namely the history behind tensor, tensor impl, storage, storage impl, and basically like how is the tensor data structure in PyTorch put together? This is a topic that I have written about in the past. For example, on my blog, I have a blog post about, you know, basics about PyTorch internals, and some of the things it talks about are how tensors are put together. So like there are these things called strides, you know, we have a concept called storage. So if you want to know more about these topics, go check out my blog post, then come up back to this podcast. Today, I want to talk a little bit more about some of the historical and design constraints that have led us to where the tensor data structure is today. So basically, given all these design constraints, if you, you know, spend enough time, hopefully, you would end up in the same situation that PyTorch is. So I sort of, there are a lot of things in tensor, right, because it\u0026rsquo;s a very traffic data structure. A lot of people have added things over the years. And sometimes it can be a bit bewildering, like why the heck are there like eight bit fields for like various, you know, variations of, you know, memory layout on the tensor? Well, you know, hopefully, knowing a little bit about the background and the constraints will help you understand, oh, yeah, I see why that\u0026rsquo;s there. It might not be ideal. But there is a constraint that causes us to get there. So let\u0026rsquo;s get to it. So the first and foremost constraint that fed into PyTorch design of tensor is the fact that PyTorch descends from th. I\u0026rsquo;ve said this before, I\u0026rsquo;ll say it again. Remember, PyTorch was originally just Python bindings to the pre existing C libraries that shipped with LuaTorch, which in turn came from the Torch 7 libraries. And why is this important? Well, we inherited a lot of the basic architecture for tensors from these libraries. And in particular, the split between tensor and storage is the sort of most prominent thing that, you know, PyTorch carries in its DNA today. I didn\u0026rsquo;t ever get a chance to talk to original Lush or Torch 7 authors. So I don\u0026rsquo;t really know why they set things up this way. But when I sort of like retroactively look at the past and like come up with my own explanations, one thing that I can say is that PyTorch\u0026rsquo;s concept of a storage was very important for, you know, enabling something that\u0026rsquo;s very core to PyTorch\u0026rsquo;s DNA, namely the ability to alias tensors together and do mutations on them. This is like very unusual. Strides especially are very unusual. Many, many other systems, tensor flow being one prominent one, only support operations on contiguous tensors. And sort of like what makes PyTorch a little spicy here is that, you know, you can actually, you know, refer to multiple tensors on the same memory, possibly with different layouts simply by adjusting the striding. So it\u0026rsquo;s something that\u0026rsquo;s very like unique to PyTorch. And we got that from the libraries that we descended from. There\u0026rsquo;s other things that we inherited from the TH days as well. For example, when tensor was just the C struct in TH, they needed some way to do a reference count. So they just put the reference count on the tensor itself. It turns out that intrusively ref counting in this way is very convenient. For example, when you\u0026rsquo;re writing bindings, because if you have a raw pointer to a object, you don\u0026rsquo;t have to like do any work with say enable shared from this to get out a owning pointer to it, right? You can just transmute the raw pointer into an owning pointer and, you know, the owning pointer will just take care of incrementing and decrementing the ref count. So when, you know, we brought PyTorch into, you know, the C++ land and re-implemented the classes, we also preserved intrusive ref counting because all of our binding code was way simpler when we had it that way. Also, we didn\u0026rsquo;t want pointers to tensors to be two words, which is, you know, what shared pointer does in C++. The second constraint, which is useful to know about on tensor is the fact that it actually is the result of merging the cafe2 and PyTorch libraries together. So if you\u0026rsquo;re a regular PyTorch user, you might not, you know, think very much about cafe2, right? It\u0026rsquo;s this other library that, you know, is graph mode only. But in fact, the same tensor representation in PyTorch is used verbatim with cafe2. There\u0026rsquo;s actually two separate user facing classes. There is a tensor class that, you know, sorry, an AT tensor class, which you use from PyTorch, and a cafe2 tensor class that you use from cafe2. And they actually have different public APIs for backwards compatibility reasons. But the both of these are what we call pimple classes, pointer to implementation classes. So they don\u0026rsquo;t actually, you know, represent the data in the object. Instead, they just contain an owning pointer to the tensor in pull object, which is the actual object that contains all the data in question. By the way, why is there the split between tensor and tensor impl? Well, it\u0026rsquo;s because you know, we are a Python project. And a lot of people when writing code involving tensors in C++, expect Python style reference semantics to work. So like, if I have a tensor y, and then I say tensor x equals y, I expect x to, you know, point to the same tensor as y, I don\u0026rsquo;t expect a copy to happen in this case. And you know, in C++ value semantics, you know, if you have a value type, like tensor impl is, you did this copy construction that would actually copy all the metadata in question. And then it depends on the semantics of the smart pointers inside what the other data does. So by splitting this into two types, and having tensor be a actual pointer type, like in the same way shared pointer is, you just write tensor, and you, you know, can assign things around. And it looks just like how things are in Python. So, you know, constraint three, I would say is that, you know, we\u0026rsquo;re our Python project. So a lot of our C++ design comes out of trying to model off of Python. There\u0026rsquo;s a great essay about this, by the way, which is on the wiki, basically, a manifesto about writing Python in C++. We, as time has gone on, for efficiency reasons, we have had to walk back some of the things we\u0026rsquo;ve done here. For example, you know, passing around tensors, as a pointer type is not so great, because they force ref count bumps, right? In Python, this is not a big deal, because Python has a gil, so the ref counts are non atomic, but atomics are kind of expensive. So you know, we\u0026rsquo;ve actually spent some time in the recent past, trying to, you know, remove as many ref counts as possible. But generally speaking, if you can write Python code, you can write PyTorch code, and the tensor class API\u0026rsquo;s are designed to make these look as similar as possible. Okay, point four. So I\u0026rsquo;m done with the historical things. But point four is, we don\u0026rsquo;t really want there to be virtual calls on tensor. And this actually has some pretty major implications. Now, I should preface this by saying, if you actually go and look at the tensor class, tensor impl class, and look at all the methods on it, actually, a ton of them are virtual. And there\u0026rsquo;s a reason for this. It\u0026rsquo;s a historical reason. But the reason why we don\u0026rsquo;t want to virtualize most methods on tensor impl is because virtual methods thwart the inliner. So you know, most operations on tensor, like, for example, querying the sizes should compile into a direct, you know, memory access at the field that contains the sizes and questions, right, it should be super fast, we should be able to get rid of all the function called goop. But you know, if it\u0026rsquo;s a virtual method, well, some subclass could have overridden the behavior in this case. And so we can\u0026rsquo;t inline in the situation, we have to actually do the V call jump. And the V call jumps are not that expensive. But you know, we call size everywhere in pytorch. So it really does add up. Why is size actually virtual then? Well, you know, this is a sort of like argument between like, you know, his history and sort of design in the pytorch code voice, the history of pytorch is that size was virtual, because when Zach originally wrote the class, it was virtual. And why was it virtual? Well, it was virtual because we had this variable thing, see my previous podcast about the life and death of variable, we had variable variables, a wrapper on a tensor. And they made this very reasonable at the time design decision, that they didn\u0026rsquo;t want to duplicate the size information between variable and the tensor that it wrapped because you know, if you duplicate the information, it can get out of sync. For example, if you resize the underlying tensor without, you know, telling the variable about it. So if you don\u0026rsquo;t want to keep them in sync, you need to change the behavior right on a tensor, you can just access the field directly. But on a variable, you have to jump to the base class, and then actually query this size there. So okay, size is virtual. Now we\u0026rsquo;ve gotten rid of variable, right, the variable tensor merge. And so this, this constraint no longer applies. And now we have a design that we can actually just force everyone to like accurately record what the size of their tensor is inside the class itself. But in the meantime, a bunch of people like went ahead and overrode size for their own needs. And so we have to like unwind that situation, solve the problem, most notably xla, cough, cough, cough. Okay, so but you know, in general, we want on methods on tensor to be virtual. And what that means is that actually, when you look at the tensor input class, it basically has all of the fields that you can conceivably want to describe, you know, what a tensor should be. So for example, we have sizes on tensors. Yes, hypothetically, you know, strange extensions to tensors, like ragged tensors, or nested tensors might not have size in the traditional sense. But you know, because size is such a, you know, intrinsic operation that we use everywhere in PyTorch, we really do want you to like have some, you know, conventional notion of size for anything you model in this way. And if you can\u0026rsquo;t model in this way, well, maybe, you know, tensor input is not for you. Another consequence of this is exactly those bit fields about memory layout, right, like we don\u0026rsquo;t want to actually have to compute the memory layout every time. So, you know, given that we know what the sizes and strides of a tensor are, that actually tells us what the memory layout is. And so we pre compute a lot of information in these bit fields so that you know, we can have fast accesses that don\u0026rsquo;t involve doing some compute, they just like check what the bit is. Okay, point five point five is extensibility. So you know, tensor is actually this is the same as the previous point, right, which is that like the devirtualization constraint is in tension with the extensibility constraint, right? By devirtualizing the tensor input class, it\u0026rsquo;s less extensible, but operations on it are faster. By virtualizing it, you can override more behavior, but then the tensor input class is less efficient. So we kind of need to play this game. And so like the the cut we have, right, is that we want basic operations, like the basic data model for tensor to be virtual, but then anything else on top, like especially operators, that can all be virtual. And in fact, it is via the dispatcher. Okay, last constraint, size and memory, I have a really funny story, which is when we were merging the cafe to and PyTorch libraries, I added a bunch of fields sort of randomly because like, I was once again unioning the behavior of cafe to and PyTorch. And then I broke some internal workflows. And what those internal workflows were doing where they were like allocating 4 million tensors. And so every word I added to PyTorch actually ballooned their memory usage by several gigabytes. So that was not very nice. And it like induced us to like spend a bunch of time trying to optimize the actual memory size of the tensor impulse truck itself, because it\u0026rsquo;s it\u0026rsquo;s really overhead, right? Like in PyTorch, you really want to just be, you know, storing memory for all of the, you know, actual data that you\u0026rsquo;re doing your deep learning on. And you don\u0026rsquo;t want to waste time or space on the metadata in tensor itself. And so we\u0026rsquo;ve done a bunch of optimizations, some very recently, for example, done by Scott Walchalk. For example, we used to store sizes and strides as out of line vectors on tensor, that\u0026rsquo;s really inefficient, because a standard vector in C++ takes three words in the structure itself, right? It takes a size, it takes a pointer to the beginning, and it takes the pointer to the end of the reserved data. So because you know, vectors can have a size that is less than the actual data that\u0026rsquo;s allocated for it. So all that needs to be stored. And it\u0026rsquo;s not really necessary. And also you don\u0026rsquo;t need to store the size for both sizes and strides, because the dimensionality of a tensor is fixed. So you know, we actually pack these fields, and we also put the sizes and strides directly in the tensor impl itself, assuming that most tensors are five dimensional or smaller. And that saves us having to do dynamic allocations when we allocate tensors. Okay, so that\u0026rsquo;s it for, you know, why tensor is the way it is. So the next time you go and look at the tensor impl class, hey, think about, you know, well, we wanted this to look like Python. So that\u0026rsquo;s why there\u0026rsquo;s a pimple method. We wanted, you know, to support all the stuff we could support from the good old torch days. So that\u0026rsquo;s why there\u0026rsquo;s storage and tensor, we merged cafe two and pi torch together. So that\u0026rsquo;s why there\u0026rsquo;s a bunch of really random features in tensor impl that don\u0026rsquo;t make that much sense. Well, that\u0026rsquo;s because some of them came from cafe do. Another example of that is type meta, which you know, there\u0026rsquo;s two like ways of representing d types in C++ scalar type, which is just an enum, and type meta, which is a pointer type that is open and extensible. And that\u0026rsquo;s because cafe two supported registering custom types to tensors like std strings, you could have a tensor full of std strings. Don\u0026rsquo;t ask me why you\u0026rsquo;d want it. Actually, it\u0026rsquo;s pretty useful in some situations. And then fourth, there\u0026rsquo;s a bunch of, you know, constraints about like, you know, efficiency, right? Like making sure that our methods can inline, making sure that the memory size of tensor impl isn\u0026rsquo;t too big, but also at the same time supporting extensibility for, you know, all of the weird and wacky other tensor types like sparse tensors and nested tensors and, you know, funk torch tensors that people want to support. Okay, that\u0026rsquo;s everything I wanted to say for today. Talk to you next time.\nEP13 Conjugate-views Conjugate-views Hello everyone and welcome to the PyTorch Dev Podcast. Today I want to talk about a new feature that is going to be landing to master soon for complex numbers, namely conjugate views. To explain what this feature is, I have to backtrack a bit and talk a little bit about complex numbers first. So what are complex numbers? Complex numbers are a form of numbers where instead of only having a single real number representing a quantity in question, you have both a real quantity and an imaginary quantity. And the invariant, right, is that the imaginary quantity, you know, when squared is negative. And no positive or negative real number when squared gives you a negative number. So that\u0026rsquo;s what makes imaginary numbers different. This sounds kind of strange. And, you know, for the longest time, neural networks don\u0026rsquo;t really use complex numbers. But in lots of, say, signal processing applications, you know, complex numbers have a lot of interesting properties that make it actually really good for, you know, doing certain types of computations. So if you\u0026rsquo;re doing some like, for example, fast Fourier transforms, complex numbers arise very naturally. And there\u0026rsquo;s also a line of research looking into how to use complex numbers for useful things. Actually, when the complex numbers project started, it was a physicist, Roger Luo, who sort of came and was like, hey, you know, I think this would actually be really useful. Took us a while to actually listen to him. But you know, we got there in the end. So when you\u0026rsquo;re doing complex numbers, there\u0026rsquo;s an operation that is really, really common, and it\u0026rsquo;s called conjugation. So what is conjugation? Conjugation says, okay, if I have a complex number a plus bi, where i is the, you know, constant that when squared gives you negative one, conjugation is taking this number and giving you back a minus bi. Now, the reasons why conjugation and complex numbers are very common is sort of beyond the, you know, scope of this podcast. But one way to think about it is, if you like, think about like, your linear algebra class that you took in undergrad, okay, if maybe if you took the theory based one, because I don\u0026rsquo;t know, they really go into complex numbers on the more practical linear algebra classes. One of the things you do is you, you know, talk about fields on real numbers. And you know, you do a bunch of stuff on them, and you learn some properties about linear algebra. And then you\u0026rsquo;re like, okay, now you can generalize to complex numbers. And you know, you have to like change all your definitions to make things work. And one of the things that happens is, you know, everywhere you were doing transposes in your, you know, old theorems, suddenly you\u0026rsquo;re doing Hermitians, you\u0026rsquo;re doing adjoins, you\u0026rsquo;re basically taking both the transpose and the conjugation of the matrix in question, whereas, you know, in the real universe, you were just transposing. And you know, you just you just need to do this to make all the theorems work out. And you know, if you\u0026rsquo;re really, really curious why this is the case, I recommend, you know, like, taking a theoretical linear algebra class and just sort of spending some time stewing with the theorems. Okay, so conjugation is a really common operation. And, you know, it\u0026rsquo;s really simple, right? Like you just are doing a negation on one part of the complex number. And so typically, right, you\u0026rsquo;re doing the conjugation because you\u0026rsquo;re about to do some other operations. So if you are doing matrix multiply, you know, a common thing to do in standard linear algebra is, you know, matrix multiply a with b transpose. Well, you know, in the complex universe, that\u0026rsquo;s going to look like something like a matrix multiply with b transpose and conjugated. And here\u0026rsquo;s something very curious happens. So if you think of conjugation as just a operation where, you know, if to conjugate a tensor, you know, you take your tensor, and you produce a new tensor with, you know, everywhere it was a plus bi is a minus bi, then this matrix multiplication operation is actually a bit less efficient than its sold version. When you did a matmole b transpose, we didn\u0026rsquo;t actually ever do the transpose. Because remember, pytorch, supports strides on tensors. So if we want to take a, if we want to take an operation like transpose, and do it without actually doing the computation in question, it\u0026rsquo;s actually an 01 operation, you just take your tensor, and you swap the strides. So instead of saying, okay, when you move in the y position, like say that you\u0026rsquo;re indexing x, y, only move one, instead, changing the y position means moving an entire, you know, row, and moving x position is what, you know, you only move one on. And by like simply switching the strides, so that instead of, you know, moving one, you move a lot in the y case, you are representing a transposed tensor. And actually, so if you\u0026rsquo;ve got a back end implementation of matrix multiply, that knows how to implicitly do transposes, for example, BLAS is, you know, matrix multiply has a flag that, you know, lets you specify if the, you know, argument is transposed or not, then you actually can just avoid having done the transposition at all, because you just, you know, say, okay, well, I want to do a transposed matrix multiply, where the right argument is transposed. And you can just call the kernel directly, and you\u0026rsquo;re all good. And we never actually do the transposition. And transposition is kind of expensive, you got to allocate memory for it, blah, blah, blah, blah. So you don\u0026rsquo;t really want to do that. Okay, but conjugation, right? Well, conjugation is weird, because, you know, conjugation actually involves negating, you know, half of the numbers in your tensor. And so strides don\u0026rsquo;t really work for this. And so you\u0026rsquo;re in this weird situation where, oh, well, sex to be me, I have to conjugate the tensor, and actually, you know, create a new tensor. And then I can, I guess I can do the transposed tricks, and then call my, you know, a complex BLAS matrix multiply implementation. But this is a waste, because actually, BLAS provides a fused matrix multiply with a transposed and conjugate on the second argument. And so like, yeah, that\u0026rsquo;s faster, because, you know, it\u0026rsquo;s just faster to have the fused operation. It\u0026rsquo;s why people like using the JIT fuser, right? Like, you\u0026rsquo;re often memory bound in these situations. And, you know, being able to do this fusion is very profitable. So what\u0026rsquo;s a poor person to do? So we hemmed and hawed a bit. And, you know, we talked to some of the experts on, you know, basically doing complex numbers with neural networks, namely Bodecker. And, you know, we talked about a few options, right? Like, one option was, okay, well, we\u0026rsquo;re just gonna, you know, provide a new matrix multiply that, you know, explicitly takes a little keyword argument that says, okay, do you want to conjugate the output? That looks really ugly, right? Like, if you\u0026rsquo;re just writing some math down in PyTorch, you want to just say x, you know, at sign y.h. And you want that to work, right? You want that you want to be able to write code that looks like math. Like, yes, in principle, we can, you know, write lots and lots of fused operations and tell people to, you know, look up, you know, some fused operation for whatever operation they want to do. But they don\u0026rsquo;t want to do that, right? They just want to write math. And then hopefully, you know, some compiler or something, some smarts in your program are good enough to actually, you know, run that efficiently in that situation. So we really want to be able to write, like this operation, and actually have it be fused in the situation. And so the next thing you tend to think about in the situation is, okay, maybe we can do some sort of lazy tensor, right? So I\u0026rsquo;ve talked about lazy tensors a little bit in the past, in this podcast. But once again, what\u0026rsquo;s a lazy tensor? A lazy tensor is like, you don\u0026rsquo;t do the operation immediately, right? You just wait and see if you run some other operation. And then if it\u0026rsquo;s profitable to like fuse in that situation, well, good for you, you were lazy, you didn\u0026rsquo;t do the original operation. And so now you can do the fused operation. But lazy tensors are a little difficult to implement. And one of the things that makes them difficult to implement is that laziness means that operations which are ordinary, ordinarily reads can turn into writes. What do I mean by that? Well, lazy evaluation, you know, as popularized by say Haskell, the functional programming language, means that you guarantee that you only do the operation once. So say you have a tensor, and you request it, you lazily conjugate it, and then you request the value of the conjugation, and no fusion is possible. Under a lazy scheme, you\u0026rsquo;re obligated to actually at this at the point in time you do the read to actually materialize the conjugate tensor, and then go ahead and do the stuff you want to do. And this makes things a little complicated if you want to, you know, be in a multi threaded environment, because okay, well, you\u0026rsquo;re doing a write on a read. So that means that you know, you actually have to start synchronizing your reads. And that\u0026rsquo;s actually kind complicated, blah, blah, blah, blah, blah. Okay, so. And also, it\u0026rsquo;s kind of different from this transposition, right? Transposition was really elegant, you just allocated new tensor with different strides. And then it just implicitly fused. Once you call the function question, namely, you weren\u0026rsquo;t doing lazy evaluation, you were doing call by name evaluation, where you were willing to, you know, do the transposition at every use site of the transposed tensor, if necessary. But like in practice, you know, most things get to be fused in this situation. That\u0026rsquo;s not, that\u0026rsquo;s not entirely true. Like a lot of operators in PyTorch don\u0026rsquo;t support non contiguous outputs. A transposed tensor doesn\u0026rsquo;t count as contiguous output. So they\u0026rsquo;re non contiguous on it, right? They\u0026rsquo;ll transpose it on the spot when they need it. But this is a good trade off for us, because most of the time, you know, a fusion is actually possible in the situation, or, you know, it just doesn\u0026rsquo;t really matter. You know, because, you know, you\u0026rsquo;re only using the transposed tensor once. So whatever, like, you know, delaying it for later, with possibility of duplication is fine. So we want something that works kind of like transpose, but for conjugation. And so conjugate views are a way to make this work. Okay, so how does it work? Well, you\u0026rsquo;ve got your tensor, tensor, and you want to make a new tensor, 01. So you want to share storage, you can\u0026rsquo;t copy storage, because then it wouldn\u0026rsquo;t be constant time anymore. And you want it to somehow represent having done the conjugation. So I\u0026rsquo;m going to cheat. And I\u0026rsquo;m just say, okay, we\u0026rsquo;re going to define another bit field on tensor that says whether or not you should interpret the storage as needing a conjugation or not. So if you have a normal tensor, where in memory, you have three and then four i, and the tensor doesn\u0026rsquo;t have the conjugate bit, then this entry represents three plus four i. But if you do have the conjugate bit set, even though the physical memory says it\u0026rsquo;s three plus four i, you actually interpret it as three minus four i. So okay, so we\u0026rsquo;ve got our 01 tensor allocation, right, you just allocate a new tensor share storage, so the conjugate bit to be one. Now what? Well, you\u0026rsquo;re done. That\u0026rsquo;s it. Okay, it\u0026rsquo;s not as easy as that. So if assuming that every operator knows how to respect the conjugate bit, right, like, basically, like, if you look at the tensor, you need to look at the conjugate bit, it\u0026rsquo;s it says, Oh, if you need to, you know, interpret the code differently, assuming that you have all the operators working this way, then you have, you know, a 01 Hermitian operation, right? You just allocate a new tensor, you swap the tensor, you swap the strides, and you set the conjugate bit. Easy peasy. And as long as all the kernels know how to deal with this conjugate bit, everything\u0026rsquo;s great. Well, making everything actually understand the conjugate bit is kind of difficult, right? Because we have a lot of operators, you know, 1700 plus, and you know, we don\u0026rsquo;t really want to be editing all of our operators to like, you know, pass in, okay, if the input is, you know, conjugated, then please, you know, unconjugated, like actually materialize the conjugation, and do the operation in question, blah, blah, blah, blah, blah. Okay, so that\u0026rsquo;s kind of difficult. So what do we do? Well, we have this nifty feature called a back and fallback. And what a back and fallback does is it lets us say, okay, whenever you see a tensor that has the conjugate bit set, run this special piece of code unless you\u0026rsquo;ve told me otherwise. So it\u0026rsquo;s a fallback, because, you know, you can override the behavior of this. But if there\u0026rsquo;s no override, if there\u0026rsquo;s no actual implementation, we call the fallback in this implementation. And we can use the fallback to implement the okay, well, you know, I\u0026rsquo;ve got a kernel, it doesn\u0026rsquo;t understand how to respect the conjugate bit. So I just have to get rid of all the conjugate bits before I call the kernel in question. And the conjugate fallback will make sure we apply this universally to all functions, even custom registered functions. So like, what does this do, right? Like, so if I\u0026rsquo;ve got a functional operation, and I want to run a operation that doesn\u0026rsquo;t understand conjugation on it? Well, let\u0026rsquo;s see. So you know, I\u0026rsquo;ve got some arguments, some of them have the conjugate bit set, I need to get rid of the conjugate bit. So I just go ahead and conjugate them, producing new inputs, that you know, whose physical memory represents the conjugation. So there\u0026rsquo;s no extra interpretation that needs to be done. And then I just go ahead and call the original kernel. Very easy. The logic is a little more complicated in the in place case, because you know, you can\u0026rsquo;t just, you know, change the conjugate bit on the tensor, there\u0026rsquo;s other tensors that may be aliasing with that storage. You know, the the the conjugation status of it is related to the storage, not the tensor. So you can\u0026rsquo;t just conjugate a tensor in place by flipping the conjugate bit on the tensor, you need to do something to the storage, namely actually conjugate the storage, but you can you can make it all work out. And it\u0026rsquo;s a pretty fun exercise to see how to do it. And then what do you have? Well, you\u0026rsquo;ve got conjugate views, right? You\u0026rsquo;ve got these views of tensors, you know, views in the sense that if you mutate the view, or you mutate the base tensor, the views, all other views to the tensor get updated. So there\u0026rsquo;s got a view, but it\u0026rsquo;s not a view in the traditional sense, it\u0026rsquo;s not a view in just striding or just, you know, swizzling around the data, it\u0026rsquo;s actually a view in terms of some transformation on the data. And this is okay, in this case, because there\u0026rsquo;s an inverse to the conjugate operation. In fact, the conjugate is a self inverse, right, a plus bi to a minus bi to a plus bi. So because it\u0026rsquo;s a self inverse, it\u0026rsquo;s really easy to go through through these things. It\u0026rsquo;s really easy to set up, you know, the bi directional lens. If you\u0026rsquo;re familiar with the functional programming literature, the bi directional lens that says, you know, when you make an update to some view, how to propagate the update back to the original thing, inverses just make this easy. And then we\u0026rsquo;ve got something that like is a view and you know, share storage. It has aliasing semantics, which is one of the reasons why conjugate views are backwards compatibility breaking. So they\u0026rsquo;re kind of an experiment, right? Like, maybe people are actually mutating their tensors after conjugating them and expecting the conjugates to stay the same. I don\u0026rsquo;t know. So that\u0026rsquo;s one of the things we need to work out by putting this in master. But like, you know, if this all works out, you know, we have an actually interesting new tool that we can use in other situations that, you know, allow us to do fusion without having to worry about the, you know, concurrency problems that lazy evaluation give us. So conjugate views, they\u0026rsquo;re not in master yet, I think, but Anjali Chordia has been working hard on actually landing it. She\u0026rsquo;s done most of the work on actually, you know, pushing this to the finish line. And yeah, I hope it is a cool feature and one that will pay off for us in the future. That\u0026rsquo;s all I have to say today. Talk to you next time.\nEP14 Automatic-mixed-precision Automatic-mixed-precision Hello, everyone, and welcome to today\u0026rsquo;s edition of the PyTorch Dev Podcast. Today, I want to talk about how automatic mixed precision is implemented in PyTorch on the request of one of our listeners. Thank you very much. So what is automatic mixed precision? AMP, or automatic mixed precision, or internally referred to as autocasting, is a feature by which when you write your models in PyTorch, we will automatically downcast some of your parameters to lower precisions so that your models can run faster. So what do I mean by that? So imagine that you\u0026rsquo;ve got a bunch of parameters, right? Your parameters are probably floating point numbers, which is the normal thing to do in this situation. And you want to like, you know, do a matrix multiply with the parameters and your input. Ordinarily, you would just do, say, a float, float, matrix multiply, and you know, that would go however so much fast. But you know, NVIDIA being the tricky people they are, they actually have a faster implementation of matrix multiply that happens if you give it a half precision input, and a floating point precision input, half being, you know, a representation of floating point numbers that uses only half the number of bits. And you know, because there\u0026rsquo;s less bits, there\u0026rsquo;s less compute to do. And so if you actually have silicon for it, which NVIDIA GPUs do, it can run faster. So if you pass it in, in this half precision way, your stuff magically gets faster. So that\u0026rsquo;s mixed precision operations. But the automatic part of automatic mixed precision, you actually don\u0026rsquo;t have to do anything to your models to get the benefits. Automatic mixed precision\u0026rsquo;s API is this context manager, you say, okay, turn on AMP, and then magically your modules use mixed precision when it\u0026rsquo;s appropriate. What exactly does AMP do? Well, the heuristic that\u0026rsquo;s applied here is actually pretty simple. Basically, AMP says, okay, when it comes to operations involving parameters, this is the situation where the extra resolution on the parameters tends to not be so useful, right? Like we use floating point 32 bit floating point numbers to represent parameters, because we need to be able to do updates on them. But as far as the computation for the neural network is concerned, most of that precision is not actually used. And so it turns out and you know, this is not obvious, you had to run experiments and show okay, actually, this is profitable. It turns out that you can just cast your floating point parameters into half precision, run your network this way, and it will use less memory, it\u0026rsquo;ll run faster, and it will train just about as well. So Michael Kareli and co at Nvidia actually did an implementation of AMP as part of their apex toolkit, you know, advanced PyTorch extensions. And at some point, you know, mkareli was like talking to me at the PyTorch devcon. And he was like, hey, you know, I want to put this in core, like, how can we do it? And at the time, we had been working on this new dispatcher thing. Yes, I talked about the dispatcher a lot, because my team composability works on dispatcher features, like that\u0026rsquo;s kind of what one of the big things we do. So I was like, oh, you know, there\u0026rsquo;s this interesting new thing called the dispatcher. And I think it gives you enough rope to actually implement automatic mix precision. And you know, we went back and forth a bunch of times with a different few different proposals. But in the end, we have this implementation, implementation of AMP. It works transparently, it has the same API that apex had, namely, context manager, you don\u0026rsquo;t have to know anything about it when you\u0026rsquo;re writing operators, it\u0026rsquo;s a complete extension on top of operator writing. So like, if you\u0026rsquo;re just a plain old fashioned operator, then some normal behavior will happen in that situation. Like, you know, you don\u0026rsquo;t have to worry about it. And that\u0026rsquo;s, that\u0026rsquo;s important too, right? Because not all algorithms have faster mixed precision implementations, like matrix multiply and convolutions, those actually have tensor core algorithms, and they can go faster and have precision, but a lot of things don\u0026rsquo;t. And so, you know, there\u0026rsquo;s no need to deal with them in that case. And then, furthermore, it\u0026rsquo;s extensible in the sense that if you have external libraries, like say torch revision, which doesn\u0026rsquo;t live in PyTorch, they can also be extended to use AMP. And it\u0026rsquo;s all extensible, right? Like sort of AMP is this like capability layered on top of PyTorch. Operators are extra pieces of functionality that are layered on top of PyTorch as well. And the dispatcher lets us, you know, put the square together, we don\u0026rsquo;t have the expression problem, we can actually do the extension in both ways, and then fill in the last corner of the square. Okay, so how does it work? Well, let\u0026rsquo;s remember what AMP wants to do, right? So what AMP wants to do is, when you turn on this mode, when you turn on this context manager, we need to change the behavior of all our operations that know about, you know, AMP, and this will be a fixed set of operations that, you know, heuristically, we know are useful to do AMP things on. And we need to change the behavior to instead of taking parameters directly. We say, Okay, well, I don\u0026rsquo;t want to take this parameter directly, I want to cast it to a half precision, and then run the operation on it. So algorithmically, that\u0026rsquo;s what we want to do. Like, sometimes, when I get an operator, I want to cast things, and then you know, use the cast. And furthermore, like, you know, if this parameter is being used a bunch, I want to cache the cast in this situation. So I\u0026rsquo;m not repeatedly converting it unnecessarily. So how do we go about doing this? So step one is how to actually intercept operations when you want to, you know, when a context manager is being set. And this is actually like the textbook use case of what we call mode dispatch keys. So what is a mode dispatch key, a mode dispatch key is a dispatch key that typically isn\u0026rsquo;t put directly on a tensor itself. But instead is something that gets put into our thread local state that you know, basically, in the dispatcher, we have a thread local state that lets us include dispatch keys and lets us exclude dispatch keys globally, no matter what the tensor inputs are. So to, you know, enable this context manager, the AMP context manager, when you turn it on, says, Okay, put the autocast key into the local TLS that says, Okay, whenever I do operations, I want to include the TLS. And then if you know, the autocast key is not in local TLS, well, then I just bypass all these kernels. The second recipe here that we need to know about is what are we going to do about all the operators that you know, don\u0026rsquo;t know anything about AMP. In this situation, we want to just sort of fall through to the default behavior, we just want to run the normal operation in the situation. So there\u0026rsquo;s another tool in our toolkit in dispatcher. And this is called a fall through kernel. So fall through kernels are kernels that we put in the dispatcher that say, Hey, don\u0026rsquo;t do anything here. And said just fall through to the next valid implementation for the dispatch key in question. And you know, why is there a next valid implementation? Well, all the dispatch keys are ordered in a sense, right? So there\u0026rsquo;s a priority, you do autograd first, then you do the CPU key. And in this ordering, autocast needs to live somewhere. And so, you know, when autocast, you know, when we when we have a kernel, and then we hit autocast, because, hey, you know, autocast mode is on, if that kernel doesn\u0026rsquo;t do anything special for autocast, fall through just says, okay, go to the next key in that case. And most typical autocast kernels are going to go ahead and do some operations, and then also do a redispatch, they\u0026rsquo;re going to say, okay, forget about doing any more autocast stuff, I\u0026rsquo;m done with autocast, go ahead and do whatever the next thing you\u0026rsquo;re going to do was. Cool. And actually fall through is implemented very efficiently, because the way we determine what, what dispatch key to, you know, call into in the dispatcher, is we actually look at a bit set of all the dispatch keys, and we just do a find the first set bit. So when you have a fall through installed for a kernel, we actually just don\u0026rsquo;t set the bit inside this bit field. And you don\u0026rsquo;t actually have to, you know, go ahead and do the dispatch and then realize, oh, there\u0026rsquo;s nothing to do here falls with the next one, it\u0026rsquo;s completely free. So you can always add these fall through keys without paying any cost. Okay, so we\u0026rsquo;ve got a way to intercept all operations when a mode is set using the TLS key. And we have a way of making sure operators don\u0026rsquo;t actually call the AMP kernel if we don\u0026rsquo;t know anything about them. Namely, we have a fall through key, and we register at this fallback, right? So any, anything that, you know, doesn\u0026rsquo;t explicitly have an autocast key just does the fallback. What about the actual implementations of operators that do have fallback keys? Well, it\u0026rsquo;s not too hard, right? So intuitively, you know, we\u0026rsquo;ve gotten all our inputs, and we need to decide, you know, whether or not we\u0026rsquo;re going to cast some of them to have precision. And then eventually, we need to call into the actual operation that is underlying the autocast implementation. So what are the steps to this? Well, you know, the naively, the meat of the algorithm, right, is like looking at an input and deciding if you\u0026rsquo;re going to do it to have precision. Unfortunately, there\u0026rsquo;s no like, cut and dry rule for how to actually decide if half precision is going to be useful or not. We have a few rules of thumb in the dispatcher tutorial, like, you know, matrix multiplies and convolutions are likely to be profitable with half precision. If you\u0026rsquo;re doing reductions, you probably want them at a higher precision because, you know, catastrophic cancellation is more of a problem. But you know, really, really, it\u0026rsquo;s, you know, testing things out and seeing what works well on actual models that you want to run things on. Okay, so let\u0026rsquo;s say that you decided that okay, this parameter should get casted to half precision, if it is a parameter. So we have a helper function that attempts the cache cast. And what it does is it says, okay, you know, is this a parameter? Namely, you know, is it a least lab variable, make sure it\u0026rsquo;s not a view. We actually forgot to put the view check in. And this really resulted in some hilarious bugs, where people were taking views of parameters and loops, and we were continually adding things to the cache. Parameters are good, because there\u0026rsquo;s a fixed number of them, you don\u0026rsquo;t have to worry about there being too many of them. And they stay live for the entirety of the computation. So there, it\u0026rsquo;s usually safe to cache them because the lifetimes line up. Okay, so you look and see if it\u0026rsquo;s a leaf, if it\u0026rsquo;s not a view, and then all you need to do is go ahead and cache, cast it and, you know, put it in a cache. And the cache is just a good old fashioned hash map. And it gets cleared at the end of every training loop, namely when you, you know, exit auto casting. And that\u0026rsquo;s pretty convenient, right? Because at the end of the training loop is when your parameters are likely to update, and therefore when all of the cast entries are likely to be invalid. Okay, so how\u0026rsquo;s that actually implemented in PyTorch? Well, there\u0026rsquo;s a bunch of operators that, you know, do need auto casting support. And actually, you know, the co-union write in this case is very regular. And so at the time that mcarelli was working on auto casting, we still had a lot of bugs in our boxed fallback, the mechanism they talked about in the previous podcast, which we use to implement conjugate views. So that didn\u0026rsquo;t sort of work out. And it was okay, because there\u0026rsquo;s only a fixed number of operators that he really needed. So instead, he just wrote a little template thing, right? So he has this template meta program that takes in the name of the operator, takes them what the type signature of the operator is, and then, you know, constructs a new wrapper function that, you know, does the operations based on some policy, right? Because some functions, we want to cast a half precision, some functions want to stay as float, some functions, you know, if there\u0026rsquo;s a explicit D type, we want to use it. This is just a template that picks apart the arguments, you know, looks through them, checks for parameters, cast them to half precision, then sets a dispatch key guard that says, okay, don\u0026rsquo;t ever go to auto cast anymore, and then redispatches. By the way, on the redispatch, typically the redispatch is going to autograd. And the reason we want redispatch to go to autograd is because autograd is going to save some inputs for backwards. And we would much appreciate it if it saved the half precision inputs, because that\u0026rsquo;s half the memory you\u0026rsquo;re spending saving things for backward. Okay, so you know, we\u0026rsquo;ve got our dispatcher, which lets us, you know, set up this autocast key, that\u0026rsquo;s a mode that only gets, you know, turned on when we need them. We talked about what to do about operators that don\u0026rsquo;t need autocasting. And we talked about what ought to do about operators that need autocasting. And actually, that\u0026rsquo;s it. Like, autocast is a really, really short implementation. There is not very much at all to it at all. And, you know, it\u0026rsquo;s a single file in our code base called autocast.cpp. You can read through it, it\u0026rsquo;s got all the interesting details. Really, the hardest thing is just, you know, figuring out what the policy you should apply on the operations should be. And shortly after we added Autocast to PyTorch Core, you know, Francesco Massa, for example, gave support for AMP and TorchVision. So it\u0026rsquo;s actually fairly well supported even throughout the library ecosystem. AMP was so influential that actually Intel is working on a CPU version of AMP, not for half precision, because there isn\u0026rsquo;t really good silicon for doing half precision on CPUs. But bfloat 16 does pretty good on CPUs, especially when you\u0026rsquo;re vectorizing. So they want a version of automatic mix precision that does bfloat 16 on CPUs. And they\u0026rsquo;re just, you know, modifying the existing CUDA autocasting code to work in this case. So that\u0026rsquo;s how autocasting works. Take your parameters, cast them to half precision, cache that cast, and then, you know, use it throughout. And once again, the way that it is integrated into PyTorch in an orthogonal way is by using the dispatcher, which lets us, you know, layer on extra pieces of functionality that you don\u0026rsquo;t have to care about unless, you know, you actually do want to care about it. And then you can write implementations for it. That\u0026rsquo;s all I wanted to say for today. Talk to you next time.\nEP15 Shared-memory Shared-memory Hello, everyone, and welcome to the PyTorch Dev Podcast. Today, I want to talk about a kind of niche topic, which actually you\u0026rsquo;re probably using, even though you don\u0026rsquo;t know about it, namely shared memory and PyTorch. What is shared memory? Well, let\u0026rsquo;s think about what happens on your computer when you want to run multiple processes. Each process ordinarily has a separate memory address space that is isolated from every other process on your system. And, you know, if you remember how your operating systems class explained it, there\u0026rsquo;s, you know, a very fancy virtual memory system that your operating system implements along with your processor to actually make this possible. So having your processes have separate memory is a really good idea because, you know, you really don\u0026rsquo;t want one process stomping over the memory of another process accidentally. For example, if you have a buggy, you know, Firefox instance, you don\u0026rsquo;t want that to, you know, go into your bank account application. That being said, sometimes it is useful to share some memory between two processes. And your operating system also has a facility for that, and it\u0026rsquo;s called shared memory. Normally, shared memory gets used when you do shared libraries. So what\u0026rsquo;s the idea behind a shared library? Well, the idea behind a shared library is that you have a bunch of libraries on your system that might be used by multiple processes. And it\u0026rsquo;s a waste to actually, you know, have separate copies of exactly the same binary in each of the processes that you want. So, you know, a shared library is designed in a way that, one, it can be put anywhere in your address space, aka it is so-called relocatable, or it has been compiled with position-independent code, FPick, as it\u0026rsquo;s called. And then, you know, using the virtual address table, your operating system only needs to hold one copy of the shared memory in physical memory, and then we\u0026rsquo;ll just, you know, map it to the various virtual address tables of all the processes that are actually using the shared library. So that\u0026rsquo;s a really common use case of shared libraries in Unix-like systems. How about in PyTorch? Well, in PyTorch, shared memory can come in handy when you have a tensor, and you want to share the contents between multiple processes. Now, this is actually, you know, a little bit tricky to do, right? Because if you\u0026rsquo;re wanting to write into the tensor, normally, if you have multiple concurrent, you know, processes or threads working on writing something, you have to do some sort of synchronization. But sort of, you know, one of the glorious things about machine learning is it doesn\u0026rsquo;t really matter if you synchronize or not. So-called hog-wild training methods actually work pretty well, and they just work by sort of, you know, YOLOing the updates without any synchronization, and things sort of just work out in the end by the magic of gradient descent. So PyTorch has support for shared memory so that you can take a contents of a tensor and share it between multiple processes on a single machine. And this is most useful usually because, you know, Python is silly, it\u0026rsquo;s got the global interpreter lock. So if you actually want to do, you know, parallel processing on a single node, you usually need to have multiple processes to like actually max out your CPU, because otherwise, you\u0026rsquo;re only going to be writing Python code on one core. Okay, so what does this look like in PyTorch itself? Well, there\u0026rsquo;s a few things that, you know, you have to know about shared memory that like lead to a bunch of things that PyTorch does to sort of make this a seamless experience. So one thing is that shared memory on your operating system is not reference counted. In fact, once you create some shared memory, it will stay there indefinitely until someone explicitly decides they\u0026rsquo;re going to get rid of it. And this kind of makes sense because, you know, shared memory is often represented as a file in a special dev shim mount point on your operating system, like slash dev slash shim. And, you know, of course, files, files don\u0026rsquo;t go away unless you actually RM them. And so this leads to a problem, which is that, you know, let\u0026rsquo;s say that, you know, I allocate some shared memory. Well, I need to get rid of it when I\u0026rsquo;m done with it. Otherwise, it\u0026rsquo;s going to hang around until the end of my, you know, operating systems, until it reboots or something like that. So you could imagine setting up your process so that, you know, if the process, you know, is shutting down, then it can deallocate all the shared memory. But this works out poorly if your process, for example, crashes for whatever reason, and none of the destructors run in that case. So actually PyTorch solves this problem by providing a sort of watchdog process. This is the shim manager, the shared memory manager. And what the shared memory manager does is, you know, when we start using shared memory inside PyTorch, we spawn off a daemonized version of this watchdog process, whose only job in life is to watch the relevant processes that, you know, are associated with this PyTorch instance. And when all of them are dead, clear all the shared memory in question that it has been told about. So in this particular case, the shared memory watchdog process is much smaller. It\u0026rsquo;s not running custom user code. It\u0026rsquo;s just getting signals from the processes when shared memory is being allocated, and when it\u0026rsquo;s being deallocated. So it\u0026rsquo;s much less likely to accidentally crash due to a bug. And, you know, it\u0026rsquo;s a way we can make sure shared memory actually, you know, gets preserved in this way. Okay, what are some other things that we need to do to make shared memory work out? Well, another thing we need to do is we need to actually, you know, back our tensors with the shared memory in question. So how does that work? Well, you know, we have a representation for tensor and, you know, inside the tensor is a data pointer that points to some data. And we represent this internally via a data pointer class, which sort of says, hey, here\u0026rsquo;s where the data is. And also here\u0026rsquo;s where to deallocate. Here\u0026rsquo;s how to deallocate it. And so the fact that the deallocator for memory stored by tensors is actually, you know, user programmable means that you can actually override, you know, where things come from. So if you\u0026rsquo;re just doing a normal tensor allocation, you just say, okay, I want the stock CPU allocator. And that gives me a data pointer that says, okay, to free this memory, just free it in the normal way. But if you\u0026rsquo;re doing shared memory, and you want to like pass it around with another process, then you can use a different allocator, which says, okay, please allocate this shared memory for me. And when it\u0026rsquo;s done, deallocate it by, you know, both deallocating the shared memory in whatever special way it needs to be. And also sending a message to the shared memory manager to say, okay, well, I\u0026rsquo;m done with this shared memory, you don\u0026rsquo;t have to worry about it anymore. And so in fact, the way we implement shared memory in PyTorch is there\u0026rsquo;s actually a few allocators. So there\u0026rsquo;s a th map allocator, which says, okay, I\u0026rsquo;m just going to give you some shared memory, and then I\u0026rsquo;m going to get rid of it, you know, unmap it the normal way when you\u0026rsquo;re done with it. There\u0026rsquo;s also a ref counted shared memory allocator, which says, okay, well, you can give me this shared memory, and I\u0026rsquo;ll actually keep track of it via a ref count that is distributed over all PyTorch processes. So, you know, if I have multiple PyTorch processes that are referring to this shared memory, I won\u0026rsquo;t deallocate it until the distributed ref count goes to zero. And so once again, you know, what does the deallocator in this case do? Well, it just says, okay, well, when you\u0026rsquo;re done, you know, decrement the distributed ref count, and then also check if the distributed ref count has gone to zero. If so, free the shared memory. By the way, how the ref counts are stored, also shared memory. And you know, it\u0026rsquo;s just the easiest way to implement this sort of thing. And of course, the the managed shared memory allocator is the one that knows about the shared memory manager. And that one does the stock behavior, but also talks to the shared memory manager to get things done. Okay, so that\u0026rsquo;s it about shared memory on CPU. But it turns out that we also support shared memory on CUDA. And the way we do that is sort of very similar CUDA API provides a way of taking some arbitrary CUDA memory, and then saying, okay, create a opaque handle, some byte string that when passed to another process can be used to get another CUDA handle to the memory in question. And so this way, you can also share CUDA memory across multiple processes. How convenient. However, CUDA shared memory works a little differently than CPU shared memory. Unlike CPU shared memory, where you know, if once you allocate it, it just stays live until you explicitly delete it. CUDA memory only stays live as long as the host process actually keeps the CUDA memory alive. And so for the longest time in PyTorch, we had this restriction that, you know, when you have some CUDA shared memory, you must make sure that the CUDA shared memory stays live in the originating process long enough for all the consumer processes to be done using it. Otherwise, very strange things will happen. And, you know, these strange things include, you know, like it being overridden with total garbage, because remember, we have a caching allocator. So we don\u0026rsquo;t actually CUDA malloc and CUDA free every time you allocate CUDA memory. We, you know, allocate a big chunk of CUDA memory, and then maybe sometime in the future, you know, we reuse it for something else. So if someone else is still referring to, you know, some CUDA IBC memory that, you know, we decided was unused in the host side, and then reused to something else, they\u0026rsquo;ll see it actually get overridden with some random data from the next allocation. So that was a kind of foot gun. And when Vitaly Fedunin joined the PyTorch project, one of the first things that he implemented was distributed ref counting for CUDA IPC tensors as well. And it works kind of similar to how CPU ref counting works, right? So there is a, you know, shared memory file, hey, you know, shared memory once again, that maintains the distributed ref count. And then there\u0026rsquo;s just a sort of polling mechanism on the host side, which just looks and sees, has the, you know, ref count gone to zero? Has the ref count gone to zero? Oh, the ref count\u0026rsquo;s gone to zero. Now I can release the tensor. There were a bunch of different possibilities we had for how to go about doing this. But polling was the sort of simplest implement. Okay, so shared memory is a way to share memory between multiple processes in your system. It\u0026rsquo;s not so useful if you\u0026rsquo;re doing multi-node training, but because Python has a GIL, it\u0026rsquo;s pretty useful if you\u0026rsquo;re using a single node and you just need multiple processes to parallelize. You probably are using shared memory if you\u0026rsquo;re using Torch multi-processing. And there\u0026rsquo;s just a few things that PyTorch does to make this work out nicely. But, you know, mostly we\u0026rsquo;re just relying on, you know, mmap support for shared memory files. So that\u0026rsquo;s all I wanted to say today. Talk to you next time.\nEP16 Stacked-diffs-and-ghstack Stacked-diffs-and-ghstack Hello everyone and welcome to the PyTorch Dev Podcast. Today I want to talk about something a little different, namely instead of talking about PyTorch itself, I want to talk about one of the tools that we use to help develop PyTorch, namely GHStack. GHStack solves a problem that goes something like this. Imagine that you\u0026rsquo;re working on some code in your project and you know you go hack, hack, hack, type, type, type, and you\u0026rsquo;ve got a working implementation and you send it up for review as a pull request on GitHub. And while you\u0026rsquo;re waiting for people to come and actually review your beautiful code, you\u0026rsquo;re like, okay, well, I\u0026rsquo;d kind of like to start working on the next feature, which is going to build on top of this patch that I did before. Maybe this patch was some refactoring or, you know, a little bit of infrastructure that you needed for the next thing you were going to work on. So, okay, back to your local development copy, you hack, hack, hack, work on your next, you know, piece of the, piece of the puzzle, and maybe you\u0026rsquo;re now done with that piece as well. Now what do you do? Well, imagine that, you know, your first PR still hasn\u0026rsquo;t been reviewed or maybe it has been reviewed, but it still hasn\u0026rsquo;t landed to PyTorch because lands on PyTorch take a really long time. Don\u0026rsquo;t ask. It just, it takes a really long time. So, you know, you\u0026rsquo;ve got the second poll request and, um, sorry, you\u0026rsquo;ve got the second patch on top of your first patch. It\u0026rsquo;s logically independent. So like, you know, it can be reviewed in isolation from the first patch, but you\u0026rsquo;d kind of like to put it out there and let people take a look at it. How can you do this? Well, GitHub doesn\u0026rsquo;t really make it easy for you to handle a situation like this because, you know, poll requests are, hey, here\u0026rsquo;s a branch and then here\u0026rsquo;s a master I\u0026rsquo;m going to compare against it. And then that\u0026rsquo;s what the diff is going to be. So there\u0026rsquo;s no easy, like built-in workflow for submitting extra patches on top of each other for review without, you know, necessarily forcing someone to review all the code from your previous patch as well. So GH stack implements what we call stack diff development, and it solves this problem, namely by allowing you to create pull requests that, um, are stacked on each other so that, you know, you can submit your first pull request, then you can submit your next pull request, which depends on the first pull request. And if someone wants to, you know, uh, review, they can review each of these pull requests separately, but the second pull request still has all the changes from the first pull request. So you can build on top of your work. Stack diff development is not really an invention of the PyTorch project. Um, you know, a lot of people have used it before in particular, if you use the code review tool called fabricator, um, developed by Facebook originally, um, that also implements the stack diff model. And really GitHub is a little behind the times and still not supporting stack diffs. I heard that they\u0026rsquo;ve got some feature development in the works for supporting this workflow, but right now it doesn\u0026rsquo;t work. And so, um, you know, we have tools like GH stack to make this easier for us. Okay. So how do you use GH stack? Well, let\u0026rsquo;s imagine that, you know, you\u0026rsquo;re doing this story that I just told you before, which is you hacked on some feature a, and then you hacked on another feature B, which depended on feature a. So normally, uh, you know, well, everyone uses get a little differently. And so like one workflow that you might do is you say, okay, well, I\u0026rsquo;m just going to just keep, you know, committing stuff like edit. Okay. Rework, you know, foo bar until, you know, you get to the end. Right. And then you like do a bunch of extra commits on top. And then you push all those commits to the pull request to be, um, you know, reviewed. And then, you know, adding new updates, isn\u0026rsquo;t so hard, right? You just make a new change, you commit it, and then you push it to your pull request. So GH stack requires you to work a little differently. Instead of maintaining a blow by blow commit history of every change you made, instead, GH stack wants you to create a single commit per, um, per logical change that you want to submit to fabricator. So let\u0026rsquo;s say, sorry, submit to GitHub. So let\u0026rsquo;s say that, you know, you\u0026rsquo;ve got three changes, right? Two like refactors that are independent of each other. And then a feature implementation, you structure these so that you have a commit one, which is refactor one, a commit two, which is refactor two, and then a commit three, that is, you know, the actual feature of request in question. And then once you have these three commits, and they\u0026rsquo;re all ready to go, you run GH stack. Well, if you need to install it, you pip install GH stack, and then you run GH stack. And what GH stack will do is it\u0026rsquo;ll look for every commit that, you know, is off of your branch from master, and it\u0026rsquo;ll create a pull request for each one. So if you had three commits, it\u0026rsquo;ll create three pull requests. Then when you want to make changes to the pull request later, you just go ahead and amend or interactively rebase them. By the way, about interactive rebasing, interactive rebasing might sound, you know, tricky and complicated if you\u0026rsquo;ve never done this sort of thing in Git before, but it\u0026rsquo;s actually very easy to use. And the way an interactive rebase works is you write git rebase dash I, and Git will give you a list of all the commits that you\u0026rsquo;ve made on top of master. And then all you need to do is say, okay, well, this is the commit that I want to edit. And this is the commit that I don\u0026rsquo;t care about. And so, you know, you say edit, and then Git rebase will drop you into a working tree with only, you know, the commit that you want to edit. And so you can go ahead and edit it, amend the commit, and then continue your base further on. So the way I tend to do interactive rebases is that I, you know, first I work on my patches, like, you know, okay, patch one, I\u0026rsquo;m done, commit it, work on patch two, I\u0026rsquo;m done, commit it, and so forth. And once I get to the end, usually what I do is, if I notice that I need to fix up on commit one, and it\u0026rsquo;s a small one, I\u0026rsquo;ll just make a little edit at the very top, I\u0026rsquo;ll commit it, so that I have a separate fix up commit, you know, standing on its own, I don\u0026rsquo;t run gh stack yet, instead, I, you know, run my build, make sure everything works. And then I do an interactive rebase to move my fix up commit to the commit that it actually logically belongs to, and then amend it in using the so called fix up option in the rebase option. And so this, you know, makes it easy for me to keep track of all the changes that I want to do, you have to make sure not to like overly merge conflict with yourself when you\u0026rsquo;re doing this kind of thing. But it gets easier with practice. So anyway, so that\u0026rsquo;s it, right? So you\u0026rsquo;ve got these three commits, and then Git gives you some tools for modifying, you know, commits in the middle of the stack. And I mostly try not to like make modifications. And you know, it\u0026rsquo;s mostly a way of me letting letting myself get ahead of myself when I\u0026rsquo;m working on code. By the way, in fabricator and mercurial land, there\u0026rsquo;s actually support for actually going backwards and forwards in history using the hg prev and hg next commands. So this is actually a much better user interface than Git. Haha, sorry, Git. Well, Git\u0026rsquo;s user interface is famously bad. So it\u0026rsquo;s not surprising if I\u0026rsquo;m bad mouthing it. So like if you wanted to amend your commit, instead of amend a previous commit, one that wasn\u0026rsquo;t at the top of your stack, instead of having to do the GitHub thing of setting up an interactive rebase, or like making a fix up commit and then moving to the right place, all you have to do in mercurial is say hg prev, and that\u0026rsquo;ll put you in the previous commit, you can go ahead and modify it. And then mercurial, if you have enough extensions installed, will automatically restack all of your later commits on top of this one. This is very convenient. And I like it better. If you want to try to like replicate this developer experiments in Git, there are some quilt tools, apparently, I\u0026rsquo;ve never used any of them. But I think they\u0026rsquo;re trying to do something similar. So anyway, so you\u0026rsquo;ve got the stack of diffs, right? So you\u0026rsquo;ve got a stack of commits, you\u0026rsquo;re on g stack on them, they all get posted to GitHub. And that\u0026rsquo;s it, edit them, and then g stack again to, you know, put some more things on. And for example, if you need to update to the latest version of master, you just need to use a non interactive rebase in this case. So, you know, you got your commits, and you say get rebase, you know, master origin master, if you\u0026rsquo;re, you know, just get fetching, like I normally do. And then I\u0026rsquo;ll just move all your commits over. And of course, you might have to resolve some merge conflicts. But you know, it\u0026rsquo;s, it\u0026rsquo;s, it\u0026rsquo;s pretty straightforward. And you know, not much more difficult than merging. One downside to rebasing in this way, is you have to resolve merge conflicts for each commit individually. Unlike a merge commit, where you just do everything all at once. This makes sense, because you know, when we actually land a stack of disks from g stack, we will land each commit separately. So they will show up durably in the final GitHub history. That\u0026rsquo;s good for us, because, you know, you went through all the trouble of making sure CI was passing on every commit. So we will go through the trouble of making sure we preserve history in the situation. Okay, so that\u0026rsquo;s basically how gh stack works. You can get it once again, by pip installing gh stack. There is one caveat, though, which is that in order for gh stack to work, you need push permissions to the pytorch pytorch repository. So most people just, you know, fork pytorch and push their stuff there. And unfortunately, gh stack doesn\u0026rsquo;t work because the way it works is that we create a bunch of branches representing what you\u0026rsquo;re trying to merge into, and then what your actual commits are, right. And the what you\u0026rsquo;re going to merge into branch has to actually live on pytorch pytorch, because if it doesn\u0026rsquo;t, when you open a pull request, you\u0026rsquo;ll open the pull request in your fork and not in pytorch itself. Okay, well, can I get right permissions? Well, if you\u0026rsquo;re working on some feature that, you know, might be useful for stacking, and you, you know, have talked to someone on the pytorch team about it, like say, on an issue, you know, you can just ask for right commits. And we basically give write commits, write access to anyone who asks for it, because you can\u0026rsquo;t actually write to master directly. There\u0026rsquo;s some complicated process by which commits are sucked into our internal build system in FV code, and then spit it back out via this piece of software called ship it. So you can\u0026rsquo;t touch the master branch, you can just create temporary branches. And so if you need to use gh stack to organize one of your PRs, just ask someone and we\u0026rsquo;ll add you to the project. Okay, so what are some things to know about when using gh stack? Well, one thing is that when you rebase your gh stack, or you make a modification to a commit that\u0026rsquo;s very early to the gh stack, we will push an update to every subsequent PR in your stack. So please use this with care, right? Like each PR you push will trigger off a full CI run for everything in PyTorch. Our CI runs are not that cheap. So you know, like try to be nice and don\u0026rsquo;t, you know, repeatedly repush your stacks when you know, you know, oh, yeah, I just need a little bit of modification here, maybe defer that till later, once you finished all of your modifications, and then push the gh stack all at once. Another common thing to do is you\u0026rsquo;ve got your gh stack, and you\u0026rsquo;re just working on the latest commit, as long as you don\u0026rsquo;t rebase it onto master, you can safely gh stack in this case, and it\u0026rsquo;ll only push the, you know, latest commit that you modified in that situation. What are some other things to know about gh stack? Well, gh stack is also an open source project. It lives on GitHub at easyang slash gh stack. Yeah, I sort of wrote this tool, because I was so mad at having to deal with not being able to do stack diffs on GitHub. So I wrote it just to solve my own problem. And it\u0026rsquo;s, you know, it\u0026rsquo;s not very much code. And you know, you can also check that out and use it on your own projects. gh stack supports other repositories, you just have to use a special command to land gh stack diffs, because the normal merge to GitHub button doesn\u0026rsquo;t work. For PyTorch, PyTorch, this doesn\u0026rsquo;t matter, because the normal merge to GitHub button doesn\u0026rsquo;t work for completely unrelated reasons related to the ship it situation. That\u0026rsquo;s something you have to know about there. Okay, so gh stack, it lets you do stack development. Stack diff development is really good. It lets you move ahead on what you\u0026rsquo;re working on without blocking on code review, or your code actually landing to master. It makes me a lot more productive. And maybe it will make you a lot more productive if you\u0026rsquo;re not just working on one off patches in PyTorch. So give it a try. That\u0026rsquo;s all I wanted to say. Talk to you next time.\nEP17 Continuous-integration Continuous-integration Hello, everyone, and welcome to the PyTorch Dev Podcast. Today, I want to talk about the continuous integration service that runs on all your pull requests in PyTorch. This service has sort of been built over many years and has gone through various different versions and is probably going to change some more in the future as well. So what\u0026rsquo;s up with that? It\u0026rsquo;s also really complicated because we test in a lot of configurations. So what\u0026rsquo;s up with that? And how can I understand how the CI works? Well, it\u0026rsquo;s not too bad because there\u0026rsquo;s a few very important constraints that went into the building of the CI. And if you understand those, you\u0026rsquo;ll kind of understand why things are set up the way they are. Okay, so let\u0026rsquo;s talk a little history first. So what did PyTorch\u0026rsquo;s CI look like at the very beginning? Well, at the very beginning, there were only really like four developers working on PyTorch. And we were running all of our CI on Travis, you know, for example, because that was what everyone used at the time. And we had a problem. And the problem was that PyTorch is a GPU accelerated library, we needed a way to actually run our code on GPUs. And none of the CI services actually made it possible to do this. Okay, so what do we do in this case? Well, we did what any, you know, enterprising hacker would do. Sumith set up a desktop box in his apartment with a GPU in it. And we set up a gen consistence to like, go ahead and run our GPU tests on that single box. And when you only have four developers on a project, this kind of works okay. And we added a few more developers. And you know, I took a box home to my apartment. And you know, we had two GPU boxes. But this clearly wasn\u0026rsquo;t sustainable, right? Like the PyTorch project, even back then, it was growing, we were getting more and more pull requests. And our, our pool, our, our, our backlog for the GPU runners was getting more and more backlogged. So Peter Nordhuis was sort of looking around for something to do at the time. And he was like, Okay, I want to build a new CI system for PyTorch. And so he was like, Okay, well, we need to be able to run GPUs. And we need to be able to scale so that it\u0026rsquo;s not just, you know, two GPUs and people\u0026rsquo;s basement in their apartment. Imagine having a basement in their apartment. And so how are we going to do this? Well, once again, because none of the CI providers provided this, we needed to just build it on top of AWS. So we did, we built an auto scaling Jenkins, you know, fleet of machines that, you know, could run GPU and CPU jobs. Fortunately for us, AWS would sell us GPU machines. In fact, it would even sell us Windows GPU machines. The only thing it wouldn\u0026rsquo;t sell us were OSX machines, because Apple is a thing. So we actually just bought a bunch of fixed runners from Mac Stadium to get that going. All right, so you know, that\u0026rsquo;s sort of the first iteration of the CI system. And you know, we\u0026rsquo;re going through a bunch more iterations. So at some point in the past, we migrated to CircleCI, when you know, that was about the time when, you know, CI providers who you could pay money to actually started supporting GPUs. And so you know, we helped CircleCI get their GPU support up and going. And now we\u0026rsquo;re kind of looking at moving again to GitHub actions, because GitHub actions is just really well integrated with GitHub. And we like that a lot. Okay, so that\u0026rsquo;s enough of history of the CI. So once we, you know, upgraded from just randomly running people things on people\u0026rsquo;s machines, to actually running things in CI, we also made some key design choices that sort of has stayed with the CI system today, even though we\u0026rsquo;ve migrated from one system to another and probably are going to be migrating again. So the first big decision we made was, hey, GPU machines are really expensive. So we don\u0026rsquo;t really want to spend time running code on GPU machines when it\u0026rsquo;s not necessary. And in particular, the most time consuming thing that is totally useless to run on a GPU is building PyTorch. Of course, you know, normally, you wouldn\u0026rsquo;t build a GPU enabled version of PyTorch on a computer that doesn\u0026rsquo;t have a GPU, because it would be kind of pointless normally, but you can do it. And if you\u0026rsquo;re, for example, building binaries, you know, you can always set up the binary build, and then send it off to another machine to actually run on, which does have a GPU. And so that\u0026rsquo;s how we set things up in the CI as well. When you run a GPU job in our CI, we don\u0026rsquo;t build it on a GPU, we build it on a CPU that has CUDA installed, but you can\u0026rsquo;t actually run anything. Once we\u0026rsquo;re done building, we actually go ahead and send it over the wire to the GPU executor through various mechanisms. Right now we send them via an ECR registry that\u0026rsquo;s in AWS, but there are a bunch of ways you could do it. And then only then we run the tests which do require GPUs. And that\u0026rsquo;s how we actually do the testing in this case, right? So GPUs are really, really expensive. They\u0026rsquo;re like 10x more expensive on AWS. Is it 10x? It\u0026rsquo;s like an order of magnitude more expensive. And they\u0026rsquo;re also more expensive on CircleCI as well. So it just makes sense to reduce the amount of time you\u0026rsquo;re running on them. Another major constraint that we had is, you know, hey, PyTorch is a really popular project. And people want to run their PyTorch programs in a ton of different situations, right? Like they don\u0026rsquo;t just want to run them on Linux, they want to run it on Windows, and they want to run it on OSX, and they want to run it on, you know, various different Linux distributions, and, you know, various different versions of Python. And so, you know, we offer to support all of these configurations. And this is kind of trouble for a CI setup, because, you know, these configurations are actually really, really, you know, complicated. Sometimes there are a lot of different moving parts. Did you know that we actually test PyTorch under different parallelization primitives? So normally, we use OpenMP, but we also support TBB, which is Intel\u0026rsquo;s Thread Building Blocks Library. And so that\u0026rsquo;s another configuration separate from OpenMP that we test under. And so making sure all the like prerequisite software is installed for all these cases can be a bit of a chore, and, you know, is wasted time, once again, if you\u0026rsquo;re doing it at CI time. So what we did was instead, we said, okay, we\u0026rsquo;re going to make a Docker image for every environment we actually want to run our CI in. And then, you know, these Docker images will just, you know, basically have all of the software you need pre-installed at the correct versions for the particular run of the CI. And so, you know, for example, when we needed to, like, figure out how to move things from a CPU machine to a GPU machine to actually run it, we actually just, you know, move the entire Docker container because that was convenient. Okay, so, you know, we use Docker to actually, you know, maintain each of the environments. And this is really convenient. And it works pretty well on Linux. Yeah, Windows and OS X, we don\u0026rsquo;t really use Docker on, but we also only really test in one configuration in these situations, because it\u0026rsquo;s kind of too hard to do it. Okay, so we use Docker for this purpose. By the way, because we use Docker for this purpose, if you\u0026rsquo;re, like, trying to debug a particular Linux failure in our CI, hypothetically, you can download the Docker image that we ran the CI in and run your code in exactly that environment. And I used to do this a bunch when I was testing very strange bugs. But it\u0026rsquo;s a little inconvenient to do. You actually need some credentials to actually access the ECR because Amazon doesn\u0026rsquo;t support, you know, passwordless ECR authentication, if you actually need it, feel like you need it, just ping someone on Slack, and they\u0026rsquo;ll be happy to give you the credentials to access the images there. So, so what have I said so far? So GPUs are really expensive. So don\u0026rsquo;t run things on GPU if you can. And, you know, we also need to run under a lot of different configurations. So we use Docker to manage these different configurations. What else? So the last constraint that I want to talk about is more of a, like, anti-constraint, in the sense that we didn\u0026rsquo;t, like, explicitly go in to engineering the system with this constraint. But it sort of just naturally happens if you don\u0026rsquo;t do anything else. And what this constraint is, is RCI doesn\u0026rsquo;t rely on any external servers. Okay, so what do I mean by this? Well, let me talk about one particular feature that we built into RCI. So one of the things that sometimes happens is someone breaks a test. And when the test is broken, you either have to revert the PLO request, or you have to, you know, put a patch in. And these, both of these remediations can be somewhat slow. Because when we have a ton of, because landing divs to PyTorch is actually kind of slow, since we have to run all of Facebook\u0026rsquo;s internal CI before it\u0026rsquo;s all okay to go. So we wanted a way to actually make it so that we could avoid running tests if, you know, someone, sorry, we wanted to fit faster escape valve to turn off test running if we knew that something was wrong, but we didn\u0026rsquo;t want to revert in this case. So Zachary DeVito wrote a little thing to make this happen. And so how do we do it? Well, one way you can imagine doing it is you set up a server that just says, okay, here are the tests that are known to be okay, here are the tests that are known to be bad. And then just make sure the CI service pings the server whenever you want to know, you know, which test should I skip? Because, you know, we know there\u0026rsquo;s a problem on master. Okay, we didn\u0026rsquo;t do that, right? Because to do that, we would actually have to like design a service and bring it up available to the public internet. And, you know, do all the things necessary to actually run the service. So you can see why this is an anti constraint, right? Which is that, you know, if people don\u0026rsquo;t want to run servers, then they will try very hard not to run servers. And so the way it\u0026rsquo;s actually implemented is using, you know, Facebook\u0026rsquo;s internal cron job infrastructure, because, hey, you know, Facebook has a bunch of, you know, services, once again, that are not publicly internet accessible, because, you know, that would be a security risk. We piggyback off of the cron job service to publish a file to S3, which once again, is a service that we don\u0026rsquo;t run, right? It\u0026rsquo;s a service run by Amazon. And that file gets downloaded when you do testing. And that tells you whether or not, whether or not a test should be skipped or not, right? So this is a sort of like Rube Goldberg contraption, whereby you don\u0026rsquo;t do the obvious thing. Instead, you do the thing that, you know, reduces the requirement for needing us to actually run a service to get things going. Another example is the CI status HUD. So if you don\u0026rsquo;t know about the HUD, it\u0026rsquo;s a little react app that basically reads out the information about CI signal for all of our configurations, and displays it in a very compact form. So it\u0026rsquo;s easy to see if any particular job has failed. So once again, this job was set up without needing, so like normally, you\u0026rsquo;re like, okay, well, I should set up some sort of service, the service will have a database, the database contains all the statuses. And you know, I\u0026rsquo;ll just render it from that database. Well, that\u0026rsquo;s not how this app works. Instead, the app is just a pure React app, there is no back end service associated with it at all. Instead, what it does is it queries Jenkins to get the list of recent jobs via an APC, just an RPC call, you know, with cores protection, sorry, cores enabled so that we can actually read the Jenkins data. And also, you know, reads out a bunch of GitHub statuses that we actually just stashed once again in S3, and then it renders that. So there\u0026rsquo;s no server, there\u0026rsquo;s no database involved here. We\u0026rsquo;re just piggybacking a bunch off of other infrastructure. So recently, we\u0026rsquo;ve been adding more support for actually putting services behind things. It\u0026rsquo;s slow going, right, because we have to make sure it\u0026rsquo;s all secure and, you know, actually make sure we administrate the systems. But you know, we\u0026rsquo;re getting there. But a lot of things that the CI works on, you know, are sort of done in the circuitous way to make things work out. Okay, so enough about constraints on the CI. What does the CI actually look like today? Well, as I said, we run a lot of stuff in a lot of different configurations. And, you know, actually, it\u0026rsquo;s sort of infeasible for us to test every combinatorial many configuration that we want to do. So what we do is like, there\u0026rsquo;s usually something weird about some particular job. And sorry, there\u0026rsquo;s usually something weird about some particular configuration we want to test whether or not it\u0026rsquo;s, oh, it\u0026rsquo;s rock \u0026rsquo;em, or, oh, it\u0026rsquo;s, you know, with ASAN turned on, we pick one particular config to, like, put that weirdness onto. And the hope is that, you know, we can, you know, we, we, there, the errors won\u0026rsquo;t be correlated. So if something fails on ASAN, it\u0026rsquo;ll fail regardless of what your Python version is or what your Linux distribution is. So we have a bunch of builds, but like we sort of like packed each of the configurations we want to test into them. What do these configurations look like? Well, I\u0026rsquo;ve already told you we support Linux, OSX, and Windows. Some other things that we need to test, we test with CUDA. We also test without CUDA. And we also test a CUDA build of PyTorch, but run on a machine that doesn\u0026rsquo;t have any GPUs. This is something that we used to break all the time because, you know, it\u0026rsquo;s subtly different. And if you make assumptions in the CUDA build of PyTorch that a GPU is always available, then this binary won\u0026rsquo;t be usable on CPU. So that\u0026rsquo;s why we have that build. We build for various different versions of Python, because our support window for Python is the most three recent versions of Python. And yes, there are relevant backwards incompatibilities in Python that we need to test for, especially in Python surface syntax, because, you know, like, for example, F strings, we couldn\u0026rsquo;t use F strings until we dropped support for like Python 3.6, I think. So, you know, we needed to make sure people didn\u0026rsquo;t actually add in features that were too new. What other things do you test? We have an ASAN configuration. ASAN only gets run on one build because it\u0026rsquo;s really, really slow to run ASAN code. And we also have some other configurations like Rockm GPUs. Actually, the Rockm GPU configuration still lives on Jenkins, because CircleCI doesn\u0026rsquo;t actually have any machines with AMD GPUs on them. So we have to run it ourselves. Actually, AMD has a data center full of servers with AMD GPUs, and they\u0026rsquo;ve graciously loaned it to us to run our CI there. Another weird CI configuration is XLA. So what makes XLA weird is that it is actually two repositories we\u0026rsquo;re doing CI on: the PyTorch repository and also the XLA repository. And so whenever you run the XLA build, we always take the latest version of the XLA repository and do that. This is kind of like bad practice, right? Like what you\u0026rsquo;re supposed to do in CIs, you\u0026rsquo;re supposed to pin versions. But XLA, you know, is constantly adding fixes, and they don\u0026rsquo;t want to have to coordinate with the main PyTorch repository. And so we worked out this compromise, whereby XLA is very responsive. If you make a change to PyTorch, and it needs an XLA change, they\u0026rsquo;ll set up a PR that fixes it for you. And then once you land your diff, they\u0026rsquo;ll just go ahead and land it straight to XLA. So the breakage on XLA is very small. And this is kind of worked out okay, because most diffs don\u0026rsquo;t break XLA. And so you know, like you don\u0026rsquo;t have to worry. But oftentimes, yeah, if you see XLA is failing, that\u0026rsquo;s probably because you know, something got landed in master and XLA just needs to catch up. So that\u0026rsquo;s it about open source configurations. We also run CI inside Facebook, and Facebook CI, you know, sort of mostly tracks open source here, like if something fails in open source, sorry, if something fails in Facebook CI, usually it means something failed in open source CI. But there\u0026rsquo;s a few cases where this is not true. One case it\u0026rsquo;s not true is if you\u0026rsquo;re making build system changes, like you add a new file, you add a new directory, you made CMake changes, Facebook has an internal different build system based on buck. So usually someone is going to have to go and fix that change for you. Another thing that is pretty unusual is the internal builds much more aggressively build on mobile platforms, we have some mobile open source builds, which are also kind of weird, and you know, worth knowing about. But Facebook\u0026rsquo;s mobile builds are also kind of weird and interesting. And so that\u0026rsquo;s another situation where you might have a error that you know, doesn\u0026rsquo;t show up on open source. But we try very, very hard to make sure that you can get all the signal in open source, because otherwise, you\u0026rsquo;re gonna have to go through, you know, very long round trips with a Facebook employee to like figure out what the problems are. Okay, so that\u0026rsquo;s everything I wanted to say about our CI, right? Like, so what is our CI? It, you know, tries to make sure we don\u0026rsquo;t build things on GPUs, it makes sure that it is scalable, because we want to, you know, scale with the team. We use Docker to manage all of our build configurations. And historically, we don\u0026rsquo;t really run any extra services, although this is changing over time, especially with the work that say Taylor Roby is doing to do better performance tracking. So that\u0026rsquo;s all I have to say for today. Talk to you next time.\nEP18 Serialization Serialization Hello everyone and welcome to the PyTorch Dev Podcast. Today I want to talk about a somewhat dry but still very important topic to PyTorch, namely serialization. Serialization is the mechanism by which when you have a PyTorch program and you have some tensors floating around or god forbid a more complicated program such as a PyTorch module or a Tor script module, it allows you to serialize this data to disk so that you can load it up again when you do another run. So in any sort of you know usual PyTorch program you are probably making heavy use of serialization because you\u0026rsquo;re doing things like you know doing your training loop and then saving your trained weights to disk so you know because like you actually want to use them later for something for example. Okay so how does serialization work? Well it\u0026rsquo;s a long story. So I think the way easiest way to understand where PyTorch is with serialization is today is we\u0026rsquo;re going to first talk about how serialization works in general in Python and then we\u0026rsquo;ll talk about how historically PyTorch did serialization based off of this and then we\u0026rsquo;ll talk about the new developments namely JIT zip file based serialization which is what more recent versions of PyTorch are using by default when you do torch.save. All right ready? Let\u0026rsquo;s dive in. So instead of answering the question how does PyTorch do serialization? Let\u0026rsquo;s ask an easier question which is how do you do object serialization in Python? And the answer to this is well there are a bunch of ways to do things but there\u0026rsquo;s one that is very popular and a lot of people use namely Pickle. Pickle is a protocol and file format for doing arbitrary Python object serialization in Python. So like if you have some class or you\u0026rsquo;ve got some object you\u0026rsquo;ve got some numbers you\u0026rsquo;ve got a list whatever you can run it through Pickle and Pickle will give you a byte stream that you can put on disk and then you can unpickle things later. How does Pickle work? Well Pickle is defined for on a per object basis and the way you define how a Pickle works is you define what\u0026rsquo;s called a reduce magic method underscore underscore reduce underscore underscore underscore or you know if you\u0026rsquo;re cool it\u0026rsquo;s actually underscore underscore reduce underscore EX underscore underscore the EX meaning that you also get to know what the Pickle version is. So you know for any given class if you want to be able to serialize something using Pickle you just define what the what the Pickle sorry what the reduce function for things should be and the way you write one of these reduce functions is actually it\u0026rsquo;s sort of recursive. You just define the serialization you want in terms of smaller more primitive objects that you want to serialize. So let\u0026rsquo;s imagine that we\u0026rsquo;re serializing a tensor. So the way we serialize a tensor is that we actually return a tuple from our reduce function and you can actually go look at this code inside torch slash underscore tensor dot pi. It\u0026rsquo;s all implemented in Python. At least one version of it is. So what do we do? Well you know we get our tensor and we\u0026rsquo;re going to construct a tuple containing all of the important pieces that we need to rebuild the tensor. So it\u0026rsquo;s going to contain our storage it\u0026rsquo;s going to store our sizes it\u0026rsquo;s going to store our strides and very importantly it\u0026rsquo;s also going to store a function that says how to take all of these you know particular pieces to that are there namely you know the size and the stride and the storage and reconstitute this into some actual tensor because when we actually want to you know load our tensor from pickle like pickle needs to know how to actually take one of these you know tuples and turn it into the actual object in question. So you do that by providing a function a rebuild function as we call it internally that takes the various pieces that you serialize one by one and reassembles them back into the whole. So that\u0026rsquo;s how pickling works in general and pickle itself is a pretty simple file format. There are plenty of tools you can use to look at pickle so if you\u0026rsquo;ve like ever thought oh you know pickles are just these opaque blobs that I can\u0026rsquo;t actually look into well okay they are binary objects but like the format is actually very simple it\u0026rsquo;s like a little stack machine that you use to just build up various data structures inside pickle. By the way if people have ever told you that like pickling and unpickling arbitrary objects is unsafe that\u0026rsquo;s because you know pickling can induce object construction and so if you\u0026rsquo;re not safe about what objects you construct via pickling then you could accidentally trigger you know remote code execution like say you unpickle an object that actually goes ahead and you know runs a shell command from whatever you pass as a constructor. So that\u0026rsquo;s something to keep in mind for as we\u0026rsquo;re getting later in this podcast. Okay so Python serialization usually done using pickle because pickle is built into the standard library everyone sort of knows about it it\u0026rsquo;s got a protocol for defining this that most people do. It\u0026rsquo;s actually a little tricky to like pickle things correctly for example imagine that you are pickling an object and in you so you have a class and you are pickling it and then in a new version of your software you add a new attribute to your class right so adding a new attribute ordinarily is a backwards compatible change because well you know like all the old users of your code weren\u0026rsquo;t using that attribute so what what what skin off their back is it if there\u0026rsquo;s a new attribute but with pickling in the mix this is actually usually a bc breaking change because any old pickles from older versions of your class don\u0026rsquo;t actually have this attribute set so when you So when you actually write the unpickling code for your code you\u0026rsquo;ll unpickle an object that is missing this attribute and so if you don\u0026rsquo;t like if you assume that the attribute is set which is a very reasonable thing when you\u0026rsquo;re writing a class it\u0026rsquo;ll break when you unpickle this old thing. Fortunately Python has another mechanism for overriding behavior in this situation there\u0026rsquo;s a magic method called set state which gets called whenever you\u0026rsquo;re you know you\u0026rsquo;re actually populating the state quote unquote for an object that\u0026rsquo;s being pickled and that doesn\u0026rsquo;t actually have a full on reduce implementation and so in that situation the way to make things bc is usually just to look for missing attributes and fill them in before you load the object. Okay long long tangent aside okay so how does tensor pickling historically how was it implemented well we did the same thing that everyone else does and we use pickle to do it so what do you expect to see well you expect to see on tensor there\u0026rsquo;s a reduce implementation and indeed there is a reduce implementation in torch slash underscore tensor dot pi and it has a bunch of functions for example it has a function that knows what\u0026rsquo;s going to do it. For example it has a function that knows how to rebuild the tensor actually this function is called v2 because in 0.4 when Sam Gross merged tensor and variable he actually you know changed also the serialization format in a backwards and forwards incompatible way so you know we had to make a new rebuild implementation. digression about forwards and backwards compatibility so backwards compatibility typically is this idea that if you serialize a tensor to you know some saved format a backwards compatible software means that when there\u0026rsquo;s a new version of the software you can load that old version of the pickle in the new version. backwards compatibility is a good idea we try very hard not to break backwards compatibility ever especially with a serialization format however there\u0026rsquo;s a very similar and also important notion called forwards compatibility so forwards compatibility means if I serialize an object from a newer version of pi torch. Can I load it from old versions of pi torch. Can I load it from old versions of pi torch. And you know it should be clear to see that like maintaining indefinite forwards compatibility means you can never ever change the serialization format but it is useful to be able to like load new tensors from older versions. So whenever we\u0026rsquo;ve broken forwards compatibility we\u0026rsquo;ve usually had some mechanism by which you can get back the old format so if you just in a pinch you need to send something to an older version of pi torch. Okay, so digression over so we have we\u0026rsquo;re on v2 of the tensor serialization and you know v2 is obviously not forwards compatible with v1 but you\u0026rsquo;d have to be running pi torch like 0.4 so that\u0026rsquo;s like ancient history and no one really cares but for zip file format this is going to be relevant in a moment. Okay, so what do we do for tensor what we do exactly this so you know we\u0026rsquo;ve got a function to rebuild tensors based on the data. What is the data it consists of storage sizes etc storage itself also has an implementation of how serialization works namely you know you just you just there\u0026rsquo;s a reduce implementation but this reduce implementation does something very interesting which is it actually calls into torch dot save to do the implementation. And so now here is sort of the first interesting thing that\u0026rsquo;s going on which is that actually pickle i.e the interface that Python gives you for pickling objects is not the same thing as torch dot save and torch dot load which is the other sort of very published mechanism for doing serialization and py torch like usually like when you look at the tutorial you don\u0026rsquo;t like directly instantiate a pickle object and pickle Python objects. And then you actually use torch dot save to actually use torch dot save to save these things. So what does torch dot save do differently. So what does torch dot save do differently. Well, if you go look at the implementation of this file also all in Python easy to look at. What you\u0026rsquo;ll find is that we actually do most of the things you\u0026rsquo;d expect right which is that we are going to create a pickler and then we\u0026rsquo;re going to feed it the data in question and then out is going to pop a byte string. What we do differently is that we want to do. What we do differently is that we want to do duplicate storages that are shared between multiple tensors so let\u0026rsquo;s imagine you\u0026rsquo;re serializing a list of tensors and you have a you know law the list of tensors actually list of views onto a single tensor. So if you you know serialize this naive way we you would you know stamp out a copy of the same data for every single occurrence and in the once you do serialized it like these would all be different tensors and if you mutated one of them the other tensors wouldn\u0026rsquo;t get mutated. So that\u0026rsquo;s bad we don\u0026rsquo;t want that we want this the sharing to be preserved during pickling. And so the way this is done is we use this other mechanism in Python object serialization called persistent IDs where basically for any given object that\u0026rsquo;s being pickled you can override the behavior for what happens in that situation. And so when we see a storage we actually record a persistent ID that records what that storage is and then for subsequent you know occurrences of that storage. We make sure they get all memorized into the same version of the storage. So okay so that\u0026rsquo;s basically in a nutshell how serialization used to work in the old days. And so what happened. So what happened is that we were building. So serialization in the old days was just targeted at eager mode right the only thing people were really serializing were tensors and maybe modules right modules with parameters. Because you know those were just Python objects but they also had tensors on them. We actually discourage people from pickling modules directly but people do it anyway. What you\u0026rsquo;re supposed to do is you\u0026rsquo;re supposed to get the state dict for the module and serialize that that\u0026rsquo;s because you know serializing arbitrary Python objects is kind of error prone. But anyway that was what people were normally using serialization for. So in comes TorchScript. So what does TorchScript need. So TorchScript is a bunch of things but one of the things it is is it\u0026rsquo;s a distribution mechanism for arbitrary PyTorch programs namely TorchScript programs that are understood by the TorchScript compiler. TorchScript compiler. And this is important because if you want to sort of ship your model to production it\u0026rsquo;s important to have a self contained file format that contains all the information you need to run the model. So the Python code and as well as the tensors and so people were like looking and they were like okay we need some serialization format for TorchScript. And oh you know there\u0026rsquo;s this interesting thing which is that PyTorch is using pickle but actually pickle is kind of a bad idea to actually serialize tensors because tensors are really big. And you actually want to like if you\u0026rsquo;ve got your data living on disk like you know a bunch of parameters you want to just map them into memory you don\u0026rsquo;t want to actually have to parse them into memory which is what you know traditional PyTorch serialization used to do. Okay so what did they decide well they decided that one they wanted to use standard file formats so we really didn\u0026rsquo;t want to be in the business of making up a new file format because then you don\u0026rsquo;t have any tools that can work with file format. And two was you know we kind of wanted you know our code to like be in the Python style right like you know there\u0026rsquo;s all this existing infrastructure for pickling and unpickling Python objects. And you know if we define a totally different serialization format rather than pickling. Well we\u0026rsquo;d have to redo all of that and we\u0026rsquo;d have to keep these in sync indefinitely. So what did they do. So they did two things. So one is that they decided we were going to use zip files for our serialization format. Don\u0026rsquo;t laugh zip files are actually really cool. It\u0026rsquo;s a really well designed file format and one of the reasons it\u0026rsquo;s really well designed is well you don\u0026rsquo;t actually have to compress things in a zip file. So you have an uncompressed zip file. What it turns out is that zip files have a bunch of really good properties. One is that you don\u0026rsquo;t actually have to read through the entire zip file to figure out where things are. There\u0026rsquo;s a manifest at the beginning that lets you efficiently index to any particular location. So if you\u0026rsquo;ve got a bunch of big tensors you don\u0026rsquo;t have to scan to all of them to actually find out where your tensor you\u0026rsquo;re interested in is. Another really good property of the zip file format is that it you know is the tensors are laid out exactly as is in memory. You can easily and map them into memory if you want to load them in your package process. And finally, like everyone knows zip files right like zip files are you know the darling compression format in Windows and like you know there are tons and tons of full tools that can work with zip files efficiently. So if you have a serialized you know thing that Pytorch gave you from torch dot save in a recent enough version of Pytorch you can just unzip it literally like unzip it use it like rename its file extension to zip and then like double click it and it\u0026rsquo;ll give you all the internal bits. The second choice they made was they were going to keep using pickle. So what does that mean. Well, remember, I said pickle is a very simple serialization format, you know, like most of the complexity involved with it is because like you can call arbitrary code to actually reconstitute these objects that are saved. But other than that, you know, you\u0026rsquo;re just saving these two poles of various other things that themselves, you know, might be two poles of other things. So what did we do. We just implemented a pickler and unpickler from C++. So inside JIT slash serialization, there is a pickler and an unpickler and it is feature for feature compatible with our Python implementation and it understands the pickle format. And this implementation is secure because unlike stock Python pickler, which, you know, we\u0026rsquo;ll just attempt to unpickle anything that you throw it. Our pickler only supports a limited set of, you know, types and all those types don\u0026rsquo;t actually do remote code execution. So, you know, you\u0026rsquo;re safe there. So, hey, so then that\u0026rsquo;s basically where we are today. Okay, so when you torch save and torch load without using the use on, you know, use the non zip file format, which which does get called occasionally, for example, if you serialize a storage just by itself directly using pickle, we don\u0026rsquo;t use the zip file format. Just just just just a fun fact. But if you are using torch dot save and torch dot load, we give you the zip file. This zip file contains a data pickle that represents, you know, metadata about the tensor in question. And then it contains, you know, a bunch of files representing the actual data in the tensor. And this works pretty well. It\u0026rsquo;s a little slower than the old school pickler, but not that much slower. And people have been pretty happy about this new serialization format. Okay, so that\u0026rsquo;s been a whirlwind tour of serialization in PyTorch, starting from our humble beginnings as a Python pickle extension, and then to our not so humble endings of also a pickle extension, but you know, also with a zip file around it. So I hope this explains a little bit about why our serialization code is kind of complicated. And also why whenever you want to make a change to the serialization format, it\u0026rsquo;s really complicated to do so because of BC and FC and also because you have to edit Python and C++. But hopefully, if that\u0026rsquo;s something you ever actually need to do, you\u0026rsquo;ll know where to look to figure it out. That\u0026rsquo;s all I have to say for today. Talk to you next time.\nEP19 native_functions.yaml native_functions.yaml Hello, everyone, and welcome to the PyTorch Dev Podcast. Today, I want to talk about nativefunctions.yaml, but I don\u0026rsquo;t actually want to talk about nativefunctions.yaml. What I really want to talk about is enough about our just-in-time compiler for people who are not compiler engineers and working on the eager portion of PyTorch. You\u0026rsquo;ll see what this has to do with nativefunctions.yaml in a moment. Okay, so what is nativefunctions.yaml? Well, nativefunctions.yaml is this YAML file named nativefunctions, which basically describes every operator supported by PyTorch. So imagine that you\u0026rsquo;re thinking about sum or add or sub. Each of these operators that PyTorch supports has an entry in this YAML file. And so this YAML file basically is a sort of canonical source of information about these operators, except for a few exceptions, which we\u0026rsquo;ll get to later. Okay, so why is there this YAML file, right? Like, if you were just writing a Python library, you\u0026rsquo;d expect, well, you know, if there\u0026rsquo;s a bunch of functions that my library supports, I\u0026rsquo;ll just write Python definitions for them. Or even, you know, if you\u0026rsquo;re writing a library and you\u0026rsquo;re doing C++ bindings, you\u0026rsquo;d expect, oh, well, you know, I\u0026rsquo;ll have a bunch of C functions that implement the functions that I need, and I\u0026rsquo;ll just register using pybind11. So, like, why do I need this separate representation? And as is always the case, when there is an abstract representation about what operators you have, there\u0026rsquo;s probably code generation lurking underneath. And in particular, native functions.yaml gets fed into a variety of different code generation pipelines, which basically stamp out all of the boilerplate necessary to support all the things that you want to, you know, you want an operator to do. And this is where, like, being JIT for non-compiler engineers is important, because, yes, native functions.yaml plays a very important role in generating our eager PyTorch bindings, that is to say, you know, the actual functions you call when you\u0026rsquo;re just running PyTorch from Python. But what it also does is it also generates bindings to TorchScript, our compiler and interpreter stack in PyTorch. And so whenever you\u0026rsquo;re, like, working on a new operator, when you\u0026rsquo;re trying to define a new operator, whatever it is you do also needs to work okay with compiler stack. And here, it\u0026rsquo;s helpful to know a little bit about what the compiler is trying to do with this information to, you know, figure out why, you know, there are certain constraints about what kinds of things you can do in native functions.yaml. So let\u0026rsquo;s take one example to start. So in native function.yaml, one of the things you do is you write down a so-called schema string for any operator you want to define. So what does this schema string look like? Well, let\u0026rsquo;s take our example of addition. So what is an addition? Well, it takes two tensors, and it produces an output tensor. And so the schema string for add basically is, like, you know, tensor add, open paren, tensor self, comma, tensor other, close paren, right? What it says is, hey, you know, here are the types of the arguments, here are the types of the outputs, pretty standard stuff. But if we look a little deeper into this type system, you know, the fact that we have this schema string, the fact that we have this JIT schema format actually says something about what we are planning to target. Because in particular, the schema is not Python types. It\u0026rsquo;s not C++ types. It\u0026rsquo;s JIT schema types. And what JIT schema types represent is sort of the intersection of all language features that are supported by Python, as well as language features that are supported by C++, and most importantly, language features that are supported by the TorchScript compiler. So let\u0026rsquo;s just take an example, right? So let\u0026rsquo;s say that I wanted to write a function in PyTorch that takes a void star pointer as its input. Well, you can\u0026rsquo;t do that. And the reason you can\u0026rsquo;t do that is while void star works as a, you know, type in C++, there\u0026rsquo;s no such type as void star in Python. Well, unless you count, you know, one of the C type types, but we don\u0026rsquo;t like most of the Python types that the PyTorch binding support are like stock types, like normal types, like integers, floats, booleans, tensors, for example. So you can\u0026rsquo;t write a function like that, right? And if you wanted to write a function that took void star, you would first have to fix both the eager code generation code to understand a void star pointer, as well as the C++ code. That would be easy because void star is very simple, as well as the TorchScript code to know how to represent a void star pointer in what we call our box format, our iValue format, which is basically a universal container for any type of, you know, object that you might actually pass into one of these functions in question. So yeah, there is a limited set of types available to native functions.yaml. And this limitation makes it easy to actually, you know, write code that works for all of the platforms that we care about. Of course, this can be kind of annoying sometimes. For example, we don\u0026rsquo;t have support for enums in native functions.yaml because how enums are defined in C++ and in Python are fairly different and it\u0026rsquo;s pretty involved. There\u0026rsquo;s no reason in principle we couldn\u0026rsquo;t solve this, but, you know, you have to actually pre-declare an enum in C++ and you have to pre-declare an enum in Python, except in Python, that\u0026rsquo;s not the Pythonic way to do enums. You just, you know, provide a string saying what option you want. So actually most enums are implemented sort of crappily using strings. And I say it\u0026rsquo;s crappily because, like, you don\u0026rsquo;t actually want to be passing around strings and doing string comparisons. In Python, it\u0026rsquo;s okay because string and turning happens. And so if you\u0026rsquo;re lucky, it\u0026rsquo;s just a point of equality. But in C++, that doesn\u0026rsquo;t happen. And so you actually do want an enum type. But we haven\u0026rsquo;t implemented it yet, right? Because it\u0026rsquo;s a little complicated to, like, work out a representation for enums that works in all the situations. By the way, if you\u0026rsquo;re interested in doing this, well, talk to us because it is something that we\u0026rsquo;ve been wanting to fix for a really long time. Okay, so that\u0026rsquo;s it about types in nativefunctions.yaml. What\u0026rsquo;s another example of something that, you know, you need to worry about in nativefunctions.yaml, not because it matters in eager mode, but because it matters in the compiler? Well, a great example of this is mutation and alias info in nativefunctions.yaml. Okay, what\u0026rsquo;s that? Well, if you ever look through the YAML file, you might notice that there are some operations that have some little weird, like, extra, like, fluff in their type signatures, right? So they don\u0026rsquo;t just take a tensor as an argument. They take a tensor, open parentheses, A, exclamation mark, close parentheses. What the heck does that mean? Well, what that means is that this argument isn\u0026rsquo;t just being read in as a pure argument. That is to say, we\u0026rsquo;re just taking it as an input. We\u0026rsquo;re also going to write to the argument in this situation. So, okay, you might say, that\u0026rsquo;s really useful for documentation purposes in eager mode. But, like, why does it matter if I specify this correctly or not? Well, it matters because, once again, we\u0026rsquo;ve got a compiler. And our compiler wants to do certain optimizations, and some optimizations might not be valid if you don\u0026rsquo;t know if an operator is mutating its arguments or not. For example, dead code elimination says that if I call a function on some operands and then I don\u0026rsquo;t use the result, I can just get rid of that operation entirely, right? Because it\u0026rsquo;s dead code. Well, I can\u0026rsquo;t get rid of this function call if the function is actually in the business of mutating the tensor. Because, you know, like, we might just be calling this function for the purpose of doing the side effect in question. So it\u0026rsquo;s actually really important to put down correct mutation information on your functions. Because if you don\u0026rsquo;t, and then your function goes into the TorchScript compiler, which it will, because the whole point of putting something in native functions.yaml is so that you get all of the support, right? Eager, C++ script. If you don\u0026rsquo;t do it right, then your compiler may just miscompile your code. It may, you know, throw away your opcalls. It may reorder them with other mutating opcalls. Bad business all around. Of course, what you really should do is just write your operator without having any mutation at all. But, you know, sometimes that\u0026rsquo;s not possible. This is a really common mistake people make when they\u0026rsquo;re defining custom operators. Because you\u0026rsquo;re, you just like, you just write a type signature down and you think, oh, this looks fine. And the, you know, PyTorch accepted it. What\u0026rsquo;s wrong with it? Well, what\u0026rsquo;s wrong with it is, you know, this downstream thing about the compiler. So if you\u0026rsquo;re thinking about, like, what kinds of info the compiler needs, that\u0026rsquo;ll help you understand, like, what kinds of stuff native functions.yaml actually needs. There\u0026rsquo;s one more thing that, like, really, really, really affects people. When they\u0026rsquo;re making changes to native functions.yaml. And this is backwards and forwards compatibility with serialization formats in JIT. In the previous podcast, I talked about serialization sort of in a general sense. And I talked about this forward compatibility and backwards compatibility concept. Well, this concept also applies to operator definitions. So stepping back a moment, when we think about forwards and backwards compatibility in PyTorch, we usually only really care about backwards compatibility because you just write some Python program and you just want this Python program to keep working when you upgrade to the next version of PyTorch. And there are a lot of changes that we can make to functions which are actually backwards compatible. For example, if we add a new keyword argument to an operator, but we give it a default, from the perspective of Python, that\u0026rsquo;s totally backwards compatible because, well, if I had a call to the function before that didn\u0026rsquo;t pass the argument, well, it\u0026rsquo;ll just get defaulted. And, you know, if I\u0026rsquo;m doing my job correctly, the default behavior will be compatible with whatever the old behavior was in that situation. Well, well, well, but remember, native functions.yaml is being used in different situations. And in particular, there are two particular situations where this is not exactly backwards compatible. And by the way, these might be just mistakes and we should fix these mistakes, but sort of it\u0026rsquo;s just how PyTorch works today. So situation one is, for the longest time, when we serialize PyTorch, so stepping back a moment, so one of the things that TorchScript does is you have a model that has a bunch of function calls, and we can serialize these function calls back into Python code. And so something very interesting happens as a result of something that a compiler wants to do, which is whenever you serialize some functions, we actually write out all the defaults to the serialized model. So let\u0026rsquo;s just imagine, like, I\u0026rsquo;m doing a matrix multiply, and I added an optional flag that says whether or not I should transpose the second argument or not. So this doesn\u0026rsquo;t actually exist in PyTorch, this doesn\u0026rsquo;t actually exist in PyTorch, but there are plenty of examples that are actually existing, I just can\u0026rsquo;t think of them right now. So in this situation, if I write a, say, matmol a b, what will actually get serialized is matmol a b true, where true says, sorry, false, where false says don\u0026rsquo;t transpose the second argument. That\u0026rsquo;s kind of weird. Why does the JIT do that? Well, one of the reasons the JIT does this is, you know, one of the things that it does when compiling your program is it tries to translate it into an intermediate representation that\u0026rsquo;s easier for the compiler to deal with. And one of the things that makes IRs easy to deal with is when they are very regular. So what do I mean by regularity? Well, it means that I don\u0026rsquo;t have to, like, you know, go ahead and canonicalize stuff every time I look at it, I can just assume that things are in canonical form. And an example of something in canonical form is a function call, which has all the defaults actually explicitly written out, as opposed to, like, implicit, because if they\u0026rsquo;re implicit, you have to go figure out, you know, what the behavior, what the defaults are and fill them in if you wanted to, like, actually write code that operated on the semantics of this function. Okay, so because this IR representation transformation happens, well, as an accident, when we reseerialize things out, we actually just lost the information about whether or not, you know, something was explicitly defaulted or not explicitly defaulted. And so we just always serialized it out. Why is this problematic? Well, it\u0026rsquo;s problematic for forwards compatibility. Recall from the previous podcast, forwards compatibility refers to if I serialize a model from a newer version of PyTorch, and let\u0026rsquo;s say that it doesn\u0026rsquo;t actually use any of the new features of PyTorch, which, you know, would necessitate using the new version of PyTorch. Can I run it on an old version of PyTorch? And so if you add this defaulted new parameter, and, well, it\u0026rsquo;s getting serialized out, uh-oh, when you, you know, try to load this model in an old version, there will be this extra parameter that your old version of PyTorch doesn\u0026rsquo;t understand. And, well, sucks to be you, the model can\u0026rsquo;t be loaded anymore. So there is a way to solve this problem, uh, in PyTorch Master, and I don\u0026rsquo;t exactly remember how we resolved it. It\u0026rsquo;s either some sort of, like, backwards compatibility, sorry, forwards compatibility, uh, well, one is we don\u0026rsquo;t really offer forwards compatibility, but I think there\u0026rsquo;s some, like, surgery you can do to fix the problem. Or it might just be that we fix this problem to begin with. But, like, the meta point here is that this was a problem for a while, and the reason it was a problem is because, you know, JIT is using this representation in a way that is different than how you normally might conceptualize it in just eager mode. And so to just understand the consequences of various changes you might make, you have to also understand, you know, what\u0026rsquo;s going on in JIT. Is this bad? Like, what if we, like, just wrote our format really, really nicely and explained all of the invariants in question, and, like, you could just read up about them and know everything? Well, yes, ideally, this would be the case. Ideally, we would have a really good backwards compatibility and forwards compatibility story, and we wouldn\u0026rsquo;t have problems like this. Great! If you want to work on this, you know, come talk to us. Like, you know, this is a really important project for PyTorch, and we\u0026rsquo;ve just, you know, been very slow in actually getting some, because who wants to work on backwards compatibility, honestly? I do, actually. But I\u0026rsquo;m always working on other stuff, unfortunately. So, yeah. So what did I talk about today, right? So I talked about nativefunctions.yaml. I didn\u0026rsquo;t really tell you, you know, how to write things in nativefunctions.yaml, and I don\u0026rsquo;t really want to in this podcast, because there are pretty nice documentation that you can look at. What I wanted to go over today was more, you know, why does nativefunctions.yaml have all of this stuff? And the reason it has all this stuff is because, well, there\u0026rsquo;s this compiler stack attached to it, and, you know, there are a bunch of constraints that, you know, we need to solve simultaneously in both cases. So if you ever find yourself wondering, you know, why is something this way? Well, maybe there\u0026rsquo;s something in the compiler that needs it to be that way. And also, I also want to emphasize that compilers are not that magical. Like, there\u0026rsquo;s not that much they\u0026rsquo;re doing. So you can understand it, even if, you know, you don\u0026rsquo;t work on the compiler on day-to-day. And, like, once you understand it, you might actually be able to look at the situation and say, hey, actually, there is no reason for it to be this way, and we can fix it. And then, you know, you can just fix it. And that\u0026rsquo;s pretty powerful, and so a generalizable lesson that applies to all of software engineering. That\u0026rsquo;s everything I wanted to say today. Talk to you next time.\nEP20 TensorIterator TensorIterator Hello, everyone, and welcome to the PyTorch Dev Podcast. Today, I want to talk about Tensor Iterator, but I\u0026rsquo;m going to go about it in a sort of unusual way, which is, imagine you\u0026rsquo;re walking into a software engineering interview, and you\u0026rsquo;re wondering what question you\u0026rsquo;re going to be asked today, and your interviewer sits down and says, okay, Edward, please add two vectors together. And I say, come again? Yes, given two vectors, A and B, add them together, giving a new vector, which contains the pairwise sum of each element in each of the vectors. And I think to myself, oh, this, how complicated could it be, right? Write a for loop, iterating over the size of the vectors, and, you know, just look at the two entries, add them together, and then, you know, set it in my output and return the result. You know, easy. Are we done? And then the interviewer gives you an evil smile, and they says, okay, now, what if you want to make it really fast, and you want to make it work in a lot of situations? And so this situation is sort of exactly the situation that tensor iterator is in, right? On its face, the job that tensor iterator is trying to solve is very simple. Namely, given two tensors, you know, do some point-wise operation on them. So, you know, given two tensors, you know, like, look at the first element in the first one, look at the first element on the second one, add them together, that\u0026rsquo;s the first element of your new tensor and keep doing it step by step by step by step. And you might think, hey, this should be really simple. And it is, sort of. But it turns out that, you know, when you\u0026rsquo;re in a library like PyTorch, there\u0026rsquo;s actually a lot of different conditions, and also a lot of different performance optimizations you might want to apply in this situation that, like, actually end up making tensor iterator very, very complicated. So the goal of today\u0026rsquo;s podcast is to talk a little bit about all of these things that, you know, go into making a tensor iterator work. So where to start? So let\u0026rsquo;s start from the very beginning. So I gave you these two vectors, and I wanted you to add them together. And one of the first things you should ask me is, well, okay, what are their sizes? Are their sizes the same? Because, you know, a tensor is not just a one-dimensional array, it\u0026rsquo;s actually a multi-dimensional array, right? Possibly with arbitrary dimensionality. And so, you know, when you want to add two things together, it turns out that, you know, adding two things together doesn\u0026rsquo;t require the two input tensors to be the same shape. In fact, PyTorch implements something called broadcasting. We got broadcasting from NumPy, which also does it. What broadcasting says is it basically simplifies the situation when you have a tensor, and you want to, like, add a single scaler to it. So if I have a tensor, and I want to add two to it, that just means, hey, add two to every single element in the tensor. And this is a special case of broadcasting. Broadcasting actually, you know, sort of generalizes to arbitrary dimensions. So let\u0026rsquo;s say that I have a five by four by three tensor, and I\u0026rsquo;ve got a sort of sample tensor of size three that I want to add for every single element in my five by four by three. Namely, I want to do it five by four times, right? 20 times, stamp in this extra three, and replicate it in all of the situations that it shows up. And broadcasting also supports that. The way you figure out how I tensor broadcast, by the way, is you sort of line up their sizes, but to the right, rather than to the left, and then just pad it out so that it goes all the way to the end. And so their sizes from the right have to match up. And then everything else gets replicated. All right. So one of the first things you have to do when you\u0026rsquo;re doing tensor iterator is when you want to add things together is actually like, we will accept inputs that don\u0026rsquo;t have the same sizes. And you need to do something reasonable in that in that case, namely, you need a broadcast in a situation. Okay, sure. So let\u0026rsquo;s say that you know how to do broadcasting, and you\u0026rsquo;ve written the algorithm to figure out what the output shape should be. You know, what else is there? Well, I didn\u0026rsquo;t tell you what the, you know, types of your inputs were, right? And, you know, normally, if I give you two vectors in an interview, you just assume they have the same type. But what if they don\u0026rsquo;t have the same type? Well, in PyTorch, we have this thing called type promotion, also taken from NumPy, which says, hey, under some situations, we are willing to add together tensors, which have different types. Once again, why is this desirable? Well, sometimes, you know, you have a floating tensor, and you\u0026rsquo;ve got an int tensor, and you just want to add them together, right? You want to treat the integers as if they were floating point numbers, and then do this addition. So there\u0026rsquo;s a table, and I\u0026rsquo;m not going to tell you this table in the podcast, but there\u0026rsquo;s a table, which imagine that you know, you have all the different d types in PyTorch, like int32, int64, int16, float, double, etc, etc. And then you\u0026rsquo;ve got another axis on the table, which is all the d types as well. And the type promotion table tells you given two inputs of these two d types, what is the output d type in question. And so this is something else tensor iterator has to, you know, compute, right? Which is that, hey, you know, what is the actual output d type, because maybe the input types of my values are not the same. Oh, but it gets better than that. Because, hey, you can also give a addition operation explicit out tensor that you want to write the results to. Does the out tensor have to match the computed d type in this situation? The answer is no, it doesn\u0026rsquo;t, it can be different. And yeah, we\u0026rsquo;re also going to promote, as necessary to like fit the output into the output d type you give us. So hey, all right, like, yeah, so you know, you asked about what the types could be, we said they could be anything. We asked about what the shapes could be, they could be anything. Does it get worse? Yes, it does get worse. Okay, so I mentioned that addition can take an optional out tensor, right? And so what does this mean? It just means that, hey, when I add these two tensors together, don\u0026rsquo;t allocate a new output buffer for the situation, just write it in place into this pre existing buffer. What happens if that output buffer aliases with one of the inputs? And this is like, actually, kind of tricky to deal with. And in general, like the sort of aliasing situations, make, you know, otherwise straightforward algorithms a bit more complicated. So in some situations, it\u0026rsquo;s okay for this aliasing to happen, right? So like, let\u0026rsquo;s imagine that I am adding a tensor in place, right? So I\u0026rsquo;ve got this tensor, I want to add two to every element in it. What actually happens in the situation is I put in the tensor as an input. And I also put the tensor in as an output. And because addition is sort of atomic, right? Like I just read out from the memory, and then I do my operation. And then I write out back to the memory without ever like looking at any of the other memory locations. This is fine. And like, nothing bad will happen if the inputs and outputs aliased with each other. But let\u0026rsquo;s imagine that my output tensor actually is strided in a funny way. For example, what can happen with strides in PyTorch is that multiple logical locations on the tensor can refer to the same physical memory, right? We talked about broadcasting, well, broadcasting exact is exactly a situation like that. So what happens if you know, you\u0026rsquo;ve got multiple logical positions pointing to the same physical location? Well, let\u0026rsquo;s say you\u0026rsquo;re processing your inputs one by one. And so okay, I want to add two to some location. So you go ahead and you read out the physical location, do the addition and write it back out. Oh, the next time you read out from that physical location again, because once again, this is one of those tensors where multiple logical positions mapped in the same physical position. Well, you\u0026rsquo;ve already clobbered the old value there. And well, sucks to be you, you just are going to get total garbage in this situation. So something else tensor iterator has to do is it needs to make sure that there aren\u0026rsquo;t any sort of illegal overlaps between the inputs and the outputs. And also sometimes, you know, with the also, the output itself needs to make sure that it doesn\u0026rsquo;t overlap with itself, which can also cause problems where you know, you write to the output location, and then you write to that output location, again, clobbering the original value. Oh, man, by the way, the problem of determining whether or not there is an overlap is actually like equivalent to like solving diaphantine equations. So PyTorch just does an approximation, it would be really, really difficult to do this properly. Oh, one more thing, this this like aliasing thing where the destination and the source could overlap. This is one of the reasons why like there\u0026rsquo;s a difference between mem copy and mem move in the C API, right? One of them is allows for aliasing and another one doesn\u0026rsquo;t. And so you have to be careful when you\u0026rsquo;re writing code to figure out whether or not aliasing can occur or not. And since PyTorch is a library, and it can be called by anyone, we have to basically assume that arbitrary aliasing can happen to anyone. All right, so we talked about shapes, we talked about d types, we talked about memory overlap, are we done? No. So I mentioned about strides, right? So we talked about how strides can be used to like implement broadcasting. And so what do I mean by that? Well, you know, in PyTorch, we support this operation called expand. And so what expand does is it takes a tensor of some size, and then expands it to be some bigger size with the same element, you know, repeated, repeatedly, but we don\u0026rsquo;t actually materialize all of this in memory, what actually happens is, you know, it just gets stamped out in duplicate copies. And the mechanism by which this happens is a stride. The stride says, you know, once you advance in some dimension, your index in some dimension, how should the, you know, physical location change, right? And so normally, in a like contiguous dense tensor, if I advance in a dimension, that means I should skip ahead however many, you know, 100 bytes, 400 bytes, whatever, to get to the next sort of chunk of data in this case. But when I broadcast, I just say, oh, that number is zero. So I\u0026rsquo;m not going to advance at all. So broadcasting is a degenerate case of striding. But in fact, striding has a lot of other possibilities, right? And you know, what it comes down to is that when I have this like flat contiguous piece of memory, there are multiple ways I can interpret it as a multidimensional tensor. And the like very like simplest example of this is when I think about how 2d matrices are represented, right? There\u0026rsquo;s this concept of row major and column major matrices, right? When you read out the numbers, you know, 12345 in physical memory, does that correspond to reading out a column or a row? And PyTorch supports both of these simply by just specifying what the strides of a tensor are. Okay, so you can\u0026rsquo;t assume that like the layout for both of your tensors is the same. And so oh, another thing tensor iterator has to do is given the two inputs, what should my output layout be, right? Because, you know, if I give you a column major tensor and a row major tensor, well, I had to make some decision about what the output should be. This is there\u0026rsquo;s a very complicated resolution algorithm for it. But like one of the properties that it wants is if like I add a column major tensor to a column major tensor, I still get a column major tensor. And similarly, if I add a row major tensor to a row major tensor, I also get a row major tensor. Is that it for strides? Not quite. Okay, so there\u0026rsquo;s something else that happens. So we\u0026rsquo;re getting out of the realm of correctness, right? Where like, we just need to like deal with all these things like shapes and d types and layout, because they\u0026rsquo;re part of the public facing specification. And we\u0026rsquo;re now getting into the how we actually make the algorithm run fast. So one of the things that is with strides that like makes them kind of slow is like if you have a really big dimensional tensor, well, you have a lot of strides. And if you want to index, you know, the indexing formula with striding is, you know, take the first index, multiply it by the first stride and add it to the second index, multiply it by the second stride, and so forth and so forth and so forth and so forth. So you can imagine with a really high dimensional tensor, indexing computation actually takes a lot of time. And in fact, we try very, very hard not to do arbitrary dimensionally indexing. And most of our helpers for doing indexing require us to know exactly what dimensionality a tensor is. By the way, that\u0026rsquo;s also the reason why like, say, eigen is actually templated on dimension size, because it\u0026rsquo;s way easier to generate efficient code in the situation. But tensor iterator is supposed to work on tensors of arbitrary dimensionality. So like, how do we do this efficiently? Well, another important optimization that we do is sometimes we have multiple dimensions, but they\u0026rsquo;re actually all contiguous, right? Like, let\u0026rsquo;s imagine that I have a contiguous tensor, a contiguous, you know, five dimensional tensor, and it\u0026rsquo;s just laid out in memory, you know, exactly 12345678910. And I\u0026rsquo;m adding it to another 5d tensor. Well, I don\u0026rsquo;t actually care about the dimensionality in this case, right? Like, if the dimensions are exactly the same, there\u0026rsquo;s no broadcasting, there\u0026rsquo;s nothing like that, I could just treat these both as one dimensional tensors and add them together. And that would give me exactly the same result. Well, okay, I get a one dimensional result, and I have to reshape it into a five dimensional tensor. But like the computation between these two cases are the same. So another optimization that tensor iterator needs to do is it needs to say, Okay, well, you know, I\u0026rsquo;ve got this n dimensional tensor, it\u0026rsquo;s got all these strides, but what I\u0026rsquo;m going to do is I\u0026rsquo;m going to coalesce these dimensions, where when I have contiguous stripes, I\u0026rsquo;m just going to treat them as one mega dimension. And so I don\u0026rsquo;t actually have to spend time doing indexing computation inside them. Oh, man. So that\u0026rsquo;s a bunch of stuff that tensor iterator does. Okay, so like, you know, we\u0026rsquo;re like looking at sizes, we\u0026rsquo;re looking at strides, we\u0026rsquo;re looking at d types, we\u0026rsquo;re looking at overlap. Is that it? Well, not quite. So remember the interview question, right? Like, so it\u0026rsquo;s like, okay, how do you add two vectors together? Oh, I will just write a loop, and I will add the elements together, and I\u0026rsquo;ll be done. And then your interviewer says to you, Okay, how do you make it faster? Well, there are a lot of things you can do to make it faster. So one thing you can do to make it faster is you can parallelize it when there are lots of data, right? So you know, what does that mean? Well, you know, I\u0026rsquo;ve got this giant tensor, I\u0026rsquo;ll just split it up into chunks into grains. And I\u0026rsquo;ll ship each chunk to some thread. And the threads will, you know, do the addition in parallel on each of them. And you know, like, if I I\u0026rsquo;m not trying to run in a like multi threaded environment, I get all the cores to myself, you know, that\u0026rsquo;s a big speed up, because, well, one, you know, the data can be shipped out in this way, without like too much interference. And two, because I\u0026rsquo;ve got all these cores, and they all have ALUs, and they can actually be easily doing computation in this case. So there\u0026rsquo;s, you know, when you run a CPU kernel, alpha tensor iterator, parallelization is something you get for free in a situation. But wait, there is more. So you\u0026rsquo;re doing your addition on a single thread, right? And it\u0026rsquo;s like, hey, you know, please add the first element, please add the second element, please add the third element. Well, we can do better than that, right? Because there\u0026rsquo;s this little thing called vectorization. See, my earlier podcast, vectorization means I can actually do chunks of, you know, multiple numbers at a time, and take advantage of that AVX silicon in my CPU. So you know, not only am I going to paralyze, but I\u0026rsquo;m also going to vectorize when I\u0026rsquo;m actually doing the side by side elements. That\u0026rsquo;s also something tensor editor takes care of. Okay, so I paralyze my code, I vectorize my code, can it go any faster? Well, yeah, it can, right? Because, you know, the whole point of running things in PyTorch is GPU acceleration, right? GPUs are these massively, massively parallel processors. And you know, they have way more parallelism than just the poor vector units on our multi core CPU can be. So another thing tensor editor does is it lets you write kernels that work both on CPU and on CUDA, while sort of saving all of the, you know, other stuff like shape and D type and, you know, layout, that common stuff, letting you just do that once for the two implementations. And that in a nutshell is, you know, most of the things tensor iterator does, there\u0026rsquo;s more stuff that I sort of haven\u0026rsquo;t really talked about and glossed over. But at a high level, tensor iterator is, you know, sort of pulling its weight in two ways. One is that it is doing all the complicated semantics for point wise operations that you just don\u0026rsquo;t think about, but like, are these things that like people rely on working uniformly for all binary operations. And two is it makes it to write reasonably efficient code, you know, when you\u0026rsquo;re writing things in PyTorch. And it does so without like needing a just in time compiler or anything like that, right? It like gets compiled once, it doesn\u0026rsquo;t take up too many much binary size, and you get decently fast kernels that work in a huge variety of situations. Not everything is great with tensor iterator. Tensor iterator is kind of slow, it does a lot of, you know, bookkeeping and that bookkeeping adds up, we\u0026rsquo;d like to make it faster. But this is one of the reasons why it\u0026rsquo;s been so hard to replace because it really is doing a lot. And, you know, ask what you can do for tensor iterator, I say. That\u0026rsquo;s all I want to say for today. Talk to you next time.\nEP21 torch_function torch_function Hello everyone and welcome to the PyTorch Dev Podcast. Today I want to talk about Torch Function, a magic method that you can put on any class that makes it possible to override the behavior of everything that happens in the PyTorch library. Torch Function was developed in collaboration with many, many people over many years. For example, at the very beginning, Dylan Biswalco made a request for subclass preservation in PyTorch and he wrote an implementation for it. We didn\u0026rsquo;t end up using this implementation, but the prototype was enough to convince us that we should fund this project. And a number of folks at QuantSight, namely Ralph Gammers, Prasun Anand, and most importantly, Hamir Abasi, actually took it and did an implementation that we actually landed in PyTorch based off of the numpy implementation called array function. So Torch Function operates very similarly to array function. So if you know how that works in numpy, you know how it works in PyTorch. So why would you want to use Torch Function? Well, let\u0026rsquo;s imagine that you are writing some code and you want to reuse the functionality in PyTorch. So for example, we\u0026rsquo;ve got all these functions, right? We\u0026rsquo;ve got torch.add, torch.fft, tons and tons of API surface like that. And you might have code that, you know, you\u0026rsquo;ve had written against the PyTorch API and you like it, but you want to do a little bit more, right? Like, so the normal tensor behavior works okay, but you want to extend a little. Maybe you want to like keep track of some extra metadata on your tensors, or maybe you want to, you know, run some extra code like logging every time you do an operation. And so you just like to, you know, subclass tensor and, you know, customize the behavior a little bit. And at the same time, still be able to, you know, run all the good old fashioned operations on PyTorch. Or maybe you want to completely override the meaning of tensor, do everything on your own, but still be able to use all of the preexisting API that PyTorch provides. And a good example of when you might want to do that is say tracing. Well, you kind of run into trouble if you want to do this. So if you are just thinking about object oriented programs in Python, ordinarily, you can just change what the methods on an object are. And because Python is duct typed, things will mostly work out, right? So like if you if I had a tensor, and it supports an add method, I can just write another object that has a different add method. And then you know, if I call add on the object in question, I will just go to my, you know, whatever my other implementation is in my new version of the object. But if I have a function, and I pass in an object to that function, ordinarily, I can\u0026rsquo;t overload the behavior of the function in that way. Because well, it\u0026rsquo;s a function. And you know, we don\u0026rsquo;t actually do a dynamic dispatch in that situation, we always call a single implementation in that case. And sure, maybe the function might call a method underneath, but maybe not, right? Like and a lot of functions in PyTorch don\u0026rsquo;t, they go straight into the C++ bindings. And so you know, there\u0026rsquo;s no opportunity for overriding behavior in Python. So that\u0026rsquo;s what torch function is for torch function is a magic method that lets you override what the meaning of functions in the torch namespace do. No matter what, you know, the object in question is. So all you do is you write a class, you put a magic method called torch function on it. And then whenever you call a function in torch, instead of doing the normal behavior, it\u0026rsquo;ll bounce to your torch function implementation. And then you can override the behavior however you like. And in fact, it does more than that. You know, originally, the only thing we wanted to do was make it possible to override functions in this way. But it also turned out that it was really helpful to have a generic protocol for, you know, overriding the behavior of all operations on tensors, not just functions, like sort of analogous to you know, like, if you want to do logging, you want to write some code that works polymorphically over every function and method on tensor, you don\u0026rsquo;t want to have to just write a single you don\u0026rsquo;t have to, you know, do an override for every single method and function you want to do. So what torch function actually does today, is it lets you override all method behavior, all function behavior, and you know, write your own custom functionality. And then, like, you know, have your code that\u0026rsquo;s written against the PyTorch APIs actually use it in this situation. Torch function is pretty useful, and it\u0026rsquo;s already been used a number of different situations. The original request that led us to implementing torch function was someone was writing some code using tensors, and they had sort of units of measure associated with the tensors. So the tensors represented physical quantities, and they wanted to, like, you know, classify tensors based on, you know, what was what. And they had a problem, which is that whenever they did an operation on, say, a voltage, like, say you had a tensor representing voltages, and they added two voltage tensors together, even if they were subclassed in the beginning, when they added those two tensors together, well, the subclass wouldn\u0026rsquo;t be preserved. So originally, the like, the pitch for this was, hey, we want to be able to subclass tensor, and we want the subclassing to be preserved whenever we do operations on classes, because that\u0026rsquo;s pretty useful, right? Like logging sort of works the same way, right? If you have a tensor, and it\u0026rsquo;s a logging tensor with extra metadata on it, well, you need to, you know, get it back another logging tensor after you run an operation on it. Otherwise, your logging will just stop. So in fact, tensors have a default torch function implementation that says whenever you have a call onto a tensor that is a subclass of tensor, we will automatically preserve the subclass for it if all the arguments are that subclass. Otherwise, we\u0026rsquo;ll just say it\u0026rsquo;s not implemented. And you\u0026rsquo;ll have to figure something out in that situation. Another situation that torch function has been used for is this tracing use case. Actually, it\u0026rsquo;s called torch.fx. So what is torch.fx? Torch.fx is a manipulation toolkit for PyTorch programs, what it does is it says, okay, you write your PyTorch program using Python, you can use torch.fx to trace it into some representation, you can do some transformations on it. And then you can reinterpret it, recompile it back into Python code that you know, you might send a torch script or something like that, right? So it\u0026rsquo;s a lightweight, easy to prototype mechanism that, you know, lets you do all the syntax manipulation in Python. And how is torch.fx implemented? Well, it\u0026rsquo;s also implemented using torch function. So what torch.fx does is it has a tracer class, the tracer class implements torch function. And instead of, you know, doing all the normal operations, when you call into torch functions or methods, what the tracer object does instead is it just writes down what happened, and then gives you a new object that is just, you know, another tracer, and then you know, you keep track of things this way. And then, you know, once you have one of these traces, you can do whatever you want to it. But the point is, you didn\u0026rsquo;t have to modify your PyTorch program at all to run it under torch.fx, you can still call regular torch functions on the tracer object, and it all works. Okay, so that\u0026rsquo;s, you know, some of the use cases behind torch function, how does it actually work? And why is it actually so effective? So let\u0026rsquo;s first talk about how it works, because it, the inner workings of torch function explain a little bit why it\u0026rsquo;s so effective. So the way torch function is implemented is it\u0026rsquo;s a purely Python binding concept. What do I mean by that? Well, think remember, in the very first episode of this podcast series, I talked about how PyTorch Python bindings work. And so in general, you know, we have this interface where a lot of code is written in Python, and eventually you cross over into C++, we translate all the Python arguments into C++ arguments, and we pass them on below. So, you know, between there, there\u0026rsquo;s like another level of indirection until you get to the dispatcher, another topic that we\u0026rsquo;ve talked about in a different version of the podcast. And so what happens is that torch function is implemented directly on the Python binding layer. So all of this extra business that, you know, gets you to the dispatcher or the dispatch keys or any of the various subsystems in question, torch function bypasses all of that, right? Like, it happens exactly when you have the Python binding layer. There\u0026rsquo;s a very pragmatic reason this is the case. And that\u0026rsquo;s because when we want to call into torch function, well, torch function is an honest to goodness Python function, right? So we need to pass on all the arguments that we were given. And so we need to actually like keep the Python representation around. So if you go any lower, you know, past the Python binding layer, you\u0026rsquo;ve lost all the Python objects, right, you just have C++ objects, and then you\u0026rsquo;d have to like reconstruct them into Python objects. And that\u0026rsquo;s annoying. So it happens at the Python binding level. But there\u0026rsquo;s a second implication to this as well, which is that we can actually also override the behavior of functions in Python itself. So what happens is we have a number of functions which are implemented in Python. So they\u0026rsquo;re not so so the way we implemented torch function was we wrote some code generated code to insert into all the Python binding sites that basically said, hey, if you see an argument that doesn\u0026rsquo;t look like a normal tensor, it like looks like some object with a tensor torch function, go call that. Well, we have a version of that that lives in Python. So whenever you have a code in Python that\u0026rsquo;s written directly in Python, you can write a little preamble at the top that says, well, if any of my tensor like arguments contain something that looks like it has a torch function, then call the torch function instead of the regular function. And so this way, not only can we bind at the Python binding layer, which is sometimes kind of low level, right? Like, you know, we don\u0026rsquo;t the Python binding level is not public per se, right? Many of the functions that you see there are in fact the public API because they coincide. But many functions are not they\u0026rsquo;re just like sort of internal things, the way that we get into the C++ binding. Well, you can also override the higher level Python operation that actually explains what\u0026rsquo;s the stuff you actually want to do in this situation. And this fact about the torch function implementation that it operates at the Python level, and it can operate both at the, you know, level of the Python bindings, but also any higher level abstractions you read it in Python, it\u0026rsquo;s actually one of the reasons why torch function is so powerful and so popular for doing applications like tracing. And that\u0026rsquo;s because it preserves the high level semantic structure of your program. We actually, you know, one of the questions that I often get about torch.fx is, you know, hey, torch.fx is just tracing, but don\u0026rsquo;t we already have a tracer in PyTorch? And indeed, that\u0026rsquo;s true. We have what\u0026rsquo;s called the autograd tracer. This tracer lives in the C++ level, it lives in the dispatcher, and it also does sort of the same thing as fx, which is that it traces things. So why then is there like another tracer that\u0026rsquo;s fx that\u0026rsquo;s built on this torch function thing? And the answer is fx gets to trace at a much higher level than the autograd tracer because it gets to interpose on actual Python functions. In fact, you know, one of the things that fx is all about is it\u0026rsquo;s all about tracing nn.modules. And because it lives entirely in the Python world, you know, it can actually, you know, record directly what the nn.module you were operating on was when this sort of thing happens, right? That would be totally impossible to do in C++ because C++ has no conception of an nn module, right? Everything has been translated into just plain old function calls at that point in time. Another implication of this is that because it happens at the Python binding layer, you have an opportunity to actually, you know, look at the Python call stack or like, you know, override the meaning of things that are not even tensors. So for example, when you call sizes on one of these fx tracer objects, we don\u0026rsquo;t have to give you an integer in C++, we would have to give you an integer because like C++, if you say you return an integer, it has to be an integer. But in Python, everything is duct type. So we can actually just return you another tracer object and like do the right thing when it shows up in a trace. Which brings me to my second reason why torch function has been so popular. And that is because it is in Python. It turns out that people really, really like to write code in Python. This was actually, it\u0026rsquo;s a little surprising that I didn\u0026rsquo;t learn this lesson given like PyTorch\u0026rsquo;s entire shtick is that like, hey, you just write normal Python and your programs work, but hear me out here. So, you know, we knew that PyTorch, you know, from a machine learning practitioner\u0026rsquo;s perspective, you know, it was really useful to write things in Python. Like that was a essential part of the DNA of PyTorch. But when we were like writing the first version of the compiler, we were like, Oh, no, Python doesn\u0026rsquo;t have strong static types. And we\u0026rsquo;re in the business of writing compiler. And you know, we don\u0026rsquo;t want to write a compiler without having static types, because compilers are complicated, you really want as much help as you can get enforcing all the invariants that you have. So you know, we decided, okay, we have to write the compiler in C++. I don\u0026rsquo;t think this is the wrong decision. Like, you know, having the compiler in C++ is really useful. But what we underestimated was the appetite for, you know, like, sort of short, easy transformations that people might want to do, you know, like, like, you know, democratizing compiler, right? If like, if you had to like, learn about type systems, and programming language theory, and, you know, lower level intermediate representations, just to like, make a little manipulation to your code, you know, that\u0026rsquo;s going to gatekeep a lot of people out of doing compilery things, when actually, you know, that\u0026rsquo;s how they should be solving their problems. And so it turned out that like, giving these tools to people and letting them do them in Python, well, so one is a lot of people needed to do stuff like this. And previously, the only way they could do it was by writing C++. And that was terrible. And the second thing is that things were simple enough that like doing everything in Python was actually tractable. And you know, people could keep track of everything that was going on. So like, hey, you know, like, if you can prototype your entire thing in Python, without having to recompile PyTorch, recompiling PyTorch. Hey, that\u0026rsquo;s a huge win. And so that\u0026rsquo;s one of the reasons why people like this a lot. And like torch function, it being a Python level extension mechanism means you don\u0026rsquo;t have to actually, you know, talk to us PyTorch core or have to rebuild PyTorch to play around with it, you can just write your Python function in your research code, write like, you know, just a stock dependency on PyTorch, no family business going on with C++ extensions. And you can do whatever you want, like sort of crazy interesting stuff. And that\u0026rsquo;s pretty powerful. That being said, there are some downsides to being a purely Python level mechanism. And the biggest downside and one that we\u0026rsquo;ve been working on recently, is that you can\u0026rsquo;t take advantage of any of the machinery that lives below the Python binding layer. And the most important piece of machinery here is autograd. So hey, if you override things with torch function, you don\u0026rsquo;t get autograd anymore. Like if you want autograd, you\u0026rsquo;re going to have to figure out how to do it yourself. That being said, we are trying to figure out how to solve this problem. And the way we are thinking about how to solve this problem is a concept called dispatch to Python. The way dispatch to Python works is that, you know, we still have this torch function binding layer that works in Python, but you can choose to go into the C++ layer. And in the C++ layer, there\u0026rsquo;s a lot of things we can\u0026rsquo;t preserve the Python, you know, status of like, you know, if you have an integer argument, that\u0026rsquo;s going to turn into a C++ integer. Sorry, we\u0026rsquo;re just going to completely forget about the original Python object in that case. But for tensors, we do record what the py object for the tensors are. So all we need to do is make sure that we preserve the idea that Oh, this is a tensor that has some extra Python behavior on it, we blast it through our C++ dispatcher layers doing autograd doing batching, everything like that. And when we eventually get to the final implementation, instead of dispatching to our CPU or CUDA implementation, we just dispatch back to Python, translate all the arguments back into the Python and call into there. And that way, you can actually also take advantage of autograd while still prototyping everything in Python. We\u0026rsquo;re still in the early days of working on this. Functorch, which is being worked on by Horace and Richard, is a sort of experimental, you know, repository working off of this to give functional transformations to PyTorch. It\u0026rsquo;s pretty cool. But you know, like, I\u0026rsquo;m hoping that this can be another really cool tool, complementary to torch function to let people further extend the behavior of PyTorch on the the inside. That\u0026rsquo;s everything I wanted to say for today. Talk to you next time.\nEP22 Why-is-autograd-so-complicated Why-is-autograd-so-complicated Hello everyone and welcome to the PyTorch Dev Podcast. Today I want to talk a little bit about the constraints slash motivations slash things we are trying to do in the Autograd engine in PyTorch. The Autograd engine in PyTorch is the part of PyTorch which is responsible for implementing automatic differentiation. This is very important for a deep learning library. If you think about the tagline of PyTorch in the past, it\u0026rsquo;s a numeric computing library with GPU acceleration with automatic differentiation. The automatic differentiation is how you can write models in PyTorch, run them, and then differentiate through them, finding out what the gradients are so you can go that way when you\u0026rsquo;re optimizing your models. It\u0026rsquo;s very, very important and it was built very early in the history of PyTorch and is still something that people use all the time today. Unfortunately, the Autograd engine is also very, very complex and that makes it difficult for people to understand how it works and it has a lot of features and a lot of, you know, sort of peculiarities and this makes it also difficult to understand. So difficult that I don\u0026rsquo;t think I could actually like technically explain what is going on with the Autograd engine in just a podcast. I\u0026rsquo;d have to actually write a blog post about it. Um, I\u0026rsquo;ve been promising to write a blog post about it for a while. Um, ever since my internals talk, but it\u0026rsquo;s just, it\u0026rsquo;s just a really, really complicated subject. So today what I\u0026rsquo;m going to try to do is do something a little simpler, which is, I\u0026rsquo;m just going to talk about a bunch of the things, a bunch of the important properties that we wanted out of the Autograd engine and some of the implications of those properties. For example, uh, one thing that, you know, we needed for our Autograd engine was it for it to be fast. Like, um, you know, we had a version of the Autograd engine that was written in Python and it was pretty slow and we weren\u0026rsquo;t saturating GPUs when we wanted to, um, run networks on it and that prompted us to port it all to C++. And so, you know, the Autograd engine lives in C++ and it uses multi-threading simply because, you know, at the time it was designed, we needed it to be fast enough to saturate GPUs on common, you know, um, uh, distributed, sorry, data parallel training, uh, regimes. So, you know, that was the only way we could get there. Another thing that, um, the Autograd engine needed to be was, um, it needed a very concise way of writing derivatives for operations. As I\u0026rsquo;ve mentioned before in many other, uh, episodes of this podcast, PyTorch has a lot of operators and, you know, one of the things that, you know, we sort of ensure is the case for every operator someone adds is that it actually has a derivative definition. And so if you had to write, you know, like multiple pages of boilerplate just to add a new operator, because that was how derivatives were going to be generated, you\u0026rsquo;d be in big trouble because like we just have way too much code in PyTorch for anyone to maintain in a reasonable way. And so to get around this problem, we actually built a code generation system for Autograd engine. This code generation system existed from the very beginning of the C++ implementation for Autograd. And one of the like sort of very, um, famous and, you know, you will probably touch it if you ever add a new operator to PyTorch files in, uh, our code base is the so-called derivatives.yaml, which is this yaml file, which for every operator, we know how to do derivatives of, you write down what the derivative of any given operation is with respect to each of the inputs in the function in question. And so most derivatives can be written in a single line. And this just makes it really easy to like, you know, write new derivatives when they are mathematically obvious. A topic that I should talk about sometime is about the code generation pipeline in PyTorch. And, you know, one of the reasons why we have a code generation pipeline, which is, you know, not the easiest thing to, to understand any sort of metaprogramming at this scale is not so easy, but in the case of Autograd, and I think in the case of most of the uses of code generation in PyTorch, it is well worth it because without it, um, C++ just doesn\u0026rsquo;t have a strong enough metaprogramming mechanisms, we would have had to have written a lot of code to just implement one of these things. Like, if we think about like, when you write something in derivatives.yaml, what\u0026rsquo;s going on here? Well, there\u0026rsquo;s a lot of things going on. For example, when you write one of these derivatives, you can refer to inputs that were given to you inside the, um, that you can refer to inputs that were given to you, um, as inputs to the forward implementation. What does that actually mean? Well, what that means is that when we\u0026rsquo;re running the forward of a, um, model in PyTorch, when you refer to an input in the backwards formula, that means we have to save that input so that it\u0026rsquo;s still available when you, um, actually, you know, refer to it in the backwards pass. So, you know, we have to save it. We have to like write a struct. We have to put a place where we can save the thing. We need to actually save it in the forwards thing. And we need to get it out again and plug it into your formula. So that\u0026rsquo;s a lot of moving parts and the code generation handles that all for you. So you could just, you know, it looks like you\u0026rsquo;re just closing over, um, closing over, you know, the input at that time. Like, you know, one way to think about derivatives is they\u0026rsquo;re like just higher order functions, but you know, in C++, that\u0026rsquo;s not so easy to do. So we have, um, a lot of things to make this simpler. Another thing that PyTorch needed to support when doing automatic differentiation was views and mutation, right? So like one of the really big things, part of PyTorch\u0026rsquo;s DNA is that you can take out views from tensors. So these views, you know, don\u0026rsquo;t allocate new data. They share storage with the original tensor in question, and you can also mutate them. So, you know, like if you want to fill in just a single row on a tensor, you could view out that row and then just run fill on it. And our automatic differentiation system actually needed to work correctly, even when people were doing views and mutation. There\u0026rsquo;s a few ways senses in which I mean, it needs to work correctly in the situation. One sense it needs to work correctly in the situation is just sort of basic correctness, which is just to say that, you know, you have a, um, tensor that you want to save for backwards so that you can use it later. And then if someone goes ahead and, you know, scribbles all over it with garbage sometime later in the forward pass, well, you\u0026rsquo;re just going to get garbage out in the backwards pass if you try to reuse that buffer exactly as is. And no, we don\u0026rsquo;t want to copy out variables when we save them because that would be expensive. And remember, we want automatic differentiation to be fast. We don\u0026rsquo;t want to like impose, uh, you know, that kind of overhead on users. And also you\u0026rsquo;d probably run on memory if we were doing that. So to make sure this doesn\u0026rsquo;t happen, we have this mechanism called view counters, sorry, version counters, which, um, uh, record, you know, what, how many mutations have happened to a tensor in question so that when we save it, we can say, oh, you know, three mutations have happened. And then when we come back, we check, you know, is it still only three mutations? If it\u0026rsquo;s five mutations, that means someone\u0026rsquo;s mutated it in the meantime, and we can give a good error message in that situation. But there\u0026rsquo;s another more important, uh, thing that we need to do to support views and mutation with automatic differentiation, which is that we can actually support differentiating through, um, mutations in some situations. For example, if I have a tensor and I, um, you know, take out a view and then write out that view with that tensor, which requires gradients, the result is that my, you know, base tensor, which I wrote into now also requires gradients, right? Because if I use it as part of my loss computation, that bit of the tensor that I wrote in using that view now contains data that, you know, tracks its provenance back to that tensor that I originally, um, requires grad from. And so there\u0026rsquo;s actually a pretty complicated apparatus in autograd. We\u0026rsquo;re making sure we can keep track of what automatic differentiation happens in the situation when you do a mutation on a view with something that requires grad. And this is, um, if you remember the podcast about inference mode, this was some extra metadata that you actually don\u0026rsquo;t need an inference mode and inference mode lets you dispense with doing that. But, you know, when you\u0026rsquo;re doing normal automatic differentiation, you need this information. And so we track it so that you can, you know, do all the things you expect to be able to do in Python. There\u0026rsquo;s some other performance stuff that we do to sort of, um, make reverse mode automatic differentiation work in a predictable way, because at the end of the day, what our reverse AD engine is, is it\u0026rsquo;s this multi-threaded C++, you know, opaque engine that like runs your code and you don\u0026rsquo;t really know like what is going on with it because it\u0026rsquo;s not written in Python, you can\u0026rsquo;t debug it. And furthermore, there\u0026rsquo;s no like direct sequence of calls you make, right? You just call into backward and then a whole lot of stuff happens in that time. So one of the things is it needs to be possible to debug problems in your autograd graph in a reasonable way, right? Because, um, yes, we say PyTorch is this eager mode framework and, um, you know, like you can just write code and write debug statements, but that doesn\u0026rsquo;t really hold true when you do, um, a reverse mode AD because, um, all this stuff is happening without any corresponding source code, by the way, tangent, a research project at Google for doing source to source automatic differentiation. One of their pitches is like, Oh, you know, we\u0026rsquo;ll take your Python program and turn it into a differentiated Python program that you can just debug directly if you need to debug problems. So PyTorch doesn\u0026rsquo;t do that. So what do we do instead? Well, we have a bunch of extra mechanisms built into AD such as anomaly mode, which, um, anomaly mode normally you use to debug why are NANDs showing up in your tensors. But another thing that it does is it, you know, keeps track of what backward operations correspond to what forward operations. So when something fails in a backwards operation, it\u0026rsquo;ll tell you, and by the way, this was the forward operation, the back trace that actually caused that situation. Another thing that we do is we have a pretty sophisticated hooks mechanism whereby you can insert arbitrary pieces of Python code at any point when you\u0026rsquo;re running your, um, backwards, uh, you know, computation and say, Hey, you know, give me what the gradient is at this point in time. And let me take a look at it, you know, maybe modify it if I\u0026rsquo;m doing some weird gradient scaling or something like that. But really, you know, I can just take a look at it and figure out if, you know, it\u0026rsquo;s what I expect or not. It\u0026rsquo;s the way of inserting say debug print statements. And so, you know, these things are not conceptually complicated, but a lot of, you know, effort is spent inside, um, the Autograd engine. So if you\u0026rsquo;re like reading the code and you\u0026rsquo;re like, Oh, what is all this hooks business and this anomaly mode business? Well, it\u0026rsquo;s not important to the core algorithm, but it is important to making sure users get a good experience when using the Autograd engine. There\u0026rsquo;s also some really unusual features that our Autograd engine supports, which also add to the complexity of the implementation. So one of these things is so-called re-entrant execution. What does re-entrant mean? Re-entrant means you\u0026rsquo;re inside some sort of procedure and you want to call back into the procedure again while you\u0026rsquo;re inside it. So you\u0026rsquo;re re-entering while you\u0026rsquo;re already inside. So re-entrant execution in the context of automatic differentiation, the Autograd engine is you\u0026rsquo;re in the Autograd engine, you\u0026rsquo;re executing, you know, your, um, backwards functions one by one. And then inside one of those backward functions, you actually execute, um, Autograd again. Why would anyone want to do that? Well, one, one answer to that is, you know, like Autograd is just this operation, right? Like it computes the derivatives of a function. And so like that just is a normal mathematical computation that, you know, you should be able to do anywhere. And in the other, in other words, grad should be composable, but there\u0026rsquo;s another like sense in which re-entrant execution is really useful. And that\u0026rsquo;s for checkpointing in PyTorch. Checkpointing is this trick for reducing the memory usage of your models that says, Hey, I\u0026rsquo;m not going to record the, um, saved variables. Remember that, right? Um, I\u0026rsquo;m not going to record the saved variables for everything in my network. Instead, I\u0026rsquo;m going to force the network to re-compute, um, the, um, variables when I actually get to them. I\u0026rsquo;m trading away, uh, compute so that I can reduce the amount of memory I use. So how do we implement re-entrant, uh, how do we implement checkpointing in PyTorch? Well, we do it with re-entrant execution. What we do is we, um, run our PyTorch program, we run the forwards, and we just don\u0026rsquo;t save anything. And then when we come back in the backwards and we need to figure out how to, uh, you know, execute the, um, backwards formula, well, we\u0026rsquo;ve failed to save anything. So what we do is we just rerun the forwards again, and then re-entrantly call backwards on it to get the, um, actual backwards, uh, uh, computation, uh, computed in this case. Um, this was implemented by Priya Goyal back in the day and people use it. And so it\u0026rsquo;s, you know, one of the most important use cases for re-entrant execution in PyTorch. There, there\u0026rsquo;s a bunch of like complicated stuff where, uh, you can actually get into these, this bad behavior where, um, you keep re-entering, uh, over and over again, and then you blow your stack space. And there\u0026rsquo;s also some logic in the Autograd engine to deal with that. One last thing that the Autograd engine supports, which is that, um, normally Autograd Autograd is this thing you think of as running on a single process on a single machine, right? Like you just run Autograd, you\u0026rsquo;ve got your entire graph. Well, in the distributed setting, we actually have an implementation of distributed Autograd, which allows you to distribute Autograd across multiple processes, uh, across multiple nodes in case, you know, your program in question is too big to run on a single processor. And so there\u0026rsquo;s a sort of like specialized version of Autograd, um, called distributed Autograd, which uses many of the same implementations, but override some important stuff that makes it possible to just run, um, Autograd in this distributed fashion. So that\u0026rsquo;s pretty cool. Also complicated in its own right. You can read more about it if you\u0026rsquo;re interested. So why is Autograd so complicated? Well, one is that there are a lot of features. There\u0026rsquo;s a lot of performance requirements. And, you know, when you put it all together, there\u0026rsquo;s just, you know, you have to work pretty hard to do something like this. Um, so that\u0026rsquo;s one of the reasons why, um, for example, in my previous podcast, it was really, you know, interesting for people to be able to reuse our Autograd engine directly because, Hey, um, you know, we\u0026rsquo;ve already done all this stuff, so you\u0026rsquo;d like to reuse it in that situation. But, you know, there\u0026rsquo;s also like something to be said about a simple implementation of Autograd that, you know, is hackable, maybe doesn\u0026rsquo;t have all the efficiency, doesn\u0026rsquo;t have all the features, but, you know, just has the core, um, algorithms for Autograd. That\u0026rsquo;s a good idea too. And we have a bug report that\u0026rsquo;s tracking this issue. So, um, hopefully you\u0026rsquo;ve come away from this with a little more appreciation of, you know, why Autograd is more complicated. And so if you\u0026rsquo;re ever looking at this code and you\u0026rsquo;re like, Oh, what is this business with hooks? What is this business with, um, you know, this view metadata? What is this business with this multi-threading nonsense? Well, hopefully, um, this podcast has given you some clues about why those things might actually be there. That\u0026rsquo;s everything I wanted to say today. Talk to you next time.\nEP23 Code-generation Code-generation Hello, everyone, and welcome to the PyTorch Dev Podcast. Today, I want to talk about code generation in PyTorch. Code generation refers to the practice of writing other scripts that generate code for you. In the case of PyTorch, these are our Python scripts, which get run as part of the build process and produce a lot of C++ files that actually make it possible to build PyTorch as a whole. Code generation is kind of the heavy guns, right? Because when you start code generating your code base, a lot of things stop working. For example, if you\u0026rsquo;ve got an IDE and you want to jump to definition, well, it, whoops, looks like your, you know, method you\u0026rsquo;re looking for is actually in a code generated file, which means it\u0026rsquo;s not in your working directory, and you have to go build PyTorch first before you can actually go look at it. So it makes things kind of confusing. And, you know, most of the time, if you\u0026rsquo;re just writing a C++ project, you try very hard not to do code generation. But in the case of PyTorch, we\u0026rsquo;ve used code gen actually from the very beginning of the project, even back in the days when A10 wasn\u0026rsquo;t even a thing. And it\u0026rsquo;s ended up being a pretty good tradeoff for us in terms of what it allows us to do. The high level of why, you know, code generation tends to be a good idea is that it lets you greatly reduce the amount of code you have to write manually in a project. If you had, you know, a, you know, hundreds of classes that you would have had to have written, you know, one by one. Well, if you have a code generation pass, you can just generate them all from, you know, a few lines of YAML. And you don\u0026rsquo;t have to worry about it. So that\u0026rsquo;s what it\u0026rsquo;s doing in PyTorch. Code generation is being used to generate a lot of code that we otherwise have to write hand by hand. It makes the framework more maintainable. But, you know, it is kind of complicated. And so I just want to talk a little bit more about what kind of stuff we\u0026rsquo;re using code generation for. Also, what are some of the pros and cons of using code generation and some other counterpoints in the design space? Because code generation isn\u0026rsquo;t the only way to skin the cat necessarily in some situations. Okay, so what are we using code generation for in PyTorch? There\u0026rsquo;s a lot of things that we\u0026rsquo;re using it for. And at a high level, the biggest way to think about, you know, why we\u0026rsquo;re using code gen for any given thing is because usually it was something that we needed that you can\u0026rsquo;t do with plain old-fashioned C++ metaprogramming with templates. So a really simple example of, you know, C++ just doesn\u0026rsquo;t support enough language features to do this is a generation of APIs like functions or methods on classes, you know, based on a small amount of data. So for example, we have a type named tensor, and it supports a lot of methods on it. And those methods essentially call into another class. It\u0026rsquo;s really a dispatch mechanism that, you know, is very uniform. So like for every method, what it does is it just, you know, takes its arguments and calls into another function that like actually does the method processing for us. And in one of the, you know, philosophies in the C++ API in PyTorch is that, you know, we want it to be possible to just write the same code you would have written in Python. So if you wrote x.add in Python, you can write x.add in C++. But C++ doesn\u0026rsquo;t have operator dot overloading. So we have to actually manually write out every method by hand whenever we want to write a class like tensor, which supports a method like this. So we don\u0026rsquo;t write out these methods by hand because we have hundreds of methods on the tensor class. This is how we use code generation to actually do this. Another example of us using code generation is when we do automatic differentiation, see my previous podcast, we need to generate a class representing the set of saved data for any given piece of autograd information. And we actually generate one class per piece of autograd per operator because autograd might save different things depending on the operator in question, right? Because there might be different mathematical values from the inputs that you need to compute the derivative in these cases. We don\u0026rsquo;t do a box representation for autograd because that would be less efficient. Instead, we just have a specialized class for each operator that only contains fields for exactly what we need. And oh no, once again, there\u0026rsquo;s no way to, in C++, conveniently generate a ton of classes with slightly different fields based on some simple specification of what the things are. So instead of having to write them all out by hand, we also use code generation to generate this code generation is also used in some cases to deal with things that don\u0026rsquo;t live in C++ at all. For example, we have a bunch of Python bindings. We do code generate the arg parsing logic for parsing the arguments from them. But we also need to generate pi i stubs, type stubs, that make the type information available for all the C bindings in question. Well, how do we do that? Well, there\u0026rsquo;s thousands of operators. So once again, we co-generate the pi i. So someone, we didn\u0026rsquo;t used to have this capability. We didn\u0026rsquo;t have any type stubs for it. And all someone had to do was just go and write an extra Python script that knew how to generate these Python type stubs. And that was it. They didn\u0026rsquo;t have to like painstakingly go through every operator in PyTorch and figure out what their type signature would be. And then saddle us with the burden of having to continuously maintain this extra set of stubs. Instead, it just gets generated by code in this situation. Some of the time, what we do is we say, OK, you want to implement an operator and you need to implement a CPU and CUDA version of this operator. And usually there\u0026rsquo;s a fixed prototype that we expect a user to implement in the situation. So we also use code generation to generate the prototypes for these functions so that, you know, you know what you need to implement downstream. OK, so those are some of the main uses for code generation inside PyTorch. So what are the benefits of using code generation? As I said, I\u0026rsquo;ve harped on repeatedly about, you know, often we use code generation when there\u0026rsquo;s no other choice. We just can\u0026rsquo;t do what code generation wants to do using just C++ templates or other mechanisms. But there\u0026rsquo;s also other reasons why code generation is something that, you know, we reach for. For one, when we build a code generation system in Python, we can actually do much more complicated things with surface syntax. For example, we have a native functions.yaml. Inside it, we have this miniature domain-specific language for specifying JIT schema, which is like something that we have to write a parser for. And, you know, we also have derivatives.yaml, which is this compact representation for writing derivatives for functions. And, yes, in principle, you can write a templated piece of code that is a parser for some arbitrary syntax. And people have done this just to show that it can be done in C++. But in general, C++ is much better at, like, modeling metaprogramming based on, like, C++ types, right? Like, that\u0026rsquo;s how, you know, partial specialization and tricks like that work. So C++, really compact code when you, like, want to look at the type structure of your C++ programs and metaprogram off of that. Really bad, horrible, awful, no-good-looking code when you want to, like, implement a parser that happens entirely at compile time. And, yes, constexpr makes things better. And the, you know, bigger your C++ version is, that also makes it better. But, unfortunately, PyTorch is still stuck on C++ 14. Hopefully, we\u0026rsquo;ll get to C++ 17 soon. But, you know, we need to work in a lot of different platforms. And that sort of puts a limit on how futuristic our C++ code can be. Another reason that we like using Python code generation is it makes it easier to write better error messages. Template error messages in C++ are famously horrible, right? Maybe if we get C++ concepts in the future, things will get better. But, like, you know, a lot of people don\u0026rsquo;t really know how to debug C++ template errors, but are perfectly fine if, you know, it\u0026rsquo;s just a Python script. And there\u0026rsquo;s, you know, albeit a complicated Python script, but it\u0026rsquo;s, you know, raising an exception somewhere. Because then you can add print statements, you can, like, look at, you know, you can tweak things around, you can print extra things out. And it\u0026rsquo;s just easier to, you know, deal with than C++. Yes, you can figure out how to do all of these things in C++. But C++ metaprogramming debugging is a skill, and most people don\u0026rsquo;t have this skill. Whereas most people do, and when I say most people, I mean, like, you know, most developers on PyTorch. Most developers on PyTorch do know how to write Python code, do know how to debug Python code. So that makes things a lot easier. A sort of similar thing related to this is that in C++ templates, you often have to do very complicated encoding mechanisms to, like, represent complicated data structures, because, like, as I said, C++ is all about, like, operating on types. And if you actually want to do data, well, you have to work pretty hard. And in Python, well, you can just write a data class and, you know, use that to represent whatever data you need to pass around. In fact, our code generation is very strongly typed Python. We use data classes everywhere, frozen data classes, and it\u0026rsquo;s fully type annotated with MyPy. And that makes it easy to also do refactors, where you just, you know, make a change to the data type, and then you just look for all the places you need to update in the situation. One last thing. With a code generation framework, we generate C++, which then is compiled by the C++ compiler, which means that if something isn\u0026rsquo;t working, you can look at the generated code and be like, hmm, is this the code that it would be written by hand? And so it\u0026rsquo;s just generally easier to reason about the performance characteristics of Python-based code generation, because you\u0026rsquo;re often trying to generate code that looks like code that you would have written by hand. And with templates, it can be obscured, because there\u0026rsquo;s this level of indirection. You\u0026rsquo;re never actually looking at the code that actually gets generated, and it\u0026rsquo;s easy to accidentally put in inefficiencies when you write things that way. I spent this whole time, like, saying what the pros of doing code generation are, but, like, there are also some very big cons, right? So I\u0026rsquo;ve talked about a few of them already, such as that code generation is complicated. A lot of people don\u0026rsquo;t really want to, like, deal with this random Python script that is generating code. If you do a bad job at maintaining your Python code that generates C++, it can be really, really hard to maintain. In fact, that was the state of the old code generation before we wrote it again with strong types. But there\u0026rsquo;s some less obvious cons to code generation as well. One is that code generation is not portable. What do I mean by that? What I mean is that, let\u0026rsquo;s say that, you know, you have some stuff that generates code for you, and then you have some external user of PyTorch that also wants to make use of this code generation pipeline. If I had a C++ template, I could just say, oh, instantiate the C++ template in your project, and then you can get whatever functionality the C++ template gave you. And they don\u0026rsquo;t have to do anything extra in their situation. Whereas if I have a Python code gen script, well, now I have to, like, actually design the code gen script to be runnable outside of PyTorch for some, you know, extra data that the user does in question. And it\u0026rsquo;s just, there\u0026rsquo;s a lot more work you have to do to make sure something is publicly available. We are doing some of this work, actually. So for external backends, we spent a long time giving only a C++ template-based API for registering extensions. But it eventually became clear to us that that just wasn\u0026rsquo;t enough. We didn\u0026rsquo;t have enough features to do it. And Brian Hirsch has been working on out-of-tree code gen for backend extenders. It\u0026rsquo;s pretty cool. I\u0026rsquo;ll post a link to it in the podcast description. But, like, you know, we spent a long time not doing this because, well, there\u0026rsquo;s a lot of work you have to do to actually make external code gen work. And I just want to talk a little bit more about, you know, I said previously that C++ templates are pretty good for doing metaprogramming based on the C++ type system, right? And it makes sense because it\u0026rsquo;s built into the C++ compiler, which knows all the vagaries of how C++ types work. And it has turned out that when we write Python code generation framework, we actually need a, like, you know, model of the C++ type system, because sometimes we just need to do administrative stuff, like conversion from one type to another. And, well, you know, the best way to do that is to actually know something about C++ types so that you can, like, you know, basically run the whatever implicit conversions or type matching that C++ would have done in this situation. So, we had to implement that. We have a crappy version of the C++ type system in our code gen. It would have been easier to do this in C++ itself sometimes, perhaps. Because sometimes it\u0026rsquo;s very easy, but, you know, when you add a little extra feature, then it becomes difficult to do something with templates. So, I spent most of this podcast being like, hey, you know, you can either do code generation or you can do C++ templates, and these are two points in the design space for doing this kind of thing. And one of the reasons why I put these as the two, like, possibilities is because both of these have the same efficiency characteristics, assuming you\u0026rsquo;ve done it correctly, right? C++ templates get instantiated every time you give them some parameters so they can generate code that\u0026rsquo;s just as efficient as if you had written it by hand, which is what, you know, a code generation would do. But there\u0026rsquo;s actually a third point in the design space, namely boxed fallbacks. So, what are boxed fallbacks? Boxed fallbacks are basically a way of writing polymorphic code that runs at runtime rather than at compile time. And the way this is done is by making sure all of the inputs to an operation in question are boxed. They\u0026rsquo;re stored in a uniform representation called an I value, and then you can actually write C++ code that\u0026rsquo;s polymorphic in a sense. By the way, if you\u0026rsquo;re used to be able to doing generic programming, say, in Python or in Java, where you just, you know, like, write some, use, like, a reflection API or something like that to write code that works no matter what the types of inputs are, you know, that you\u0026rsquo;re also taking advantage of the fact that those languages, their internal data representations are all boxed. They\u0026rsquo;re all uniform. So, you can just write runtime code that does this. C++ doesn\u0026rsquo;t have that. So, we have to actually turn things into their box representations before we can write this uniform code. Boxed fallback code is often way simpler to write. I recently, Brian, once again, he\u0026rsquo;s been working in this space. So, he\u0026rsquo;s the expert. Brian has been, you know, taking some code that we used to do in CodeGen and writing it using a boxed fallback, namely some CPU fallback code. So, what does this do? It just says, hey, I want to run an operation, but I don\u0026rsquo;t have it implemented for XLA. So, I\u0026rsquo;m going to cast it to CPU and then run the operation on CPU and then put it back in XLA. And it\u0026rsquo;s really, like, easy to do the boxed fallback version. You just do the obvious thing. You, you know, iterate over the arguments. You look for ones that are XLA tensors, convert them to CPUs, call the actual thing, and then, you know, iterate over the results and turn them back into XLA. Very, very simple. You\u0026rsquo;d have to do quite a lot of work to, like, write the code generation version of it. And you\u0026rsquo;d probably have to do less work, although still some amount of work, to write the CPUs plus template version. The boxed fallback is very simple. It\u0026rsquo;s easy to debug as well because you can add print statements in the normal way. There\u0026rsquo;s no templates involved. The problem is it\u0026rsquo;s less efficient, right? Because you\u0026rsquo;re boxing things up and you\u0026rsquo;ve got this little interpreter that, you know, has to go and look at what the types of everything are. So, boxed fallbacks, simple. And, you know, they work at runtime. So, they, like, work even when you can\u0026rsquo;t see the code in question that you might need to do. But it\u0026rsquo;s less efficient. So, you probably only want to use them in cases where efficiency isn\u0026rsquo;t important. And CPU fallback is definitely one of those cases because, well, you\u0026rsquo;re falling back to CPU. So, like, you don\u0026rsquo;t expect it to be fast. You\u0026rsquo;re just trying to make it work at all in the first place. So, that\u0026rsquo;s most of everything that I wanted to say about code generation. One of the open questions that I have as a programming languages person is, is there a way for us to have the best of both worlds, right? So, I had this picture of, oh, I can metaprogram things ahead of time and it\u0026rsquo;s kind of complicated, but it\u0026rsquo;s really efficient. Or I can write this interpreter that does everything at runtime. It\u0026rsquo;s simple to write, but less efficient. Can I have the best of both worlds, for example, by writing an interpreter and then partially evaluating it so that I can get the fast compile time version? Well, I can\u0026rsquo;t easily do this if I write my interpreter in C++, but maybe if I write it in a different language, it\u0026rsquo;ll be easier to do. That\u0026rsquo;s something that I\u0026rsquo;ve kind of been thinking about, although we don\u0026rsquo;t really have any concrete projects for dealing with this. That\u0026rsquo;s everything I wanted to say for today. Talk to you next time.\nEP24 torch.nn torch.nn Hello, everyone, and welcome to the PyTorch Dev Podcast. Today, I want to talk about Torch.nn, PyTorch\u0026rsquo;s public API for actually building neural networks. Of course, if you are a user of PyTorch, Torch.nn is one of the very first things you actually learn how to use, and there are lots and lots of documentation about all sorts of ways to use modules in PyTorch. And as this is a dev podcast, I\u0026rsquo;m not going to really talk about how to use Torch.nn so much as if you are a maintainer or a potential contributor to the library, and you want to make modifications to Torch.nn, well, what are the kinds of things you\u0026rsquo;re going to have to worry about? What are some of the philosophies behind the design of this component, etc? So let\u0026rsquo;s dig in. So Torch.nn, as I said previously, provides the NN module abstraction, most importantly, which is how most people put together their deep learning modules. Why does Torch.nn exist? Well, it exists because when you are setting up your modules, model, you have a lot of computations that you want to do, you have a lot of parameters. And you need a convenient way to keep track of all your parameters. Because for example, when you are doing optimization, you need to iterate through all your parameters, and you know, apply the gradient you computed for each of them to the result. And so if you\u0026rsquo;re a purely functional person, like in Jax, actually having to like manually keep track of all your parameters, and you know, like a global spot in your application gets kind of annoying when your model gets very big. And so what Torch.nn does is it gives you a convenient object-oriented like interface that automatically can collect up all the parameters for you, so that you don\u0026rsquo;t actually have to keep track of it yourself. You can just ask, hey, what are the parameters of this model? And it\u0026rsquo;ll tell you all of them. Pretty cool, right? Another thing that is really important about Torch.nn is unlike many of the other pieces of PyTorch, which we\u0026rsquo;ve moved to C++, because, well, you know, C++ is faster, we\u0026rsquo;ve tried very hard not to actually move Torch.nn to C++. And so if you crack open the Python files in PyTorch itself, because, hey, you know, how is convolution implemented? Well, it\u0026rsquo;s still a, you know, plain old Python class that you can, for example, copy paste into your own project and tweak however you need. And so another reason why Torch.nn is in Python is it\u0026rsquo;s more hackable, right? Like a lot of times you are, you know, doing something that someone has done before, but maybe with some tweaks. And there\u0026rsquo;s nothing wrong with copy pasting code and research code. It\u0026rsquo;s probably the fastest way to get going. And, you know, long term maintainability isn\u0026rsquo;t as much of a concern. And so we wanted to make sure this was still something that people could do when they wanted to do those things. Of course, getting all these features to work ends up being pretty complicated. So if you\u0026rsquo;ve ever cracked open module.py, the module that actually implements module for real, it\u0026rsquo;s actually really, really long, and there\u0026rsquo;s tons and tons of stuff going on. So let\u0026rsquo;s just talk about the most important things that it\u0026rsquo;s doing. So one, I said that modules are able to collect all parameters. How do we know if something is a parameter or not? Well, in PyTorch, there\u0026rsquo;s a parameter subclass of the tensor, which is how you make this distinction, right? So anything that is a parameter, and you put it into a module, we will keep track of it. Anything that\u0026rsquo;s not a parameter, just a plain old tensor, we won\u0026rsquo;t keep track of that. In order to keep track of all the parameters you put on the module, we need to override the behavior of what happens when you modify fields on your modules. So most modules override behavior of set adder and get adder to basically say, hey, when you set an attribute on my module, is it a parameter? And if it is a parameter, then we actually just go ahead and, you know, put it in our record of all the parameters that are on the module. So that\u0026rsquo;s another piece of like complication inside the implementation of module. Some other thing modules need to support while modules support being transitioned from one device to another, traditionally, the way that you like allocated module on CUDA is you first allocate it, and then you run dot CUDA on it. So another thing that modules need to support how to do is, you know, find all of the things in the module, all the tensors, and not just the parameters, but also other buffers and also any recursive sub modules that are also part of this module, and also make sure things get called on them. And so there\u0026rsquo;s a, you know, little helper function called underscore apply, which knows how to iterate over what essentially is every tensor in the module and apply an operation to each occurrence of it. Another thing that modules implement are hooks, hooks are ways of just interposing in on the behavior of modules without having to manually write in code in every location. And to implement this, well, you know, when you define a module, you write a function called forward. But when you want to actually invoke a module, you don\u0026rsquo;t call the forward function directly, you call the operator call, like underscore underscore call, like just a planal function call on the module directly. And that call does a bunch of work, it like processes hooks and figures out all the sort of administrative stuff before actually calling the forward implementation to do the actual thing you want to do. So there\u0026rsquo;s a lot of goop in module.py. But you know, if you just keep these three things in mind, right, like we need to keep track of the parameters. So there\u0026rsquo;s overriding behavior of set editor and get editor, there\u0026rsquo;s implementations of these functorial operations, which operate over all the tensors on the module. And then there\u0026rsquo;s a bunch of hooks and interposition that, you know, let people tweak the behavior of modules without having to edit them manually, you\u0026rsquo;ll actually, you know, be able to understand a good majority of the lines of code in modules up high. There\u0026rsquo;s really only two other things you have to worry about. One is serialization, right? Like a really important thing to be able to do is once you have your module, and you have trained it, you want to dump all the parameters to disk so you can use them them again later. Well, similar to how we keep track of all the parameters, there\u0026rsquo;s also a notion of sets of things that actually get persisted when you serialize a module, the recommended API for doing this is state dict, which just gives you a dictionary mapping from key names to tensors that says all of the parameters in question, you can also technically pickle the module directly, although this is a lot more fragile, because pickling requires you to actually maintain exactly the same name of the module, and exactly the same module that the module is defined in module in the Python module sense. One last complication, when writing modules in PyTorch itself, is most modules in PyTorch are what we call torch scriptable. What\u0026rsquo;s torch script? Well, torch script is our compiler for PyTorch models. And essentially, what it lets you do is if you have a torch scriptable model, you can translate it into torch scripts intermediate representation. And then you can, for example, ship it in a like Python agnostic form, or you can also run some optimizations on it. And because torch script is a compiler, but Python is really complicated, there\u0026rsquo;s some restrictions that apply when you want to write modules, because you need to make sure they\u0026rsquo;re actually torch scriptable. The most obvious restrictions are that there\u0026rsquo;s a limited set of types you\u0026rsquo;re allowed to use, because the interpreter in torch script doesn\u0026rsquo;t support arbitrary types. And you also have to make sure that the set of Python you use inside your forward function is the set of Python that is actually understood by torch script. Although torch script actually does support a lot of Python features. So chances are normal things you do are going to be understood. One of the more unconventional things about how torch script compiles modules, it\u0026rsquo;s it\u0026rsquo;s actually a staged computation. So when I imagine compiling an NN module, you could imagine compiling an NN module, including the constructor and the forward implementation. But that\u0026rsquo;s not actually how torch script works. What torch script does is it first instantiates the module as a normal Python, so you actually construct the module. And only once you\u0026rsquo;ve constructed the module, do you actually then attempt to compile the forward implementation on it. There are some benefits to doing this. In particular, because the initialization of the module happens in ordinary Python, you can go wild with anything you want in this case. And you know, there\u0026rsquo;s no restrictions on the initialization code for the modules, you can do anything you want. And furthermore, once you\u0026rsquo;ve actually initialized all the attributes on the class in question, torch script has a much more accurate much more accurate picture about what the actual parameters on your class are. So if you have some weird situation where you know, if you pass in a parameter, and it\u0026rsquo;s true, you allocate a parameter. And if it\u0026rsquo;s false, you don\u0026rsquo;t allocate the parameter. Well, Tor script can handle this fine, even though Tor script is statically typed, and you need to know exactly what all the fields on your module are. So that\u0026rsquo;s some of the things you have to be aware about when you\u0026rsquo;re working on modules in NN module. What else? Well, there\u0026rsquo;s been some new developments in NN module. Shocking, I know, because everyone and their dog subclasses from modules. So when we make changes to the class, we have to be very careful, because there\u0026rsquo;s a lot of people who will be very unhappy with us if we ever break backwards compatibility on modules. That being said, we\u0026rsquo;ve been able to come up with some new things that like make modules easier to use. One of the coolest new additions is the concept of lazy modules. I\u0026rsquo;m authored by EM Castillo from preferred networks. What lazy modules do is solve a common problem that you have when you\u0026rsquo;re trying to construct a model, which is that you don\u0026rsquo;t know how big the parameters should be. Because you know what\u0026rsquo;s going on while you\u0026rsquo;re passing in some input of some known input size, and it\u0026rsquo;s going through your model. And at some point, you\u0026rsquo;re like in the middle of the model, and you need to provide an FC layer. And that FC layer needs to know how big the input is, because the parameter in question is going to be, you know, the size of the input times the size of the output. But you have no idea what the input size is going to be like, you know, you\u0026rsquo;ve run a pile of convolutions, who knows what the result is going to be. And you don\u0026rsquo;t want to have to manually, you know, compute what the sizes at that point in time. So prior to lazy modules, you had to suck it up and like add some print statements to figure it out what it was. With the lazy module, you just say, okay, well, lazy FC, with what the output size is supposed to be because that\u0026rsquo;s not specified. And then the first time you run the forward on the module in question, it says, hey, this input is size x. Okay, now I\u0026rsquo;m going to evaluate the now I\u0026rsquo;m going to allocate the parameter, because I know what the size of the input is. Another really interesting recent development is for the longest time, you couldn\u0026rsquo;t actually allocate a module directly on CUDA. And so we forced everyone to like allocate on CPU first and then move it to CUDA. This wasn\u0026rsquo;t too bad when models were small, but people are really excited about really big models. And sometimes the models are so big, you can\u0026rsquo;t even fit them on a single machine. So how the heck are you going to construct a module in that case when it\u0026rsquo;s too big to fit on your machine. So what Joel Schlosser has done is he\u0026rsquo;s added a new device keyword argument to all the modules in PyTorch. So what does this mean? So if you are constructing a module in PyTorch, and you pass in device equals CUDA, when you construct it, instead of constructing a module on CPU, and then moving it to CUDA, what it will do instead is it\u0026rsquo;ll directly construct the module on CUDA. This patch was super simple, right? All we did was like, edit the initialization code to actually respect the device. But, you know, I don\u0026rsquo;t know why we hadn\u0026rsquo;t done it before. But you know, Joel actually made it happen. And we\u0026rsquo;re hoping that throughout the rest of the PyTorch ecosystem, people will start following this convention. And so given an arbitrary module, you can just pass in device and get the module on the device in question. One of the like cool interactions with this other feature that we\u0026rsquo;ve been working on called meta tensors is if you say device equals meta, what you\u0026rsquo;ll get is you\u0026rsquo;ll get a constructed module, but all of the tensors will be not allocated, there\u0026rsquo;ll be meta tensors saying what their sizes are. And then you can do post facto analysis on it in this situation. One of the open questions for us with the NN module design, there\u0026rsquo;s a few things. So one problem that is coming up for us soon is we actually do need some sort of functional version of modules because sometimes you\u0026rsquo;re doing sort of higher order training, or you\u0026rsquo;re doing APIs that only work on purely functional programs. And in those situations, like the very stateful nature of PyTorch NN modules doesn\u0026rsquo;t work so well. So that\u0026rsquo;s one thing like given a module, can we turn it into its functional version? Another open problem that has been plaguing us for a while is many of the weight initializations in PyTorch are very out of date, like they basically hearken all the way back to LuaTorch days. And the research has gone beyond and figured out that there\u0026rsquo;s better ways to initialize weights in these ways. And we\u0026rsquo;re stuck in a hard place because well, on the one hand, we like to update the initializations. But on the other hand, if we do that, lots of people\u0026rsquo;s, you know, pre existing models, might break because well, they may be expected some particular initialization. We have some ideas about how to fix this, like imagine some sort of like version that you can specify, hey, I want weight initialization version three, and that comes with all the updates and you just explicitly opt into it. But no one has really implemented this yet. And something I\u0026rsquo;m kind of interested in seeing done at some point. That\u0026rsquo;s everything I wanted to talk about NN module today. Talk to you next time.\nEP25 Mobile-selective-build Mobile-selective-build Hello, everyone, and welcome to the PyTorch Dev Podcast. Today, I want to talk about mobile selective build. PyTorch is a project that is trying to do a lot of things. And one of the more unconventional things that the project tries to do is we use the same code base that you use for doing your, you know, good old fashioned Python training loops in Python on your regular desktop. And we also use this code base for actually deploying PyTorch mobile models on mobile, like so that you can run some, you know, image model on your phone and get a result back without actually having to go back all the way to a server. So this is kind of crazy, actually, because mobile is a completely different universe than server side programming. And there\u0026rsquo;s one particular aspect of it that I want to talk about today, which is selective build, namely the fact that when you are writing applications that go on mobile, binary size is really, really, really important. On server, binary size isn\u0026rsquo;t that important. It is kind of important because if your binary gets too big, like say four gigabytes big, then a lot of tools like the debugger stop working. But it takes a lot of code to get to four gigabytes. On mobile, this isn\u0026rsquo;t really the case. You really, really want your app to be as small as possible. Because you\u0026rsquo;ve got people who are downloading your app on really, you know, shitty cell phone connections. And if your app takes up a lot of binary, then they\u0026rsquo;re not happy. And without really having any sort of clamp on the binary size, the easy thing for a software project to do is just keep going and going in binary size. And so there\u0026rsquo;s very stringent restrictions about binary size, people will yell at you if binary size increases too much. And it\u0026rsquo;s in this context, that PyTorch designed selective build. So what is selective build in PyTorch? Well, this is the concept that, hey, PyTorch comes with a lot of operators, right, a lot of support for many, many different operations. And half the time, you\u0026rsquo;re not using even half of these operators for any given model, right? If you\u0026rsquo;re like doing a ResNet, oh, so old fashioned, but if you\u0026rsquo;re doing a ResNet, there\u0026rsquo;d probably only be 20 or so operations that you actually need out of PyTorch\u0026rsquo;s, you know, more than 1000 operators. So what\u0026rsquo;s the idea? If you are shipping some models to mobile, and you know, what the set of models you want to do are, well, don\u0026rsquo;t ship all the operators ship only the operators that you actually need to run on mobile, and you\u0026rsquo;ll get big binary size savings, and everyone will love you. And also, all of the people who are, you know, frantically working on adding new functionality to PyTorch, they don\u0026rsquo;t have to worry about going over some binary size limit, because all that stuff isn\u0026rsquo;t actually going to be used. Now, ordinarily, when you are building some application for mobile, typically, the way you do it is you build everything statically, and you statically link everything together. And static linking has this interesting property, which is that we know exactly what is being used inside a statically linked application. So if a function is not being used, we can actually just prune it away. And all of linkers will do that automatically in that situation. You can\u0026rsquo;t do this, by the way, for a dynamic library, because a dynamic library offers a public API. And anyone else, even people you know nothing about, could make use of any of the exposed functions in your dynamic API. So usually, everything has to be put in. So if a static library can be done this way, why doesn\u0026rsquo;t, you know, elimination of operators that you don\u0026rsquo;t need happen automatically in PyTorch? Well, there\u0026rsquo;s two reasons. So one is that when we run, when we run models on mobile, we\u0026rsquo;re running them via an interpreter, either the TorchScript interpreter, or the light interpreter, which is a sort of pared down version of TorchScript that has less support, but, you know, is smaller in binary size and runs a little faster. So when you have an interpreter, one of the things in the interpreter loop that you need to do is you need to, you know, look at your op code, which says, hey, run this operator, and then have a giant switch statement for all the operators that you understand and, you know, have a call to each of them. And obviously, static linking isn\u0026rsquo;t going to know that, well, this particular branch, which is doing some, you know, mish activation or whatever, isn\u0026rsquo;t actually ever going to be used by your model, because it can\u0026rsquo;t know, there\u0026rsquo;s no way for it to know. So we need to tell the interpreter, hey, you know, these ops, you don\u0026rsquo;t need to compile in, you can\u0026rsquo;t get it automatically with static linking. But let\u0026rsquo;s say you wrote your model directly in C++, which is something you can do. And you could actually use to deploy models, although most people don\u0026rsquo;t, because it\u0026rsquo;s a pain in the ass to update native code on mobile, because you have to, you know, build an entirely new version of your app, it\u0026rsquo;s much easier to just push an on the wire update, for some data that just is your, you know, serialized model. But let\u0026rsquo;s say you did do that. Hypothetically, static linking should get you what you want in this case, right? Well, not quite either. So in PyTorch, we use this operator registration mechanism to make it possible for people to sort of insert in, it\u0026rsquo;s like a form of dependency injection, like if you load up the LibTorch CUDA library, then all calls to torch.add suddenly have the ability to call into CUDA, as long as they\u0026rsquo;re passed by CUDA tensor. And this is done via dynamic dispatch. And the important thing is that in order to make this dynamic dispatch work, we have to register an implementation of the operator at library loading time. And what happens when you do that? Well, that\u0026rsquo;s a static initializer in the library. And once again, the compiler cannot eliminate this, because it doesn\u0026rsquo;t know if this arbitrary piece of code that gets run at library startup might actually, you know, do something important that you can\u0026rsquo;t dispense with. So okay, by the way, that\u0026rsquo;s why you need like whole archive if you\u0026rsquo;re linking against PyTorch statically, because otherwise, they\u0026rsquo;ll just drop all the static initializers if nothing in the object file in question is referenced. It\u0026rsquo;s, it\u0026rsquo;s pretty nutso. But you know, that that\u0026rsquo;s the way it is. Okay, so we need a way to actually figure out what operators that our model needs, and then apply this to a build of PyTorch, so that we don\u0026rsquo;t, we don\u0026rsquo;t actually send them when we\u0026rsquo;re building the application for mobile. Okay, so let\u0026rsquo;s take these in two steps. So first, what operators does our model need? So if I have a TorchScript model, my TorchScript model is serialized in some machine readable form. And so at the first level, it\u0026rsquo;s really easy to figure out what operators a model needs, right? Like we just go to this serialized format. And for every operator call in it, we just say, Okay, well, I see an ad, so I need ad. And then Oh, I see a convolution. So I need convolution, etc. Easy to get a list of operators that the model needs. But there\u0026rsquo;s a problem with this, which is what if your operator uses other operators. And this is really, really common in PyTorch, because we have a lot of like really small, cheap operations that you can use to sort of massage things into the correct form, like viewing and reshaping. And many, many operators use this. And so if you\u0026rsquo;re doing one of these things, well, you also need to be able to track what those uses are, you need some sort of dependency graph from operators to operators. So how is this done? Well, the way we do this is we actually have a LLVM based static analysis. So what you do is you take PyTorch, you compile it with Clang, producing LLVM bitcode for all the object files. And then our static analysis goes through and looks through all of these, all of the bitcode looks for things that look like operator definitions. They\u0026rsquo;re easy to find because there\u0026rsquo;s a specific API call you use to register the operator. So it just looks for instances of that API call. And then it, you know, spiders that code until it finds all the dispatcher calls, which mean that, hey, I have a dynamic dependency on some other operator, and then generates that into a YAML. That\u0026rsquo;s pretty interesting. Most people don\u0026rsquo;t want to compile PyTorch with LLVM bitcode to actually get this analysis graph. So we also have the YAML checked in for an easy kickstart. If you don\u0026rsquo;t actually want to, you know, run this pass. By the way, this pass is supposed to be updated by a bot. But the last I was I was checking for this podcast, the last time it was updated was February this year. So you know, if you\u0026rsquo;re running into a problem with the open source mobile selective build, like something\u0026rsquo;s missing, and it shouldn\u0026rsquo;t be just rebuild from scratch. The instructions are there. It\u0026rsquo;s pretty simple. I\u0026rsquo;ll also link it in the episode notes for this podcast. By the way, there\u0026rsquo;s another way to get the way things your ops needs, which is some sort of dynamic tracing. And we actually debated a lot when we were trying to decide what to do for figuring out what ops memory needs. So what how does dynamic tracing work? Well, instead of trying to statically read out the operators your model needs by looking at the TorchScript model, just run the TorchScript model. And when you run it, you\u0026rsquo;re going to hit a bunch of operations and record what operations you see. And then that gives you exactly the set of operators you need. So no need for, you know, this dependency graph analysis. Life is easy when you\u0026rsquo;re dynamic. Of course, there\u0026rsquo;s a problem with this, right, which is you need representative inputs for your model. And well, maybe that\u0026rsquo;s not a big deal. If you\u0026rsquo;re like deploying these models, because you want the representative inputs anyway to test that the model doesn\u0026rsquo;t crash. But if there\u0026rsquo;s say control flow in your model, then a single representative input might not actually cover everything. So you need to make sure you actually fully cover it\u0026rsquo;s like a code coverage problem, right? You need to actually cover every operator that\u0026rsquo;s actually used to make sure that you\u0026rsquo;ve gotten everything. Okay, so that\u0026rsquo;s how you get all the ops your model needs. How do we actually apply this to a build of PyTorch? So as I said, static linking doesn\u0026rsquo;t let us, you know, do this automatically. So what we actually have to do, so you have to, you know, take these operator registrations or things that would otherwise force the compiler to include a code in question, and make sure that we have a way to say, okay, don\u0026rsquo;t do that when we don\u0026rsquo;t need it. So a lot of operator registrations are done via code generation, see one of my previous podcasts. So in that case, it\u0026rsquo;s very simple, we actually just feed in the YAML file that says all the operators we need to our code generation. And the code generation says, Oh, you know, the selective build says that I don\u0026rsquo;t need this operator, so I\u0026rsquo;m just not going to generate a registration call. And if I don\u0026rsquo;t generate a registration call, then the code that it calls is now dead, because there\u0026rsquo;s nothing actually calling it, and then it\u0026rsquo;ll get pruned away by the static linker, no problem. Unfortunately, there are some registration calls that don\u0026rsquo;t actually get generated by code gen, they\u0026rsquo;re just done manually via our, you know, very nice and intuitive m.def or m.imple syntax. So for this, we have a very clever scheme, which is called the selective name macro. The basic idea behind this macro is that when you build PyTorch, we also dump all of the operators that are supported into a constexpr string. And so we actually have this constexpr function, which can basically take in an operator name and say, hey, is this included in the giant comma separated constexpr list of, you know, all the operators that are allowed or not. And what the selective name macro does is it just applies this constexpr function to the name that you are registering. So you wrap selective name around the name you want to register. And if it is in the constexpr list, you let it go through no problem. And if it\u0026rsquo;s not, you generate a, you know, basically a dummy type that says, hey, don\u0026rsquo;t actually do this registration. And because this all happens in compile time, then the compiler knows, oh, okay, now I\u0026rsquo;m just not going to generate any code for this at all. We had to do this a little especially because in C++, you can\u0026rsquo;t actually pass strings to templates directly. So you know, we have to make sure this gets all resolved into a Boolean, which we can then pass into a template. There\u0026rsquo;s one last detail, which is actually pretty important when you\u0026rsquo;re trying to understand how the selective build system works, which is how this integrates into your build system. So in CMake, everything is fine, you just do a CMake build of PyTorch, with the particular operators that you want it to ship. And then you know, there\u0026rsquo;s no problem. But at Facebook, we actually have multiple apps, and all these apps want to use Facebook. And so we actually have this problem, which is that we want different sets of allowed operators, depending on which app we\u0026rsquo;re doing. And the build system we use at Facebook, namely buck has a constraint, which is that you\u0026rsquo;re only allowed to have one copy of any given library at any given time in the build system. And this is just to make sure people aren\u0026rsquo;t like, doing some sort of Node.js style disaster where there\u0026rsquo;s like a bazillion copies of the same dependency everywhere. But that\u0026rsquo;s a problem for us, right? Because, you know, there\u0026rsquo;s only one PyTorch library, but each of the apps wants a different version of the PyTorch library in this situation. So what do we do? Well, we cheat, we actually generate multiple copies of the PyTorch library for each version of the app that we need in this situation. And we don\u0026rsquo;t we don\u0026rsquo;t generate a copy of everything, just the relevant parts that actually contain the operators. This used to be just some glue code, which did the registrations. So it was a very small bit of code that we like had to recompile for everything. But we\u0026rsquo;ve actually expanded this to recompile all of PyTorch. Because as I said, we want selective D types, and D types are like sort of coded into the operators themselves. So there\u0026rsquo;s no like registration mechanism we can use for D types, we have to handle this actually by recompiling the kernels in question. There\u0026rsquo;s a kind of funny alternate universe, where instead of like recompiling the entire library for the sets of operators you want to do, you could also just modularize library. So they have you have one library for convolution, another library for add, another library for sub, etc, etc, etc, etc. So isn\u0026rsquo;t that like the, you know, good software engineering way to like, you know, deal with the system, and then you only depend on the libraries you need. Well, yes, this kind of works. And actually, Cafe2 used to do this. And there\u0026rsquo;s a problem, which is that one, building libraries takes a while, right, because you have to link them. So it\u0026rsquo;s like takes a minute a pot. And so that would be really, really slow. And second, well, people just don\u0026rsquo;t write code this way. They don\u0026rsquo;t generate a 1000 libraries for 1000 really small pieces of functionality, and then you know, mix and match them for what you actually want to do. And a lot of the ecosystem is not set up to do this properly. So for example, we have to load iOS applications into Xcode to actually, you know, work on them. And if we actually generated a library for every operator, it would crash Xcode, because there\u0026rsquo;s just too many libraries. So you know, yeah, don\u0026rsquo;t don\u0026rsquo;t do no JS style stuff in, in mobile. One final thing I want to say, so the selected build for mobile is intended to be something that you don\u0026rsquo;t really have to worry about if you\u0026rsquo;re developing PyTorch. But sometimes it rears its head. And the most common situation it rears its head is you\u0026rsquo;re working on a kernel, you modify some of its implementation details, implementation details, so that it\u0026rsquo;s calling some new operator. And then some guy comes to you and says, hey, my rando mobile, like application stopped working. And that\u0026rsquo;s usually because there\u0026rsquo;s some YAML somewhere that describes the set of ops the model needs. And it\u0026rsquo;s out of date, right? Because you changed what the dependency structure of the model is. And so now there\u0026rsquo;s a different way. There\u0026rsquo;s a different set of operators that are needed. And you have to tell the YAML file, hey, this is a new thing, you have to rerun the analysis pass. A lot of these things are checked in for better or for worse. Fortunately, it\u0026rsquo;s really easy to regenerate this YAML files. And also the PyTorch edge developers are very friendly and very willing to help in these situations. So you can just reach out and you know, learn how to do it. And there\u0026rsquo;s also ample documentation internally for this sort of workflow. Okay, that\u0026rsquo;s everything I want to talk about today. Talk to you next time.\nEP26 PyObject-preservation PyObject-preservation Hello, everyone, and welcome to the PyTorch Dev podcast. Today, I want to make good on a promise that I made on the very first episode of the podcast, namely, how the heck when we bind C++ A10 tensors to Python, can we make it so that the Python object doesn\u0026rsquo;t go away when the Python object goes dead? Namely, how do we preserve the Py object? This podcast is going to be a little technical. So the way that I\u0026rsquo;m going to do it is I\u0026rsquo;m going to first explain how the trick works, which will sound really simple, stupidly simple, in fact, but with a lot of complexity underneath. And then we\u0026rsquo;ll just go on a wild romp on various aspects of how C Python works and how C extensions work with Python to explain all of the more subtle moving parts of how Py object preservation works. Because yes, it does sound very simple. And it is simple. There\u0026rsquo;s just a lot of T\u0026rsquo;s to cross and I\u0026rsquo;s to dot. All right, so where should we start? So just to remind ourselves about what the problem is, imagine you have two objects, two ref counted objects, the ref counted separately, you know, so object A has got some ref count three, object B has got some ref count two. And what you\u0026rsquo;d like to do is you\u0026rsquo;d like to set them up so that object B stays live as long as A has non zero ref count, and object A stays live as long as B has non zero ref count. So let\u0026rsquo;s imagine that these are one of the objects is our C plus plus tensor with a C plus plus reference count. And another object is our PI object representing the C plus plus tensor. And it has a Python ref count. This puzzle basically devolves into how do we make sure that we actually keep the C plus plus tensor and the PI object around in the same linking their lifetimes together. Before we explain the solution, it\u0026rsquo;s helpful to think about two solutions that don\u0026rsquo;t work. One solution that doesn\u0026rsquo;t work is to have the C plus plus object have a strong reference to the Python object, and the Python object to have a strong reference to the C plus plus object. Why doesn\u0026rsquo;t this work? Well, you have a reference cycle. You call in your class about reference counting, that reference counts are very nice, but they have a problem, which is that if you have objects that refer to each other, those objects will never get garbage collected unless you break the cycle in some way. So if we have C plus plus refer to Python, Python refer to C plus plus, that\u0026rsquo;s a cycle that\u0026rsquo;s straight out, we will just never garbage collect the objects in that situation. Another solution that doesn\u0026rsquo;t work is to have one of the objects have a strong reference to the object, and another object have a weak reference to the other. So for example, have the C plus plus object have a weak reference to the PI object, and the PI object have a strong reference to the C plus plus object. This is what PyTorch does today. And it doesn\u0026rsquo;t work because if all the references to the Python object go dead, the Python object will get deallocated because well, the C plus plus object, even if it has references to itself only has a weak reference to the Python object. So it doesn\u0026rsquo;t stay live in that situation. Okay, so how can we solve this problem? Well, we\u0026rsquo;re going to use a little trick. And the trick is resurrection in Python ref counts. What does resurrection refer to? So resurrection refers to the fact that when you\u0026rsquo;re doing ref counting in Python, if the ref count for an object goes to zero, you can still resurrect the object from the dead by simply making sure that a new reference to the object gets taken out while you\u0026rsquo;re deallocating the object. When this happens, CPython will say, Oh, object is still live and will abort the rest of the deallocation process. With resurrection as our tool, we now have enough tools to actually solve the circular reference problem once and for all. Here\u0026rsquo;s how it works. So in the beginning, we\u0026rsquo;ll set things up just as we do today, where we have a C plus plus object and a Python object. And the Python object has a strong reference to the C plus plus object, but not vice versa. This goes on for a bit while we have references. And at some point, the Python object is going to go dead. Whereas the C plus object is still live, because that\u0026rsquo;s the situation we\u0026rsquo;re worried about. When the Python object goes dead, we don\u0026rsquo;t immediately deallocate it. Instead, we look at the reference count of the C plus plus object and say to ourselves, is this reference count greater than one? Because, well, if it\u0026rsquo;s one, then it\u0026rsquo;s solely owned by the Python object in question. But if it\u0026rsquo;s greater than one, that means someone else has a reference to the C plus plus object. And that means we shouldn\u0026rsquo;t kill the Python object. So when this happens, we will abort the deallocation, and we will flip the ownership so that the C plus plus object owns the Python object instead of vice versa, thus saving the Python object from getting deallocated. And you know, because it has no incoming references, giving it the ownership in the only way that\u0026rsquo;s possible. There\u0026rsquo;s one last thing, which is that C plus plus reference counting traditionally doesn\u0026rsquo;t support resurrection, because it\u0026rsquo;s kind of a difficult thing to do in a thread safe manner. So what C plus plus, so what we\u0026rsquo;ll do is if I ever use my C plus plus object to take out a new owning reference to the Python object, and this shouldn\u0026rsquo;t be too hard to do because you had to call some API with a C plus plus object to get the Python object in question, then you can actually just flip the ownership back so that the Python object refers back to the C plus plus object once again. And then you can do this as many times as you want, as many times as the Python object goes dead while the C plus plus object is still live. And so we wrote this up in a patch, we put it in PyTorch master. And so now if in PyTorch master, you say assign a variable to the grad field of a tensor, the grad field, by the way, is stored in C plus plus. So it isn\u0026rsquo;t a good old fashioned py object field. It\u0026rsquo;s a actual field in C plus plus. So you store a tensor in there, and then you delete all references to it from Python, you will still retain, for example, the dict properties that you put on the tensor in question. So no more lost py objects. So that\u0026rsquo;s it. That\u0026rsquo;s how py object preservation works. Feel like you want a little more, perhaps? Well, let\u0026rsquo;s dig into a little bit about why this actually works. And the first question that you might ask is, hey, Edward, so it\u0026rsquo;s kind of cool that there\u0026rsquo;s this py object resurrection mechanism. By the way, it was Sam Gross who came up with this technique. He was the one one who told me about it, and let me actually implement this in this way. So why does resurrection exist in Python in the first place? And the answer is finalizers. What is a finalizer? So in Python, what you have all these objects, and sometimes they go dead. And sometimes you want to clean up after an object after it goes dead. For example, if you open a file, when the file object goes dead, you might want to close the file in that situation. Of course, what you really should do in that situation is a context manager to guarantee the file gets closed. But if you don\u0026rsquo;t use a context manager, the file will still get closed when it gets deallocated because of the finalizer. So Python supports arbitrary finalizers, you can write whatever code you want. If you want to write a Python object and write some finalization code on it, you can just write the magic method underscore underscore del on it. Cool. So there\u0026rsquo;s a problem, right? So finalization is when the object is dead, and we\u0026rsquo;re trying to get rid of it. So the finalization code can do anything. So what happens if you accidentally, like, you know, or purposefully, you know, put out a new reference to the object you\u0026rsquo;re being finalized somewhere else? Hmm, well, that\u0026rsquo;s a bit that so for a while, this was kind of skeevy. And eventually, there\u0026rsquo;s this PEP safe object finalization, which said, Okay, what we will do is we will resurrect the object when this happens. So we will make this a valid thing to do. And we\u0026rsquo;ll just mark the object as Oh, this object has been finalized. And so I\u0026rsquo;m never going to finalize it again. So so you have the environment that an object only gets finalized every once. So this by this way, like, you know, we don\u0026rsquo;t have to worry about objects being in strange, half deconstructed states, and then escaping into the outside world, because we just run the finalizer, the finalizer resurrects it, we just stop the allocating. And then we wait until later when the object actually becomes dead to deallocate it. So this is why resurrection works. But it also poses a question for pi object preservation, which is, if finalizers can only run once, I better not run my finalizers when I\u0026rsquo;m doing this one of these resurrection things. And actually, it\u0026rsquo;s a little difficult to arrange for this to be the case. Because let\u0026rsquo;s explain how deallocation works in C Python. So in C Python, when you define a any type of Python object, there are a bunch of TP fields, which define the various behavior you want to do. So there\u0026rsquo;s like TP init that says what to do during construction. And for our purposes, there\u0026rsquo;s one that\u0026rsquo;s very important. TP dialog. What is TP dealloc? It just says how to deallocate an object when you call into it. And so when you like write a C extension custom pi object, you\u0026rsquo;ll typically provide a TP dealloc that, you know, like looks into the C++ fields of whatever it is, you\u0026rsquo;re implementing in the pi object and actually deletes them so that you know, we deallocate them. And at the end of the day, it actually also deletes the Python object altogether. Okay, so that\u0026rsquo;s kind of cool. What about when you subclass a Python class in you know, say Python, and this is relevant to tensor, because we don\u0026rsquo;t actually let people use the C bound object called tensor base directly, we actually subclass it into tensor. Well, Python subclasses have their own special deallocation implementation called subclass dealloc. And this deallocation method sort of takes care of all of the random things that you know, Python objects actually support. So there\u0026rsquo;s a good reason why we subclass tensor into a Python subclass, which is that if we didn\u0026rsquo;t do that, many things that people would expect to work on objects such as you know, writing to arbitrary fields on the object, using weak references, doing finalizers, all those things wouldn\u0026rsquo;t work, right? Because those things are actually handled by the implementation of the Python subclass. And we would have to like manually replicate them in our C implementation if we wanted them to work without subclassing. So we\u0026rsquo;ve got a problem, right? So what happens when I deallocate an object, I call the tp dealloc for the most specific subclass that the object is in question. And that\u0026rsquo;s going to be the Python subclass in the case of tensor. And what does it do first? Well, it runs finalizers. And I don\u0026rsquo;t want to run finalizers because they might be resurrecting this object. So what\u0026rsquo;s a poor person to do? Well, we need to somehow override the tp dealloc for all subclasses of tensor base to make sure that they first check if resurrection is going to happen and bail out entirely before the deallocation process has a chance to mark the object as having been finalized. Do we have a way to do that? Fortunately, yes. In Python, you can define a metaclass. What is a metaclass? A metaclass is a way of customizing the behavior of classes when they get subclassed. So if you imagine, like a class constructor is something that gets called when you construct an object, a metaclass constructor is something that gets called when you construct a class as part of the metaclass hierarchy. So what do we do? We define a new metaclass for tensor base. And so when we subclass tensor from tensor base, the metaclass gets run. And what it does is it just overrides the tp dealloc to replace sub subclass dealloc with our own thp variable subclass dealloc. It actually looks very similar to subclass dealloc, right? It still needs to clear out slots. It still needs to deallocate the dictionary. It still needs to run finalizers. But before all that, it checks if we are going to resurrect the object by looking at the ref count of the C++ object. It\u0026rsquo;s a little unsatisfactory because I actually went ahead and looked at CPython and copy pasted all the code for subclass dealloc to make this all work out. But it works out in the end because actually a lot of Python binding code like Cython, for example, replicates this because remember what I said, if you just do a very simple C object from Python, you don\u0026rsquo;t get dictionaries, you don\u0026rsquo;t get slots, you don\u0026rsquo;t get any of that stuff. So you want that all working, you have to actually write code for it. And so Cython, for example, does replicate all this logic so that it looks like it without you having to subclass from Python. So that\u0026rsquo;s one of the complications that arise from doing subclass preservation. What\u0026rsquo;s another complication? So another complication is that weak references are a little bit of a problem. So I said earlier that we need to be able to intercept whenever a strong reference is taken out to the pi object from the C++ object, because we need to fix up the ownership in that situation. If the C++ object owns the pi object, I need to flip it back around. So the pi object owns the C++ object. And ordinarily, it\u0026rsquo;s easy to interpose on this. But there\u0026rsquo;s one case you can\u0026rsquo;t interpose on it. And that\u0026rsquo;s a weak reference. A weak reference lets you take a reference to an object that you know, will go dead if that object goes dead. But if the object is still alive, I can use it to manufacture a strong reference into the object. And there\u0026rsquo;s no way to hook into this behavior. So if someone\u0026rsquo;s got a weak reference, they can get out a reference to the pi object, even if I\u0026rsquo;m still in this flipped state where the C++ object owns the pi object. This is mostly harmless, unless then the C++ object goes dead while the strong reference from the weak ref stays live. And then you\u0026rsquo;re in this awkward situation where the C++ object gets deallocated, because there\u0026rsquo;s no resurrection for C++ objects. Fortunately, there\u0026rsquo;s a simple workaround for this situation. You just need to like, ask to fix the reference direction. And so I added a new method to tensor that lets you do that if you\u0026rsquo;re using weak references. But actually, none of our tests failed because of this. So I\u0026rsquo;m suspecting that no one\u0026rsquo;s actually going to run into this in practice. One last thing. So Python has this thing called a garbage collector. And actually, what it does is it makes it so that if you do have cycles in entirely Python objects, you can actually garbage collect them in that situation. So they\u0026rsquo;re not actually going to be lost to these either forever. By the way, this doesn\u0026rsquo;t apply for C++ shared references. If you have a cycle there, you\u0026rsquo;re just flat out of luck. So GC cycles are kind of interesting in Python because we also need to handle them correctly under the assumption of resurrection, right? If I have a cycle in Python, but it turns out that if I were to deallocate this object, then I would have resurrected it from, you know, some C++ object that\u0026rsquo;s live, that Python object needs to be treated as a root, right? I can\u0026rsquo;t actually deallocate the cycle because that would just leave everything in a broken state. But the way that the, you know, cycle, the way that garbage collection works is if I try to resurrect it at the point in time, I\u0026rsquo;m deallocating, it\u0026rsquo;s too late, because I might have actually started deallocating all the other stuff in the cycle because Python is just going to be breaking the cycle using TP clear. That\u0026rsquo;s the way you break cycles. So what\u0026rsquo;s a poor person to do? Well, all you need to do is make sure that when Python is doing garbage collection, any object that is resurrectable gets treated as a root. And ordinarily, a GC just has a fixed set of roots that it knows to traverse down to find where everything is. But Python is special, it needs to do a first pass, a pre-pass before the actual traversal pass in GC to determine what all the roots are. And this makes sense because, you know, you could have arbitrary references to py objects from random places in C++ that Python knows nothing about. And so in general, Python doesn\u0026rsquo;t know what your roots are. So it simply defines roots to be any object that has a ref count greater than all the ref counts coming into it from other Python objects. So if you just make sure that something gets treated as a root, and that\u0026rsquo;s pretty easy to do, you just don\u0026rsquo;t traverse its members in that situation, then you\u0026rsquo;re all good. And so we also not only do we override tp dealloc, but we also override tp traverse in the meta class to make sure we check for resurrection before we traverse into the sub members. Okay, so that\u0026rsquo;s how py object preservation works. I\u0026rsquo;m hoping to release a little sample open source project that shows you how to do this trick, you know, in a very compact way, because I think this will apply to any project that is binding C++ objects to Python. That\u0026rsquo;s all I wanted to say for today. Talk to you next time.\nEP27 C++-frontend C++-frontend Hello, everyone, and welcome to the PyTorch Dev Podcast. Today\u0026rsquo;s topic is a listener request, namely a discussion about the trade-offs behind the design of the C++ frontend. So before we start, I have to first explain what I mean by the C++ frontend, because there are a number of different ways you can interpret this. In one sense, the C++ frontend is the tensor class that is inside PyTorch and is used to actually undergird the implementation of all our kernels and all the plumbing that is in PyTorch. So this is tensor provided by the A10 library, originally developed by Zachary Dorito. And it\u0026rsquo;s a really important piece of what we think of as the C++ frontend. So I\u0026rsquo;ll spend some time talking about the philosophy there. But there\u0026rsquo;s a second part to the C++ frontend, and this was added after A10 by Peter Goldsboro. And what it is, is basically everything else beyond, you know, just the tensor class. Because if you think about PyTorch\u0026rsquo;s library, we don\u0026rsquo;t just provide a tensor, we also provide a module abstraction and an optimizer abstraction that you can use to easily structure your neural networks. And you know, people use tensors a lot, but they also use modules a lot. And so that matters a lot when you actually want to write real code. But we\u0026rsquo;re going to start with talking about tensors, because that\u0026rsquo;s simpler, and it sets the stage for some of the design constraints that happened when we were designing the rest of the C++ frontend. Okay, so let\u0026rsquo;s talk about A10. So where did A10 come from? So A10 came from this idea that, hey, we were writing all of our internal code in PyTorch in this very terrible language called TH, where we had various macros for your tensor types. And it was all done in C. And you had to write your code, and then compile it multiple times for every D type you wanted it to be supported on. And you had to manually ref count. And it was all terrible. And so the model behind A10 was, okay, let\u0026rsquo;s use C++ instead of C, and use the abstractions that C++ gives you to actually make a nice API for doing manipulations on tensor. But it went a bit further. So there were a number of other tensor libraries in C++ at the time, eigen being one of the most influential ones. And we didn\u0026rsquo;t want to do that. We the idea that Zach had was we want to have a tensor type in C++, that is just tensor, it doesn\u0026rsquo;t record any D type information, it doesn\u0026rsquo;t encode any dimension information. And the really important thing about doing it this way, is now you can write polymorphic code on various D types and various dimension sizes without having to template your code. Because, well, you know, when you\u0026rsquo;re writing C++, if you have a type, and it\u0026rsquo;s got some parameter on it, like you\u0026rsquo;re you doing a vector, and it\u0026rsquo;s got some, you know, type of the elements in it, if you want to write a function that is generic on the types, you have to write a template function, because C++ is going to instantiate it for every copy of the element type you use. And it gets worse and worse, because the templates don\u0026rsquo;t actually get type checked, you have to wait until they actually get instantiated with the type in question before they get type checked. So it\u0026rsquo;s just much harder to write code in C++, if you are using templates, that is until C++ concepts come around. But you know, we were C++ 11 at the time. So oh, so much trouble. Like, and one of the things that makes it really hard for newcomers to C++ to write C++ is the really horrible, obscure template error messages. So if we just don\u0026rsquo;t put that information in tensor, if we type erase tensor, then people don\u0026rsquo;t have to worry about that. So that was the like first main innovation of a 10, which was don\u0026rsquo;t do templates, just type erase everything. And it\u0026rsquo;s okay, things will work out in the end. Another really important philosophy that went into the design of tensor is we really wanted it to look as much like Python as possible, right? So if you like wrote some code in Python, like, I have a tensor.addb.mullc, right? Like that\u0026rsquo;s something you could write in Python, no problem. We wanted that to be exactly the same way in C++. So people who came in not knowing very much C++, but needing to write their code in C++, because remember, this was at the time we were trying to start moving all of our Python code into C++. So we were in desperate need of C++ programmers. But everyone knows how hard it is to actually find grizzled C++ veterans that know everything about the ownership model in C++. There\u0026rsquo;s just like not that many of them. So the closer to Python, we could make the code, the easier and more accessible it would be for people to start writing kernels in C++. And so one implication of this is tensor, like AT tensor, as seen in PyTorch, is not the traditional notion of a C++ type, which is a value type, where if I were to like do a copy construction on it, an actual shallow copy would happen. No, it\u0026rsquo;s a reference type. So we actually organize most of the main user visible types in PyTorch into two types, a tensor type, which is the reference type. So if you copy it, you just, you know, are copying the pointer, and then tensor impl, the impl type, which actually contains all the metadata in question. And so you\u0026rsquo;ll see this separation in storage, storage, storage, impl, and also in modules, module, impl, module. So you get reference semantics, equality works the way you expect it to in Python, and people are pretty happy. One last thing about the C++ API, which is that we want our calls to look a lot like Python. And for the most part, function calls are the same. But one thing that Python has that C++ doesn\u0026rsquo;t is keyword argument support. So we needed some way to actually simulate keyword arguments. And I\u0026rsquo;m getting my timeline a little bit mixed up here, because we added keyword argument support to the C++ API after we actually did the initial version of A10. In particular, the reason why A10 didn\u0026rsquo;t have keyword argument support was it wasn\u0026rsquo;t obvious how to do it. And the sort of most important structure that gets used everywhere in PyTorch, tensor options is designed explicitly to let you do this sort of keyword argument style, argument passing in Python. How does it work? It\u0026rsquo;s just a struct. It can be default constructed to have nothing in it. And then you can set via setter methods, various attributes on it. So like tensor options dot d type, blah, dot device, blah, we\u0026rsquo;ll set up things so that you actually get a tensor options with that d type and device set, but maybe not the other keyword arguments. And we actually designed tensor options to be a value class. So you don\u0026rsquo;t have to worry about like mutation or someone mutating it under you. It always functionally returns you a new tensor options. It\u0026rsquo;s only two words large. So it\u0026rsquo;s not a big deal to keep creating new copies of this tensor options. Okay, so I\u0026rsquo;ve established the basic ground rules that you know, the A10 library wanted, right, which is that no templates, don\u0026rsquo;t don\u0026rsquo;t do templates. So it means we need a type erase tensor and make the tensor API look as much like Python as possible. We actually even wrote a manifesto about this like about this writing C++, writing Python in C++. So with these two constraints in mind, let\u0026rsquo;s fast forward a little bit in time to when Peter Goldsboro was working on the C++ front end proper, namely module support. So at the time, there was a project going on at Facebook research, the StarCraft project, they were doing reinforcement learning for StarCraft. And they have a problem, which is that, you know, what they needed to do was they they needed they had a simulator for StarCraft, an actual game instance of StarCraft, actually, and they needed to feed it information from the reinforcement learning model that they were training at the time. And they needed this to go as fast as possible, because you know, like the faster you can be the simulator, the faster you can actually do training. And so CPU overhead really mattered here. And parallelism and multi threading really mattered here because they were running lots of simulators. And this was just completely impossible to do in an efficient way in Python. And so they actually started writing a little layer on top of the A10 library, which remember recall only had tensors and that without that\u0026rsquo;s it, all it is is a tensor library, I\u0026rsquo;m called Autograd PP to make it possible to do automatic differentiation on these things, and to you know, actually structure modules. And so at the time, Peter Goldsboro was like, you know, hey, C++ front end is a really good idea. And there are a lot of people who might be interested beyond the StarCraft project. And we took the, you know, learnings from their version of the C++ front end and built it into the C++ front end that actually you can use today as part of FITurch proper. So we ran into a few questions when we were trying to figure out how exactly modules should work in C++. Like there are a number of problems. For one, we already have modules in Python. If we want modules in C++, does that mean the Python module should call into the C++ modules? Well, maybe that\u0026rsquo;s not such a good idea, because a lot of people take modules in PyTorch, they copy paste them into the research code, and they hack on it. This hackability is really good when you\u0026rsquo;re writing Python. And if we actually moved all the implementations into C++, then you know, well, people can\u0026rsquo;t just copy paste things, right? They actually have to compile some C++, or like look up an old version of PyTorch where there was still the Python implementation. So we decided we didn\u0026rsquo;t want to get rid of the existing Python modules because hackability was really important there. Another question was, could we write a transpiler to take these Python modules and transpile them into equivalent C++ modules? And that just seemed like too much complexity for things to be worth. So we decided, okay, we\u0026rsquo;re just going to reimplement all the modules that are in Python in C++ for better or for worse, because now you\u0026rsquo;ve got two versions of the code, you got to update both of them in this situation. We have another problem when you\u0026rsquo;re trying to implement modules in C++, which is that, you know, Python has all of this meta programming stuff. If you recall my previous podcast on torch.nn, I was like, hey, you know, what does module do? Well, it tracks parameters. And really, the like, most important thing it does is track parameters, so that you can collect them all up and pass them to your optimizer. But the way Python does that is by overriding the meaning of setting attributes on the module so that it can like then, you know, sideband, like recorded in some field that says what all the parameters of a module are. Well, how the heck are you going to do that in C++? The answer is you can\u0026rsquo;t. So you need to adjust the API a bit. So the way the C++ render works, right, is it asks you to register parameter when you register a parameter. And that just sets up the extra metadata tracking necessary to tell what the parameters in question are. Another problem, which is similar to the quarks problem from the tensor case, is that modules also often have a lot of arguments that you want to like express like keyword arguments. And unlike factory functions, which tensor options is sort of oriented towards, which have a fixed set of keyword arguments to occur everywhere, every module is a little different. So there\u0026rsquo;s a bit of work in the C++ API to make it easy to define, you know, options objects that you can, you know, use setters to set in what the options should be, and then eventually pass it to the module in question to make things work out. And one last thing, right, modules, we argued a lot about whether or not they should have reference or value semantics. In the end, right, Python and C++, right, like these Python modules should look the same as the C++ version. So all modules also are split in the module, module impulse split. And that\u0026rsquo;s why there\u0026rsquo;s a macro that you need to call to actually, you know, bring the module into the question. So what is what what\u0026rsquo;s the upshot? Well, we started off writing the C++ front end for tensor. And we had some design principles, namely write Python and C++. And we extended it to modules in C++, perhaps a little imperfectly, because modules are a lot more complicated. But we were still trying to consistently apply this idea to the entirety of the C++ front end. And I would say that\u0026rsquo;s sort of like the the main idea, right, like you\u0026rsquo;re not going to get exactly the best performance that you could have gotten by writing really idiomatic C++, you\u0026rsquo;re going to get something pretty good, and certainly much better than like, if you were writing Python, and had to, you know, worry about the gill. And that\u0026rsquo;s good enough for a lot of researchers. That being said, there are some performance challenges to writing code in this way. And actually, I\u0026rsquo;m Scott Walchuk, a engineer over in core infra, who has been on loan to us on the project has been working on reducing overhead in our framework. And some of the stuff that raises a lot of overhead is related to writing Python like C++. So let\u0026rsquo;s just check out a few of these. So one problem that we have is that ref counting is really slow. Why is ref counting really slow? Well, Python ref counting is actually really fast. But there\u0026rsquo;s a trick behind it, which is that because there\u0026rsquo;s a global interpreter lock, Python ref counts are non atomic, because you can just assume that they\u0026rsquo;re going to be protected by this lock. In C++, ref counts are typically atomic, because you want your ref counted objects to work across multiple threads. So you know, you actually implement the ref counts as atomic things. And incrementing and decrementing atomic fields, that is expensive, because you have to tell the processor to actually send the cache line back to the main memory in question. Oh my god. So like, that\u0026rsquo;s, that\u0026rsquo;s a huge hit. So excess ref counts are a problem. And one of the difficulties about writing a code in the Python style, where you only have the tensor concept, which is a pass by reference type of shared ownership type, is that, well, a lot of the times people are just going to start, you know, doing ref count bumps willy nilly, because that\u0026rsquo;s kind of what you did in Python, where it was cheap. Well, it\u0026rsquo;s not so cheap in C++. And we\u0026rsquo;ve actually developed a really interesting way around this problem. So conventionally, the way you would have solved this problem in C++ is that you would have, you know, made a strong distinction between the, the actual thing that contains the data, data, and a shared pointer to that data in question. And then you would force everyone to use the right pointer, whether it\u0026rsquo;s a raw pointer, or shared pointer, or unique pointer, or some arena allocated pointer, you\u0026rsquo;d force everyone to like do all this juggling around. We came up the problem with, we\u0026rsquo;ve got this tensor type, everyone expecting is expecting to be able to do const tensor ampersand. So we can we have to have an actual tensor at the end of the day, can we reduce the amount of ref counts going on in this case? And the answer is yes, because we actually implemented ref counting ourselves using an interest with pointer class. And what we can do is we can build wrappers on top of tensor, for example, maybe owned tensor, which dispense with the ref counting, because the ref counting ends up being, you know, an incref or decref call. So you just skip the ref counting when you\u0026rsquo;re in this container type, depending on what\u0026rsquo;s going on. So for example, if I have a maybe owned tensor, which is actually just a reference to some tensor, it\u0026rsquo;s non owning, then I have the destructor of maybe own tensor, just leak the tensor when it gets destructed. So don\u0026rsquo;t trigger the normal destructor of tensor, which would decref to skip the decref entirely. And you can actually build a bunch of other things, there\u0026rsquo;s actually a PR out for also exclusively owned tensor, right? So this is kind of like unique pointer. But unlike unique pointer, it\u0026rsquo;s piggybacking off of a shared pointer. So you know, when you know you only have that pointer, you don\u0026rsquo;t have to actually incref and decref it, but then you can promote it into a regular shared reference. That\u0026rsquo;s very much like unique pointer in this case. But at the end of the day, it\u0026rsquo;s still a tensor. And so you can still, you know, forget about all of these pointer distinctions and passed around constant references to tensor without having to rewrite all your code. So yeah, I would say if we were going to do this project again, we would probably think about not writing all our code in C++, and perhaps writing it in some language, and then writing a compiler stack to compile down to the actual machine code we want. And you know, figuring out how to make it run really fast. And because we because compilation time is a huge problem, you don\u0026rsquo;t actually want to be like spending a lot of time compiling. But that\u0026rsquo;s a huge infrastructure outlay. And I don\u0026rsquo;t think there\u0026rsquo;s any way we could have gotten to the point we are today, not using this concept of writing C++ in Python. So I still think it was a really good call. It saved us a lot of template headaches. It really made it possible for a lot of people to write code in our framework in C++. But you know, like, there\u0026rsquo;s always something better you can do. That\u0026rsquo;s everything I wanted to say today. Talk to you next time.\nEP28 torchdeploy torchdeploy Hello, everyone, and welcome to the PyTorch Dev podcast. Today, I want to talk about Torch Deploy, a way of deploying direct Python code in production environments where you can\u0026rsquo;t wait for the gill. So what is Torch Deploy? So Torch Deploy is our answer to a question that we were asking, which is that, hey, it turns out that in a lot of cases, we don\u0026rsquo;t really care that the Python interpreter is slow. Yes, the Python interpreter is slow, but maybe it\u0026rsquo;s a very experimental model, or it doesn\u0026rsquo;t matter that much. And we just want to be able to run it in a multi-threaded environment. That is to say, the only sin CPython committed with this particular Python program is just that there is a global interpreter lock, which means you can\u0026rsquo;t run it in a multi-threaded fashion. Besides that, Python is fast enough. And this is often true in a number of cases. And I\u0026rsquo;ll link to an analysis which was done on a number of models showing that, hey, you know, Python doesn\u0026rsquo;t actually really matter as far as performance goes. So if you want to run a bunch of models in the same process, and being in the same process is pretty important because it just simplifies management of memory. And you know, you can make sure things get shared in an easy way, you don\u0026rsquo;t have to go to shared memory across processes. So single process, but you want multiple Python threads running in parallel inside this process. How can you do it? Well, torch deploy is the answer to this question. The use case of torch deploy is pretty niche. And we haven\u0026rsquo;t really tested it that hard in production cases. But it is being tested in CI and PyTorch. And so if you\u0026rsquo;re dealing with code that interfaces between the boundary between C++ and Python, namely C++ code that ordinarily doesn\u0026rsquo;t call into Python, but you know, does you want it to call into Python, for example, dispatch to Python, a project that I\u0026rsquo;ve been working on recently, then you\u0026rsquo;re probably going to run a foul torch deploy as an torch deploy is going to have to make you think about how to structure your code correctly. Fortunately, it\u0026rsquo;s not too hard. So I\u0026rsquo;m going to tell you a little bit about how torch deploy is implemented. And then some of the consequences for when you\u0026rsquo;re designing stuff in PyTorch that might interact with torch deploy. Okay, so what is torch deploy? So torch deploy is a way to run multiple Python interpreters in your process, without them sharing any state, so that you can run them each with separate gills. And technically, Python 3.9 sub interpreters are also an attempt at doing this sort of thing. But sub interpreters are trying to work with a single copy of the Python in your address space. And it\u0026rsquo;s sort of not complete, like they haven\u0026rsquo;t actually gotten it. So that each of the sub interpreters is has it got its own state so that you don\u0026rsquo;t have to do the same gill to protect everything. So torch deploy sort of takes a really heavy hammer at the problem. And it says, Okay, well, it\u0026rsquo;s too hard to refactor CPython, so that the like interpreter specific state is separate, and I can you know, create as many copies of it. So I\u0026rsquo;m just going to take the whole honking Python process in its entirety, and stamp out multiple copies of it in my process. Ordinarily, you can\u0026rsquo;t do this because, you know, Python is going to be some shared library. And if you load a shared library multiple times, well, the normal thing to happen is you only load it once, right? The whole point of a shared library is a shared library, you only load it once it like shows up in one place. And it provides symbols for all the things that, you know, it defines as being exported. So what do you do with torch deploy? What we do is we build a special version of Python, that\u0026rsquo;s got all of its stuff bundled up. So all the modules and all the Python code that you need to actually run Python. But most importantly, it\u0026rsquo;s built hiding all of the symbols. So you don\u0026rsquo;t actually export any symbols directly from it. There\u0026rsquo;s just going to be like a single fixed entry point that we\u0026rsquo;re going to access with dl sim when we deal open this library. So we have this like blob of code representing a Python shared library that has doesn\u0026rsquo;t export any symbols. And what we can do now is we can, whenever we need a new copy of the Python interpreter, write it out to a new dynamic library file. Because you know, remember, if it\u0026rsquo;s the same dynamic library, then the dynamic linker, the system dynamic linker is going to deduplicate all of them. So write it to a fresh library name, and then deal open it without resolving any of the symbols. And then manually use dl sim to pull out the one or two symbols that you actually care about for actually doing access into the interpreter. And so all of this is mediated by a interpreter class that sort of represents the like small set of things you can do to actually run code in your specific Python interpreter. And the most important thing that it lets you do is it lets you take I values PyTorch\u0026rsquo;s internal representation of, you know, like boxed values that take any sort of shape or size, unless you feed it into the Py interpreter, so that they turn into Py objects inside. So what does this picture look like? So when you load up torch deploy, and you have multiple Python interpreters going, each of them has a corresponding dynamic library that is their own copy of the Python. And because it\u0026rsquo;s their own copy of Python, nothing is shared at all. And so they can all have separate gills. It\u0026rsquo;s not just by the way, the CPython library that\u0026rsquo;s in there, you also need PyTorch\u0026rsquo;s Python binding code, because the binding code links directly against CPython\u0026rsquo;s API. And so like, because we\u0026rsquo;re hiding all the symbols, they can\u0026rsquo;t live in our library itself. So those also get compiled into this binary. And we end up with multiple copies of most of the code in torch slash c circ when you\u0026rsquo;re using torch deploy. So this is an important segue into some of the limitations and consequences of torch deploy being set up this way, when you\u0026rsquo;re trying to write code in PyTorch. So one really important thing is, because we\u0026rsquo;re loading multiple copies of the PyTorch library, Python, the Python part of the PyTorch library, when we have multiple torch deploy interpreters, it\u0026rsquo;s important that these don\u0026rsquo;t access any shared state. And that shared state actually can\u0026rsquo;t deal with multiple copies of the library hanging around. This is important because we don\u0026rsquo;t actually want to have multiple copies of A10, the tensor library, or any of the like pure C++ code, that C++ code, we want to have shared across all the interpreters. And in particular, for example, if you have a code inside the Python library, that for example, registers an operator to the dispatcher, that\u0026rsquo;s a no go under torch deploy, because remember, you\u0026rsquo;re gonna have multiple copies of the torch deploy library, each of those libraries, when you load them, are going to run their static initializers, and each of them are going to attempt to register whatever operator it is you are trying to define inside them. And the dispatcher doesn\u0026rsquo;t like that, right? It only wants an operator to be registered exactly once. There\u0026rsquo;s also another problem that shows up when you\u0026rsquo;re in a situation like this, which is, let\u0026rsquo;s say you\u0026rsquo;re in some C++ code, it doesn\u0026rsquo;t really have anything to do with Python. And you need to somehow get to Python. Like for example, you\u0026rsquo;ve got a C++ struct that was defined inside PyTorch proper, but it has a possibility to contain a reference to a Python object that might be associated with one of these PyTorch, these interpreters. And say you need to deallocate that Py object when this happens. Well, if there isn\u0026rsquo;t a dynamic dispatch to the correct interpreter, you aren\u0026rsquo;t even going to know which interpreter you should actually do the PyDecref on, right? Because each interpreter has its own state, each interpreter might even have its own representation of the Py object in question. So you need to make sure you can figure out which one you can actually get. And so in a previous podcast, I talked about Py object preservation. And I mentioned how there was this thing that we needed to do, which is that when we flip the ownership so that tensors own Py objects, we needed to be able to deallocate the Py object when the C++ tensor died. And so to figure out which interpreter we the Py object is associated with, we have to make an assumption. And the assumption we\u0026rsquo;re going to make is that for all tensors in PyTorch, there is going to be exactly one Torch deploy interpreter that actually has a Py object representation for this. This isn\u0026rsquo;t always used to be the case. In a previous implementation, we actually had it so that every Py interpreter could have its own Py object. So it was a one to many relationship. And that was just kind of a disaster, because you have to like go and deallocate each of the Py objects corresponding to the C++ tensor, if they happen to be owned, and you have to take out the gill locks for each of them in turn. And there\u0026rsquo;s just lots and lots of opportunities for deadlock in the situation. But if you can assume that any given tensor only belongs to a single Py object Py object interpreter, well, one, you can still store the Py object on the tensor itself because it\u0026rsquo;s guaranteed to be unique. And two is, well, because there\u0026rsquo;s one interpreter, you can also like have the tensor remember what the interpreter that it\u0026rsquo;s corresponding to is. And then you can always use that to like do virtual calls into to figure to do things that require the Python API in that situation. So I\u0026rsquo;ve been using this multiple times for different things. So when we did Py object preservation, we use the Py interpreter object, which we\u0026rsquo;re storing on tensors, which points us to the correct interpreter for torch deploy. What we are using that for is using that to decref the Py object when it goes when the C++ tensor goes dead. But in a more recent piece of work, dispatch to Python, we\u0026rsquo;re using the Py interpreter to figure out how to call into the Python interpreter, so that we can actually take a call to a C++ operator, and turn it into a call back into the Python interpreter. So what\u0026rsquo;s the idea? It\u0026rsquo;s the idea is that we have this dispatcher hierarchy, it\u0026rsquo;s got all the C++ code, and maybe at the very bottom, you want to override the behavior of an operator and call back into Python. Well, how do you know which Python interpreter to call with torch deploy? Good thing the tensors know what the interpreters they\u0026rsquo;re corresponding to are. So you just look for a single tensor object that\u0026rsquo;s got a Py interpreter, and then use that to do the virtual call into the correct interpreter. So there\u0026rsquo;s a pretty important corollary to this, which is that once you associate a tensor with an interpreter, it is always associated with that interpreter, even if the, you know, interpreter goes away, like, because we decided to unload it, that tensor is permanently associated with that interpreter. And that makes it easy to make the interpreter recording thread safe, because there\u0026rsquo;s a hazard. The hazard is, if you have multiple threads, and they\u0026rsquo;re all trying to, like, basically allocate a Py object for a tensor at the same time, there\u0026rsquo;s no intrinsic synchronization to this. And the fact that only one of them can win. And the fact that only one of them can win. And once they win, it\u0026rsquo;s permanent means that you can just do a plain old compare and swap and force the other threads to fail if they lose the race. One last complication when doing these sort of virtual dispatch tricks, unlike traditional C++ code, where you sort of load up all your libraries, stuff happens, and then shutdown kind of happens at the very end. And it isn\u0026rsquo;t really that important. And it doesn\u0026rsquo;t really matter if you clean up after yourself in the situation, because the process is going to die very soon. Torch deploy interpreters can be spun up and spun down. And when they\u0026rsquo;re spun down, you will unload the dynamic library that\u0026rsquo;s associated with them. And that\u0026rsquo;s important, because if you have any, like spare references to functions from that dynamic library, well, all those functions are going to become invalid once the library gets unloaded. And so this is so we don\u0026rsquo;t actually use virtual methods to implement the PI interpreter object, we use a homegrown V table like implementation with an extra feature that lets you disarm the function pointers when the library unload happens. So normally, you\u0026rsquo;ve got a bunch of function pointers, they all look great. And when you unload the process in question, we replace all of the function pointers with no op function pointers that live in the base library, so that if anyone else tries to interact with the Python interpreter, after it\u0026rsquo;s died, we don\u0026rsquo;t just you know, seg fault, we can do a no op operation in some cases when it\u0026rsquo;s benign, or raise a good error in the situation. So a lot of tricky stuff going on here, but torch deploy is a pretty cool bullet in our toolkit for letting multi threaded Python processing happen in a single process. That\u0026rsquo;s everything I wanted to say today. Talk to you next time.\nEP29 CMake CMake Hello, everyone, and welcome to the PyTorch Dev Podcast. Today, I want to talk about CMake, or perhaps want to talk about is too complimentary a word of it. Really, what you\u0026rsquo;re going to listen to today\u0026rsquo;s podcast is me ranting about how CMake is terrible. And oh my god, it is terrible. And there\u0026rsquo;s two parts to today\u0026rsquo;s podcast. So the first part is going to be short. And it\u0026rsquo;s basically like, dude, I know you like have this preference about where you want to file to be. But like, seriously, just follow this small set of rules for when you\u0026rsquo;re adding new files to PyTorch, and you won\u0026rsquo;t run a file to CMake gods, and everyone will be happy, and you will not have to deal with CMake. And the second part of the talk, which is going to be much longer, because there\u0026rsquo;s just so much random shit that is wrong about CMake is like, okay, actually, you got to do something about the CMake. Like you got to make a change, you got to add a new library, blah, blah, blah, blah. How do you actually go about understanding the monstrosity that is PyTorch\u0026rsquo;s CMake configuration? So in order, first off, what is CMake? So CMake is a build configuration system. So what it does is you write CMake list files that describe, you know, aspects of your build system, most importantly, you know, what your source files are, and what libraries you want to build are, and then CMake will generate some sort of actual, you know, build file, usually either make or ninja. When you build PyTorch, by default, we generate ninja files, because ninja is way better than make at running builds. Anyway, so it generates an actual file that it hands off to some other system that just knows about how to build things quickly. And the reason for this two stepness is that CMake deals with all the grody information about, oh, you know, what are packages, what are flags, like, how do you detect things? How do you, you know, generate files for both ninja, but also Microsoft Visual Studio, depending on what platform you\u0026rsquo;re on, etc, etc, etc. And then a system like ninja can be really simple, really short. And it\u0026rsquo;s just, you know, I\u0026rsquo;ve got this graphic dependencies, how do I build it as quickly as possible in the correct order. Okay, so if you\u0026rsquo;ve ever written any serious open source software, you may know of the thing where build systems are just generally a complete disaster. And PyTorch\u0026rsquo;s build system is no exception. I like to kind of think of it, this is because, well, you know, we\u0026rsquo;re all here to write software, right? Like we\u0026rsquo;re all here to write the an awesome deep learning library. And so every moment spent not doing CMake is time spent well, every minute spent doing CMake. And that ends up in the sort of very tragedy of the common situation where the CMake is terrible. And it actually like legitimately like people would be more productive if the CMake was better structured, but no one knows CMake, no one has the time to deal with it. People are just cargo culting changes whenever they need to do something. And so things just get worse and worse without anything, anyone working on it. So if you are being beaten down by the establishment, and you don\u0026rsquo;t have time to raise a revolution, there are some easy things you can do to reduce the risk of, you know, running into a CMake disaster. And I really only have one rule here. And the rule here is abide by the existing PyTorch structure, and don\u0026rsquo;t try to do anything fancy. And when I say do anything fancy, I mean, like add a new directory to put your files in. Why do I give this advice? Well, I give this advice because the way CMake is set up, right, is we have to do a lot of, you know, telling the build system what files to actually compile in. And so sometimes the list of files that you want to compile in is written out by hand, like one by one by one by one by one. So in some file in some directories, if you add a new file, you will need to add that file to some list somewhere that says, Oh, here\u0026rsquo;s a list of all the files. And in some other places, it\u0026rsquo;s done using a glob. And so if you just add the file to the directory, the glob will pick it up. And in a very, very restricted set of cases, do we do a recursive glob that looks into all subdirectories. So if so if you don\u0026rsquo;t want to have to edit the build system, then if you don\u0026rsquo;t add any new files, that\u0026rsquo;s like a surefire where to make sure you don\u0026rsquo;t have to edit the build system. Excluding that, well, if you don\u0026rsquo;t add a new directory, if you just add a new file, hopefully a glob will pick it up. But if it doesn\u0026rsquo;t pick it up, you know, just find one of the other files in the directory, grep for it. And that\u0026rsquo;ll tell you whatever file you need to edit it. And you know, cargo culting it in that way usually isn\u0026rsquo;t too painful. Ah, but if you want to add a new directory, well, you\u0026rsquo;re actually going to have to understand a little bit about how CMake works if a recursive glob isn\u0026rsquo;t picking it up. So just, you know, if you don\u0026rsquo;t have time to deal with the build system, just don\u0026rsquo;t friggin add a new directory to PyTorch. Yes, I\u0026rsquo;m sorry, like PyTorch\u0026rsquo;s structure sometimes doesn\u0026rsquo;t look so good. Sometimes you really want to add a new folder because you think that it\u0026rsquo;s going to make things organized better. And so if you really, really think this is important, then listen to the second half of this podcast, or try to explain the method behind the madness of CMake lists. But if you don\u0026rsquo;t have time, just don\u0026rsquo;t do it, please. Oh, and one more extra tip. So when you add a new file, CMake has to actually pick it up. And when a glob is being used, CMake won\u0026rsquo;t automatically pick up the change because it doesn\u0026rsquo;t repeatedly rescan the directory every time you build it, that would be expensive. So you have to manually re-trigger CMake. And when you\u0026rsquo;re using setup.py to build PyTorch, you can just pass the dash dash CMake flag to cause it to pick it up. This is one of the reasons of the debate between whether or not a glob is better or a list of explicit files is better. If you do a list of explicit files, you have to actually edit the CMake list to add the new file, that\u0026rsquo;ll trigger a CMake rebuild automatically. But if it\u0026rsquo;s a glob, you have to like, you know, pass dash dash CMake, just a little thing to be aware of if you\u0026rsquo;re ever adding new files to PyTorch. Okay, so if I\u0026rsquo;ve duly scared you off of, you know, doing CMake modifications, that\u0026rsquo;s great, you can stop listening to this podcast. So now I want to talk a little bit about like, what the heck is going on with the build system in PyTorch. And so there\u0026rsquo;s a few things that, like, are historically important to understand about why the build system is so freaking complicated. So one is that, and this is an ongoing constraint, and you will have a very hard time getting rid of this constraint, is that PyTorch needs to be built under multiple build systems. So it\u0026rsquo;s not just the open source CMake that you\u0026rsquo;re building it, but there is also a buck based build system that is run inside Facebook for building PyTorch for suicide. There is also a second buck build system that is built for Facebook, but when you\u0026rsquo;re running PyTorch on mobile and other sort of exotic devices, there is a third Facebook build system, which is run when you are building PyTorch for running it on Oculus. And there is a fifth Bazel build system, which someone requested for us. And so we maintained for them because, you know, buck, Bazel, they\u0026rsquo;re basically the same thing. But like, you know, if you\u0026rsquo;re using Bazel, you need an actual Bazel build system. So there\u0026rsquo;s so many build systems, and each of them sort of is re implemented. And there\u0026rsquo;s some work to be done to so we have some stuff to like deduplicate a configuration between them. In particular, there\u0026rsquo;s this concept called append file list, which lets us read out file lists from Bazel files into CMake. And we use this in a few cases, not everywhere, but in a few cases. But in general, like when you are doing a build system change, it is not just a CMake change, you also need to change all of the other build systems. And that can be quite a tall order, especially if you don\u0026rsquo;t work at Facebook, and you have no way of running any of the Facebook internal build systems. So find your favorite Facebook employee and make them actually do the build system change for you. The second important thing to know about our build system is that it is the unholy mash together of the Cafe 2 build system and the PyTorch build system. Remember when I said that we merged PyTorch and Cafe 2? Well, this is one of the things we merged, right? We took the two build systems, smashed them together. And we didn\u0026rsquo;t really get very far in like refactoring everything. So for example, you might be wondering where the Torch library is defined would be very reasonable for Torch to be defined in Torch slash CMake lists. Well, it is not defined in Torch slash CMake lists. It is defined in Cafe 2 slash CMake lists. Why is that the case? Well, because it used to not be called LibTorch, it used to be called Lib Cafe 2. And eventually we renamed it to LibTorch, but no one bothered moving the CMake definition from Cafe 2 CMake lists to Torch CMake lists. I really hope these parts of the podcast eventually become obsolete. But I\u0026rsquo;m not holding my breath because as I said, no one really likes working on CMake. And the last reason I would say that our build system is very complicated is a sort of intrinsic problem with CMake itself. So the CMake model historically is set a bunch of global variables in a crappy imperative programming language and then stuff happens, right? Like literally it\u0026rsquo;s like, you know, set this, set this, set this, blah, blah, blah, blah. Modern CMake involves, oh, define a dependency graph, which, you know, says the library structure that you want to build. But, but really like you\u0026rsquo;re still setting tons of variables along the way to like figure out how you\u0026rsquo;re going to set things up. So what makes CMake hard to understand is that like there\u0026rsquo;s this program and it\u0026rsquo;s setting a ton of variables. The order in which these variables are set matters. You\u0026rsquo;re sort of stepping in and out of various subdirectories for different CMake lists. And so if you want to understand what any given definition is, you have to understand the trace of all the possible CMake files that were included before that might have set that variable in question. So that means that there\u0026rsquo;s a lot of non local action going on. Like I said that the torch library definition is in cafe two dots slash CMake lists. Where is the files that the torch library includes defined? Well, not cafe two slash CMake lists. That one\u0026rsquo;s actually in the much more reasonably placed A10 circ A10 CMake lists. So you have to like be willing to follow the breadcrumbs to find where things are defined. Fortunately for you, because CMake is a really crappy imperative programming language. It also is very dumb. So for example, if you are looking for a variable that is being set somewhere, you can just grep for that variable, and you will find it, you don\u0026rsquo;t have to worry about like, oh, some sort of meta programming thing going on, generating these variables on the fly, just search for the variable, and you will find where it is defined, I guarantee you. So modifying CMake or like sort of understanding how the CMake works, usually just involves like, you know, doing a lot of grepping around to find all the places where a particular variable is set. One last note, not everything is in CMake lists.txt. We also have some .cmake files that contain various configuration in the aptly named CMake slash folder. And there\u0026rsquo;s a lot of actually very important stuff going on there. Like, you know, the stuff that\u0026rsquo;s responsible for calling our code generation scripts, say previous podcast. So, you know, be sure to check those out as well. But I don\u0026rsquo;t really recommend trying to sit down and read through all of our CMake end to end, although you\u0026rsquo;re certainly welcome to try. And if you successfully do it, you\u0026rsquo;ll have a very good idea of how everything is set up. But it\u0026rsquo;s usually better to just use this tactical idea of, you know, like, looking, like, find the definition that matters. In CMake\u0026rsquo;s case, there\u0026rsquo;s actually really only one definition that matters, right? Add library. Add library in CMake says define a library that is the thing that, you know, I want to build. So every, like, you know, dynamic library that you see dumped in Torch slash lib, whenever you build PyTorch, there\u0026rsquo;s going to be an add library declaration for that. So you can start there and then start looking at what things refer to this library, like what properties am I setting on it? What files am I saying are it? And then start tracing back the variables. And so you don\u0026rsquo;t have to worry about the ridiculous folder structure that\u0026rsquo;s going on. Okay, so I\u0026rsquo;ve talked a lot about how our build system is terrible. Let\u0026rsquo;s say that you are very enthusiastic, and you think you can help fix the build system. How might you go about doing that? So there are a few avenues that I personally would go about looking at if I were tasked with this task. So first, I would try to unwind the directory structure, actually try to put the definitions of libraries in the places that make sense for them. And what you will find challenging about this is that we actually don\u0026rsquo;t have that many libraries in open source. So for example, we have this A10 directory, and you\u0026rsquo;d expect there to be a library named A10 in open source, but we actually don\u0026rsquo;t have an A10 library at all. Why is that the case? Well, we used to, but it turned out that there\u0026rsquo;s no reason to have a separate LibA10 dynamic library alongside the LibTorch dynamic library. This is something that\u0026rsquo;s useful inside our Facebook build systems, but inside CMake for the open source binary distribution, it\u0026rsquo;s not useful. So we actually just merge them together. So there\u0026rsquo;s a single LibTorch.so that contains all the A10 code as well as the Torch code. So you\u0026rsquo;ve got this problem, which is that the physical directory structure doesn\u0026rsquo;t line up with the dynamic library structure. And that might not be a big deal if you can, you know, define A10 to be, say, a static library, and then you bundle up a bunch of static libraries into a dynamic library. But in old versions of CMake, this was kind of buggy. And so you\u0026rsquo;ll need to figure out like, um, what the, you know, uh, earliest version of CMake you can use to actually do this properly is. Second is that there is this concept of modern CMake, right? Modern CMake says, don\u0026rsquo;t set CMake CXX flags global variable, which twiddles all of the CXX flags for every target defined in the CMake list, because that\u0026rsquo;s a global property. And you have no way of controlling the visibility on per target basis. Instead, look for target underscore functions, which actually define, uh, you know, a property, but only for a specific target. And I would probably start going and trying to like reduce the visibility of everything. And that\u0026rsquo;s kind of a like tall order, right? Because there are so many targets. And there are also a lot of different build configurations you can build PyTorch under. So it\u0026rsquo;s a little not tricky to like, make sure you\u0026rsquo;ve gotten them all right. Something that would be kind of nifty is if there are a way to, um, you know, basically look at the output of CMake, right? Because as I said, CMake doesn\u0026rsquo;t actually do any building. It just produces files that actually build your software in the end. So if there is a way to like run CMake, get the output, and then just, you know, say, okay, I\u0026rsquo;m going to refactor CMake. And I\u0026rsquo;m going to like ensure that the output is always the same. Or if I change the output, it\u0026rsquo;s done. So in a semantics preserving way, right. And then I could like iterate on changes to CMake without having to actually go through the rigmarole of actually building PyTorch under every configuration under the sun, I just need to like, you know, make sure that I don\u0026rsquo;t change what the outputs and questions are. And so that\u0026rsquo;s like, like, so for the simple case of refactoring CXX flags, um, now I just, you know, if I want to like put these into targets, well, I can use the, um, you know, output and the make file to see, oh, where were these CMake, these CXX flags applied in the first place. And just make sure when I do the refactor, I\u0026rsquo;m continuing to apply it in all those cases, or maybe, uh, I\u0026rsquo;m removing it only in places where I know it\u0026rsquo;s not necessary. Oh, and one last thing, don\u0026rsquo;t change things in the CMake randomly and then pray that it works out. Like, yes, the CMake is really complicated. Yes, there are a lot of parts moving parts to it, but like fundamentally CMake is a very simple language. Like it is basically someone went into the process of designing a language without wanting to design a language. And so like, that\u0026rsquo;s why the if statements also look like functions because it was like, Hey, I\u0026rsquo;m not a language designer, but I\u0026rsquo;m just adding features. But the good side of that is that, um, CMake is actually simple and you can understand it. And so if you need to make a change to CMake, just make sure you actually understand the change you\u0026rsquo;re making and then do it. Don\u0026rsquo;t just randomly make changes and hope it works out. That\u0026rsquo;s just going to waste a lot of time when you\u0026rsquo;re trying to do things. Uh, I could probably rant for CMake a lot longer, but that\u0026rsquo;s really all I wanted to say for today. Talk to you next time.\nEP30 TorchScript TorchScript Hello, everyone, and welcome to the PyTorch Dev Podcast. Today, I want to talk about the TorchScript compiler, also known as the JIT compiler. It\u0026rsquo;s a little bit hubristic to think that I could explain an entire compiler stack in a 15-minute podcast, but I\u0026rsquo;m going to try anyway. Unfortunately, there\u0026rsquo;s plenty of resources for you if you want to dig in deeper than, you know, the short amount that I\u0026rsquo;m going to be able to talk about today. In particular, there is a really good overview document in the JIT directory, also linked in the episode notes, that is basically going to cover everything that I talk about in this podcast and everything else in more detail. So my goal here is to sort of just give the big picture, tell you a little bit about the Torch Script compiler, even if you don\u0026rsquo;t necessarily know anything about compilers. So it\u0026rsquo;s going to be a mix of, you know, here\u0026rsquo;s what a compiler is, and also, hey, you know, here are some of the interesting things that are going on in TorchScript specifically compared to other compilers in question. So I\u0026rsquo;m going to structure this like you would structure a normal compilers course. Don\u0026rsquo;t tell William Bowman I said that. He structures it in the other direction, which is we\u0026rsquo;ll start from parsing and then go successfully lower as we, you know, progressively lower into simpler and simpler representations until we get to the interpreter, which actually is responsible for running your TorchScript programs. And so each step is like a traditional step in a compilers course, where, you know, first we do parsing, then we do lowering, then we talk about optimizations, then we talk about actual code generation. Although in the case of PyTorch, we don\u0026rsquo;t actually generate x86 code from your TorchScript models, we just interpret them. And each of these steps, we\u0026rsquo;ll talk about, you know, some of the things that are going on, and how to understand what they\u0026rsquo;re doing from the perspective of a PyTorch developer. So let\u0026rsquo;s start by first setting the stage for what TorchScript is about. So what is TorchScript? So TorchScript is a way of taking Python programs that you\u0026rsquo;ve written, and re-representing them into a form that, you know, is not Python, but is an actual honest-to-goodness IR that we can do optimizations on, which we can easily package up and send and run on, say, C++ services that don\u0026rsquo;t lay against Python at all. You may recall in a previous podcast, I talked about TorchDeploy, a technology for making it possible to run Python programs from multiple threads inside a single server process. Before TorchDeploy, there was TorchScript, and TorchScript took a much more direct route, which was saying, hey, if you want to run your model in a multi-threaded fashion, we\u0026rsquo;re just going to get rid of Python entirely. And so in order to do that, we need to actually have some way of representing our program, and have it be runnable from C++ without a Python interpreter at all in the loop. Oh, and by the way, we also want to do some optimizations, like Fusion, to make our programs run faster in this situation. So TorchScript, in other words, is a graph mode for PyTorch. PyTorch is all about eager execution, but TorchScript actually lets you take your PyTorch program and put it into a machine-processable graph representation that we can do transformations on and that we can actually execute in this way. But there\u0026rsquo;s a step further with the TorchScript compiler, which is that we want to actually be able to capture the control flow and other features of people\u0026rsquo;s programs that otherwise you couldn\u0026rsquo;t get from, say, just tracing your program. So in the very first version of TorchScript, we implemented getting PyTorch programs simply by just running your eager PyTorch mode program and seeing what operators were called, and those were the operators that we actually put into a trace. So TorchScript wants to be able to handle your beam search or your while loop or your, you know, if conditional. It wants to handle all of those things. And so it basically wants to capture a kind of, you know, high fidelity representation of your program, even if, you know, on a single eager mode execution, you might go down one path or you might go down different paths. So it wants to capture something that can describe all of the possible traces of your program in this situation. So with that in mind, what this means is that when you talk about the TorchScript compiler, you have to talk about an actual parser. That is to say, you know, we can\u0026rsquo;t, you know, do the easy trick, easy way out of just tracing your code and getting out a representation of all the things that got run in runtime. Because as I said, there might be an if condition, there might be a while loop, and there\u0026rsquo;s no way to trace all the possible different versions of it, unless you\u0026rsquo;re in a language that supports abstract interpretation, which Python is not. Okay, so, so what does this parser look like? So we\u0026rsquo;ve got our Python code, and we basically need a Python parser. And so in fact, there\u0026rsquo;s two parsers that TorchScript supports. There\u0026rsquo;s one parser that\u0026rsquo;s written in Python, and it\u0026rsquo;s based entirely off of the stock Python AST module that lets you, you know, take some Python code, and blurts out an AST. We also have a reimplementation of this in entirely in C++. So it\u0026rsquo;s a lexer. That\u0026rsquo;s a thing that takes in a string and reduces into a bunch of tokens so that the parsing stage, which organizes this into a parse tree, can do it more efficiently. We have a lexer, and we have a parser that knows how to parse Python, implemented in C++. And remember, that\u0026rsquo;s because TorchScript is all about being able to run PyTouch programs in contexts where Python is not allowed. As a side note, this actually is very important code because we don\u0026rsquo;t actually serialize some sort of, you know, random bytecode format when we want to save TorchScript models to disk. And, you know, remember, this is one of the things that, like, TorchScript is designed to do. It\u0026rsquo;s, you know, take your model, put it into some format so you can load it up into the model server somewhere else. We actually save honest-to-goodness Python code as our serialization format because it\u0026rsquo;s easy to debug, it\u0026rsquo;s easy to modify if you need to. You don\u0026rsquo;t need special tools to deal with it. But, you know, that\u0026rsquo;s only because we\u0026rsquo;re on server, and it\u0026rsquo;s not a big deal to parse Python code when you\u0026rsquo;re loading up your model. On mobile, where binary size is at a premium, see my mobile podcast episode, we don\u0026rsquo;t want to pay that. And so there\u0026rsquo;s actually a different version of the serialization format that\u0026rsquo;s used by mobile, and that\u0026rsquo;s actually a, like, good old-fashioned bytecode format that\u0026rsquo;s easy to parse in, so you don\u0026rsquo;t need, you know, something that understands Python syntax to parse it. Okay, so you\u0026rsquo;ve done the parsing stage of your program, right? And so given a Python program, now you have this AST that looks a lot like the surface syntax, but it\u0026rsquo;s, you know, in tree shape, it\u0026rsquo;s easy to look at, and it\u0026rsquo;s got all of the language features from Python that, you know, you actually support. So, like, you had a while loop with a break, you know, you\u0026rsquo;re going to have a AST that has, okay, here\u0026rsquo;s the while loop, and then inside it, there\u0026rsquo;s a statement, and that\u0026rsquo;s the break statement. And so the next thing you need to do in any honest-to-goodness compiler class is you want to take this, you know, sort of direct reflection of the surface syntax as an AST and lower it, de-sugar it into a simpler representation that\u0026rsquo;s easier to do processing on. This is just like, you know, the very standard thing you do in compilers because people want tons and tons of features in their surface language, right? Like, the more features, the better. Like, invent a new syntax, you know, do all sorts of fancy things. And as a compiler writer, like, this is a big problem for us because we need to write code that can work no matter what features you use. And so the easiest way to make our life easy is to, you know, take all of this, the surface syntax that, you know, all our users want and then condense it down into a smaller set of syntax that, you know, we only, we only have to worry about when we\u0026rsquo;re writing our passes. So, uh, there, there\u0026rsquo;s this transformation. There\u0026rsquo;s a bunch of optimization passes because sometimes we have to do non-trivial analyses to figure out how to like re-jigger things into the simpler format, but eventually you get to what we traditionally call TorchScript IR. So what is TorchScript IR? So if you know what LLVM IR looks like, TorchScript IR has a lot of similarities to it. It\u0026rsquo;s SSA. That means that for any given variable you define, there is a single definition site for it. So you don\u0026rsquo;t have to worry about, you know, you\u0026rsquo;re like, you\u0026rsquo;re an optimizing optimization pass. You\u0026rsquo;re like, okay, who defined this variable? You don\u0026rsquo;t have to worry that there\u0026rsquo;s multiple possibilities, like one in this if branch and one in this else branch. SSA means that, oh yeah, um, there\u0026rsquo;s only ever going to be one of these things. Another thing about, um, the TorchScript IR format is, um, it does understand, uh, conditionals. These are actually added after the fact, because remember, um, tracing, you don\u0026rsquo;t have any conditionals. They all go away. Um, and the way they\u0026rsquo;re modeled is that, uh, instead of a good old fashioned, uh, you know, CFG style setup where you have a bunch of blocks and they have labels and then you have phi nodes for when blocks, um, enter in, uh, from multiple possible entry points. Instead, what we just do is we, we, we, we, it\u0026rsquo;s more of a structured can flow flow control style where like when you have an if statement, there\u0026rsquo;s two sub blocks associated with it that represent the computation that gets run in the first case and the second case. And they, um, you\u0026rsquo;re responsible for passing in the inputs. And then when you, uh, exit, you have to say all the variables you want to return. And then the if statement itself does return values and it returns all of the, um, sort of values that get carried out of the loop. So unlike, um, LLVM SSA, uh, we don\u0026rsquo;t have phi nodes. Instead, those are sort of done implicitly via these, what, whether the, what they\u0026rsquo;re known in the literature as basic block procedures in the situation. Two more important things to say about the LLVM, uh, the, the, uh, TorchScript IR. So one is that, um, although we simplify the aspects of the Python programming language, so we have less features, um, we still have a really big instruction set. Every, um, there\u0026rsquo;s a bunch of, you know, like when you have an IR like this, there\u0026rsquo;s, there are going to be operations, primitive operations or prim ops for short, which don\u0026rsquo;t have an implementation inside of the IR itself. Instead, the compiler stack defines what these operations should be. And, um, every operator that\u0026rsquo;s defined in PyTorch, recall the native functions.yaml podcast, that that\u0026rsquo;s the list of all the operators. Every operator is a valid instruction inside of TorchScript IR. This is kind of a pain in the neck for compiler writers who don\u0026rsquo;t really want to, you know, deal with like over a thousand operators. And hopefully in a future podcast episode, I can talk with Zachary DeVito about some recent work he\u0026rsquo;s been doing about Mintorch, which is reducing the set of operations in PyTorch. But okay. So we have this really big primitive operator set, um, but it\u0026rsquo;s in SSA form. We\u0026rsquo;ve got control flow in a structured manner that\u0026rsquo;s simplified. So there\u0026rsquo;s only a few control flow operations. And one last thing is that this IR supports mutation and aliasing. What do I mean by that? So when you write PyTorch programs, you can take out views on tensors, right? Like you can say tensor open bracket, zero close bracket, and that\u0026rsquo;ll give you the zeroth row of your tensor. And if you mutate that, like you say, um, X dot add underscore blah, then the base will get updated as well. And TorchScript can handle programs that do mutation. It can handle programs that do views. And we don\u0026rsquo;t have a functionalization pass that removes all of these things. So the IR needs to also know about the concept that some operations might have side effects. No, you cannot move operations around willy nilly because, you know, if you move a use of a tensor before you mutate it, that\u0026rsquo;s, you\u0026rsquo;re going to see the old version, not the new version, um, that\u0026rsquo;s going to be semantics changing. So really like what TorchScript IR is, and maybe this is, um, you know, not the best point in the design space, but it makes a lot of sense. If your goal is to, um, uh, like package up as many Python TorchScript models as possible into this representation is, you know, we support all the operators in PyTorch and we support a bunch of control flow. We support mutation and views and, um, we, uh, but otherwise it\u0026rsquo;s an SSA format. So it\u0026rsquo;s still possible to do optimizations on this. So once you\u0026rsquo;re in an IR, the next thing you want to do are optimizations. And we do all the sort of basic optimizations like people optimization, that sort of thing, but there\u0026rsquo;s two really interesting optimizations that are very specific to PyTorch. So one is specialization. So what do I mean by specialization? Well, when you write Python code in PyTorch, typically you don\u0026rsquo;t give it very detailed types, right? Like for example, you have a bunch of inputs and they\u0026rsquo;re just tensors and you don\u0026rsquo;t really know anything else about what they are. Actually, in reality, they\u0026rsquo;re probably all floating point tensors of, you know, dimension three, but you don\u0026rsquo;t, we don\u0026rsquo;t know that when we\u0026rsquo;re parsing the TorchScript IR. And so there\u0026rsquo;s this concept called the profiling executor, uh, rewind a sec. So this is a bad thing for us because if you want to optimize your code, if you want to generate kernels, the more, you know, about what the D types are, the sizes are, the dimensions are, the more, you know, about these things, the easier it is to generate good code. Like let\u0026rsquo;s say you\u0026rsquo;re doing like fusion and you want to fuse a bunch of point-wise operators together. Well, you can\u0026rsquo;t actually do it unless you know, for example, what the dimensions of things are, because if the dimensions don\u0026rsquo;t match, you might actually need to do some broadcasting in this situation. So what the profiling executor does is it runs through your code on some inputs and it says, Oh, here are what the types of everything are. And then it introduces this information into the TorchScript IR. Um, and it does so in an interesting way. So it\u0026rsquo;s not, um, the way you would think, which is like, take your IR and then generate a specialized version of it. Instead, we take the IR as is, and we insert a bunch of what we call guard, um, statements. And what these guard statements say is if it is the case that something has this type, then do this, otherwise bail out and do something else. And so inside the, um, segment of the code where the guard is okay, we actually now, um, are able to optimize under the assumption that, you know, it\u0026rsquo;s floating point and has these sizes. And at the same time, we haven\u0026rsquo;t changed the semantics of the program, because even if you feed it something that it wasn\u0026rsquo;t expecting, you\u0026rsquo;ll still get, uh, a valid result. You\u0026rsquo;ll hit the bailout path in that situation. Another interesting optimization we do is derivative splitting. And this is because, um, PyTorch programs, uh, often are differentiated because you\u0026rsquo;re doing gradient descent. Now, unfortunately, um, TorchScript, uh, can\u0026rsquo;t make use of the standard derivative definitions that are defined in derivatives.yaml because, you know, those are basically just, they\u0026rsquo;re just C++. It\u0026rsquo;s glorified C++ in there. And TorchScript is, you know, this IR, it needs its own IR definition. So, um, unfortunately, we weren\u0026rsquo;t able to like put the derivatives in a, um, form that could be used by both TorchScript and the traditional old eager mode. So there\u0026rsquo;s a set of extra definitions for doing symbolic automatic differentiation in TorchScript. But these definitions are not complete because as I said, there\u0026rsquo;s a lot of operators in PyTorch and, uh, it, you know, it\u0026rsquo;s just hard to actually keep coverage with that. So for the things that TorchScript knows how to symbolically differentiate, um, derivative spilling bunches them all together so that it can, you know, go ahead and generate derivatives in those case for everything else that it doesn\u0026rsquo;t know about. It keeps those separate so that we can run the good old fashioned autograd system. Yes. Um, so we\u0026rsquo;re, we\u0026rsquo;re compiling your code, but you know, we don\u0026rsquo;t necessarily compile everything away. And in particular, if you\u0026rsquo;re going to run, uh, AD code in TorchScript, we still use the eager mode autograd executor in this situation. And so those, um, things that don\u0026rsquo;t support symbolic differentiation, they\u0026rsquo;ll just go through the normal autograd mechanism. And there\u0026rsquo;s a very complicated way of making sure symbolic AD and, um, uh, eager AD work together harmoniously in the situation. And we should honestly write a research paper about this, but we\u0026rsquo;ve been lazy and haven\u0026rsquo;t gone ahead and done it. Okay. So you\u0026rsquo;ve optimized your program, right? So we\u0026rsquo;ve gone from parsing to, uh, lowering to IR, and then we\u0026rsquo;ve optimized the IR. What\u0026rsquo;s the last thing? Well, uh, program is useless unless we actually run it. So we need to be able to run our programs. And the way this works is, as I said, we don\u0026rsquo;t actually, uh, co-generate x86 code from your TorchScript programs. Although maybe this would be a good idea. And, um, some people have looked into it. What we do instead is we just have an interpreter. So we take our, uh, IR and we compile it into a bytecode format. It\u0026rsquo;s a very simple bytecode format. Um, it, all it does is it just does some register allocation and the register allocation is really dumb because, uh, we, we don\u0026rsquo;t really, we\u0026rsquo;re not really storing things in like hardware registers. We\u0026rsquo;re just sort of like using the registers in an easy way to keep track of what intermediates are hanging around. But the thing that is, is important about the registers is that we use them we need to know when tensors die because we need to deallocate them promptly. And so that\u0026rsquo;s something that happens during the final, like, uh, compilation of, um, TorchScript IR into what\u0026rsquo;s called code blocks. So we do that. And then to actually execute your TorchScript program, we do something that should be very familiar to you if you\u0026rsquo;ve ever started the JVM, which is we just have a good old fashioned stack machine. So what do I mean by a stack machine? So a stack machine works is if you want to call a function, you push all the arguments you want, uh, to call the function with onto some stack, right? And you call the function and that function is responsible for popping off all those arguments and then pushing on the return values to the stack stack based machines are very nice because they give you a uniform calling convention that doesn\u0026rsquo;t, uh, that works no matter how many, um, arguments and returns you have. Like if you wanted to actually, uh, do it some other way, then you would have trouble like, you know, finding memory to put all your arguments or returns depending on what the situation was, because remember the interpreter doesn\u0026rsquo;t know anything about what operator is going to execute. It\u0026rsquo;s, you know, running in a loop and going over each instruction and being like, okay, now I got to do this one. Now I go, now, now I got to do that one. And it doesn\u0026rsquo;t know ahead of time. Oh, this is a function that only takes two arguments. And what are these arguments that we\u0026rsquo;re passing in on the, uh, stack? Well, these are I values, um, which I\u0026rsquo;ve talked about in previous podcast episodes, right? It\u0026rsquo;s a box representation of, uh, either a tensor or maybe an integer or some other primitive formats that just let us work polymorphically over them in the interpreter. And that\u0026rsquo;s a whirlwind tour of the short script compiler. I promised 15 minutes, you got 20 minutes, uh, but that\u0026rsquo;s everything I wanted to say. As I said, check out the overview document in the JIT folder. I mean, it contains tons and tons of details, way more information than I talked about in this podcast. That\u0026rsquo;s everything I wanted to say today. Talk to you next time.\nEP31 TH TH Hello, everyone, and welcome to the PyTorch Dev Podcast. Today, I want to talk about th, the previous library that was used to implement all of the kernels in PyTorch. This is something of a historical episode because there isn\u0026rsquo;t really that much code that is still in th in PyTorch today. But it\u0026rsquo;s still a kind of interesting historical example to look at. And we still do have some th code. So if you\u0026rsquo;re an unlucky person, and you\u0026rsquo;re, say, trying to add a new d type or trying to deal with our storage Python bindings, you might still need this knowledge one way or another. So that\u0026rsquo;s what this podcast is going to be about. So what is th? Well, as I\u0026rsquo;ve mentioned in previous podcast episodes, PyTorch is not a project that was written entirely from scratch. It took all of its code from LuaTorch, the previous iteration of the framework, which was a bunch of C code bound to Lua. So we kept the C code, that\u0026rsquo;s the th library, and we bound it to Python instead of Lua. And that\u0026rsquo;s how PyTorch came into being. When you ask the question, why do we want to port all of our th code to C++? We have to understand a little bit about how th was put together in the beginning. And the most important constraint for the construction of the th library is that it was written in C. This posed a challenge for the library in a few ways. One is that the tensor library th needed a way to write algorithms that would be generic over multiple d types. Like suppose you\u0026rsquo;re writing a matrix multiply, and you want it to work for both floats and doubles. In C++, you could use a template to templatize over the d type in question, and then instantiate code multiple times for each version of the d type you want. But in C, there\u0026rsquo;s no such mechanism, right? I had a friend in grad school who was like, yeah, C++ is a really terrible language, but it\u0026rsquo;s really convenient to be able to have a reusable vector container that you can use on different types. And so th had this problem. The problem was they wanted a bunch of tensors that were for different d types, but there was no good way to actually write them all out without actually having to write out all the code, you know, N times where N is the number of d types in your code. And so the way th decided to solve this problem, and also the reason why th is kind of universally loathed and something that we\u0026rsquo;re trying to get rid of, is that it decided that the problem could be solved with macros. So here\u0026rsquo;s how th decided to solve the problem. Let\u0026rsquo;s imagine that you\u0026rsquo;re writing some C code for an algorithm, say matrix multiply, and you want to write it in a way that it\u0026rsquo;s generic over the d type in question. So instead of writing float or double inside your program, you instead write a scalar, you say, okay, well, everything is some unspecified scalar type. And don\u0026rsquo;t ask me how it\u0026rsquo;s going to be defined, but it\u0026rsquo;ll somehow be defined. And you write your code all in this generic way. When you write functions that should be externally visible, you also use another macro, the th tensor underscore macro to say, hey, I\u0026rsquo;m defining a generic function, I don\u0026rsquo;t know what its name is, I\u0026rsquo;m going to tell you what the name of it is later. So the where we\u0026rsquo;re where we\u0026rsquo;re going here is we\u0026rsquo;re going to actually give macro definitions for scalar and for th tensor underscore that basically expand these to the appropriate thing. So if you\u0026rsquo;re doing a float tensor, then the scalar will be a float and th tensor will become th float tensor. But if it\u0026rsquo;s a double, then scalar will be double and th tensor will become th double tensor. And then the trick is, we have a C code, it refers to all these macros. And what we will do is we will define the macros to be float, include the C file, this is very unconventional, right? Normally, you only include header files, but here, we\u0026rsquo;re actually going to include the honest to goodness C file, include the C file, and then undef the macros, redef them to the next D type we want to instantiate with, and then include the C file again. And we\u0026rsquo;ll keep including the C file with different settings of the macros until we\u0026rsquo;re done instantiating all of the D types that we want. So yeah, there you go. This is the most important thing to know about th. And in terms of code structure, all the C files that are instantiated multiple times in this way, they live in generic slash folders. And these aren\u0026rsquo;t all in th or thc, there\u0026rsquo;s also a folder in torch ccirc for doing Python bindings that also is written this way. And so you know, whenever you see the generic folder, that just means it\u0026rsquo;s the C code, C++ now, because we made it into C++, that gets stamped out multiple times in this way. Doing things this way also meant that it was easy to generate a new tensor type for every instantiation, we had a struct th float tensor and a struct th double tensor, etc, for each of these things. And those were the those were also instantiated in the same way. And this also caused some problems when we wanted to write generic code, because well, there\u0026rsquo;s these structs are all different in all of these cases. And so one of the things that we did early on when porting to C++ is we unified all of these different D type structs into a single struct that was polymorphic, because while we don\u0026rsquo;t actually need to store floats or doubles directly in the struct, we only ever store a data, a pointer to the data in question. So that\u0026rsquo;s something that you can easily write a single struct that works in all cases for. I actually don\u0026rsquo;t think this macro instantiating strategy that the old th libraries was too bad. It\u0026rsquo;s actually a pretty nice way of adding on a fake parameterization system to a language that doesn\u0026rsquo;t natively support it, aka C. And I don\u0026rsquo;t really, I can\u0026rsquo;t really think of other ways you could have gone about doing this. Actually, my PhD thesis at Stanford was about Backpack, which was this module system we retrofitted onto another programming language called Haskell. And it also operated by very similar ways. You had a bunch of sort of types and functions that you left unspecified, and then you instantiated them with an actual implementation later when you wanted to do the code in question. And why did we do it this way? Well, we did it this way because we didn\u0026rsquo;t really want to make major surface changes to the language in question. So it turns out you could do my PhD thesis and see just with macros. Who knew? There\u0026rsquo;s a few other things about the th code that are good to know, although they\u0026rsquo;re less major than this macro system. So one is that th, because it\u0026rsquo;s written in C, has to be manually ref counted, because you don\u0026rsquo;t have a concept of constructors or destructors, which C++ programs use to implement RAII. RAII is probably one of the other sort of killer features of C++ because who wants to do manual reference counting. It\u0026rsquo;s also a big problem though, because with the automatic reference counting, you can\u0026rsquo;t easily tell when you\u0026rsquo;re doing these ref counts. And so it\u0026rsquo;s easy to write code that does a lot of unnecessary ref counting. So, you know, it\u0026rsquo;s a double-edged sword, right? Like when you wrote th tensor code, it was easy to get the ref counting wrong, but at least you could see it all in one place. And then when it\u0026rsquo;s, you know, all implicit and hidden away in these classes, it\u0026rsquo;s easy for people to forget, oh yeah, there\u0026rsquo;s actually cost to ref counting, bumping willy-nilly. I guess this is one of the reasons why Linus Torvalds still writes all of Linux in C, because C++ is just this terrible language that like has all of this, you know, extra stuff that happens automatically, and it\u0026rsquo;s easy to forget about, and you write really slow code. Anyway, so we had a manual ref count in C, and that was also a pain. And it was especially painful when you had error conditions, because you had to make sure you freed all the temporaries when the error condition fired. Because we were actually, in the old days, when it was C only, we would crash the process when you hit an error like this. But very early on, when we started porting things to C++, we were like, okay, we\u0026rsquo;re going to do everything at C++. And then when you hit an error, we\u0026rsquo;re going to raise an exception. So we can recover it from it, and not just crash your Python process when this happens. One last thing that\u0026rsquo;s interesting about th, and actually sort of has propagated its way to our Aton ports, is a lot of the neural network operations that we supported have a lot of buffers that get passed from forwards to backwards. So what are these buffers? Well, basically, they\u0026rsquo;re extra outputs from a function in the forward pass that you don\u0026rsquo;t actually use. Like from the perspective of a user, these buffers are invisible, you don\u0026rsquo;t see them, they just invisibly get passed to the backwards function, where they get used. And a lot of the times, they don\u0026rsquo;t actually do anything useful. They\u0026rsquo;re just like scratch space that the kernel in question uses. Why do these buffers exist? Well, it turns out that back when we were in LuaTorch, we didn\u0026rsquo;t actually have a caching allocator for CUDA. So allocating CUDA memory was very slow, and it was very expensive. And one of the first new pieces in PyTorch that also was one of the really important pieces for making our CUDA programs run fast was adding a caching memory allocator. So that so in Lua, you know, you really wanted to not have to allocate memory willy nilly. So if you allocated this buffer, and then you saved it for later, that was actually a benefit, because you wouldn\u0026rsquo;t have to do this allocation again later. PyTorch doesn\u0026rsquo;t have this requirement. So if you ever see these scratch buffers being passed around, that\u0026rsquo;s just useless memory usage, and you should just get rid of it. So that\u0026rsquo;s really all you need to know about th. I\u0026rsquo;m not going to labor on because we have a process of porting th operators to ATEN operators that has gone pretty far. We\u0026rsquo;re very, very close to getting rid of all the legacy th code, and no one else is going to have to have the c code inflicted on it. There\u0026rsquo;s also there was also a lot of legacy code gen that was written specifically for the c library. We\u0026rsquo;ve also gotten rid of all of that. So you don\u0026rsquo;t really have to worry about that anymore as well. There\u0026rsquo;s one thing that I regret a little about porting all of our th code to ATEN, and that\u0026rsquo;s the loss of static typing in call sites. One of the things that is kind of expensive in modern PyTorch is when we dispatch so we have to go look at all the tensors and figure out oh is it CPU CUDA and go to the right one for the right d type in that situation. th didn\u0026rsquo;t have this problem because there was a separate type for every th float tensor, th double tensor, etc. And you always wrote code knowing exactly what your d type was right because everything was in one of these c files where you\u0026rsquo;re going to instantiate the macros. So calls in th, while they couldn\u0026rsquo;t get inline because inlining isn\u0026rsquo;t really a thing in c, you could still actually just compile them as normal function jumps without any fuss and muss. And we have swung back around to wanting to be able to do this in PyTorch proper when performance matters, but it\u0026rsquo;s a bit harder because you know we don\u0026rsquo;t have we don\u0026rsquo;t really want to template all our code and so it\u0026rsquo;s just kind of annoying to actually make sure these things work. One thing we\u0026rsquo;ve been looking at is maybe we can use very small amount of you know just in time compilation techniques. No, not the JIT compiler for PyTorch, but like good old-fashioned polymorphic inline caches that might make it possible to like speed this up. But that\u0026rsquo;s something just speculatively we\u0026rsquo;ve been looking at. Okay, so you know about th. That\u0026rsquo;s everything I wanted to talk about today. Talk to you next time.\nEP32 XLA XLA Hello everyone and welcome to the PyTorch Dev Podcast. Today I want to talk about Torch XLA, PyTorch\u0026rsquo;s integration with TensorFlow\u0026rsquo;s XLA compiler. What is XLA? XLA is an optimizing compiler that sits below TensorFlow, our most favorite deep learning framework, and its sort of purpose in life is several things. So one is it\u0026rsquo;s intended to have a lot of optimizations, the idea being that XLA\u0026rsquo;s intermediate representation, HLO-IR, doesn\u0026rsquo;t need to have that many operations, and then XLA will know how to compile them and optimize them in a way, even though you may have expressed your programs as having lots of itty-bitty operations. XLA is supposed to be able to fuse them together and give you good performance. The second thing XLA is supposed to do, and the one that really made PyTorch want to integrate with XLA, is it\u0026rsquo;s the only stack that can actually run Google TPUs. TPUs are a deep learning hardware accelerator that is developed by Google. There\u0026rsquo;s a lot of free TPUs. Google loves getting people to use them, and if you want to use them, you\u0026rsquo;ve got to use XLA. And so if you want to use PyTorch and use TPUs, well, Torch XLA is your guy. Torch XLA has a lot of people who have worked on it, both on Facebook\u0026rsquo;s side and on Google\u0026rsquo;s side, and two big people I want to call out who were very historically important to this development are Ailing Zhang on the Facebook side and Davide Libense on the Google side. They both have very big influence on the XLA project. Okay, so how exactly does Torch XLA work? There\u0026rsquo;s a big, big problem when you want to take PyTorch, which is ostensibly an eager mode framework, and hook it up to XLA, which is a graph compiler. It takes in a graph of operators that is already preset and compiles it into some efficient form, which is that PyTorch eager doesn\u0026rsquo;t actually have any graph representation. Well, see my previous podcast talk about TorchScript, where if you Torch script your model, then you can get a graph mode representation of it. But one of the things that going into the XLA project, they wanted to make possible was they wanted it to be possible for people fashion PyTorch eager scripts, and run them straight on XLA without having to do very many modifications. In other words, the goal of this integration was functionality, we wanted, you know, to have as much stuff work as possible on XLA with as little work as possible that you want to do. This is a double edged sword. I\u0026rsquo;ll talk about it more later in the podcast. So how can you run eager mode code directly, while still feeding it to a graph compiler? Well, the big idea that everyone comes up in the situation is to use some sort of lazy tensor. Now, I don\u0026rsquo;t mean lazy in the sense that you had a tensor and you were going to materialize it at some point, but you\u0026rsquo;re just waiting for someone who actually needs it to use it. XLA is Torch XLA is a really, really lazy operation, what it does is it doesn\u0026rsquo;t run anything when you run your model until the very end when you want to do the optimizer step. And that\u0026rsquo;s the point when the trace of operations that you\u0026rsquo;ve run during this period of time actually gets sent to XLA and gets compiled. Or, you know, hopefully, we\u0026rsquo;ve already seen this trace before, because your program maybe isn\u0026rsquo;t too dynamic. And that\u0026rsquo;s when it gets compiled into XLA and, you know, done. So what are we doing? So you\u0026rsquo;re running a bunch of PyTorch ops, it all looks like normal PyTorch. But under the hood, we\u0026rsquo;re constructing XLA\u0026rsquo;s HLO IR. And at the very end, that\u0026rsquo;s when we actually send it to XLA. Lazy tensors in this way are very reminiscent of DINET, another framework, where the idea was, you wrote your C++ code, you ran it, you ran every single example one by one. And then they had this thing called automatic batching. So they\u0026rsquo;d look at all the traces, and then batch them up so that you could run them more quickly. So Torch XLA also runs very much this way. When you run your PyTorch program for a single iteration, we always are constructing the HLO graph from scratch every time around. So actually, XLA does support dynamic execution, right? Like if you do something a little different on the next run, you\u0026rsquo;ll just get a slightly different HLO IR, and we\u0026rsquo;ll just compile it to some other thing in that situation. So what exactly goes into making a lazy tensor work? Well, there\u0026rsquo;s a few very important things. So one is that you need to interpose into the calls into PyTorch. when you call a bunch of operators. And where this interposition happens is pretty important, because we want to also do training on XLA. And that means we need to be able to differentiate our graphs. And Torch XLA takes the approach of reusing PyTorch\u0026rsquo;s Autograd engine. And because it reuses the Autograd engine, what we need to be able to do is we need to run our program lazily, the forward pass, and then also lazily run the backwards pass to generate the operations for all the things that the automatic differentiation needs, and only then run the entire code in question. So Torch XLA gets integrated in the dispatcher, because the dispatcher is the point that\u0026rsquo;s low enough in PyTorch\u0026rsquo;s stack of functionality to observe both the forward and backward forms of the AD pass. So once you get to the bottom, the XLA dispatch key, that\u0026rsquo;s just what processes tensors that are XLA of the XLA device. So we go into Torch XLA. And what Torch XLA does is it takes all the arguments and figures out how to construct a corresponding HLO IR node for the PyTorch operation that was done. So basically, there\u0026rsquo;s a translation of the PyTorch semantics into the terms of the XLA semantics. And you might imagine that we would construct HLO IR directly at this point in time, but that\u0026rsquo;s not quite what happens. What actually happens is there\u0026rsquo;s a intermediate IR that gets built by Torch XLA, and it\u0026rsquo;s intended to be very fast to build. And then once we\u0026rsquo;re done, we first check if, this IR matches something exactly that we\u0026rsquo;ve seen before. And if that\u0026rsquo;s the case, then we don\u0026rsquo;t need to do any compilation. We don\u0026rsquo;t need to translate into XLA\u0026rsquo;s HLO IR. We can just directly reuse the pre-computed trace. Otherwise, they do a very simple elaboration into XLA IR. And this just makes it possible to run XLA programs pretty quickly, even if XLA\u0026rsquo;s HLO IR isn\u0026rsquo;t designed to be built very quickly and repeatedly in this way. And that\u0026rsquo;s really it. So most of Torch XLA is just the massaging of PyTorch operators into XLA form, inserting the things that you need, smoothing over differences in semantics. But deep learning frameworks are all very similar. So in a lot of cases, things match up pretty closely. There is one place, though, that things don\u0026rsquo;t match up very closely. And that\u0026rsquo;s PyTorch\u0026rsquo;s support for views. Recall, PyTorch\u0026rsquo;s support for views means that if I have a tensor, I can take out a view on that tensor. And then no matter if I mutate the view or the original base tensor, the change is reflected in the view or the base, respectively. So XLA doesn\u0026rsquo;t actually natively support this. It has some support for aliasing and mutation, but not to the degree that PyTorch does. In other words, it doesn\u0026rsquo;t really know about strides. Strides are a very PyTorchy, Torchy, Lua Torchy, Torch 7, you know, lineage thing. So how exactly do we translate these PyTorch programs to XLA? Well, what we do is the functionalization pass that I talked about in one of my earlier podcast episodes. So what we do is we keep track of all the aliases when PyTorch makes them. And then when someone updates an alias, we just go and look at all the other aliases and reapply the update in those cases. And this happens lazily so that we don\u0026rsquo;t actually have to keep track of all the aliases that are on the tensor. This works pretty well. And so you can mutate to your heart\u0026rsquo;s content, and we are still able to translate to XLA. I mentioned earlier that Torch XLA\u0026rsquo;s integration favors functionality or performance. And another way that this is favored is that XLA has a CPU fallback. Because PyTorch has a ton of operators and XLA, HLO, while cool, doesn\u0026rsquo;t have that many operators. One of the selling points of HLO IR is it\u0026rsquo;s pretty small, and it\u0026rsquo;s easy for backends to target. Actually, that\u0026rsquo;s why a lot of, you know, of the burgeoning new hardware accelerators often target XLA, because that\u0026rsquo;s a very easy place to start. And well, you know, if you do XLA, then you\u0026rsquo;ve got TensorFlow. And TensorFlow is a very important framework to support when you\u0026rsquo;re doing this sort of thing. So PyTorch XLA has a fallback. So what the fallback says is if there\u0026rsquo;s some op and we don\u0026rsquo;t know how to translate it to HLO IR, we\u0026rsquo;ll just go ahead and immediately run the XLA graph to get out what the output would be, translate that output into a PyTorch CPU tensor, and then run the good old-fashioned PyTorch CPU operation, and then go back and put it back into the XLA graph. So, you know, that\u0026rsquo;s not going to be very fast, right? Like, you know, you don\u0026rsquo;t, you\u0026rsquo;re seeing less of the graph to optimize, and you have to, you know, go ahead and like, if you were on TPU, you have to move it back to CPU so you can do the fallback. But at least your program runs. And in a lot of cases, that\u0026rsquo;s all you need. You didn\u0026rsquo;t care that much about performance. You just needed to get it working in the first place. That being said, sometimes all of these conveniences can make it hard to make your Torch XLA models go fast. So we\u0026rsquo;ve had some experience working with people who wanted to get their stuff running on TPU. And one of the themes that happened is that sometimes their code would just run really slowly. And why was that? Well, oh, okay, there was, you know, a if statement somewhere inside their model. And that was causing Torch XLA to have to recompile many, many different traces every time it went one way or the other in the if statement. And yeah, you have to like rewrite your model a little so that the traces don\u0026rsquo;t change over time, so that you can reuse the XLA traces. And that can be a little challenging. It\u0026rsquo;s a bit different than say Jax, where Jax provides you this JIT combinator. And what the JIT combinator says is you\u0026rsquo;re going to run the JIT combinator once on this model that you\u0026rsquo;re going to run. And whatever it is that you traced at that point in time, that\u0026rsquo;s what you\u0026rsquo;re going to have compiled. So there\u0026rsquo;s no expectation that things are going to work dynamically. There\u0026rsquo;s no expectation that, you know, every time you go through a new batch, you\u0026rsquo;re going to JIT again, like, you know, obviously, you JIT once and then run it many times. For better or for worse, right? Okay, I want to talk about some nuts and bolts about general PyTorch development, you might have had your eyes glaze over, because you\u0026rsquo;ve never, you know, interacted with XLA. And whatever, like, do I have to care as a PyTorch developer? And the answer, unfortunately, is yes, because XLA is in our CI. And so if your PRs are not passing XLA CI, well, we are not going to let you land them. That being said, there are some peculiarities to the XLA CI. XLA lives in a separate repository, because we have a lot of Google people who work on it, and they all need commit access. So it\u0026rsquo;s in a separate repo from PyTorch PyTorch, which only Facebook people can directly land to. So how did we set up the CI? Well, there\u0026rsquo;s the right way to do it. And we did the wrong way. But it was pretty easy, which is PyTorch will pull whatever the master build of XLA is at any given point in time for your peers. Crazy, right? Like you\u0026rsquo;re never supposed to do that in CI. But that\u0026rsquo;s what we do. And what makes it work is we have a lot of dedicated people on call for XLA, like Iling, like Jack Sao, who, when someone has a PR that\u0026rsquo;s making a change in an operator, and that operator is affecting XLA, because there\u0026rsquo;s some translation in XLA, and now it\u0026rsquo;s changed, and it needs an adjustment, you can just sort of send up the bat signal be like, hey, we need some XLA work. And usually an XLA PR will show up, you know, in short order. And then what just needs to happen is you land the PyTorch PR. And then once the PyTorch PR is landed, the XLA PR is landed as well. The XLA CI has some pretty nifty features. For example, they have this thing called torch pin. So like if you\u0026rsquo;re making an XLA change, and it needs to be against a specific pull request from PyTorch, well, you add this torch pin magic file that says a PR name. And then when your CR runs, it\u0026rsquo;ll be run with respect to that PyTorch\u0026rsquo;s pull request and not master in that situation. And yeah, sometimes this means that we break the XLA bill temporarily when things land. And usually, if that happens, you just are like, hey, you know, is there an XLA change? And usually there is. So the XLA change lands, and then everything\u0026rsquo;s green again. That\u0026rsquo;s really the most important thing, like just knowing who to talk to, to resolve XLA errors, and someone will help. Don\u0026rsquo;t worry, you don\u0026rsquo;t have to know everything about XLA. There\u0026rsquo;s also some cool stuff coming up in the space of XLA integration. So one thing that Brian Hirsch has been working on is an external code gen in PyTorch PyTorch that XLA can use. And we\u0026rsquo;ve actually landed most of the pieces of this. Previously, XLA actually had its own sort of homebrew code gen with a homebrew parser for native functions.yaml that generates all of their definitions, because there were a lot of boilerplate to write, especially with CPU fallback, right? Because every operator needs to have a CPU fallback. And it\u0026rsquo;s very, very boring. You just translate all the tensors to CPU, run the CPU operation, translate them back to XLA. So we have a shiny new code gen in PyTorch. And we\u0026rsquo;ve been, one, trying to make it possible for people outside of PyTorch PyTorch to make use of our code gen, and also provide a much nicer, you know, backend generic mechanism for overriding operators in the way XLA wants to. Because actually, what has happened is XLA is our most famous and most successful backend extender of PyTorch. And people were actually copy pasting XLA\u0026rsquo;s kind of janky code gen for their use cases. So Brian\u0026rsquo;s got this new thing. It\u0026rsquo;s pretty cool. We\u0026rsquo;re working on moving the users from n equals one to n equals two. And there will soon be lots of documentation and more pitches about it. Another cool thing that\u0026rsquo;s coming up is Alex Suhan has been working on refactoring Torch XLA into what we\u0026rsquo;re calling the lazy core. Because XLA is this lazy tensor functionality, which, you know, like records what functions got run when you\u0026rsquo;re doing in PyTorch. And this is something that a lot of other backends want to use as well, right? Because anytime you have a graph backend that can\u0026rsquo;t run things in eager mode, by the way, don\u0026rsquo;t do that. Like, hey, hardware accelerators, support eager mode, support streams, it\u0026rsquo;s a good idea, really good programming model. But let\u0026rsquo;s say that you can\u0026rsquo;t, right? Well, you need something like XLA\u0026rsquo;s infrastructure for recording the graph so that you can actually run it. And so lazy core is the part of XLA that, you know, doesn\u0026rsquo;t have any of the XLA lowerings, but has that generic infrastructure for actually recording lazy tensors. And so he\u0026rsquo;s got a branch that which has split these out into two pieces. And Brian and Alex are working on merging this into PyTorch core so that generally people can use it. Okay, so that\u0026rsquo;s a whirlwind tour of XLA. That\u0026rsquo;s everything I wanted to say for today. Talk to you next time.\nEP33 Expect-tests Expect-tests Hello, everyone, and welcome to the PyTorch Dev Podcast. Today, I want to talk about expect tests. Expect tests are a form of tests that have a characteristic property, which is that when the test fails, you can automatically ask the test framework to update the test for you to accept the new output. And so sometimes they\u0026rsquo;re called golden tests, because there\u0026rsquo;s some golden version of the output, and you can refresh the golden version based on directly what the test tells you. So imagine that you\u0026rsquo;ve got some program, and under some cases, it raises an error, you might not want to hard code the error as a string to compare against because well, what if someone edits the error message, then you\u0026rsquo;ll have to go and find all the places and update you know, the error message that has been hard coded in those places. And often what people will do is they\u0026rsquo;ll write a regular expression instead, in this case, to match for some singular important piece. But you know, if you completely rewrite the error message, that\u0026rsquo;s no good either, because the regex will probably fail, and then you\u0026rsquo;ve got to go and go to each of the sites and manually fix them all. So what expect tests say is that, hey, when this happens, when your test fails, you just rerun using expect test except equals one as an environment variable or something like that. And then the expect test will automatically go through all of the sites that were wrong and modify them so that they have the new output in question. And then for example, you can just run git diff to go look at the changes that were applied and see if you like the changes or not. It makes it really easy to write tests that track internal implementation\u0026rsquo;s details really closely while still making it not so painful to update the tests as those implementation details change. Expect tests long predate PyTorch and this podcast. My personal story with expect tests is I first ran into them while working on GHC, the Haskell compiler. GHC had expect tests and the way they were used was to test the error messages that GHC gave because, you know, one of the things that Haskell is very famous for is a very strong type system. And so we work very hard when you have a type error to actually give you some useful information in this case. And, you know, there are tons and tons of test cases testing what happens when things are mistyped. And, you know, when Simon Payton Joes goes and rewrites, you know, exactly how unification works or, you know, some other major subsystem in the type system, chances are a lot of error messages are going to wobble. And having accept tests meant that it was really easy to just go ahead and update all the error messages and then just like eyeball them and see if they made sense or not. I actually, during my PhD, um, ported, um, a version of this mechanism over to Cabal\u0026rsquo;s test suite where, you know, Cabal also, you know, has a test suite where you want to run lots of different, um, Cabal packages and see if they compile or don\u0026rsquo;t compile. And it\u0026rsquo;s kind of a pain to, you know, write exactly what you expected it to happen and accept tests made this easier to do. I\u0026rsquo;m also reminded of a conversation that I had with Ron Minsky at Jane Street, um, where he was describing to me some of the stuff they were doing with except, uh, except tests in, um, Jane Street. And, um, their model was that, um, unlike what GHC was doing, which was, we had these files that were the expected values. And so when we refresh the tests, we just updated those files. What they were doing was they were actually storing the expected strings inside the test files directly. There are a lot of good reasons to do this. Imagine you\u0026rsquo;re writing a test case, right? So you like set up some functions, you do some operations, and then you want to like check that the output, um, makes sense. It\u0026rsquo;s much easier for a code reader and a code reviewer to understand what the test is doing overall. If the actual expected value of, you know, whatever string comparison is directly in the test file, right? Because you just read what the test setup is, and then you read what the output is. Now, this is a little challenging if you want to, you know, do the main property of expect tests, which is that you can actually update the test output automatically. So what you basically have to do is you have to write some code that knows how to update, you know, your OCaml code, or in PyTorch\u0026rsquo;s case, your Python code, and rewrite the source code so that you actually can put in the new value in question. But if you can get past your distaste for doing this kind of thing, it\u0026rsquo;s really, really helpful and makes accept tests a lot easier to understand. One of the, um, sort of complaints, uh, with, um, you know, expect tests that, you know, are in an extra file is they\u0026rsquo;re just these random files. And when, um, you know, you have to update these test files, um, it\u0026rsquo;s basically, you\u0026rsquo;ve got this big directory of all these expected things, and they\u0026rsquo;re all wobbling, and you have no idea why they\u0026rsquo;re wobbling or not. Actually, a lot of Onyx tests, when we originally wrote them, we\u0026rsquo;re done in this way, because it was easy to do it this way. And I don\u0026rsquo;t know, I think they\u0026rsquo;re not very interpretable, and people decided they didn\u0026rsquo;t like it very much. But, you know, the answer is just put the expect test directly in your source code. There\u0026rsquo;s another important function to doing it this way, because, uh, you, when you are putting it directly in your source code, you don\u0026rsquo;t want your expect test to be too long. And figuring out exactly what you are going to test against, what you\u0026rsquo;re actually going to record in your test file, that\u0026rsquo;s pretty important, right? Because you don\u0026rsquo;t want it to be too long, but you don\u0026rsquo;t want to be too short to not capture the important things you want to do. So actually, when you are going in and writing an expect test, you need to think about what exactly it is you want to output in this situation. And so when I\u0026rsquo;m bringing up a new subsystem, and I want to write some accept tests, I\u0026rsquo;m usually going to actually spend some time designing a text format that describes the internal state of my system in enough detail that I can actually test the important things, but, you know, condensed enough so that someone reading over the code can understand it. And that\u0026rsquo;s one of the first things that I do. And then, you know, I tweak it as I go ahead and, you know, see more test cases. So as you can imagine, expect tests have been a little sort of pet project of mine regarding testing for a while in PyTorch. I added a really simple version of them that just wrote things to files at the very beginning. People sort of used it, they didn\u0026rsquo;t like it very much, because things are out another file. And when I was writing GH stack, see my previous podcast about stacked diff development in PyTorch. One of the things that I needed to do was write a test suite for GH stack. And this was kind of not so easy, right? Because what is GH stack doing? GH stack is taking a bunch of commits and then pushing them to GitHub. And like, well, for one, like, how do you even test in this situation, right? You don\u0026rsquo;t want your test suite to be creating tons of repositories and issues on GitHub. By the way, we solve this by like creating a crappy fake in memory implementation of GitHub\u0026rsquo;s GraphQL and REST, like API, so that we could fake implementation so that the tests could actually run against them. But that\u0026rsquo;s a story for another day. And then once you\u0026rsquo;ve done all that stuff, you need to actually like stand back and be like, hey, what the heck happened? Did GH stack actually create the pull requests that were necessary and push the commits that were necessary? And that was sort of the point where I was like, okay, I\u0026rsquo;m actually going to sit down and spend some time writing a module that will help me do inline expect tests. And then I\u0026rsquo;ll also sit down and write a representation for the state of Git repositories and pull requests so that I can write these tests in a straightforward way. And so I did. And the way that the implementation worked was that when a test failed, we would catch the exception that was raised by the error. And this is this backtrace would contain a line number to the call to assert expected inline that that\u0026rsquo;s the name of the method on the test class that expects tests give you a backtrace with the line number of the thing in question. And then what we do is we go open up the Python file, go to that line and search for the string in question. And so there\u0026rsquo;s a convention that I picked for our expect test implementation, which is that we only ever substitute triple coded strings. In principle, we could substitute single coded strings, but then it might be easy to end up in a situation where you have like multiple strings on a line. And so then it\u0026rsquo;s like, which one do you replace triple coded strings don\u0026rsquo;t really have that problem. You\u0026rsquo;re not very likely to have multiple triple coded strings in one line. So we find the triple coded string, and we do the substitution on it. And then we write out the Python file back to the end. And that\u0026rsquo;s basically the crux of how it all works. There\u0026rsquo;s a funny implementation detail about pre Python 3.8, which is that prior to Python 3.8, the actual line number for the backtrace is for the end of the statement in question. So if you have a multi line statement, it gives you a pointer to the last line of the statement question. So you actually have to run the records backwards, you you\u0026rsquo;re like, Okay, well, starting from the end, look for the string in question, and then do the substitution. Details. But once you do that, all that, you have a implementation of expect tests, you have, you do a simple good old fashioned string comparison to check if the value equals the string in question. And if you don\u0026rsquo;t like it, or you want to update it, then you just do this regex on the source file Python source file, and it updates it. And then you can go and take a look in your favorite git diff viewer to see what the changes. And this is really, really easy, and makes it super great for like, you know, writing tests without having to laboriously write down all the things you expect. So I like expects tests a lot. They\u0026rsquo;re really powerful. And they let you write tests in a lot less time, especially if you write a lot of tests that involves setting up some state, running something and then looking at what the results are. A few words of advice for when you\u0026rsquo;re setting up accept tests. So I\u0026rsquo;ve already talked about some of the common problems, right? So one is that you don\u0026rsquo;t want the representation to be too long, because then it\u0026rsquo;s going to be like, oh my god, like, what is all this stuff. So you want the representation to be actually legible by humans. And that means you have to spend some time designing it. Some other more basic things that you need to be careful about when you\u0026rsquo;re doing expect tests is one, you need to make sure the outputs actually deterministic, right? Like if you\u0026rsquo;re putting a timestamp in the output, that\u0026rsquo;s bad because well, it\u0026rsquo;s going to change every time and your expect test is just not going to work. If you\u0026rsquo;re like writing the output format from scratch, this is not a big deal, right? You just don\u0026rsquo;t put timestamps in. But sometimes there\u0026rsquo;s non-determinism in your algorithm. Like for example, automatic differentiation in Python and PyTorch runs in a multi-threaded fashion. So it\u0026rsquo;s not guaranteed what order your backward nodes will run. And so if your trace involves recording logs when things get run, well, just be aware that that is non-deterministic and you might have to do some canonicalization, for example, to make sure the expect test all works out. Sometimes you can just sort of mask out the text that you don\u0026rsquo;t like. So it\u0026rsquo;s like, hey, I know this thing is non-deterministic. So before I do the string comparison, I\u0026rsquo;m going to go ahead and replace it with some placeholder token. And that token, you know, will always be the same no matter what I\u0026rsquo;m doing. By the way, it\u0026rsquo;s a pretty good idea to make sure your code is deterministic. It pays off in a lot of other ways. And so ease of use with expect tests is just yet another payoff. Okay, so nuts and bolts of using expect tests in PyTorch. The default test case that PyTorch provide already contains expect test functionality. So all you need to do is call the relevant function. And the most common one you\u0026rsquo;ll use is self dot assert expected inline. Assert expected means it\u0026rsquo;s going to be an expect test. And inline means that you\u0026rsquo;re going to put the string directly inline inside of your source code program. There\u0026rsquo;s also variants that work for if an exception is raised, what you expect the expect exception text to be, just check the expect test module in PyTorch to see what API options are available to you. The module that implements expect test itself is actually pretty self contained. And I copy pasted it between gh stack and PyTorch because I didn\u0026rsquo;t feel like making a separate package to do this. But if this is code you are interested in, shoot me a tweet, and I\u0026rsquo;ll figure out what I can do about actually publishing it for real. That\u0026rsquo;s everything I wanted to say for today. Talk to you next time.\nEP34 vmap vmap Hello, everyone, and welcome to the PyTorch Dev Podcast. Today, I want to talk about vMap. vMap is a feature that was popularized by Google Jacks, which lets you write code without thinking about batching, and then automatically make your code batched. So let\u0026rsquo;s imagine that you want to, you know, add some tensors together, and then do a matrix multiply on them, maybe run a convolution on them. What vMap says is you can write it as if you were writing this computation on a single batch, really no batch dimension at all. And then you can vMap over it to so-called vectorize it, that\u0026rsquo;s what the v in vMap stands for, so that all these operations transform into their vectorized operations. And in many cases, many ops in both PyTorch and NumPy and Jacks are automatically batched in the sense that if you attack on extra dimensions to the beginning of the tensor, the operations semantics will say, okay, I\u0026rsquo;ll just treat those as batch dimensions and process them. There are a lot of operations that don\u0026rsquo;t do that. For example, operations that only take a single batch dimension or operations that change their meaning when you add more or less dimensions. Matrix multiply is a particularly bad offender in this front. And vMap makes it so that you don\u0026rsquo;t have to like worry about, oh, yeah, if I want to add a batch dimension, I can\u0026rsquo;t use matmul anymore, I have to use bmm instead. And all it says is no, just write the single example version, and I will automatically translate it into the batch version as necessary. So how do you implement vMap? There are a number of different ways, but I\u0026rsquo;m going to talk about the particular implementation that PyTorch\u0026rsquo;s vMap implementation uses, because it\u0026rsquo;s the one I know best, and it\u0026rsquo;s most relevant if you want to develop PyTorch. So in PyTorch, when I want to vMap over a tensor, what I do is I introduce a new concept called a batched tensor. A batch tensor can be thought of as a regular tensor, but with some of its dimensions marked as being so called batched dimensions, which don\u0026rsquo;t participate in normal computation in the way that you normally imagine. So let\u0026rsquo;s imagine that I\u0026rsquo;m talking about a square matrix, right, you know, A by B, and I want to batch it, so I have a batch dimension on it. So ordinarily in PyTorch, if I asked what is the dimension of this, you know, batch by A by B tensor, I would tell you three. But with a batched tensor, the batch dimension is considered a private implementation detail, and so I don\u0026rsquo;t get to see it. So if I ask what the dimension of a batch tensor with one of these batch dimensions in it is, I actually only get two, because logically, when I\u0026rsquo;m looking at this tensor, I want to just be able to do single operations on it. And so when I say, hey, what\u0026rsquo;s the size of it, I should see only a single instance in question. But under the hood, what the batching tensor is doing is it\u0026rsquo;s translating your operations on this domain, this single example domain into the multiple example domain. And that\u0026rsquo;s why, you know, we do need to have a tensor that stores all of the various batches in question, you just don\u0026rsquo;t get to see it as a user. This distinction between logical and physical dimensions is very helpful, because it helps you sort of keep straight what is going on in the logical universe, namely what you see as a user, and what is going on in the physical universe, aka what operations are actually happening. So give another example, when you want to do a sum, a reduction sum, you can say what dimension you want to do the reduction on, right? So let\u0026rsquo;s imagine once again, you\u0026rsquo;ve got a two dimensional tensor, and you want to do a reduction on the first dimension, so dim equals zero. So if you have a tensor that\u0026rsquo;s, you know, a by b, you just say, okay, sum, open paren dim equals zero, and that\u0026rsquo;ll do the reduction in the first dimension. But what if this tensor gets batched? Well, if this tensor gets batched, then it\u0026rsquo;s not correct to write dim equals zero to reduce the 0th logical dimension, because what that\u0026rsquo;ll do instead is reduce over the batch dimension. And those of you who have seen some of the marketing copy for name tensors may recognize this as like a similar problem that name tensors were trying to solve, right? So name tensors answer to the solution is, okay, don\u0026rsquo;t say that you want to do a reduction over dimension zero, say that you want to do a reduction over the height dimension, let\u0026rsquo;s say. What vmap says instead is, no, no, no, no, you can still use numeric designations, we just won\u0026rsquo;t actually ever, you know, make the batch dimensions visible to you. So you can say, oh, I want to reduce over dimension zero. And if you have a a by b tensor, that\u0026rsquo;ll be a, you have a batch by a by b tensor, that\u0026rsquo;ll still be a, maybe a batch one by batch two by a by b tensor, it\u0026rsquo;ll still be a. And the vmap process will adjust the index, so that from the logical idea of zero to three or four or whatever it needs to be, depending on where you\u0026rsquo;ve inserted the batch dimensions, when you\u0026rsquo;re doing the actual interpretation on the inside. And really, that\u0026rsquo;s all there is to it to the vmap implementation in PyTorch. So we have a vmap dispatch key, you don\u0026rsquo;t know what dispatch keys are, go listen to one of my earlier podcasts about the dispatcher. We have a vmap dispatch key, which interposes in on vmap when you want to do an operation. And when you have one of these batch tensors, which get created when you use the vmap operation, right? So when you vmap over a tensor, on the inside of the vmap, we give you a batched tensor, which will do the batching for you. And when you, when you do, you hit the vmap dispatch key, it does the translation from the logical into the physical thing in question. And then, you know, it redispatches and the physical operations just get handled in the same ordinary way you used to see them handled. Another way I like to think about this problem is that I\u0026rsquo;m doing a sort of functional transformation on my API calls. And this is, this is very much the Jack\u0026rsquo;s interpretation for vmap, which is that I\u0026rsquo;ve got my program, it has all of these calls to add, mull, matmull, whatever. And what the vmap call does is it transforms this into a corresponding vectorized program, vadd, vmull, vmm, assuming those were actually operations, which they typically aren\u0026rsquo;t. But like, if you had a vectorized version of add, um, and a vectorized version of matmull, um, you just translate to those versions, but otherwise your program stays very similar. So, like, like, in a sort of very mathematical sense, you\u0026rsquo;re in this sort of world of single example functions. And there\u0026rsquo;s this extra world of, um, multi-batched functions. And there\u0026rsquo;s a mapping of every function in the sort of single example world into the batched world. And so, as long as you, like, say how to do this translation, then, uh, you can just take your program of single example calls and then project it into this other world. And, um, like, if you were a Haskell-er, you\u0026rsquo;d call this a type of functor. It\u0026rsquo;s not a functor on hask, per se, but it\u0026rsquo;s a functor on, um, valid tensors. If that didn\u0026rsquo;t mean anything to you, don\u0026rsquo;t worry. But, like, the picture that I want you to have in your head, right, is you\u0026rsquo;re taking all these function calls and you\u0026rsquo;re replacing them with vectorized function calls. And you might do this multiple times if you, for example, vmap multiple times. This looks pretty different from the physical implementation, right? Because the physical implementation, um, keeps track of what batch dimensions are on tensors. And, uh, what it does is it actually, you know, it\u0026rsquo;s a little more efficient. It, like, collapses all levels of vmaps into a single batched tensor. But there\u0026rsquo;s another implementation you could have done for vmap, which is you have a single batch tensor, which handles a single batch dimension. And you just repeatedly wrap each time. And so if you, you know, did a vmap of a vmap of a vmap, you would end up with batch tensor containing a batch tensor, containing a batch tensor, which contains an actual tensor. And so in this way, you can think of this sort of like as you\u0026rsquo;ve got this chain of, uh, control where the first call hits the top level batch tensor, which does a transformation and then transforms that operation into a vectorized operation, which then passes into the second batch tensor. And then when you have, well, the second batch tensor is asked, hey, I\u0026rsquo;ve got this vectorized operation, can you vectorize it again for me? And you end up with a vectorized, vectorized operation and so forth and so forth until you bottom out and there\u0026rsquo;s no more batching to be done. By the way, this is what, um, when Jack says that its functional transformations are composable, this is what is meant, which is that when you apply the transformation, um, to the operation, you get back a thing that you can apply the transformation again to. So it\u0026rsquo;s like, it\u0026rsquo;s like, it\u0026rsquo;s an endo functor, in other words, and it\u0026rsquo;s really profitable to, um, realize that even if the implementation involves these like batch tensors and, um, you know, they\u0026rsquo;re doing all this bookkeeping and they\u0026rsquo;re intercepting operator calls, it\u0026rsquo;s really helpful to think about the actual semantics as just morally replacing these operations. So whenever, like, I\u0026rsquo;m in a situation where I\u0026rsquo;m like, I\u0026rsquo;m not sure what vmap is supposed to do in this case, instead of like trying to run a batch tensor, like object in my head, instead, I just think about, oh, well, you know, like, what would I like modify these API calls to look like when I did it this way. And that usually tells me what I wanted the behavior to be. So to give an example of this, um, a classic problem when you\u0026rsquo;re doing v mapping is how to handle random number generation. So let me explain what the problem is. So let\u0026rsquo;s say that you\u0026rsquo;re doing a v map. And at some point during the v map, you make a call out to a random number generator. So you like say torch random, give me a buffer filled of random numbers, and then maybe say add it to one of these batch tensors. And so there\u0026rsquo;s a problem, which is what do, what is the semantics of this for each batch in the batch tensor, do I separately generate random numbers, and then, uh, you know, perturb them all differently. So this is like sampling noise, and then you\u0026rsquo;d want the noise to be different across batch dimensions. Or am I sampling the noise once, and then applying the same noise to every batch in question, sort of shifting everything exactly the same way. And so there is probably something that the naive implementation of your code would do, um, that is to say replicate the random numbers, uh, in each case. But that\u0026rsquo;s not a good way to think about what you actually want the semantics in the situation to be, right? So if we think a little bit further, and we say, okay, well, you know, what kinds of transformations to the API calls do I want to have happen in this situation? Um, we quickly see that the replicate the noise the same way everywhere corresponds to when I don\u0026rsquo;t modify the random number generation call. So I just do a plain old stock random number generation call. I modify the ad into a vectorized ad. And what that is going to do is broadcast the random number generator, which we call wasn\u0026rsquo;t modified at all. So it\u0026rsquo;s going to be made at the logical size, not the physical size. And that broadcasting is what causes the random number generation to be reused for every batch. Whereas the case where I, um, do a new random number generation for every single batch corresponds to transforming the random call into a call that, um, has a batch dimension. And then I don\u0026rsquo;t have to do broadcasting when I add things together later. And so there\u0026rsquo;s two reasons why this is a really useful way of thinking about it. So one is that it gives you a way of thinking about how you might actually implement this. And the way you can implement this is by doing a mode key. So normally the problem is, is that, uh, dispatch in PyTorch is based on the types of tensors. And so Randon has a hard time dispatching to batching, uh, batch tensor VMAP because it doesn\u0026rsquo;t take any tensors as input. So it doesn\u0026rsquo;t know, oh, what the VMAP should be. And we have a way of working around this, which is a so-called mode, um, which is, hey, when you turn on this mode, like AMP, automatic mixed precision, see previous podcast, all operations are affected by this, even if, you know, there\u0026rsquo;s no input, uh, dependence at all. In JAX, this is called omni-staging, if, if you are curious. So if you make, uh, VMAP a mode, then you can interpose in Randon and then like look at what the state of your, um, you know, VMapping is. And then, you know, uh, generate the Randon appropriately. And this is pretty nice because it turns into sort of the common way to fix this ambiguity, which is, if you wanted the random number generation to be generated once per all the batches, make sure you generate it before you actually call the VMAP. So make sure you call it outside of the VMAP. And if you call it inside the VMAP, we\u0026rsquo;re just going to assume that you wanted the random number generation generated anew every time because, well, you\u0026rsquo;re doing it inside the example in question. And that, that maps very nicely to the mode cell implementation. JAX solves this a little differently. They force you to pass an explicit random number generator object to disambiguate these cases, which does disambiguate the cases and is more expressive. But if you\u0026rsquo;re like a very mutable person, um, uh, moving things before and after function calls sort of makes sense as a way to control when effects should happen. It\u0026rsquo;s like flipping a coin, right? Like if you want to flip a coin once or you want to flip a coin many times inside of a loop, well, you would either, you know, flip it once outside the loop or you would move it inside the loop to flip the coin many times. So, you know, the analogy of VMAP as a loop also works here, even though there\u0026rsquo;s side effects involved. So what are some things that are wrong with the current implementation of VMAP in PyTorch? So there is one big problem, which is that, um, it is not fully composable. So VMAP is set up in a way that it is composable with itself. So we can VMAP as many times as you want and BatchTensor knows how to handle this. And it composes with Autograd in one specific way. Namely, if you wanted to VMAP your code and then run Autograd on it, that\u0026rsquo;s okay. And that\u0026rsquo;s supported by PyTorch. Um, and this is because dispatch keys have a fixed order, so you can\u0026rsquo;t reorder them. Now, the problem is sometimes you want to run Autograd first and then VMAP over the Autograd. And this is very useful for doing this thing called per sample gradients, which I\u0026rsquo;m not going to explain in this podcast, but you can look it up if you\u0026rsquo;re interested in it. But composing them in this different way. And no, it\u0026rsquo;s not the same thing. These, these operations are not commutative. So, like, whether or not you do VMAP first then grad or grad then VMAP has implications on the performance of your code. So to solve this, Richard Zhou, the original author of VMAP in PyTorch, and Horace have been working on a new version called Functorch, where instead of being forced to have a fixed order that, um, transformations like this are applied in PyTorch, um, bash then VMAP, um, instead you just have a stack of transformations like Jax. Functorch is unabashedly, um, taking a lot of inspiration from Jax and let you compose them in whatever order you like. And that\u0026rsquo;s pretty cool. And, um, you know, Jax has a lot of good ideas there. There is a good thing about our implementation though, right? Which is that because we compress all VMAP layers into a single representation, um, we have to go less loops through, like, the translation. Because we can just do the translation all in one go. It makes our batching rules a little more complicated, but, um, it reduces the sort of fixed overheads in question. And so for PyTorch, we do care about this because we\u0026rsquo;re in an eager mode framework. We don\u0026rsquo;t usually ask people to use a JIT combinator to, like, get rid of all these fixed overheads. So there\u0026rsquo;s still a utility to this, but sometimes you do want, like, wild flexibility and then being able to compose things in whatever order you want, uh, however you like, um, is a useful capability. So I hope I\u0026rsquo;ve explained a little bit about how VMAP is implemented and some of the various ways that I think about VMAP and also other sort of sorts of functional transformations in PyTorch. By the way, there\u0026rsquo;s an old podcast about functionalization. You can also think of that as a functional transform in the same sense as VMAP. That\u0026rsquo;s everything I wanted to say for today. Talk to you next time.\nEP35 Random-number-generators Random-number-generators Hello, everyone, and welcome to the PyTorch Dev Podcast. Today, I want to talk about random number generation in PyTorch. Random numbers are a very important component of deep learning. You use them when you initialize your weights. You use them when you use layers like dropout, which will randomly zero out connections. And in general, the concept of stochastic gradient descent is predicated on this idea that you\u0026rsquo;re going to sort of randomly process batches in your input data set, and, you know, this randomization, well, how do you do it? You use the random number generators in PyTorch. So there\u0026rsquo;s some basic facts about random number generators in any sort of numeric library that PyTorch chooses to, and the most important concept is that although you normally in idiomatic usage just say torch.randan, and then you just get a vector full of normally distributed random numbers, which actually is happening under the hood, is that there is a random number generator, an explicit generator object, and you\u0026rsquo;re just using the implicit sort of global generator in a situation. But really, you can create these objects explicitly and use them to sort of separate the random number generation in question. And so when you want to know a bit more about how the random number generator in PyTorch is implemented, you want to look at these generator objects, which are implemented differently depending on if you\u0026rsquo;re generating numbers on CPU or CUDA. And these contain the important state and the important functions for interacting with the state for the various algorithms that you\u0026rsquo;re going to use to generate random numbers. The most important devices on PyTorch are CPU and CUDA, and we use different algorithms for them. Sorry, so that means that if you train your model or test your model on CPU and then move it to CUDA, you\u0026rsquo;re going to get different random numbers. We\u0026rsquo;ve talked idly about maybe implementing CUDA\u0026rsquo;s algorithm on CPU, but no one\u0026rsquo;s done it so far. So on CPU, we just use a good old-fashioned Merseni Twister RNG. That\u0026rsquo;s a pretty high-quality pseudo random number generator. It isn\u0026rsquo;t cryptographically secure, but it\u0026rsquo;s fast to run. A lot of people use it, and it has pretty good statistical properties. On CUDA, we use a different RNG called Philox. So Philox is used in CUDA because it has a really interesting property. Its internal state can be entirely represented as a seed and then offset into the random number stream that was generated by that seed. Why is this an interesting property? Well, a Merseni Twister traditionally involves some sort of random number generator state, and then every time you sample random numbers out of it, this state changes so that you move some of the random bits around, and then you do the same thing over and over again. And so the state is bigger than the seed, which typically is just a 64-bit integer, which means that it\u0026rsquo;s easier to have a higher periodicity. That is to say when the random number generator starts looping over itself in that situation. So Philox doesn\u0026rsquo;t need to have some state that you\u0026rsquo;re going to put random numbers in. Instead, it will just calculate the state right off the bat when you start your CUDA kernel based off the seed and the offset. And this is important because it means that we don\u0026rsquo;t have to persistently keep a CUDA tensor around representing the rnj state of a Philox tensor. Instead, because the seed and the offset are totally are very small, they\u0026rsquo;re just, you know, a single 64-bit integers, we can send them every time we do a CUDA launch directly using the, you know, scratch space that CUDA kernels allow for sending small amounts of data directly to the kernels without having to do a device, a hosted device copy. So what happens when you use a Philox RNG? Well, we first query the generator object representing a CUDA, CUDA RNG, we get out the seed and the offset, the office tells us how far along we\u0026rsquo;ve gone in the random number state, we send these via our CUDA kernel launch to CUDA, you use CURAND init to initialize a local scratch space. So okay, I lied, there is a scratch space, but you just reinitialize it from scratch. And this is okay, because what\u0026rsquo;s going to happen right after that is CUDA is going to like use it over and over again, because you\u0026rsquo;re going to do something like fill a entire random number, entire buffer full of random numbers so you can amortize the cost of this state initialization. And then back on the host side, the host is supposed to statically know how many random numbers your algorithm is going to use. And this is usually not too hard to figure out. Like for example, if you\u0026rsquo;re, you know, filling a random vector full of random numbers, the amount of random numbers you\u0026rsquo;re going to use is exactly the length of that vector times, you know, however many random numbers it takes to generate a single element. So you increment the offset by however many random numbers you would have used. And so the next time you launch a kernel, you\u0026rsquo;ll start at the next part of the random number generation stream. And you don\u0026rsquo;t have to worry about, you know, reusing old numbers in the old case. Speaker 1: There\u0026rsquo;s also some fancy stuff for handling CUDA graphs. This is a bit of a digression, but I just want to put it out there, which is that CUDA graphs, which are a way of recording a bunch of CUDA kernel launches, and then launching them directly without having to pay for kernel launch costs or any of the, you know, sort of code that PyTorch has to run to actually get to the CUDA kernel launch. Speaker 1: Those hard code the parameters that you launch kernels with. And so what that means is that the seed and the offset are traditionally hard coded into the kernel launches. And so if you want to then rerun these kernels later via CUDA graph, you would replay exactly the same random number generators. So there\u0026rsquo;s a little trick that we do, which is we, when you\u0026rsquo;re doing CUDA graphs, there\u0026rsquo;s an extra bit of CUDA memory that we do to add an extra offset that you can use to basically program your, you know, CUDA graph fixed seed and offset otherwise to go to some other offset, because you want to run your code again, but with different random number generators the next time. Okay, digression over. At some point, I\u0026rsquo;m going to do a podcast about CUDA graph support in PyTorch, but this is not that podcast. Speaker 1: So I\u0026rsquo;m, so we have generators, we have a CPU generator, we have a CUDA generator, these generators use the, you know, impl idiom that tensor and storage also use. And you may notice that CPU state and CUDA state are pretty different. So in fact, there\u0026rsquo;s two different generator classes, and you know, they, they inherit from a common interface, but this interface doesn\u0026rsquo;t actually have a virtual method for getting random numbers. And if you think about it, this makes sense because, well, you know, like what good is a virtual method that like directs you between CPU or CUDA when like on CUDA, you can\u0026rsquo;t even call virtual methods. Like that\u0026rsquo;s just not a thing you want to do in CUDA. So like, although like standard object oriented design would say, oh yeah, you know, you want some method that can get you a different random number, depending on what generator you\u0026rsquo;re using. In reality, what you need to do is you need to refine the type, you need to figure out which kind of generator you have at the very beginning of your kernel. So you cast a generator into a CPU generator, recruit a generator, and then just directly access the fields based on what you need. And so that\u0026rsquo;s how most of our kernels are written, right? So you hit the kernel, you have this type erase generator, you figure out what generator it is, now you have a, you know, more specific CPU generator, and then you use the fields directly. One random side note, our random number generators do have locks on them. And we never really agreed whether or not PyTorch\u0026rsquo;s generators are thread safe or not. Historically, we did protect them with a mutex. This is like back to the TH days. So they\u0026rsquo;ve kept the mutex as time has gone along. One common anti pattern, which you should be careful about, is the mutex is just protecting the RNG state. So if you\u0026rsquo;re like doing something like Philox, you don\u0026rsquo;t actually need to hold on to the RNG lock for the entirety of your CUDA kernel launch. You just need to take out the lock and then update the offset and then you don\u0026rsquo;t need the lock anymore. So, you know, try not to like lock the entire things, right? The lock is just for accessing the internal state. But at some point, we should probably figure out how to get rid of the locks, because they\u0026rsquo;re not really adding much. You probably should deal with locking concurrent access to a generator yourself if you\u0026rsquo;re sharing a generator across multiple threads. In Python, this is hard to do because, you know, there\u0026rsquo;s a global interpreter lock, so you\u0026rsquo;re usually not running on multiple threads anyway. And that\u0026rsquo;s most of the important stuff about the generator state in PyTorch. Right. There\u0026rsquo;s these generator classes. They contain the state necessary for generating random numbers. And then various kernels use that state to actually, you know, run the algorithms and output, you know, random floats or random doubles or whatever it is that you need to do. There\u0026rsquo;s some interesting stuff also on the front end, which is how to generate random numbers given a, you know, like sort of uniform a set of random bits. Right. Like, for example, if you want to generate a random double, you can\u0026rsquo;t just take a, you know, a random integer and then cast it into a floating point bit pattern directly, because that would just be totally non-uniform. Right. Because like most of doubles bits space is taken up encoding NANDs. So you\u0026rsquo;d get NANDs most of the time. So there\u0026rsquo;s like a bunch of algorithms for doing this sort of thing. And I\u0026rsquo;m not really going to really tell you about all of them. You can like read through the the source code and like check them out there yourself. They\u0026rsquo;re, they\u0026rsquo;re actually pretty short and they have cool names. And like, you can read the Wikipedia articles about how these things go. There is one thing that is kind of interesting that I do want to point out. And that\u0026rsquo;s when we want to generate normally distributed values. So like your good old fashioned torch dot rand n, um, we use this thing called the box muller transform. And the way the box muller transform works is that, um, you, uh, sample two uniform doubles between zero and one, and then you sort of look at what the, um, the sort of, uh, angle and the, um, length of the vector pointed by these things are, and you can use that to get out the, um, you can use this to get out the, uh, normal, normally distributed samples. But the thing is that to do one of these box muller samples, you have to first sample two doubles and you get out two new doubles. And that\u0026rsquo;s a little awkward if you, you know, only wanted one normally distributed double. So the way that this, um, works is actually most of our RNGs have an extra little bit of state, which is a cached normally distributed value. And so, um, if you be, because it\u0026rsquo;s like, okay, well, I got these two random numbers, but I only needed one of them. The next time I ask for it, I\u0026rsquo;ll give you that instead of having to like sample two doubles to produce only one, that would be bad. And, you know, you want to reduce the amount of RNG you chew through in this case. That\u0026rsquo;s like, that\u0026rsquo;s why there\u0026rsquo;s, you know, these, um, next normal, uh, fields on the generator state. It\u0026rsquo;s for dealing with normal numbers and normally distributed numbers and, you know, normal distribution is really important. So like, it\u0026rsquo;s worth special casing, this kind of situation. Another thing that is kind of interesting about, um, you know, like transforming these random numbers is that, um, the boundary conditions can be pretty nutty. Like, you know, people actually care, uh, when you\u0026rsquo;re sampling a floating point number, if you\u0026rsquo;re zero inclusive or exclusive, and if you\u0026rsquo;re one inclusive or exclusive, and this is like, because like dividing by zero is pretty bad. And like, yes, maybe this only happens, you know, uh, one in every two to the 32 times, but like, yeah, that\u0026rsquo;s bad. And we\u0026rsquo;ve had a bunch of like very nasty bugs where like, if you like run the thing, like 40 million iterations, like once upon a time, it gives you an impossible value. And you know, over time we\u0026rsquo;ve fixed a bunch of these. So that\u0026rsquo;s another thing that like, you have to be careful about when you\u0026rsquo;re working on random numbers. Okay. So that\u0026rsquo;s most of everything I wanted to say about random numbers. There\u0026rsquo;s one last thing I wanted to say, which is sometimes you want to build, uh, what if you want to like take your own RNG and then sort of re-implement all of the functionality in PyTorch on top of it? Like, you know, basically plug in your new, uh, like cryptographically secure RNG instead of mercenny twister and then like get out normal numbers and, you know, exponential distributions and all that stuff. Well, this is something that Pavel Belovich needed to do for, um, CSPNRG, which was specifically for cryptographically secure random numbers for, um, some of the crypto projects that are going on on top of ATEN. And, um, they, so this is kind of tricky, right? Because as I said earlier, there\u0026rsquo;s no virtual interface for getting numbers. If there was a virtual interface for getting numbers and the performance was acceptable, you could just, you know, virtualize the generator object and then swap out your own generator object whenever you wanted like a CSPRNG, or just, you know, want to do something besides mercenny twister, but we can\u0026rsquo;t do that because that\u0026rsquo;s too slow. We need direct access to the generator state when we\u0026rsquo;re doing one of these vectorized things, because we\u0026rsquo;re doing it in a fast loop and, you know, we need everything to inline in that situation. So what\u0026rsquo;s actually happened is all of our transforms, our random number transforms, our templates. And, um, so once you define your custom, uh, RNG, you, um, instantiate all the templates for your RNG. And then that ensures that everything gets in line and you get a fast implementation in the situation. And so all you need to do is just make sure your generator has a, um, you know, uh, distinct dispatch key. And so we\u0026rsquo;ll make sure you will dispatch to your particular, um, you know, random number algorithms instead of anything else. That\u0026rsquo;s pretty nifty use case of the dispatcher. Um, Sebastian and I used to argue a lot about whether or not generators should have dispatch keys or not, but like, this is pretty nice. So I like it personally, at least. Okay. That\u0026rsquo;s everything I wanted to tell you about RNGs. Talk to you next time.\nEP36 TensorAccessor TensorAccessor Hello everyone and welcome to the PyTouchDev podcast. Today I want to talk about tensor accessor, a way of accessing elements for tensors when the dimensionality and dtype are known. In previous podcasts I\u0026rsquo;ve talked a little bit about the API design principles behind our C++ API and one of the characteristics of tensor in C++ is that it is completely type erased. You get to know you have a tensor but you don\u0026rsquo;t know what its dtype is and you don\u0026rsquo;t know what its dimensionality is. Doing things this way makes polymorphism easy because you don\u0026rsquo;t have to write templated code but this type erasure has costs, namely performance costs and it\u0026rsquo;s for this reason that like other C++ libraries that do tensor computations often do in fact encode this information directly in. So for example Igen, a very well-known and popular library known for its fast implementations of kernels, uses fixed dimensions inside the tensor itself. So what\u0026rsquo;s the problem? So the problem is when you don\u0026rsquo;t know what the dimensionality of a tensor is and what you don\u0026rsquo;t know what the dtype of a tensor is, in order to do operations on this tensor safely, you have to do dynamic checks. So if you want to, you know, retrieve an actual element like an honest goodness single element from the tensor in question, you are going to have to say you\u0026rsquo;re going to want to fetch it into some dtype like float or double. And technically speaking, unless you provide an unsafe API, you need to test that the dtype of the tensor actually matches what you want to read the element out of. Otherwise, you can read out a complete garbage silently in this situation. And so if you think about like the data pointer API in tensor that actually does in fact do a dtype check whenever you do this. Similarly, when you want to index into a tensor, well, if you don\u0026rsquo;t know what its dimensionality is, then you have to actually write code that knows how to loop over all the indices you want to do and multiply with the strides in question. And so, you know, because you don\u0026rsquo;t know how many dimensions there may be, right? So you can\u0026rsquo;t write a fixed index calculation in this situation, you have to write a loop that can handle all the sizes in question. And so if you\u0026rsquo;re a tensor iterator, and you know, you\u0026rsquo;re doing a lot of hard work to make sure you can write an algorithm and work with arbitrary dimensionality, that\u0026rsquo;s cool. And tensor iterator is kind of complicated, but it does that all for you. But if you\u0026rsquo;re just writing a good old fashioned kernel, you probably don\u0026rsquo;t actually need this generality, you probably only are writing kernel that only works for some set of dimensions, etc. So if you want to do lots of low level manipulations to data in your tensor, and you don\u0026rsquo;t want to go through all the overhead that tensor would be and yes, you could write a loop over a tensor and then say directly x open square brackets, index close square brackets equals blah. But trust me, you really don\u0026rsquo;t want to write your kernel that way. It\u0026rsquo;s really, really slow, because each of these indexing operations is actually going to give you another tensor back, even if it\u0026rsquo;s actually a scalar, it\u0026rsquo;s a single number, you\u0026rsquo;re going to do an entire dynamic allocation. And that\u0026rsquo;s the case. So if you want to do this sort of thing fast, what do you do? And so the sort of like very easy way to handle stuff in a situation is to get out a raw pointer and do the manipulations on it. It\u0026rsquo;s the obvious thing to do, right? Because you know, what are CRAs? Well, CRAs are, okay, they\u0026rsquo;re not exactly the same thing, because the like, type size is different. But like, a CRA is basically a pointer to some memory. And then you know, you just operate on the memory. So what do you do? So if you have a tensor object, you can call data pointer to get out a raw pointer, that is going to give you a fixed d type. So it\u0026rsquo;s going to check what the d type is. And then you can just poke it, you know, index into it the same old way you\u0026rsquo;d have indexed into any sort of array, and, you know, work with the data in the tensor that way. There are a few implicit assumptions that are going on when you do things this way. So one is that you are probably assuming that the data in question is contiguous. Why are you probably assuming that the data in question is contiguous? It\u0026rsquo;s because handling strides is actually a pain in the ass. And so you probably aren\u0026rsquo;t going to go through all the rigmarole of doing strides exactly correctly, with the pointer in question. If you do it this way, you\u0026rsquo;re probably more likely to just, you know, directly compute some linear index, or you know, you have a one dimensional tensor, and you just can index directly, and you\u0026rsquo;re not going to handle that. So whenever I see kernels that are written directly using raw data pointers, I usually assume that they are assuming contiguous inputs. The only exception is if I\u0026rsquo;m like FFIing out to some external library, where they have to take a data pointer, and then they take them into strides as the other things in question. So raw pointers, very easy, but typically only used for contiguous tensors. But what if you want to do some accesses, and you happen to know that you want to handle strided things directly, you don\u0026rsquo;t want to actually go through the process of taking a possibly non contiguous tensor, you know, allocating memory to contiguify it, and then run your kernel on it. Contiguifying a tensor, by the way, you know, is kind of slow, and it uses up memory. So if you can just directly fuse your computations directly on the input tensor, that can save you quite a bit of computation. And this is what tensor accessor is for. So what is tensor accessor? Tensor accessor is a specialization of tensor, where the d type and the dimension of your tensor are fixed. However, we don\u0026rsquo;t make any claims about the sizes or stripes. So the sizes and stripes continue to be, you know, sort of built into the class in question. And so if you look at what the representation of tensor accessor is, it\u0026rsquo;s very simple, it consists of a data pointer, it consists of a deep, the sizes, and it consists of a pointer to the strides. In fact, tensor accessors are really lightweight, and they don\u0026rsquo;t involve any dynamic allocations, because they\u0026rsquo;re also non owning, unlike regular tensors, which, you know, guarantee that the data pointed to stays live, the lifetime of the tensor in question, they\u0026rsquo;re non owning, so they\u0026rsquo;re really cheap to allocate. And lastly, right, as I said, they have statically known d type and dimension, the statically known dimension is important, because it means that we can implement index calculation without doing any loops, right. So like how it\u0026rsquo;s implemented in PyTorch is, it\u0026rsquo;s actually a recursive template, where, you know, like the tensor accessor for n is computed by doing the tensor accessor for n minus one, and then, you know, adding on the indexing for the last dimension that we\u0026rsquo;re processing. And then there\u0026rsquo;s a base case for tensor accessor, 1d tensor, where you can just linearly index in that situation. By the way, this is a nice thing about being in C++, in the battle days of th, these fast indexing operations were manually specified for every dimensionality. So there\u0026rsquo;s like a 1d fast index, a 2d fast index, 3d fast index, 4d, and so forth. Tensor accessor also optionally supports declaring the pointer as restrict. What that means is a pointer that\u0026rsquo;s restrict is guaranteed not to alias with any other pointers that are in scope. And sometimes that can unlock easier compiler optimizations. We use this very rarely, but it\u0026rsquo;s often useful in CUDA, where non aliasing is a useful guarantee. There\u0026rsquo;s also a variation of tensor accessor called packed tensor accessor. So I said tensor accessor is non owning. So it, you know, contains a pointer to the sizes, which are actually stored in the good old fashioned traditional tensor in question, and a pointer to the strides, which are also stored in the old tensor in question. But sometimes we want to send these like, you know, raw pointers plus metadata to CUDA kernels. And with CUDA kernels, you have to send all this information. If you have this pointer to some random CPU memory, well, of course, your CUDA kernel is not going to be able to access it because CUDA kernels can only access CUDA memory. So you have to pack everything up into the parameter list that and you know, is going to be sent along with the CUDA kernel launch, and packed tensor accessor basically just packs all of the sizes and strides along with the data pointer directly into a, you know, compact representation. Remember, it\u0026rsquo;s fixed dimension, so we can allocate precisely the amount of fields we need to actually do this sort of thing. And then, you know, you can ship them all to CUDA all at once so that CUDA can then use these to compute the indexing. And for CUDA, like computing indexing is pretty cheap because, well, you know, it\u0026rsquo;s CUDA, and you\u0026rsquo;ve got tons and tons of little processors that are doing these computations in parallel. You\u0026rsquo;re more likely to get hosed by, you know, memory bandwidth, because you know, you\u0026rsquo;re accessing stuff all over the place. So let\u0026rsquo;s just step back a moment. So suppose you\u0026rsquo;re writing a kernel in PyTorch, and you need to actually do some manipulation on the data in question. Well, um, there are a few things you can do, right? One is you can like directly use the tensor API. And that\u0026rsquo;s okay, if you\u0026rsquo;re going to just call a bunch of other like sort of accelerated operations, but it\u0026rsquo;s a bad idea if you actually want to do like element by element operations. Then there\u0026rsquo;s raw pointers, which are sort of the easy and obvious way to do things, but they don\u0026rsquo;t, uh, do any of the bookkeeping for strides for you. So usually people only do them when they assume contiguous inputs. So you\u0026rsquo;ll see, you know, um, run contiguous on the input and then get out a raw pointer and do something with it. And finally, tensor accessor knows about sizes, knows about strides, and so can let you do fixed dimensionality indexing on tensors that might have, you know, wacky layout without having to do the, you know, sort of indexing math all by yourself. It\u0026rsquo;s handled for you automatically under the hood. One, uh, current limitation of tensor accessor is that we don\u0026rsquo;t define any operators on them. So once you go from a tensor to a tensor accessor, uh, you can\u0026rsquo;t like a view the tensor and you can\u0026rsquo;t, for example, reshape it. Actually, we had an old version of packed tensor accessory called THC device tensor. That was part of the THC library. And this, uh, tensor did have a bunch of operations on it. And there\u0026rsquo;s no reason you can\u0026rsquo;t implement these operations in particular, anything that\u0026rsquo;s a view really good match for tensor accessor, right? Because tensor accessors are non owning anyway. So you\u0026rsquo;re usually just fiddling around with the size and strides. So this would be a really nice feature to add to PyTorch. No one has really done it yet, but it would be useful. Another thing that I\u0026rsquo;ve been thinking about is, um, sometimes we get to know that a tensor is some dimensionality, um, fairly early in the stage of a sort of multi, uh, operator composite function. And it would be nice to not have to keep, you know, doing the dimensionally check, uh, locally at the kernel site whenever you need to use it. Like it would be nice to like do it once and for all at the beginning of a composite kernel and then pass on this information statically to the kernel you\u0026rsquo;re going to call later on. Of course there, this is rife with difficulties, right? Like if you want to be polymorphic over the D type in this way, your kernels have to be templated, but it\u0026rsquo;s a kind of interesting problem about, you know, like how can you write code that doesn\u0026rsquo;t need to be template instantiated, but can still propagate, um, type information like this. And so maybe, you know, having some sort of, uh, like fixed dimensionality, but the D type isn\u0026rsquo;t fixed tensor type might let you do that, but I don\u0026rsquo;t know. That\u0026rsquo;s something that I\u0026rsquo;ve been thinking about. That\u0026rsquo;s everything I wanted to talk about today about tensor accessor. Talk to you next time.\nEP37 Anatomy-of-a-domain-library Anatomy-of-a-domain-library Hello everyone and welcome to the PyTorch Dev Podcast. Today I want to talk about the anatomy of the domain libraries that we also work on here at PyTorch, namely TorchVision, TorchAudio, and TorchText. I\u0026rsquo;m not going to talk about the libraries in particular, any one library in particular, but TorchVision is definitely the most well-developed and most featureful domain libraries. So many of the things that I\u0026rsquo;m going to say are based off of things that I know about TorchVision. All right, so here\u0026rsquo;s a question. Why do we have domain libraries? Like why isn\u0026rsquo;t PyTorch just one giant repository that contains, you know, how to do distributed computing and how to do operators and how to do profiling and tons and tons of stuff? And then, you know, what\u0026rsquo;s, you know, throwing in a little bit of, you know, image processing operators or, you know, text processing models? Like, you know, multi-head attention is in PyTorch Core. Why can\u0026rsquo;t everything else be? And so there\u0026rsquo;s a few reasons why the domain libraries exist as separate libraries from PyTorch Core. So one is that in particular domains like image and text and audio, they\u0026rsquo;re often very domain specific gadgets. These gadgets don\u0026rsquo;t really make sense in any other context. Like, for example, in Vision, you need to have a JPEG decoder because you are commonly working with images that are in JPEG and you need them to be actually pixels so that you can start doing, you know, deep learning on them. And it would be pretty strange for PyTorch to come with a JPEG decoder and a, you know, wave decoder and, you know, every, you know, file format known under the sun. So the domain libraries exist because there\u0026rsquo;s a lot of extra stuff that you need to actually do work in one of these domains. But we just don\u0026rsquo;t want to keep shoveling everything into the main PyTorch library because that makes your wheels bigger. And it just, it can get pretty out of control, especially because there are lots and lots of things you might want to do. So domain libraries give us an easy escape valve where we can say, oh yeah, you know, this stuff is great. We want to support it, but it just doesn\u0026rsquo;t go in the main PyTorch library. It\u0026rsquo;s going to go in one of these extra libraries. And yes, sometimes there are operators that like, you know, might be useful in multiple domains, but usually it\u0026rsquo;s pretty obvious where they should go. The point about JPEG decoding is also another good point because another thing that you often need when you\u0026rsquo;re working in a domain is there are a bunch of other libraries that you might actually need, like FFmpeg or LibAV or Pillow, et cetera, et cetera. And once again, it would be pretty suboptimal if, you know, when you installed PyTorch, it also got all of these dependencies along. So another good metric for, you know, should I make a domain library or, you know, should I not is, you know, are there any dependencies you need? If there are no dependencies, well, PyTorch might be a good place to put it because, you know, PyTorch tries to keep a very slim dependency set, only the bare minimum that you actually need. We actually even got rid of our NumPy dependency in 1.9. This was accidental. We didn\u0026rsquo;t actually mean to do this, but when people realized this is what happened, okay, sorry, we broke a bunch of people\u0026rsquo;s code, but like, it\u0026rsquo;s better for PyTorch to not actually have a required dependency on NumPy. So if we don\u0026rsquo;t even want to depend on NumPy, well, we certainly don\u0026rsquo;t want to depend on FFmpeg, and domain libraries also let us do it this way. Another reason why domain libraries exist is they actually have different contribution models than PyTorch main repository. If you\u0026rsquo;ve ever submitted a pull request to PyTorch PyTorch, you may notice that after, you know, code review and all that regular stuff, someone actually has to go ahead and import that diff into Fabricator. That\u0026rsquo;s Facebook\u0026rsquo;s internal CI system. And then only then, like, there\u0026rsquo;s some, you know, complicated LAN process that, you know, if you\u0026rsquo;re external to Facebook, you\u0026rsquo;re not really privy to, but eventually, maybe a week or two weeks later, your PR gets merged. Oof, that takes a really long time. Hopefully, like, it\u0026rsquo;s not too bad. One of the things that I\u0026rsquo;ve worked on a lot is making it easy for open source people to work on PyTorch. But yeah, that can be quite a bit of a lift. Unlike PyTorch PyTorch, all the domain libraries don\u0026rsquo;t directly sync with Facebook. So we actually have many external contributors who have direct commit access to these repositories, you can land stuff a lot easier. And sort of, there\u0026rsquo;s a sort of calculation we\u0026rsquo;re doing here, which is that why does PyTorch PyTorch, you know, insist on every, you know, commit you land, also immediately going out to Facebook production when you land it? Well, it\u0026rsquo;s because PyTorch has a lot of moving parts, there are a lot of systems that depend on it. And so we can help de-risk this by like continuously deploying our changes, and like just seeing as soon as possible, when things break, because there\u0026rsquo;s a lot of moving parts, there are a lot of interactions. It\u0026rsquo;s better to, you know, learn about them early rather than, weekly release where like, oh my god, there\u0026rsquo;s so much stuff, and we broke everything, and no one has any idea what broke what. But domain libraries, one, have less applicability, right? Like you\u0026rsquo;re not going to use a text library if you\u0026rsquo;re doing a vision processing task, unless you\u0026rsquo;re like, you know, doing labeling or something like that. Deep learning is all sorts of, you know, interesting cross-pollination. But, and furthermore, there\u0026rsquo;s way less code in there, right? Like it\u0026rsquo;s mostly stuff that is specific for the domain question. And so it\u0026rsquo;s not so bad to just periodically sync. In this case, it can be a little bit troublesome, but it\u0026rsquo;s less bad. And so, you know, sort of these repositories live on separate ends of the scale. So yeah, if you want to move fast, and you want to like, be able to like, you know, sort of work on things very rapidly, it\u0026rsquo;s usually a lot easier to do that inside domain library than outside. Okay, so that\u0026rsquo;s a very developer specific viewpoint on domain libraries. And the next question I want to answer is, what does a domain library do, right? Like, so when I talked about what PyTorch is as a project, well, what do we do, we give CUDA accelerated operations that have automatic differentiation, and you know, a bunch of like extra stuff to make it possible to do stuff around it, like, distributed and stuff like that. So domain libraries are really very similar to many of the things that we do in PyTorch core, right? So one of the bread and butter things for a domain library is it implements operators, like ROI align that don\u0026rsquo;t make sense in a general context, but are very useful in the, you know, context of the domain in question. Actually, in the old days, even Torch Vision used to be a pure Python project. So actually, these operator implementations would just become compositions of stuff you found in PyTorch core. But as time went on, you know, there\u0026rsquo;s a need to have accelerated kernels. And so Torch Vision and most of the other domain libraries are proper C++ libraries. And they come with actual optimized operator implementations for various situations. And these are also done with autograd support, because obviously, you want to train your models. And yes, we provide CUDA kernels, because GPU acceleration is a really important thing of what, you know, makes deep learning tick today. So that\u0026rsquo;s very normal. But there\u0026rsquo;s also some operators that you\u0026rsquo;ll find in a library like Torch Vision that are unusual, like not sort of what you\u0026rsquo;d expect to see. So for example, one of the things you need to do a lot in domain is you need to be able to encode and decode the file formats for your domain, like, you know, the JPEG example I gave earlier. And as I said, you know, what a domain library is doing for you is it\u0026rsquo;s getting all the dependencies. So most of our domain libraries don\u0026rsquo;t actually implement the nuts and bolts of encoding and decoding, because there are plenty of good open source libraries for doing this. But what you know, the domain library is going to do is it\u0026rsquo;s going to take care of getting the dependencies for you, either, you know, like, because there\u0026rsquo;s some other conda package that does it for you, or, or maybe it\u0026rsquo;s some library that\u0026rsquo;s very annoying, like socks. And, you know, you like if you had to install it yourself on Windows, that would be really annoying. But fortunately for you, Torch Audio actually just bundles it with the binaries in question. So you just can use Torch Audio directly and get those implementations. And, you know, sometimes we even like create custom objects for representing various concepts in them, there\u0026rsquo;s a API in PyTorch called Torchbind for representing these things. And so you know, it\u0026rsquo;s both the data model, as well as operations for working on them. There are a bunch of other things, though, beyond operators that a domain library does. So for example, domain libraries often come with models, and it\u0026rsquo;s especially important, they come with pre trained weights, pre trained weights are wonderful, right? Because not everyone can be Google and have a bazillion, you know, TPUs to like train your model. Well, yeah. So, you know, pre trained weights lets you you know, if you don\u0026rsquo;t have that many GPUs, you can like use something that someone trained on a big data set, and then like, go and fine tune or like, you know, try to put things together that way. So, you know, envision, there are plenty of vision models, like the good old fashioned ResNet, but then a lot of more modern models. And, you know, Torch Vision, the intention is to actually track, you know, the models as things go on, and just be a one stop shop, like, okay, you\u0026rsquo;re a researcher, you need a, you know, reference implementation, because you want to compare against some baseline, cool, Torch Vision\u0026rsquo;s got you covered. Or, you know, maybe you want to take some model and then tweak it. Well, you can also look in Torch Vision and get the models that way. Similar to models is data sets, right? We talk a lot about in deep learning how, you know, models are the stuff we\u0026rsquo;re training, but a model is only really as good as the data you feed it. And there are a ton of, you know, well known data sets that, you know, are done for various tasks. And Torch Vision makes it easy for you to like, get all those data sets in a, you know, uniform API, and then feed them to data loader, which you know, you can use to kick off the rest of your PyTorch program. And you know, like, they even have reference scripts, right, to like, show you how to do the end to end training, you actually need to establish a baseline, or you want to do some sort of, you know, ablation study or something like that. There\u0026rsquo;s a few other things that like are not as obvious. So one is that, as I said, domain libraries often need various dependencies, and they take care of making sure all these dependencies are available for you. And one of the important things that, you know, makes this possible is we actually do distribute binary packages for domain libraries, right? Like this is, this is probably like one of the hardest things about like running a domain library is when release runs around, and you need to build binaries. And like, building binaries is very complicated, because you need to do it on all the platforms, and you need to get all the dependencies, you make sure they\u0026rsquo;re linked correctly, and stuff like that. And so working inside a domain library, that is one of the things that they do for you. It\u0026rsquo;s also one of the reasons why it\u0026rsquo;s a little, it\u0026rsquo;s a little hard, we\u0026rsquo;ve been stuck at three domain libraries, plus a few experimental ones for a while, because it is a lot of bring up to get all the packaging going. But it\u0026rsquo;s one of the value ads of, you know, working inside one of these domain libraries. And finally, and this used to not be true, but it is increasingly becoming more true, is our domain libraries are compatible with deploying to mobile. At some point, I should do a podcast about, you know, what\u0026rsquo;s going on with mobile and PyTorch. But like, suffice to say that, you know, you can take your PyTorch models and run them on the phone. And we are doing this at Facebook. And domain libraries, right? Well, they contain stuff for doing images and audio. Well, those are very much the types of things you\u0026rsquo;d might to do on your phone. And so actually, you know, Torch Vision is compatible with actually running on the phone, despite being in a separate repository. You know, that\u0026rsquo;s kind of ridiculous. And I don\u0026rsquo;t have time to talk about how that all works. But it\u0026rsquo;s pretty cool. And it\u0026rsquo;s another one of the things that a domain library does for you. So I talked a lot about, you know, why the domain libraries exist and what they do for you. And I want to go back and reexamine this question, which is, well, you know, it sounds great to have the domain libraries being these separate modules that are external from PyTorch PyTorch. What did we give up when we did this? And in particular, the thing we gave up is that these libraries have to be loosely coupled with PyTorch. This should be a familiar conundrum to anyone who has ever had to deal with a system where you were do you had multiple components that had different release cycles, right? Like if you are in the situation, you\u0026rsquo;re not in a mono repo where everyone is running off of the latest version of everything all the time. Well, you know, you can\u0026rsquo;t just land a change to some base library like PyTorch PyTorch, and then expect to be immediately able to use it in your library, right? The base library has to go ahead and do a release. And then you have to go and update your stuff to actually use it. That being said, PyTorch is not very ABI compatible. So we whenever we do a new version of release of PyTorch, we always do a new releases of all the domain libraries as well. So we do have some level of coupling, right? Like so if you\u0026rsquo;re looking at like TorchVision CI, it actually runs against PyTorch nightlies, right? And because the APIs that the domain libraries use don\u0026rsquo;t change that much, most of the time this is working. And actually, the PyTorch main CI itself also actually tests against TorchVision. So and one of the CI jobs, we will go and we will build TorchVision from scratch. Remember, TorchVision is not that big of a library. It only does stuff for vision. It\u0026rsquo;s not like, you know, a gargantuan library like PyTorch is. So it doesn\u0026rsquo;t take that long to compile. And then we can quickly test and make sure that stuff works. But there are some APIs in PyTorch, which sort of move a lot that we change them a lot like tensor iterator. And, you know, it would actually be kind of useful to be able to use these tools in domain libraries. But then stuff will break all the time. So people just don\u0026rsquo;t do that. They only work on the stable APIs. This would be kind of nice to like make some improvements on. Like maybe sometimes you might want to, you know, write a new binary operation that\u0026rsquo;s very specific for vision. But today, mostly, if you need something like that, you\u0026rsquo;re just going to go land it in PyTorch itself. And so you know, it\u0026rsquo;s just a little hard to coordinate changes across multiple repositories. So people will, people generally have evolved the code to not require this in this way. I\u0026rsquo;m almost done with talking about domain libraries. One last thing I want to say is that, you know, when you\u0026rsquo;re working on domain libraries, the users matter a lot. I\u0026rsquo;m so I\u0026rsquo;m here I\u0026rsquo;m talking to, you know, you developers who, you know, like like writing code and don\u0026rsquo;t know that much about machine learning, right? So when you\u0026rsquo;re working on domain libraries, right? You\u0026rsquo;re very close to the actual research that\u0026rsquo;s going on the domain, right? Because I talked about how like the libraries provide models, they provide data sets. And like, so you need to actually be keeping track of what\u0026rsquo;s going on on the research side. A really good example of this is Francesco Massa, the main maintainer of torture vision. Francesco does a wonderful job taking care of torture vision. And he also does research on the side or maybe half like, you know, it\u0026rsquo;s one side is tortures and the other side is research. There are a lot of really cool papers that Francesco has been a part of and you know, this is like, I think of this as one of the like, big reasons why torture vision is so successful is that we have someone at the helm, who you know, knows a lot about implementing framework stuff, but also knows a lot about the research stuff. Me, I\u0026rsquo;m, you know, always in core, like, you know, C++, you know, core abstractions land. And I actually don\u0026rsquo;t have to train models very often in my job function. But you know, in domains, you gotta be doing that sort of thing. That\u0026rsquo;s everything I wanted to say for today. Talk to you next time.\nEP38 Default-arguments Default-arguments Hello everyone and welcome to the PyTorch Dev Podcast. Today I want to do something a little different. Normally when I do these podcasts I talk about various aspects of PyTorch, but this time I want to talk instead about a general programming languages concept in this case, namely default arguments, which is interesting in its own right and has interesting implications on various design problems that we have in the PyTorch library. So to start, I have to explain what a default argument is. Chances are you know what default arguments are, but I just want to spell it out for a moment. So default arguments are a feature in many programming languages whereby when you are writing a function, you have a bunch of arguments and some of the arguments don\u0026rsquo;t have to be specified. Instead they have defaults, usually specified at the definition site, and those defaults, if you don\u0026rsquo;t specify the argument, the argument takes on the value from that default. So Python supports default arguments. It\u0026rsquo;s in fact the only way to implement overloads in Python without using some sort of like fancy decorator business or anything like that. The point of default arguments in language design is one, they\u0026rsquo;re a very compact way of defining overloads, right? Like so normally if you want to write a function that has that can take three arguments, four arguments, or five arguments, you have to write each of these overloads as separate definitions and implement them differently. But default arguments say, oh, I can define the four overload version in terms of the five just by filling in the last argument with the default and then calling the five in that case. So it lets you write one definition instead of end definitions. Another important function for default arguments is they give you the ability to retroactively add more functionality to an API without breaking backwards compatibility with any clients who were using it before, right? So if you have a bunch of people who are calling the function with two arguments, you want to add a third optional argument, well, if you make it a required argument, all those call sites break. But if it\u0026rsquo;s defaulted, if it\u0026rsquo;s optional, then all those original call sites keep working, and they will just, you know, do what the default functionality is in that case. And we use this feature a ton in PyTorch, because every once in a while, we want to add a few more knobs to, you know, some sort of function or other. And, you know, if we had to create a new function, every time we wanted to do this, we\u0026rsquo;d have tons and tons of functions, and it would be hard to find things. So being able to add extra features onto the existing names, because the names are a limited namespace is very useful for us. And this is inextricably tied to another language feature that Python supports, which is keyword arguments, right? So keyword arguments lets you add new functionality, and also do so in a non positional way. So you like can just say explicitly, what variable name you want to specify for the argument in question. So put it in other words, default arguments are a way of canonicalizing multiple overloads to the maximum arity a function may take. Let\u0026rsquo;s unpack that statement. So what do I mean by canonicalization? Well, you know, to canonicalize means to put something in a form that is the same, no matter how you express it, right? So when I take a two argument function, and then fill out its default arguments, so it\u0026rsquo;s five argument, I\u0026rsquo;m doing a canonicalization process, I\u0026rsquo;m canonicalizing all my function calls so that no matter how many arguments they took, I always see them with five arguments. And arity is the technical term for number of arguments a function takes. So max majority just means that we always canonicalize to the maximum number of arguments. I\u0026rsquo;m emphasizing this because I\u0026rsquo;m going to flip this around later in the podcast. One more thing I want to say is that default arguments imply overloads, but overloads are a more general concept than default arguments, right? So like in C++, you can define overloads manually, anything you could have written using default arguments, you can also do using overloads. And in some languages, there\u0026rsquo;s just no overloading at all. So it\u0026rsquo;s not a question of do they support default arguments or not? No, no, no, there\u0026rsquo;s just no overloading. And common reasons why people don\u0026rsquo;t want to put overloads in their language are it makes type inference more complicated, or you know, it just sort of makes it lets people write code that might be too complicated, you know, it\u0026rsquo;s too overloaded. So Haskell is a good example of a language that doesn\u0026rsquo;t have overloading. But another one is golang. They also don\u0026rsquo;t believe in overloads. So default arguments are pretty handy. We use them a lot in PyTorch. But we\u0026rsquo;ve also had a lot of trouble that has come from, you know, sort of taking the very Python centric approach to default arguments. So I want to explain a few of the problems that we\u0026rsquo;ve run into over the years. So one problem, which I have also mentioned in the serialization podcast, is that we have a forward compatibility problem with TorchScript serialization. Okay, so what\u0026rsquo;s going on here? So when you write a PyTorch model, and you TorchScript it into some sort of like representation for the IR in question, when we serialize it, in old versions of PyTorch, we serialize these with all the default arguments written out, right? Why do all the default arguments show up when you serialize? Well, remember that default arguments are canonicalization mechanism, right? So by the time we\u0026rsquo;ve gotten to the TorchScript IR, there\u0026rsquo;s actually only one representation for any given call. And that\u0026rsquo;s the canonical form, which is the max arity, as I said. And when we serialize, and we look at these function calls, well, they\u0026rsquo;ve already been canonicalized to max form. So the simplest and easiest thing to do is to serialize them back out into the TorchScript model format with all of the arguments, because that\u0026rsquo;s what the input IR had. And this is a forwards compatibility problem. So forwards compatibility refers to when you do something, does it work with previous versions of the software? And so the problem is, if I add a new optional argument to a function, I will start serializing code that has this argument explicitly filled in. But old versions of PyTorch won\u0026rsquo;t have that argument, and they will choke when that new argument shows up. So this is a sense in which like canonicalizing in this way, like reduces the amount of, you know, implementations in the back end that are possible, right? Like previously, if I had a function that could only deal with four arguments, as long as I passed it only four arguments, it would be fine. But once I pass it this fist argument, even if it\u0026rsquo;s the default value, even if it would have behaved exactly the same way as the four argument version, I\u0026rsquo;m stuck because, you know, the back end, the server doesn\u0026rsquo;t actually know that this is the case. So the TorchScript serialization FC problem is one manifestation of troubles with default arguments. But actually, there are other manifestations as well. So let\u0026rsquo;s talk a little bit about XLA and backend extensibility. So backend extensibility says that you can define your own custom device on PyTorch, like XLA or, you know, anything else, and then define implementations for all the operators in PyTorch. And how do you define these implementations? Well, you define the max arity implementation for any given function. So if a function has a bunch of defaults in it, you have to write a function that handles all of the defaults. So what do you think happens when I add a new defaulted argument to a function in PyTorch? Well, the backend extensions all break, right? So like, if you\u0026rsquo;re, if you\u0026rsquo;re remembering that like XLA, you know, things break in XLA, well, that\u0026rsquo;s because usually people are adding new things to the schema. And because our current API, if we\u0026rsquo;re doing backend extension requires you to implement once again, the max arity implementation, whenever this happens, someone has to go to XLA and add support for new argument in question. And that can even be just as simple as like testing if it\u0026rsquo;s the default value. And if it\u0026rsquo;s not raising an error, but they have to intervene because the APIs require you to provide the max thing. So it\u0026rsquo;s, it\u0026rsquo;s strictly BC breaking from the perspective of the server. One last example is let\u0026rsquo;s say that you\u0026rsquo;re in FX. So FX is our transformation framework in PyTorch. And you want to, you know, do a bunch of ad hoc transformations on your model to like get it into some other form. Maybe you want to shard it or, you know, you know, you want to view some things. Very common feature for FX passes is they\u0026rsquo;re very specific. They\u0026rsquo;re very domain specific. So you\u0026rsquo;re not like trying to write a general pass that\u0026rsquo;ll work in all cases. There\u0026rsquo;s probably some particular use case you\u0026rsquo;re looking at, and you\u0026rsquo;re going to ignore most operators and only the few operators you really care about are the ones you\u0026rsquo;re going to do. And so if you\u0026rsquo;re doing one of these FX passes and previously an operator like had two arguments, you might write your FX fast under the assumption that when this operator shows up in your IR, there\u0026rsquo;s going to be two arguments in it. And once again, if I add a new optional argument to it, and so now it gets canonicalized in the IR to have three arguments. Well, oh, no, all your old, you know, code doing this transformation pass is broken because well, it wasn\u0026rsquo;t it didn\u0026rsquo;t know how to it doesn\u0026rsquo;t know how to deal with this third argument, even though this third argument, most of the time, if you use the default values would have been semantically equivalent to the two argument version. So all three of these examples are the same side, they\u0026rsquo;re just, you know, different sides of my three sided dice, which isn\u0026rsquo;t a thing, but right, it\u0026rsquo;s, um, the problem is default arguments are really good for maintaining client compatibility, they\u0026rsquo;re really good for maintaining compatibility with the caller of code, but they\u0026rsquo;re really bad at maintaining compatibility with the so called server, the implementer of the code, right? Because, uh, under the sort of Python model, uh, you have to deal with all the arguments, because immediately what happens once you have called one of these defaulted, uh, functions is you get all the arguments and now you\u0026rsquo;re expected to handle them all. Okay, so how did ToroScript solve the serialization FC problem? I think I claimed in a previous podcast that it wasn\u0026rsquo;t solved. It actually is solved now the the fix landed within the last few months. And they did a very, very cool and useful hack. And this is canonicalization to low arity. So what do I mean by that? So imagine that, you know, I\u0026rsquo;m doing one of these calls. So I might go through Python and Python is going to go ahead and canonicalize to max arity because I don\u0026rsquo;t have a choice. That\u0026rsquo;s how default arguments work in Python. And it\u0026rsquo;s going to went my way through my system. And eventually I\u0026rsquo;m going to get to serialization time. And I\u0026rsquo;m going to be like, Hey, I need to write out some code that represents this argument in question. What should I write out? And so what, uh, canonicalization to lower arity says is, Hey, let\u0026rsquo;s look at the arguments and see if they\u0026rsquo;re actually the defaults. And sometimes, you know, they\u0026rsquo;re going to be dynamically computed. So I\u0026rsquo;m not going to know. So I, I have to actually, you know, pass in a real tensor. A lot of times they\u0026rsquo;re constants. And so I can just compare the constant against what the default is supposed to be. And, Oh, look at that. The last two arguments are actually the defaults. And so in this situation, what I will do is I will chop off those arguments and serialize the lowest arity, um, version of the function that accurately describes the semantics of the column question, right? So basically like, look at the suffix of arguments, all the defaults get dropped and there you go. And so you can see that this solves the FC problem because even if I, you know, call some code and it fills in the default that was new in my new version of PyTorch, and then, you know, wasn\u0026rsquo;t supported by the old version, as long as it\u0026rsquo;s the default value, PyTorch will know to remove that argument, um, in the end. And then I will end up with a, you know, lower arity function that my old extension knows how to do. And we can actually play this trick again for like all the other cases we haven\u0026rsquo;t yet, but like one of the reasons I\u0026rsquo;m recruiting this podcast is, um, a recent realization that we should apply this same technique in those other cases. So like if you\u0026rsquo;re a backend extender and, you know, you have written a function that only knows how to deal with some amount of arguments, ideally we would, you know, chop off the defaults so that your code would still work in that situation. And of course, um, this is kind of hard to do in C++, but we are working on this new Python, backend extensibility mechanism called Torch Dispatch. And there we actually can do this, and it\u0026rsquo;s not too hard to do, and we should do it. So there you have it, right? So default arguments are this way of canonicalizing your function calls to their max arity form, but max arity is bad for servers, right? It\u0026rsquo;s good for clients, it\u0026rsquo;s bad for servers. And so what you want to do instead is if you are transitioning back across the sort of abstraction boundary to the extensibility point on, on the backend, um, a good technique to apply in this case is to re-canonicalize back to lower arity, chopping off the default arguments that are not necessary. There\u0026rsquo;s like a sort of meta lesson that I took from this, right? Which is that, you know, we designed our, uh, API, our JIT schema API off of Python language design because PyTorch from the very beginning was a Python language, um, library. And so we assumed that overloading was possible. We assumed all these things and doing that, you know, gave us a very nice, easy to use API for users. And it was kind of bad for backwards compatibility and forwards compatibility. Right. And, you know, when a lot of people complain about, um, how Golang like doesn\u0026rsquo;t give you any toys and like, it doesn\u0026rsquo;t let you do overloading and, you know, it\u0026rsquo;s really ugly writing code in Golang. Um, but I kind of do think Golang has a point, right? Which is that it\u0026rsquo;s simpler to do backwards compatibility and forwards compatibility if you don\u0026rsquo;t have any of this stuff, right? Because if you don\u0026rsquo;t have default arguments, then like, if you want to add a new version of the function that has another argument, you\u0026rsquo;re just going to make a new function for that. And you just don\u0026rsquo;t run into any of these problems, right? Like the language design of Python, um, puts you into a situation where you have to remember to re-canonicalize to lower arity. But like, if you have just separate functions, you don\u0026rsquo;t have to deal with that. But of course, doing it this way is ugly and verbose. And so at PyTorch land, we want the best of both worlds. So, you know, we need to strike a balance and the hack of, you know, going to lower arity is a pretty good balance in my opinion. One last thing, which is that my PhD thesis was basically on exactly this topic. And I was very happy. I didn\u0026rsquo;t have to worry about overloads because Haskell doesn\u0026rsquo;t have overloads. And like, once again, like very easy. And we, we had to deal with like other stuff like type classes, type classes. Oh my God, such a, such a pain. All right. So that\u0026rsquo;s it for today. Um, I want to explicitly credit, uh, Dmitry Julgakov. Um, uh, we had a chat before this podcast recording and he helped, uh, me solidify some of the things that I want to say here. Um, that\u0026rsquo;s all I wanted to say for today. Talk to you next time.\nEP39 CUDA-graphs CUDA-graphs Hello, everyone, and welcome to the PyTorchDev podcast. Today, I want to talk about CUDA graphs, an NVIDIA mechanism for reducing kernel launch overhead and, you know, sort of putting all your CUDA kernels together into one megakernel that you can run really fast. So why does CUDA graph exist, right? So to understand this question, we have to think a little bit about how the CUDA programming model works. So the way the CUDA programming model works, and see my previous podcast about enough CUDA to be dangerous, the way the CUDA programming model works is we have a bunch of kernels that the CUDA, you know, GPU knows how to run. And you run your host code, regular old CPU code code, and you figure out what kernels you want to run, and you queue them on a stream. And, you know, like whenever the CUDA driver gets these kernel launches, it actually goes ahead and runs them on your GPU. And so if your data is really big, and, you know, like it takes a long time to run various things in the GPU, after a short launch latency, the latency that it takes to get to the first CUDA launch, then you will basically just queue a bunch of kernels to be run on the stream. And, you know, CUDA will just go ahead and try to, you know, run them as fast as possible when the previous work gets done. But sometimes, um, your code is too small, and it runs too fast, or maybe NVIDIA\u0026rsquo;s graphics cards are way too fast. And you\u0026rsquo;ve got a problem, which is you just can\u0026rsquo;t keep up with the GPU, you can\u0026rsquo;t feed it enough to keep it utilized. And, um, you know, when you\u0026rsquo;re in this regime where your tensors are really small, and you have a lot of itty bitty kernel launches, the kernel launch overhead actually can be pretty killer. And so CUDA graphs are a solution for this problem. What a CUDA graph lets you do is it lets you take a whole bunch of kernel launches and bundle them up into one giant mega kernel launch, so you don\u0026rsquo;t have to deal with the kernel launch overhead. And, you know, you can, you\u0026rsquo;ve gotten rid of all that overhead, you\u0026rsquo;ve gotten rid of the overhead of running the host code, so your CPU overhead is also lower, your CPU utilization is also lower. And then you can just go ahead and, uh, you know, run this over and over again. Okay. So that\u0026rsquo;s the concept behind CUDA graphs. But if, um, I told you, Hey, uh, I need you to go implement CUDA graphs for me. Um, you might think about it a bit and then you might realize, actually, this is not so easy to do, right? Like, so normally, um, and like, if you\u0026rsquo;re say ML commute at Apple, uh, you know, this is what you actually did. Normally what you would imagine is, Hey, you know, I want some sort of graph representing the entirety of the computation that I want to do. And then I\u0026rsquo;m going to feed it to some sort of, you know, internal engine, et cetera. And that\u0026rsquo;s going to, you know, go ahead and, you know, compile it into one monocernel that you can go ahead and send to NVIDIA. But there are no such graph representation exists for CUDA, right? Like CUDA was designed from the very beginning as a streaming, uh, API. And so what\u0026rsquo;s actually going on, right? Is like in PyTorch, we\u0026rsquo;ve got loads and loads of Kuna kernels all over the place. They, they don\u0026rsquo;t even necessarily have to be, um, you know, like have a, uh, publicly visible name. They can be in an anonymous name space and they\u0026rsquo;ve got all of these, like, you know, parameters that you\u0026rsquo;re calling them with, right? Like all the tensors that they want to operate on various, you know, parameters that you\u0026rsquo;re passing on the parameter buffer to the kernel, like, you know, whatever scaler you want to multiply things by or anything like that, like how the heck would you actually assemble a graph like this? And so CUDA graphs, like, you know, many other wonderful technologies, such as the JIT Torch Script Tracer requires you to go and run your CUDA kernels first and record a CUDA graph that you actually then can run again in the future. That being said, there is a API in CUDA graphs for explicitly, um, building CUDA graphs and doing modifications to modifications to them after the fact, but that\u0026rsquo;s not the preferred way of generating a CUDA graph. The preferred way of generating a CUDA graph is to actually run your code once, and then you actually get a bunch of CUDA kernel launches. And by the way, like when you do these CUDA kernel launches, um, you know, we\u0026rsquo;re going to record everything about how you launch them, right? So like what tensors you\u0026rsquo;re passing to them, what parameters you\u0026rsquo;re passing to them, all of that, we\u0026rsquo;re going to just record as is. So that means that it\u0026rsquo;s totally hard coded. Like if you use some CUDA memory inside your region of CUDA, uh, calls that memory is going to be the very same memory that a subsequent run of the CUDA graph is going to use. Because remember, um, Nvidia has no idea what the meaning of the parameters you\u0026rsquo;re passing to the CUDA kernels are. Like it\u0026rsquo;s totally flexible. You can, you can pass anything you want. You can pass any structs you want. So CUDA has no way of actually just swapping out pointers if you want it to like, you know, use different memory the next time you run it. So when you\u0026rsquo;re doing CUDA graphs, you have to like, you know, make sure that you allocate your memory in a persistent way so that the next time you want to run your code, you can reuse that memory for that. So the model behind CUDA graphs, right, is that you, you run your CUDA code, um, with a special setting on the memory allocator so that, you know, it gets kept for later. And then, uh, once you get done, you get this CUDA graph and, um, for whatever the input CUDA tensors are, you have to go fill them in with whatever the new inputs you want to run. And that situation is, and then you can say, okay, Nvidia, go run your CUDA graphs and bang, bang, bang. It\u0026rsquo;ll go ahead and run the kernels exactly as they did previously. Oh yeah. And one last thing, because, um, you know, how exactly do, um, CUDA graphs know, uh, what kernels to actually record? Well, actually they\u0026rsquo;re stream based. So remember, um, the stream in CUDA is this queue that keeps track of all the operations and what ordering they need to run in. Right. So if you put things on the same stream, they\u0026rsquo;re guaranteed to run in the order they got put in the stream. Of course, if you have multiple streams, then they can run in any order. And it\u0026rsquo;s a little hard to use streams correctly, uh, because like, it\u0026rsquo;s a very like fine grain form of parallelism and like sometimes physically your GPU just can\u0026rsquo;t do it, but it is a useful API. And so CUDA graphs, um, when you record, you\u0026rsquo;re not recording globally, every CUDA launch, you\u0026rsquo;re actually recording CUDA launches on specific streams. Um, and, um, PyTorch is not that great at being very stream friendly. Like, so, you know, PyTorch by default runs on the default stream. The default stream synchronizes with everything. It\u0026rsquo;s very easy to use. You don\u0026rsquo;t have to worry very much about it, but, um, like, you know, sometimes you want to have streams and then you have to actually write your code differently. And it\u0026rsquo;s easy to get this wrong because if you forget to do it and someone runs your code on the default stream, chances are things are just going to work out. So, you know, M. Currly, who is the NVIDIA guy who has been working a lot on CUDA graph support in PyTorch. Um, he\u0026rsquo;s also had to fix a bunch of stream bugs, especially in our Autograd engine, um, to make everything all work out. So that\u0026rsquo;s basically most of what you needed to know about CUDA graphs, right? So they, um, they are a way of running a bunch of CUDA kernels all together at once and, um, they hard code all the parameters. So that just leads to some, you know, UX problems that you have to be aware of if you want to use them. I want to recap something that I talked about in the random number generators podcast, which was about the Philox random number generator, because this has a very interesting interesting interaction with CUDA graphs. This is kind of bonus material. So like, I\u0026rsquo;ve already said the most important thing about CUDA graphs, but this is, I think this is interesting and I want to talk about it a bit. So I said that, you know, everything gets hard coded and in particular, um, the random number state gets hard coded when you run your CUDA graphs. Okay. Think about it. Right. So what I said in the RNG podcast is that the CUDA RNG state actually lives on CPU. It doesn\u0026rsquo;t live on CUDA. It lives on CPU. And you, um, just, uh, you pass the seed and the offset directly in the kernel parameters. And then, uh, on the CUDA kernel, it actually sets up the Philox state and then does sampling on it. And it\u0026rsquo;s pretty cool. And it\u0026rsquo;s very nice. And it\u0026rsquo;s a complete disaster for CUDA graphs, because what that means is you\u0026rsquo;re actually going to get the same random numbers every single time you run your CUDA graphs. And okay, maybe that\u0026rsquo;s okay, but like, usually that\u0026rsquo;s not okay. And you really do want different random numbers every time. So how the heck do you solve a problem like this? So clearly you need some way of actually feeding in what part of the sequence or the seed or something like that inside CUDA memory, because, well, you know, you\u0026rsquo;re going to totally hard code the, um, you\u0026rsquo;re going to hard code the parameters, right? So it can\u0026rsquo;t be anything passed in the parameters. Well, there\u0026rsquo;s only two ways you can pass information to a CUDA kernel, either by the parameters or by memory on the CUDA device. So if it can\u0026rsquo;t be in the parameters, well, it has to be on the device, but then, uh, how exactly can you get it to the device? Like, do I have to, um, you know, when I launch my kernel, uh, first do a host, a device copy of the RNG state to CUDA memory, and then, uh, run the kernel that way. Uh, that doesn\u0026rsquo;t sound so great. Um, to be fair, it wouldn\u0026rsquo;t be that bad because remember it\u0026rsquo;s all async. And so, um, you can trigger this, uh, well, as long as the host memory is pinned, which is not too hard to arrange, you could just trigger it asynchronously and then like have the transfer happen whenever like CUDA gets around to doing it. But there\u0026rsquo;s a better way to do it. And the better way to do this is to pass in a pointer to a little bit of CUDA memory that doesn\u0026rsquo;t say what the seed or the offset should be, but instead is an offset correction. So what\u0026rsquo;s the idea? So we\u0026rsquo;re going to put on a restriction. The restriction is that, um, if you want to use CUDA graphs with RNGs, uh, RNGs, you have to reuse the same seed because the seed we\u0026rsquo;re sending up with the parameters. So the seed is hard coded. We can\u0026rsquo;t do anything with it. But what you just want to do is right. When I do subsequent calls to the CUDA graph, all I want is to, you know, advance the random number stream, however far, uh, you know, I had advanced, you know, via my previous consumption as well. Right. So there\u0026rsquo;s only this, you know, extra bit of information, just the offset that I want in the situation. So what I can do is, uh, so when I\u0026rsquo;m running normal PyTorch code and there\u0026rsquo;s no CUDA graphs involved, I\u0026rsquo;ll send a little bit inside the parameters field saying, Hey, this is a non-capturing. You can just do use the seed and the offset directly, and you don\u0026rsquo;t have to do anything about it. But let\u0026rsquo;s say that I am in capturing mode. Then I\u0026rsquo;ll do a different bit and I\u0026rsquo;ll send a pointer to the memory. That is the offset that I want to do and say, Hey, Hey, Hey, um, when you compute the RNG state, use the seed, use the offset, but also use this extra offset read out from memory to like do the adjustment. And at the very beginning, the adjustment is zero, right? Because like whatever the seed and the offset were at the time I was recording is the correct one. But then later when I want to rerun the CUDA graph, all I need to do is do a, you know, uh, host to device, um, setting of that little bit of offset to be whatever the current state of the RNG is. And now I can run my CUDA graph and, um, the CUDA graph is going to read out the, um, you know, the offset from this memory and now offset the random numbers exactly how I need them to be. And there\u0026rsquo;s one last thing I need to do this, right? Which is I need to know how many random numbers my CUDA graph consumes, but that\u0026rsquo;s not too hard to figure out. You just record what the RNG state was at the beginning and what the RNG state was at the end. Um, this was not obvious to us at the very beginning. And, um, you know, I\u0026rsquo;m clearly Natalia and I like spent a while thinking about how to actually solve this. Um, but I think this solution is very elegant. Um, and it\u0026rsquo;s just, you know, once again, it comes out of having to solve the problem of, well, CUDA graphs, hard code, everything in the parameters, actually in an old version, apparently someone was actually going into the CUDA graph post facto and editing all of the RNG parameters to update them to the new thing. This was terrible. It was a bad idea and like needed to solve this problem. Okay. So that\u0026rsquo;s the end of the fun technical digression. So CUDA graphs. So like, how can you actually use them in practice? So we\u0026rsquo;re working on landing the last PRs that actually give a nice user API, but there is something, you know, that is very important about CUDA graphs, right? Which is if you want to deploy them, you want to use them in a production setting, you need to be able to run your code, um, you know, initially to actually get the CUDA graph in question. And so this is why like, um, things like torch deploy are actually very important for CUDA graphs, right? Because like, if you want to use CUDA graphs to like do say GPU inference, because that\u0026rsquo;s a situation where overhead matters a lot, you still need to bootstrap the CUDA graph at the very beginning. And then, you know, then you can run it. And, you know, if you, uh, uh, you don\u0026rsquo;t, if you can run Python code in your environment, and that\u0026rsquo;s what torch deploy is all about, then you can just run the slow Python code to get the CUDA graph, but then pass it off to some C++, uh, you know, engine that just repeatedly runs the CUDA graph, uh, in the future. Right. And that, that\u0026rsquo;ll be really good. And, you know, you, you, you use the Python for the slow initialization and then everything else doesn\u0026rsquo;t even need to touch Python at all. And that\u0026rsquo;s like, I think one of the main draws of CUDA graphs. All right. That\u0026rsquo;s everything I wanted to say for today. Talk to you next time.\nEP40 Functional-modules Functional-modules Hello everyone and welcome to the PyTorch Dev Podcast. Today I want to talk about functional modules, a way of taking NN modules and turning them into purely functional stateless versions that you can pass parameters into explicitly. Before I start with this podcast, I had to explain why this is something that we\u0026rsquo;ve been thinking about recently. So one of the projects that is going on in PyTorch is Functorch. Functorch is a sort of replication of Jax\u0026rsquo;s functional transforms but on top of PyTorch. And one of the problems that is challenging for Jax is the way they have set up these functional transforms like grad and vmap require you to explicitly specify what arguments you want to vectorize over or differentiate over. And this makes it challenging to do a NN module style API like what PyTorch has. I have a previous podcast about how NN modules are designed. The short version of it is that, you know, why do NN modules exist? They exist because we want an easy way of keeping track of all the parameters for various modules in question. And so rather than forcing people to like remember what all the parameters are, you can just put them as properties in the module, and then the modules will collect them all together. And then you can pass them to say the optimizer when you want to do the steps. This is really, really convenient. And you know, NN modules are a very enduring part of PyTorch\u0026rsquo;s front end API design. So what causes the problems with the functional API in question? Well, to answer this question, let\u0026rsquo;s look at the sort of very most basic operation that you can do on a PyTorch program, namely compute its gradient with respect to the parameters. Now, if you think about how this is done in PyTorch normally, what you do is, you know, you have your modules, you get your input from your batch, you feed into the modules, out pops out some final loss, and then you do dot backward on it, right? It\u0026rsquo;s a very imperative API, the dot backward triggers the automatic differentiation. And then all of the parameters get a grad field populated. And that\u0026rsquo;s what the optimizer will read out for when you actually want to, you know, do the step update. So there\u0026rsquo;s no need to know anything about the parameters in question ahead of time, or no need to actually, you know, collect up a list of all the parameters. Everything will just get put where you need them to be directly on the object itself. And so when you want to do optimizer updates, all you need to do is iterate over the list of all parameters. And of course, you know, what does NN module do, it lets you easily get a list of all the parameters. Now let\u0026rsquo;s flip this over and think about what it would look like to have a version of grad, which is an actually functional API, because this is what Jax provides, we also have a functional version of grad. And sometimes it\u0026rsquo;s very convenient, because you don\u0026rsquo;t want to actually be mutating your tensors, you just, you know, want to get the sort of mathematical conception of a gradient, right, take a function, and then compute the function that gives the gradient for you. When you\u0026rsquo;re doing sort of higher order business, this is often the easiest way to conceptualize your program. So in this setting, instead, what you have is you have a function, and you say, okay, I want to differentiate the output of this function, with respect to some of the inputs of the function. And now the implicitness of NN modules is a downside, because well, you know, your function normally has takes in a bunch of arguments. And if you have a function that takes in everything as arguments explicitly, you can just say, okay, I want to differentiate the first argument, and the second argument and the third arguments, which would just happen to be the parameters in most cases. But with an NN module, these arguments aren\u0026rsquo;t arguments at all, they are living implicitly inside of your NN module objects. And unless you have a pass that knows how to look into the NN modules and say, hey, actually, there\u0026rsquo;s also live inputs, input arguments, in this module object you pass into me, there\u0026rsquo;s no way that it actually will know about these things. And so it will look to the sort of, you know, function as if these are just tensors that you\u0026rsquo;re accessing, you know, sort of from out of scope, they\u0026rsquo;re like free variables from your function. And, you know, normally, you don\u0026rsquo;t differentiate with respect to free variables, except, like, you know, the whole point of training your model is to, you know, do differentiation with respect to the parameters. So actually, if you use torch.autograd.grad, you can do this, and the correct thing will happen. And there\u0026rsquo;s a trick that the autograd engine does in order to make this all work out, which is that when you do a .grad, you have to specify explicitly what arguments you want to differentiate with respect to. And it doesn\u0026rsquo;t matter if you actually pass them to the function or not, because you don\u0026rsquo;t pass in a function, you just pass in the output. And then the autograd engine knows, like, for every input you passed in, look for, you know, the uses of it in the in the history. And that\u0026rsquo;s how things get implemented. So there\u0026rsquo;s no, there\u0026rsquo;s no higher order function per se. Instead, we\u0026rsquo;re just sort of relying on, you know, very detailed knowledge of the object identity to, like, work out what the function it is that you wanted to differentiate was in that situation. And this trick works okay for grad. And it doesn\u0026rsquo;t work so great for, say, Jacobian. So if you like, try to do this for Jacobian, it just doesn\u0026rsquo;t actually work. You can\u0026rsquo;t compute Jacobians on functions that involve NN modules. There\u0026rsquo;s also other examples of this being a problem. So another example is when you want to ensemble model models. So what is ensembling? So ensembling is the idea that more heads is better than one. So if you had one network that, you know, was computing the answer to your problem, well, it might improve the performance. If you have multiple copies of this network, all with different parameters, and you run them all on the input, and then you sort of decide based on some voting mechanism, which one you like best. And sometimes this actually is helpful. And there\u0026rsquo;s some theorems that talk about, like, you know, idealized situations like this, where they show, yes, in fact, doing an ensemble is strictly an improvement over each of the models individually. So when you want to ensemble like this, you would ideally want to run the computation vectorized if all of the modules in question were exactly the same, right? Because each of them is doing the same thing. And you just really want to vectorize over the parameters. So you\u0026rsquo;d like, you have this parameter, but it\u0026rsquo;s not just a single parameter, it\u0026rsquo;s a stack of parameters, one per each of your modules. And that\u0026rsquo;s what you want to vectorize over. So there\u0026rsquo;s another functional transformation that lets you do this. It\u0026rsquo;s a vmap. But to vmap a function, you have to pass in what arguments you want to vmap over. And once again, if these parameters are actually parameters in your NN module, there\u0026rsquo;s no way to pass them in because your NN module is just directly accessing the parameters on that module. And, you know, your vmap has no way of sort of interposing in on it. Because the way most of these transformations works, the way that like a grad transformation works, and the vmap transformation works, right, is that when you say you want to differentiate with respect to or vectorize with respect to some argument, we take those arguments, and then we wrap them up in some sort of special object, like a batch tensor or gradient tensor that says, hey, we want to do some extra work when you do operations on this. And, well, if those things are completely in the middle of nowhere, on top of a module, there\u0026rsquo;s no way to actually update them. So how do functional modules work fix this problem? Well, a functional module is is a proposal that says, okay, given this NN module, what I want to do is I want to split it. And the way I want to split it is I want to first take out the parameters, right? Because one of the most important things a module does is give you, you know, a way to track all the parameters. And then I want to somehow, and I\u0026rsquo;ll give a example of how you could implement this, somehow have a version of the forward code for each of these modules. But instead of accessing the parameters that were stored on the modules themselves, instead get the parameter values from an extra argument that is passed in explicitly to the modules in question. And so you can see that if you have a way of, you know, taking a regular NN module and turning it into this functional version, that also solves your problem of V mapping or grading over it, because, well, the parameters are now explicit arguments. So you can just, you know, V map over them or grad over them, and you\u0026rsquo;ll get the thing you actually want to do. So how exactly could you do this? So Albin, you know, has this very simple way to do it, right? Which is, if you want to, you know, run a module like this, you need some sort of dummy module, you get in all your parameters, you sort of edit the module to replace the parameter settings with the explicitly passing parameters, and then you just run forward. And you know, if you need this operation to be idempotent, you should reset the state of module to whatever it was before when you\u0026rsquo;re done. So that\u0026rsquo;s a very cheap and cheerful way to implement modules in this way. And of course, you know, it might also be useful given one of these functional modules, and then a list of its parameters, it might be useful to reconstitute it back into an original NN module if you don\u0026rsquo;t need this functional version in this case. So this is a possibility. We\u0026rsquo;re not super keen on it. One of the reasons why it\u0026rsquo;s a little fuzzy to work with is it sort of, it gets rid of this notion that NN modules are objects with a, you know, sort of persistent identity, right? Because, you know, NN modules are built out of, you know, good old-fashioned Python object-oriented programming. And in, you know, object-oriented programming, when you have a object, you know, that object has some distinct identity, and it\u0026rsquo;s not fungible with another object that just happens to have all the same properties, but, you know, is a different identity of object, right? Like, if you mutate one of them, you don\u0026rsquo;t expect the other one to get mutated in this case. But with a functionalization API, you\u0026rsquo;re expecting to be able to, like, take these modules and then, like, decompose them into their parts or recombine them back into, you know, an NN module. And you\u0026rsquo;re expected to sort of not necessarily care that the new NN module you got back is not the same thing as the one you had before. And that is a little bit different from how the existing APIs in PyTorch work. There\u0026rsquo;s also other ways you could go about dealing with this problem, right? So another idea, which is a sort of API idea is, um, imagine that you are writing one of these functions, right? And instead of directly, um, instead of directly calling into the module via some, you know, sort of global variable, instead, you might be required to pass in the module as an argument into the function in question. And so the module, right, has a bunch of code, but it also is a glorified container that contains a bunch of tensors. And so you ought to be able to say, Hey, I want to V map over one of the parameters in the module in question, or I want to grad over one of the parameters. So, so like sort of, instead of just having to like get deal with the tensor or list of tensors module of tensors, also fair gain. Of course, once again, you still have to make sure that the, if you\u0026rsquo;re doing any wrapping or anything like that, you actually make use of the wrapped version of the tensor in, in the internals of your function. And this is why I sort of like the, the trick that we do in PyTorch for grad, which is we say, actually, um, the sort of association of a tensor input as actually being an input is independent of the uses in question, right? There\u0026rsquo;s some independent weak map that keeps track of the things that are going on. That might actually be a better way of implementing, uh, like this extra behavior rather than wrapping objects in this way, because then you can make sure that all uses of the object, no matter if it happens to be stashed somewhere else, will be run with that metadata in question. So it\u0026rsquo;s very different than how Jax and how PyTorch actually implement a lot of the things right now, which is like, you know, create a tensor, which wraps the other tensor in question. One downside to this weak map approach is it puts a lot of, um, uh, stress on how well your language supports weak references, because like if you just used a normal map and, you know, when you, uh, whenever you like did new operations, you kept, uh, adding things into this map, you would obviously leak memory in the situation because you\u0026rsquo;d never deallocate anything. So you need to make sure that, you know, when tensors go out of scope inside your program, they also get removed from the weak map. Maybe some sort of hybrid approach where, you know, inputs are done via the weak map, but, um, intermediate results are done by actual wrapping. Maybe that is an easy way to make sure that the memory management works out okay in this case. As a parting note, I want to mention how the Jax ecosystem does with this problem. So, um, Jax can\u0026rsquo;t do NN modules the same way PyTorch does. And so they have a library called Flax, which, um, you know, gives a module like abstraction and sort of the key idea for their work is they just want to completely avoid, um, the Python object-oriented insanity. So they\u0026rsquo;re just sort of translating, you know, the code you write, which looks kind of object-oriented, but is done via data classes under the hood into usual good old-fashioned pure function calls that Jax knows how to transform in an easy way. And so Flax actually has its own version of VMAP, which directly takes the module as an argument in this situation. Okay. So that\u0026rsquo;s what\u0026rsquo;s going on with functional modules in PyTorch. If you have any thoughts, this is very much something that is in progress. Um, Richard and Horace have been working on it. So if you have any comments, please let us know on the issue that I will post in the podcast notes. That\u0026rsquo;s everything I had to say for today. Talk to you next time.\nEP41 Double-backwards Double-backwards Hello, everyone, and welcome to the PyTorch Dev Podcast. Today, I want to talk about double backwards, the way that PyTorch implements higher order differentiation in PyTorch. What\u0026rsquo;s higher order differentiation? Well, normally, we think of differentiation as just the thing we do in order to figure out, you know, how we want to update our gradients and parameters. And, you know, as machine learning people, we just leave it at that, right? Like, it\u0026rsquo;s just an optimization problem. But, you know, differentiation comes to its roots in calculus, right? Like, it talks about the rate of change of quantities. And if you can talk about the rate of change of a quantity, you can talk about the rate of change of the rate of change of a quantity, and so forth and so forth. So like, you know, in high school calculus, right, you you can have a function that models position, differentiate it, and you get velocity, differentiate it again, you get acceleration, etc, etc. So what are some use cases for higher order differentiation in deep learning? Well, there are many use cases of this, actually, it\u0026rsquo;s actually a very popular feature, although it doesn\u0026rsquo;t show up in like simple models. So one good example of this is this concept called gradient penalty. The idea behind gradient penalty is that sometimes when you are working on your model, you will have a example that causes the gradient to have a really, really, really huge step. And maybe that\u0026rsquo;s bad, right? Maybe you just don\u0026rsquo;t want to do that. Maybe you want to make sure that any given input doesn\u0026rsquo;t influence the state of your parameters too much. And so the bigger the gradient, the worse the solution is. Well, if you\u0026rsquo;re just, you know, doing a good old fashioned single order differentiation on your program, then there\u0026rsquo;s nothing you can do, right? Because you just compute the gradient. And then well, you got your gradient, maybe you can just clip it before you actually apply it. But what you can do, if you have higher order differentiation, is you can actually apply a penalty, you can say, hey, so I want to reduce this loss, but I don\u0026rsquo;t want to reduce this loss, if it will cause the gradient to blow up too much. So I can have a like combined loss that takes into effect both the, you know, loss in question, whatever it is that I want to train on, you know, the accuracy of my network, but also will successively penalize, if you know, the gradient gets bigger and bigger. And I can then, you know, via the magic of automatic differentiation, find the exact quantity that will minimize my, you know, sort of joint loss involving the true loss, as well as the penalty on gradient. And how do I do this? Well, I have to do this with higher order differentiation, right? I have to first differentiate my program to get the gradient. And then I have to use the gradient with my regular loss and differentiate again, to find out how I can minimize this combined loss. Another example of higher order differentiation being useful is in metal learning. So what\u0026rsquo;s the concept behind metal learning? Well, meta learning, as the name suggests, is learning to learn. So it\u0026rsquo;s all about, you know, training a neural network to train a neural network really good. And what does this often look like? Well, you know, normally, when you think of how you differentiate a model, you have a training loop. And what you do is you, you know, you run your model forwards, you run the model backwards, you get the gradient, you apply the gradient to the optimizer, and then you go back to the loop and you go again. And then there are going to be some hyper parameters associated with this training loop. And typically, you just have to find those by like, just trying a bunch of things, you know, like, change the hyper parameter, and then try again. Well, in metal learning, what you\u0026rsquo;ll do is you\u0026rsquo;ll, you know, run this training loop. And then this training loop itself, you will run an optimizer to optimize, you know, some hyper parameter, maybe some aspect of the model architecture. And that in that so the entire training loop is embedded inside a bigger training loop, which is training, you know, the overall, you know, how well the neural network learns in this case. And once again, you know, you have to do a normal gradient computation inside the inner training loop. And then the outer training loop needs to, you know, do a gradient again, on the inner training loop. And one last thing, it\u0026rsquo;s commonly the case that you might need to compute a Hessian, when you are, you know, doing some mathematical applications. That\u0026rsquo;s the square matrix of second order partial derivatives. Second order means you need a higher order differentiation to actually compute this value. So you just can\u0026rsquo;t do it unless you have support for this. Okay, so how does double backwards actually work in pytorch? So this is going to be a long explanation. So I\u0026rsquo;m going to take it in parts. So the first part is I want to first explain how regular 80 works. If chances are, if you\u0026rsquo;ve, you know, seen any like in depth tutorial on pytorch, you already know this, but it\u0026rsquo;s going to set the stage for double backwards. Next, I\u0026rsquo;m just going to introduce how exactly the double backwards user API works, because it says something about the implementation. And then finally, I\u0026rsquo;m going to tell you how double backwards. Okay, so let\u0026rsquo;s get started. So how does regular automatic differentiation works? So what\u0026rsquo;s the model? So the model is you have a bunch of parameters, these parameters are written with requires grad true. And then whenever you do operations that involve these parameters, you record information about what operations were done, you know, in the literature, this is called, you know, writing it to the Wenger list that, you know, records all the operations. So we record operations as we execute them. And then when finally, we call backwards on the loss, we traverse this graph in reverse order, and run the operations in, you know, sort of backwards order, computing the derivatives, propagating it through until we get the gradient in the end. There\u0026rsquo;s a lot of math that explains why it goes backwards, and you know, what exactly the meanings of these operations are, but this is not sort of relevant for just understanding how double backwards works. There\u0026rsquo;s two other details about this process, which are worth noting. So one is that whenever we like process this graph backwards, we actually eagerly deallocate the recorded gradient info, whenever we\u0026rsquo;re done processing it. And why is this the case? Well, because normally, in a normal training loop, you run backwards once, and then you just use the grads that are accumulated into the parameters to actually do the optimizer update. So you don\u0026rsquo;t actually need this, you know, sort of reverse graph anymore, right? Like once you\u0026rsquo;ve used it, you\u0026rsquo;re done with it, and you don\u0026rsquo;t need it anymore. So we can save memory by just deallocating it as we go along. And it\u0026rsquo;s also really useful because if there are reference cycles, well, deallocating the grad info can break those reference cycles. Second, is that there\u0026rsquo;s something very interesting that goes on when we run the backwards, which is that the backwards formulas for various functions may involve uses of the inputs in question, right? Like if you multiply a times b, the gradient is grad a times b plus a times grad b. And if a was a parameter, well, technically, requires grad equals true says that you\u0026rsquo;re supposed to record grad info for this situation. But we don\u0026rsquo;t do that, because like, it\u0026rsquo;s very unlikely that you\u0026rsquo;re actually going to, you know, run backwards again, right, you\u0026rsquo;re going to throw everything away, and then run your pytorch program again, on the next batch in question. So by default, we disable the propagation of grad info, when we\u0026rsquo;re actually executing backwards. Okay, so hopefully, you can see where this is going. So when you want to use double backwards in pytorch, the user API for it requires you to do two things. So one is it says, okay, first, you have to pass this flag called retain graph, what does retain graph do? It says, don\u0026rsquo;t get rid of the grad info as you process the backwards in question. Why, you know, is it important to retain the graph info? Well, it\u0026rsquo;s because you know, when we do a double backwards, we might need it again in that situation. And the second thing they tell you to do is to pass in create graphs equal true when you run backwards. And what does that do? It says, okay, actually, please do report gradient infos as you compute the gradients through the backwards graph in question. And once again, why is that useful? Well, it\u0026rsquo;s because you\u0026rsquo;re going to want to differentiate it through later. And so what double backwards then says is, okay, so you you run backwards with these two arguments. And then at the end of doing the backwards, you get a grad, but this grad actually has a grad info on it, it has recorded all of the history necessary in this case. And you can now use it as part like, for example, gradient penalty, right? So now that you have the grad, you can add it to your loss. And then this entire mondo thing, you can actually just go ahead and do another backwards on it. And this is why we call it double backwards, right? You call backwards once you get some grads, you do some stuff with the grads, and you call backwards again. And that\u0026rsquo;s the double in question. Sometimes I find this process a bit mind bending. And one of the things that like sort of helps me retain my sanity when this happens is I imagine that actually, when I run the backwards the second time, I don\u0026rsquo;t actually care about the first backwards, as in I can reason about the second backwards without making reference to the first backwards. Why is that the case? Well, let\u0026rsquo;s imagine that instead, we were doing a functional transformation on our program. So once again, I\u0026rsquo;m using the sort of Jack\u0026rsquo;s terminology, but it\u0026rsquo;s really useful because it gives a good idea intuitively of how this all works. So in my basic PyTorch program, I write explicitly a bunch of operations that perform the forward pass forward operations one by one by one, right? Like, you know, take my parameters, you know, do some convolutions on them with the inputs, etc, etc, until I get a loss. And this is my program. And then in PyTorch, you just have to write dot backward, and then it gives you the backward. But when we, you know, tell people about how 80 actually works, we say, you can imagine this backward call expands into a second program, like imagine copy, pasting in the second program after your first program that goes ahead and runs all the steps, but backwards and with all the operations replaced with their gradients. And so, you know, this, this composite program involves running a bunch of stuff forwards, running stuff backwards. But if you look really carefully at all the operations in question, they\u0026rsquo;re just good old fashioned, you know, operators on PyTorch tensors. The backwards functions are not anything special. when you take the gradient of a multiply, it just uses multiplies and adds. When you take the gradient of a convolution, you get, you know, convolution backward, but convolution backward is a good old fashioned function. And more importantly, it itself has a gradient. So you can differentiate that as well. And so whenever I have a double backwards that happens in the situation, I just imagine this, you know, big graph, right, that has fours and backwards. And then I just forget that I ever knew that, you know, this was a separate fours and backwards. I just imagine that some poor grad student in the 80s had to like manually derive all the backward steps themselves. So I\u0026rsquo;ve just written it all out. It\u0026rsquo;s this opaque program. I know nothing about it. And then I just apply automatic differentiation to this program again. And well, lucky me, you know, what does 80 know how to do? Well, it knows how to handle any, any program that consists of a bunch of operations that, you know, primitively, I know how to differentiate, that gives me my double backward program. And actually, when I\u0026rsquo;m like reasoning about what graphs look like in double backwards, I like, you know, writing a simple gradient penalty example, you know, writing out the backwards, and then writing out the backwards of that, you know, fours backwards program, and that gives me a graph. And I can use, usually use that to reason about some of the weirder things that happen in this situation. So on the one hand, we\u0026rsquo;re done, right? Like double backwards is just, you know, doing backwards again, you know, what\u0026rsquo;s the big deal. But actually, there\u0026rsquo;s a reason why higher order differentiation is kind of mind bending to implement from a, you know, if you\u0026rsquo;re just purely looking at it from a Wenger tape perspective. So one of the things that is really mind bending is that when you do higher order AD, you actually need to reuse things from the graph of the first 80. That that\u0026rsquo;s why we had to do retain graph, we\u0026rsquo;re not allowed to throw away any of the grad infos for the original program. Because when I\u0026rsquo;m looking at my backwards program, well, you know, one is eventually things go back to the loss. And sorry, not the loss. Eventually, things need to make use of various parameters that may have been defined by the original graph in question. So going back to the multiply example, right? The gradient doesn\u0026rsquo;t only make reference to the gradient of x, and the gradient of y, it also makes reference to x and y, like derivative formula says, hey, you need to know what these quantities are from the original network to actually compute the gradient in the situation. And if those things require grad, then when I use them in the backwards graph, then I need to, you know, go keep going past them, right? Like, like, it\u0026rsquo;s a data dependence. And when I do backwards on my like, like composite program, that sends me back to the original graph. And so that\u0026rsquo;s pretty important. And because it\u0026rsquo;s very interesting, what happens in the situation, which is that when I go and I traverse parts of the backwards graph that were used again, for the backwards in question, I have to flip it again. I\u0026rsquo;m not explaining this very clearly. So I\u0026rsquo;m just going to leave you with a very impressionist picture of what happens. So you have a forwards graph, right? When you differentiate it, the graph sort of turns upside down, because your backwards graph is exactly the same thing as the original graph, but going back in the reverse direction. When you differentiate that graph again, well, you flip it back. And it looks just like your good old fashioned forward graph in question is actually the linear approximation, because, you know, you\u0026rsquo;re not only doing the slipping, that\u0026rsquo;s the part where we\u0026rsquo;re doing reverse mode AD, but you\u0026rsquo;re also taking the linear approximation. So what happens, in other words, what happens in double backwards situation, is you end up having to recompute the original forward graph, but with different parameters for, you know, what the inputs are, because they\u0026rsquo;re coming from different places in your graph. And this is one of the reasons why symbolic automatic differentiation, which might want to be done by systems that aren\u0026rsquo;t tape based, have such a, you know, sort of, like, it\u0026rsquo;s actually really tricky to do it all correctly, because there\u0026rsquo;s all of this stuff going on. And you can\u0026rsquo;t just assume that, like, you know, when you had some program that you compiled for forwards, that\u0026rsquo;s it, that\u0026rsquo;s the only thing you need to compile it for the sort of transformation can reuse things, you know, unpredictably. And one of the really nifty things about PyTorch\u0026rsquo;s design for double backwards is it can support an unlimited number of double backwards operations, as long as you don\u0026rsquo;t ever clear the graph when you do these things. And sometimes in other situations, when you want to, like, optimize, you have a problem, which is that you need to know ahead of time, how many times you\u0026rsquo;re going to differentiate your program, because if you don\u0026rsquo;t know that, then you can\u0026rsquo;t actually, you know, safely get rid of variables, because they might get reused at higher levels in question. All right, so I won\u0026rsquo;t claim that you will completely understand double backwards at this point. But hopefully I\u0026rsquo;ve given you the main idea, right, which is that, you know, you don\u0026rsquo;t have to think about double backwards as this mystical thing. It just is running backwards again on a program that happens to have been generated partially by backwards. But this actually causes some very intricate behavior, if you actually want to dig into it, but at a very high level. And when you look at the implementation, that\u0026rsquo;s all there is to it. All right, everyone, that\u0026rsquo;s everything I wanted to say for today. Talk to you next time.\nEP42 Intro-to-distributed Intro-to-distributed Hi everyone and welcome to the PyTorch Dev Podcast. Today I\u0026rsquo;m doing something a little special, which is that I have Shen Li from the Distributed team over at PyTorch here to come talk to us about PyTorch Distributed. Shen, do you want to introduce yourself? Hello everyone, this is Shen. I work on PyTorch Distributed Package. Super happy to be here. All right, Shen. So I just want to get started. Can you just explain to us what Distributed Training is and why it\u0026rsquo;s so important for PyTorch? Of course. I would say Distributed Training is using multiple GPUs or machines to collaboratively train the same model. By the way, this is just my personal view, not an official performance definition. I can try to elaborate on that statement. Yeah, tell me a little more. Yeah, let\u0026rsquo;s start from the motivation side. Why do we need multiple GPUs or machines to train one model? Well, it is because driven by the advances in deep learning applications, people are using larger and larger data sets to train larger and larger models. It\u0026rsquo;s possible that the data does not fit in one machine, or maybe the model does not fit in one machine. Or even if they both can fit in one machine, you might still want to leverage more resources to finish training within a shorter period of time. So that\u0026rsquo;s why we might want to use multiple GPUs or machines to train a model. Okay, so let\u0026rsquo;s say that, you know, I happen to have a giant cluster of machines in my back pocket, and I want to use them all. How do I go about doing that? So there are, like, a lot of different tools out there. It depends on, like, whether you are a framework developer or an application developer. If you are an application developer, you can choose the right tool in PyTorch. There are DDP, RPC, pipeline, etc. If you are a framework tool, then there are, like, a lot of different things you need to consider to make sure that the distributed training can work efficiently. So when going beyond one GPU and one machine, the communications are, like, inevitable. And communications are usually very slow. And so when you\u0026rsquo;re working on that, if a communication blocks computation, there will be, like, low device utilization and, hence, low efficiency. So that\u0026rsquo;s, like, one challenge you need to handle if you want to work on distributed training. Would you say that dealing with the cost of communicating over nodes is the biggest problem when working on distributed training? I guess that\u0026rsquo;s one of the biggest problems, because the main delay of distributed training comes from two sources. One is, like, computation, and another is communication. And there are actually a lot of tools just trying to handle that. One fortunate thing is that since communication and computation are using different resources, so they can actually overlap. They can basically run concurrently. So that\u0026rsquo;s, like, one benefit we can try to explore to speed up things. So earlier in the podcast, you told me that if you were a user of distributed, you had a bunch of options for what you could do. So what are, like, I think one of the things that people find bewildering about distributed is how many things you can do, like, how many different options for setting things up. Could you just, like, tell us at a high level, like, how these all get put together, how you decide to choose one or the other? Oh, sure. Yeah, there are, like, a lot of different options. For data parallel, there are vanilla data parallel. Hang on a sec. So tell us what data parallel means in this context. Oh, sure. With vanilla data parallel training, each GPU holds the replica of the model and consumes a split of the input data. And models are synchronized using communications. So basically, models are replicated and the data is sharded. And the entire model gradients and parameters are communicated across replicas to make sure that they are synchronized. So this is, like, vanilla data parallelism. And vanilla model parallelism is the opposite, where data will flow through all devices and model is sharded across devices. And the communication is only responsible for transmitting the activations and its corresponding gradients at model sharding boundaries. And, of course, there are, like, more complicated data parallel and model parallel schemes. And there are also, like, hybrid parallel schemes that combines both data and model parallelism. So that\u0026rsquo;s, like, a very high-level description of data and model parallelism. And beyond that, there are, like, advanced versions. Like, in PyTorch DDP, it\u0026rsquo;s a vanilla data parallel plus some optimization. I can try to go a bit deeper into that. So, as I mentioned, like, communication might be one of the main things people need to deal with if you\u0026rsquo;re working on distributed training. And one, like, natural thing to optimize distributed training is try to overlap communication with computation as much as possible. And because communication side communications are the main sources of delay in distributed training, And, overall, the communication delay will also, like, grow with the cluster size. And since they are using different type of resources, it\u0026rsquo;s often possible to run them concurrently. And, actually, existing distributed training technologies like DDP are using such optimizations. What DDP does is that, say, when you are synchronizing the gradients of layer I in the backward path, You can just go ahead and do the computation on layer I minus one to compute the gradients. In that way, the computation and the communication can overlap. So, are you saying that you, like, sort of, there\u0026rsquo;s a train of computation going on? So, at any given time, each of the layers is processing a different set of data in this situation? Not exactly. So, yeah, we can try to open up the backward path a bit and see how the communication got plugged into the backward path. So, in the backward path, we have a layer, we have a model of multiple layers, right? And the backward path is going to flow from the last layer all the way back to the first layer. And it\u0026rsquo;s like a stack of layers. And then the communication\u0026rsquo;s responsibility here is trying to make sure that the gradients on all the model replicas are the same after the backward path. So, how we can do that? Like, one solution is that we just run the local autograd engine and making sure all the gradients are ready on each process. But they\u0026rsquo;re going to be different because they are consuming different data, input data. And then we can basically run, say, an R-reduce to communicate the gradients to make sure that they are the same. But this is going to be slow because you see that there\u0026rsquo;s no overlap at all for computation and communication. Because I\u0026rsquo;m waiting for the gradients to get completely computed before I start the next batch of processing. Is that right? Yeah, exactly. And in this case, basically, the GPU is going to be busy for a while to do the computation. And then when you start the communication, and then the computation resource on the GPU is going to be idling, just waiting for the communication to finish. So, basically, at any individual point of time, there\u0026rsquo;s only one type of resource that is busy. And this is bad. This is what we are trying to avoid. Okay, so how do I fix it? So, in DDP, the solution is that we are organizing the gradients of the model into buckets. So, for example, if you have, say, 20 layers in your model, it\u0026rsquo;s possible that you organize, say, last five layers into one bucket, and then the next five layers into another bucket. And then, when you finish computing the gradients for the last five layers, you can put the gradients of those five layers into the bucket, and then kicking off the communication of that bucket. And at the same time, you can continue to do the computation for the gradients for the next five layers. So, in this case, the computation of the next five layers and the communication of the last five layers will be wrong in parallel. So, to put it in other words, the thing that happens, right, is that although your model has a lot of parameters, we manage to compute some of these parameters before other parameters. And so, if we can go ahead and start updating those parameters that were already done computing before we\u0026rsquo;re done with everything else, we can get ahead of having to wait for everything to be done and then doing the synchronization in that case. Yeah, yeah, exactly. That\u0026rsquo;s a very good summary for data parallelism. And going back to your question, like, what are the options of distributed training in the market? And there are, like, other things like pipeline parallel, sharded data parallel. They actually, many of them are actually exploring the same basic idea of trying to overlap communication with computation, just like what DDP does. But they do that for different things, like for pipeline parallelism. What pipeline parallelism do is that for every mini-batch, you\u0026rsquo;re going to divide one mini-batch into multiple micro-batches. And then the model is basically sharded across multiple devices. And then you\u0026rsquo;re going to fit the first micro-batch into the first model shard. And then when finished the computation on that, you\u0026rsquo;re going to move the activation from the first device to the second device. And then you can fit the second micro-batch into the first model shard, and et cetera. So the pipeline is basically going to run. And in this way, it is able to basically keep multiple devices running in parallel. And also, when you are doing computation, say, micro-batch I, and you can concurrently launch the communication for the activations generated by micro-batch I minus one. So in this way, the pipeline can work and make computation and communication overlap. But it\u0026rsquo;s actually based on a similar idea of trying to overlap things. So to summarize, it seems that, like, first, at a high level, you have to decide what you\u0026rsquo;re going to paralyze over. Are you going to paralyze over the data? Or are you going to paralyze over the parameters? But even once you\u0026rsquo;ve made that choice, there are a bunch of optimizations you can apply for overlapping computation. And all those optimizations result in tons and tons of possibilities for how you can go about doing your distributed training. And I\u0026rsquo;m guessing, like, it\u0026rsquo;s different depending what model you\u0026rsquo;re trying to train as well, right? Right, right. I think that\u0026rsquo;s a correct statement. And one thing I want to add is that, yeah, initially, you need to make a decision. Whether you need data parallelism or model parallelism. And usually, when models are small, data parallelism will be sufficient. And when models are large, you usually want to combine model parallelism with data parallelism. Because the data set is usually very large. And if you just have, say, one model replica in the entire cluster, it is possible to get up to a higher speed. But usually, having a higher data parallel width will also have you to speed up training a lot. All right. So I want to turn our attention now towards the state of distributed in PyTorch. Because I think the discussion that we just had could apply to a distributed framework anywhere in, you know, like TensorFlow or PyTorch or any of the other deep learning frameworks. So what is different about PyTorch distributed? Like, how did PyTorch distributed come to be the way it is today? We started working on PyTorch distributed, I think, since 2019. And the first feature we developed is distributed data parallelism. And at that time, data parallelism is the most dominant distributed training technology. And so far, it\u0026rsquo;s still the most dominant distributed training solution. And later, when with the advances in the community, and people started to deal with larger and larger models, we started to realize that vanilla data parallelism is not sufficient. Because the model cannot, it\u0026rsquo;s possible that model won\u0026rsquo;t be fitted into one GPU and one machine. So we started to think about, oh, we need to add a model parallelism feature into the package. And that\u0026rsquo;s when we started to think about, oh, how do we do a generic model parallel solution in PyTorch? And we come up with the idea of adding, say, RPC remote procedure call. Basically, what we are trying to do here is to make sure that everything that user usually used for, say, forward, backward, and optimizer in local training can be represented using distributed APIs. And that\u0026rsquo;s also what we are focusing on when developing the RPC package. So basically, with the RPC package, what you can do is you can wrap some part of the model into user function and use RPC to launch that function on a remote worker. And RPC will be responsible for things like serializing and deserializing the tensors and also take care of the distributed autograd. And also there is a counterpart for counterpart optimizer for the local optimizer where the distributed optimizer can automatically reach out to all the participating processes and getting their parameters updated. So that\u0026rsquo;s like the second step we take after data parallelism. And after that, we are also starting to build more and more higher layer features on top of RPC in PyTorch because RPC is a very low level raw API, which is flexible, but it\u0026rsquo;s not that easy to use. If you want to use RPC, you will have to do things like decompose your model and write a lot of code to make it work. And ideally, we want to make sure that when you have a model that can train locally, the same model, maybe with a larger size, can train on distributed environment as well. So we need to have higher level APIs to make that happen. And things we added so far are like pipeline parallelism. And we\u0026rsquo;re also working on things like intralayer sharding to make sure that you can not only shard in the model based on the operator boundaries, you can also say shard one operator and play that across multiple processes. One of the themes in my podcast has been that PyTorch, you know, originally was designed as an eager mode framework. And so whenever we build any features, you know, we always try to figure out how it can work on eager mode first and then other modes of operation come later. And, you know, some of the things you described, right, like building the higher level API for distributed, you honestly have a harder job than some of your competitors who, you know, can assume there\u0026rsquo;s a graph representation because you need to work with eager mode PyTorch. Yeah, that is true. And actually, that\u0026rsquo;s the ongoing discussion in the team. We are thinking like which, we are thinking with, we are collaborating with the compiler team and we are thinking about like which layer that we should be able to like extract the graph from the forward pass. And based on that to divide the model shard, to divide the model and do the model placement. So far, we don\u0026rsquo;t yet have a good answer. But things like TorchFX and JET IR can also, can definitely be helpful. But we haven\u0026rsquo;t decided yet whether those should be the solution where we build on top of or we need something else. All right. Well, thank you very much, Shan, for joining me. I\u0026rsquo;m hoping that we can do more of these interview style podcasts in the future. Thanks, everyone, for listening. Talk to you next time. Talk to you next time. Bye.\nEP43 API-design-via-lexical-and-dynamic-scoping API-design-via-lexical-and-dynamic-scoping Hello, everyone, and welcome to the PyTorch Dev Podcast. Today, I want to talk about lexical scoping, dynamic scoping, and how these programming languages concepts relate to library design in PyTorch, specifically with regards to backwards compatibility and other questions. When I talk to people about working on PyTorch, sometimes I get questions from people who knew me before I joined the PyTorch project as a Haskell developer working on compilers. And they\u0026rsquo;d ask me if I was doing any programming languages stuff here in machine learning land. And I\u0026rsquo;d always be very happy to answer people and say yes. In fact, I use programming languages concepts all the time as a developer on the PyTorch project. And today\u0026rsquo;s podcast about lexical and dynamic scoping is an example of how I use these concepts from programming languages to reason about some actually fairly complicated API design questions that, you know, as a Python library, PyTorch has to answer when we want to, you know, talk about how we\u0026rsquo;re going to design an API in question. So to start with, I need to explain what is lexical scoping? What is dynamic scoping? So lexical scoping is, so when we talk about scoping, we\u0026rsquo;re typically talking about how to resolve what the meaning of a variable is. So when I have a function, and I refer to the variable x, you know, how do I know what x is? Lexical scoping says that the value of x is whatever is lexically closest that defines the x in question. And when I say lexically closest, I mean, imagine you\u0026rsquo;re looking at the source code of your program, you see the x, you know, your eye wanders up outside of the enclosing blocks until you find a block that actually defines the x variable in question. And that definition is going to be the one that your actual use of the variable is going to point to. In contrast, dynamic scoping is a form of scoping where the reference to x doesn\u0026rsquo;t actually refer to, you know, whatever is lexically obvious. Instead, there\u0026rsquo;s a concept of an implicit, you know, global variable, if you can think of it that way, which sort of gets changed whenever you do an assignment. So what the value of x will be is not what you saw, you know, in the lexical scoping, but in fact, whatever the caller to, you know, your function, set the variable to be when you, when you before you actually called in the function. So you have to look at the call stack to figure out what the value of a dynamically scoped variable is. And so very concretely, in the Python programming language, there\u0026rsquo;s no native support for dynamic scoping, but a lot of use cases that people use for context managers, you know, that\u0026rsquo;s the with statement where you can with blah, and then inside your inside of this block, something different happens because of the context manager. Context managers are a very easy way to like implement dynamic scoping, because what you do is, when you enter the context manager, you set some global variable to some value, when you exit, you reset it to its original value. And that\u0026rsquo;s basically equivalent to having done a dynamically scoped variable assignment. And of course, you know, regular old variable references in Python are done lexically. If you import modules and use identifiers from those, that\u0026rsquo;s also done lexically. Okay, so up until this point, this is something that, you know, you might have gotten told about in your programming languages class in undergrad. So what the heck does this have to do with PyTorch API design? So the first thing I want to talk about is a sort of case study in what happens when you want to change the semantics of a library, or in this particular example\u0026rsquo;s case, the Python language itself, and why, you know, whether or not you choose to do this with lexical or dynamic scoping, has pretty big implications on how usable the thing is. So here\u0026rsquo;s how the case study goes. So back in Python 2, the Python developers made a bad decision. And the bad decision they made was that they defined the slash operator to mean integer division. This was a very understandable mistake to make because languages like C defined a single slash to be integer division. But what they found was that, like, lots of people were using Python to, like, calculators and stuff like that. And they\u0026rsquo;d always ask things like, what is one divided by two? And Python would helpfully or unhelpfully, from your perspective, say zero. And that was very unexpected. So the Python developers decided, okay, we want to change what the meaning of division is. We want to change it from integer division to true division, so that if you divide one by two, you don\u0026rsquo;t get zero, and secu, you get 0.5. Obviously, this is BC breaking. So how are you going to deal with a problem like this? Well, you want some way, when you have a BC breaking change, to let people opt into the new behavior before it becomes mandatory, and then only at some later point in time, namely Python 3, make it required. So, you know, there\u0026rsquo;s this intermediate time when you can change the meaning of your program to switch from, you know, integer division into true division. So how exactly did Python do this? Well, Python actually needed to introduce a special mechanism called a future import to make this happen. So the way the future import worked was that there\u0026rsquo;s this special module called future, and you could say, from future import division, and then what that would do was it would be changed the meaning of all of the slashes inside your current module to go from division to true division. Now, if you\u0026rsquo;re like me, and you\u0026rsquo;re thinking, you know, why the heck do I have to introduce an entirely new language feature? So future is not a module. It is like a special language feature that changes how the Python bytecode interpreter interprets your program. And why the heck do they have to introduce this new feature? Why couldn\u0026rsquo;t they just have set, well, like something like, okay, instead of importing division from like the normal module, import division from the, you know, like true division module. The same way, you know, if I had a function, you know, if I had a function, and I wanted to change the function semantics, I could have a v one of the module, and a v two of the module, and I could just pick which module I imported that function from to get one version or the other. Well, the reason they needed to do this was because the division operator actually isn\u0026rsquo;t a function. What division in Python, the sugars into is a call into a magic method and whether or not the sugars into a call into the magic method div, or the magic method, true div depends precisely on your version of Python, and whether or not you import future division. So, in effect, the way that the meaning of division was defined was not by lexical scoping, which, in fact, in some languages like Haskell, the meaning of division is lexically scoped. It\u0026rsquo;s provided by this prelude module that, like, is implicitly imported by your program, and that\u0026rsquo;s how you tell what the meaning of division is. That\u0026rsquo;s not the case in Python. Division always desugers into one of these method invocations, and method invocations, well, they\u0026rsquo;re not really lexically scoped or dynamically scoped. Instead, it\u0026rsquo;s a form of dynamic dispatch where you ask the object what the meaning of the operation should be. And so to change the method invocation that happens in this case, you actually need some actual, you know, juice from the language itself. And so that\u0026rsquo;s why the future mechanism exists. So, Python had this problem. The problem they had was that they wanted to change the meaning of a method invocation in a backwards incompatible way, but they had no way of letting people opt into it one by one. So, they introduced a language feature letting you change the meaning of the method from one thing to another. In PyTorch, we often want to make BC-breaking changes to methods, but unfortunately for us, there\u0026rsquo;s no way to implement a same future-style mechanism inside PyTorch. You just can\u0026rsquo;t do it because it requires language support, and Python didn\u0026rsquo;t give us language support to do this. The best approximation for this is to have some sort of global flag, which you can use to toggle between the old behavior and the new behavior in question. But notice, this is very different from what future import division does, right? Future import division only affects the division operators inside your module. If you import some other module that\u0026rsquo;s using old-school integer division, that integer division stays the same way that it used to be. So, it\u0026rsquo;s very local. You can reason about what the meaning of division operators is simply by just looking at the top of your file. With a global flag, you don\u0026rsquo;t actually know what the meaning is without walking up the call stack and looking for someone who actually set the global at some point in time. And so, we actually try very hard not to do this in PyTorch. And the reason why we do that is going to become clear in my second case study. Case study two, device context manager. To explain this case study, I have to first explain what a device context manager is. And this is a little tricky because there\u0026rsquo;s no such thing in PyTorch, but it is a thing that has been requested over and over again by many different users. So, here\u0026rsquo;s what this hypothetical mechanism would do. When you write PyTorch programs, you often want to write your program in such a way that you have both CPU code and CUDA code. So, what does this look like? Well, you know, like you have your script. You want to debug it and test it on CPU. And then at some point, you want to rerun it again on CUDA. And if you know anything about, like, PyTorch\u0026rsquo;s API, we don\u0026rsquo;t exactly make this easy to do. You have to actually plan your program out and, like, explicitly, like, you know, parameterize over the device in question. And then, you know, toggle that with your options. If you just sort of write, like, really plain straight line code, you\u0026rsquo;re probably ending up hard coding that it operates on CPU or CUDA. So, the device context manager is this concept that lets you write the naive code, like, allocate a bunch of tensors with no device argument, do a bunch of operations on them, and then implicitly change the meaning of the factory function. So, that if you, you know, use this context manager and say, hey, set the default device to be CUDA, then whenever you do any inner calls to the factory functions in question, they will actually produce CUDA tensors instead of CPU tensors. So, this is a decent example of dynamic scoping in action, right? Like, when you use one of these context managers, it\u0026rsquo;s not just the, like, local calls to factory functions that are in your module that would be changed from CPU to CUDA. It\u0026rsquo;s also all the inner calls to, like, all the modules you might be instantiating and everything else. And this is kind of desirable, right? Because, like, one of the things that people find very annoying about how things have to be done today is you have to, like, plumb the device you want down recursively into all of the, like, creation functions that you\u0026rsquo;re doing. And in this case, this is, like, all of the submodules in your modules. By the way, we used to not actually let you plumb device down, but Joel Schlosser very recently landed a patch to PyTorch that makes all modules take a device argument so you can change what the device is, you know, at module construction time. Before that, you had to actually always construct your module on CPU and then move it onto the device you wanted. And that\u0026rsquo;s kind of inefficient, and a lot of people didn\u0026rsquo;t like having to do that. So anyway, so this device context manager would let you change, for example, where your modules get allocated without having to actually explicitly pass in this device argument. And so a lot of people would like this. It would make things very convenient, and we don\u0026rsquo;t want to do it. Why don\u0026rsquo;t we want to do it? Well, the reason we don\u0026rsquo;t want to do it is because of the fact that it, you know, actually recursively goes down and all of your calls in the call set change their semantics, right? This is, like, both a blessing and a curse. The blessing of it is that you don\u0026rsquo;t have to coordinate with anyone to change the device. You just set this context manager, and then magically the meanings of all of your factory functions change. The curse of it is you don\u0026rsquo;t have to coordinate with anyone. So if someone writes some code that, like, assumes that TorchEmpty is just going to give you a CPU tensor because when I tested the code on my machine, it gave me a CPU tensor. Like, you know, how difficult could this possibly be? That code is going to unpredictably break. And in practice, this code unpredictably breaks because we have a janky version of device conics managers called set default tensor type, which you can actually use to change the default tensor type from CPU to CUDA. Please don\u0026rsquo;t do this. We really hate this function. We want to get rid of it. But this one, people always post forum posts being like, hey, I did this thing. And, like, my code, some code library code that I\u0026rsquo;m calling doesn\u0026rsquo;t work. So the, like, problem with untyped dynamic scoping is that it is a global tax on all code written in your library. If you have primitive function calls that are modulated by some dynamic scope by a context manager, everyone who writes library code is obligated to make sure that their code works under all possible settings of the context manager. So in this case, whenever I write a bare torch.empty and not bare torch.empty device equals CPU, I\u0026rsquo;m obligated to make sure that this will work even if you do a CUDA device. And maybe this is, like, possible, and maybe this is even the right tradeoff to make. But historically, PyTorch doesn\u0026rsquo;t have this requirement. And so a lot of code is not written under this assumption. And so if you wanted to add a device conics manager and you wanted to do it right, and when I say right, I mean, like, this conics manager actually works in, like, 99% of all the situations you use it in. You actually have to go and painstakingly audit all of your Python code to make sure that it\u0026rsquo;s actually doing the right thing in this case. Blech. So, like, you know, dynamic scoping leads to unpredictable effects because it, like, lets you reach into code that wasn\u0026rsquo;t expecting to be modulated. Sometimes this is a good thing, right? Like, it saves you from having to explicitly pass arguments around. If you\u0026rsquo;re Emacs, you know, actually, like, you love dynamic scoping because it makes it so easy to just set some variables and then use them later inside somewhere else without having to muck about with function signatures. But, like, this implicitness also comes with a cost. Okay, I have one last case study, and this relates to Torch function and also a sort of new mechanism proposed by NumPy for handling factory functions. So, a little bit of backstory here. So, Torch function is this thing where you can write an object, you put a Torch function magic method on it, and then whenever you pass these objects into Torch.cat, Torch.ed, any of the functions in the Torch namespace, we\u0026rsquo;ll actually just call this magic Torch function method so that you can override the meaning of operations involving tensor subclasses. So, this is very useful, and you can use it to implement all sorts of interesting tensor-like objects without having to actually, like, you know, monkey patch all of, you know, PyTorch\u0026rsquo;s functions to, you know, do something different in this case. But there is a problem, and the problem is Torch function is predicated on the idea that any given function operation takes in an actual tensor as an argument. Because the way it, like, does dispatch is in the very Pythonic dynamic dispatch style, we look for an object that has a Torch function on it, and that\u0026rsquo;s the Torch function implementation we call. So, what happens when you have a function that doesn\u0026rsquo;t have any tensor arguments? And an example of that is a factory function, right? Torch.empty, which just takes in a list of sizes and gives you a tensor in question. So, custom classes have a problem, which is they need to also somehow override these factory functions, but they have no way of doing so because their standard mechanism of overriding is via dynamic dispatch. But there is no dynamics dispatch in this situation. So, there are a bunch of ways to solve this problem. As the saying goes, if the mountain won\u0026rsquo;t come to Muhammad, Muhammad must go to the mountain. So, if you, you know, want dynamic dispatch and the factory function doesn\u0026rsquo;t have dynamic dispatch, well, turn it into a call that does have dynamic dispatch. So, we have a bunch of functions on tensors like new empty and new zeros, and, you know, you can use those in place of the good old-fashioned Torch function, Torch factory function in the main namespace. And that will indeed work. And then you just have to define those things in your Torch function to get things going. And this just preserves the same property, right, which is that you are using the objects that are lexically in scope to do the dynamic dispatch to get to the implementation you want. There\u0026rsquo;s an elaboration on this idea, which is a NumPy proposal at this point in time, which instead of directly, like, creating new variants of methods for tensors for all the factories\u0026rsquo; functions instead, wrap them up into a module call. So, given a tensor, you can extract out a module that corresponds to the, you know, type of module that you would have called the factory functions on, but this one is specialized for the subclass in question. So, what does this look like? So, I\u0026rsquo;ve got a tensor, I want to create a new tensor, so on this tensor, I call the module accessor, which gives me a Torch module, something that looks like Torch, so it\u0026rsquo;s got empty, and it\u0026rsquo;s got ones, and it got zeros on it. But this module is special, because if I call zeros on this module, I will actually get a tensor that is of the same subclass as whatever my original tensor that I got this module out from, from the beginning. So, same idea, right? Use the lexically scoped values to get out the module and then do the dynamic inspection on the module itself. So, you just don\u0026rsquo;t have to, like, shove everything into the method namespace. Of course, there\u0026rsquo;s another way to do this, and that\u0026rsquo;s using a context manager. And this is actually more likely than you might think. So, in previous podcasts, I\u0026rsquo;ve talked about Functorch, a method for doing, you know, functional transformations on PyTorch programs. And in Functorch, there\u0026rsquo;s a very natural place where a context manager would be applied, and that\u0026rsquo;s when you use one of the higher order combinators, like vmap, to actually do an operation on a tensor. So, when I enter the vmap, what I\u0026rsquo;m effectively going to do is I\u0026rsquo;m going to basically turn on the vmappiness. And what that also means is that I might very reasonably want to override the behavior of all the factory functions as well implicitly when I do this. And this is actually very natural, and, in fact, in JAX, this concept is called omni-staging, where, in previously, JAX only did data-dependent control flow, but at some point in the future, they realized, hey, actually, it\u0026rsquo;s really useful to be able to, you know, override the behavior of all these free functions, and so, you know, let me just go ahead and do that. And so, that\u0026rsquo;s called omni-staging in JAX. So, which of these is the right thing? Well, if we look back to our previous case study on Device Context Manager, PyTorch said, hey, you know, we want explicitness. We don\u0026rsquo;t want, we\u0026rsquo;ve got all this code that\u0026rsquo;s been written already that doesn\u0026rsquo;t think that you\u0026rsquo;re going to, like, change the meaning of things under your feet. So, like, you know, let\u0026rsquo;s just make sure that you keep doing things explicitly. And so, we don\u0026rsquo;t really want to add this context manager. But then, when we look at this, you know, Torch function module case, you know, there is a solution that you can do to, you know, stay with the lexical attitude, which, honestly, is PyTorch\u0026rsquo;s attitude. But you can also see that there is a lot of merit to doing the dynamic scoping. And these problems of backwards compatibility don\u0026rsquo;t, you know, they\u0026rsquo;re not as pressing because although you might not have written your code so that it works correctly under CPU or CUDA, with VMAP, well, you know, you\u0026rsquo;re explicitly asking for VMAP in this case. So, one is, you\u0026rsquo;re probably going to, like, make sure all the code you\u0026rsquo;re calling is stuff that works correctly in this case. And two is that VMAP actually, you know, is very carefully written so that, like, the code on the inside looks exactly like you\u0026rsquo;re doing a single example case. So, it really is supposed to work, even if you, like, change out the semantics of everything. It\u0026rsquo;s just, you\u0026rsquo;re just, you know, adding these batch dimensions in a way that, like, your code should be indifferent to. So, what\u0026rsquo;s the right answer? Well, I don\u0026rsquo;t really know. When I talk to people and they ask me for device context manager, you know, I used to call over Greg and Greg would, like, no, we\u0026rsquo;re not going to do this because everyone\u0026rsquo;s code is not going to work in this case. Well, maybe. If you\u0026rsquo;re willing to put in the work to make this all work correctly and all the library and all the ecosystem, I think, you know, some dynamic scoping might actually be pretty helpful. But there\u0026rsquo;s a lot of work, and I want to see this work actually, you know, have an honest attempt for this. That\u0026rsquo;s everything I wanted to talk about for today. Talk to you next time.\nEP44 pytorch-probot pytorch-probot Hello, everyone, and welcome to the PyDigitive podcast. Today, I want to talk about PyTorch ProBot, a simple bot based on ProBot that we use at PyTorch to do various operations on GitHub. So what\u0026rsquo;s the point of having a bot on GitHub that will do actions for you automatically? Well, as some members of the Rust community have put very eloquently, a bot is a really good way of codifying otherwise very mechanical, very easy, but like, you know, time consuming tasks that humans would otherwise have to do into an easy to do framework that will do it automatically for you, right? So it\u0026rsquo;s kind of like a lint rule, right? Where when you have a linter in part of your CI, you don\u0026rsquo;t have to then manually say, hey, you know, I think that line length is too long in your pull request, the machine will automatically do that for you. And you know, you can save human bandwidth for things that actually matter. Well, ProBot is exactly this, its goal is to automate things that are otherwise easy to do. And, you know, save our time for doing things like actually reading the issues you send all of us. There\u0026rsquo;s a few pieces of functionality that we currently have implemented in PyTorch ProBot. There\u0026rsquo;s three. The first is what we call CCBot. So CCBot is very simple. When you have an issue on PyTorch, you can add labels to it. Well, CCBot lets you maintain a subscription to any number of labels that you want to get CC\u0026rsquo;d on. And then when someone labels an issue that way, well, CCBot will edit the issue and CC you on it. So that\u0026rsquo;s very useful, because otherwise, there isn\u0026rsquo;t actually a good way to watch a label on GitHub. And you don\u0026rsquo;t actually want to be, you know, waiting through all of the issues on GitHub. And if you are, you know, even if you are pretty good about like looking over the issue list by hand, if you have a lot of labels you want to keep abreast of, well, that\u0026rsquo;s a pretty complicated, you know, search query that you need for that situation. So it\u0026rsquo;s easier to just get them all in your inbox, and you can decide what you want to do with them. I subscribe to a lot of issues this way. Really, it\u0026rsquo;s there\u0026rsquo;s too many issues in PyTorch for any one person to process. So CCBot is a really good way of making sure you get CC\u0026rsquo;d on the stuff you\u0026rsquo;re interested in, even if you\u0026rsquo;re not keeping an eye out for them in the ingestion point. The second piece of functionality that Probot does is a label bot. So what label bot does is if x is labeled with blah, then also label it with blah. And one of the like, use cases we do for this is for high priority. So the way that high priority works in the PyTorch repository is once again, we have a lot of issues, a lot of these issues are very minor and don\u0026rsquo;t really matter that much. And to make sure we don\u0026rsquo;t lose the important issues in the big sea of issues, we have a high priority label. So what you do is, when you think something is something that should actually get fixed, you can label it with high priority. Now, the problem is, people don\u0026rsquo;t necessarily always agree on you know, what high priority is. And we also have a socialization problem, which is like, you say you\u0026rsquo;re new to the PyTorch project, and you know, you want to know whether or not something is high priority or not, how the heck are you actually going to know this, right? Like, you\u0026rsquo;re going to be like, oh, well, I don\u0026rsquo;t really know what it means to be high priority. And then you might be conservative, and you might not not mark an issue as high priority when actually it is high priority. And the problem is no one else is reading the issue, because you were the one who was supposed to triage it. And then we just lose that issue to the sands of time. So the idea behind the label bot is well, whenever someone marks something as high priority, we also add a label triage review. And what that means is that in our weekly triage meeting, we need to go over this issue and discuss why we think it\u0026rsquo;s high priority. And you know, you know, what, what we\u0026rsquo;re going to do about it, actually, not so much what we\u0026rsquo;re going to do about it. But just, you know, why is it high priority? And the function of this, because most of the time when people label things as high priority, they stay high priority, like I\u0026rsquo;d say 90% of issues are like that. But the point of this is that everyone can easily see, hey, here are all the high priority issues that are going on. This is what we collectively as a team think of as high priority. It\u0026rsquo;s a really good way of socializing issues in this way. But we couldn\u0026rsquo;t do this. If you know, when someone labeled something as high priority, we didn\u0026rsquo;t also say, please review it in the triage meeting. And finally, there\u0026rsquo;s a new feature that was developed by Eli Uriagas and Sam Estep for triggering CI jobs when a label is added. So one of the things that we have as a problem is we want to build on a lot of configurations, but actually building all those configurations is pretty expensive. So we don\u0026rsquo;t want to actually build everything initially. And if there is some exotic configuration that you think your PR actually needs testing on what you can add a label to your pull request, and that will trigger extra texts and how those tests triggered, well, that\u0026rsquo;s done by probot once again. So that\u0026rsquo;s really it probots logic is not that complicated. And I just want to talk a little bit about the probot framework, which I decided to use after much humming and hawing, you\u0026rsquo;ll see why in a moment, and also some sort of meta points about how probot was designed. And you know, why I think these design ideas are actually good ones for the framework. So first, why probot? And actually, this was a not an easy choice for me, because probot, the framework is a JavaScript framework. And well, you know, we\u0026rsquo;re PyTorch, we\u0026rsquo;re a Python shop. So I would have ideally liked it if I could have written my bot in Python. But probot won me over by a number of pretty useful features that I, in fact, did appreciate a lot when I was developing this extension. So for one, I can actually, when I\u0026rsquo;m work developing the probot framework, I can actually run my node app locally. So like, you know, I\u0026rsquo;m hacking on my laptop, I got my source code, I made a change, and I can run it. And then you know, I\u0026rsquo;ve got my GitHub app going. And I can actually associate it with a real GitHub repository, and do you know, smoke testing by modifying the GitHub repository, that triggers some hooks, which get bounced to my local instance on my laptop, processed, whatever, you know, API calls I\u0026rsquo;m going to do, and then actually see it show up on GitHub. And the way this is done is they have this like reflector service, which knows how which is, you know, like, if you install one of these dev instances, you register the reflector service as the host name, because typically your MacBook isn\u0026rsquo;t publicly addressable. And then it bounces the request back to your, you know, local instance, which is, you know, subscribe, which which has subscribed to the reflector directly. That\u0026rsquo;s pretty awesome. And it made developing very easy, because normally, when you\u0026rsquo;re developing these hooks, it\u0026rsquo;s very annoying to like generate synthetic events, because, oh, you know, you got to like, go and trigger the hook and then download it from GitHub, and then save it to some fixture, blah, blah, blah. Here, you can just like directly just muck around with the repository and see what actually happens with Probot on the fly. That\u0026rsquo;s pretty nice. Probot also has some opinions about testing, mostly, you know, based on mocking. Mocking isn\u0026rsquo;t my favorite way of doing testing, because it\u0026rsquo;s very manual, you have to like, you know, create the fixtures, create what the outputs are, and you know, get that all going. But it\u0026rsquo;s a very convenient way when you\u0026rsquo;re dealing with an external service like GitHub. And no, you don\u0026rsquo;t actually want to be hitting the actual GitHub endpoint API, if you\u0026rsquo;re actually, you know, running your test suite. Of course, what we even better is if someone wrote a crappy reimplementation of GitHub, with support for the GitHub API and the GitHub hooks and the GitHub notifications, so that I could like just stand up a like local copy of GitHub, and then you know, test against that. Well, so I can always dream, I actually have like a very small implementation of a very small fragment, that is what I need for implementing GH stack. But you know that you can hear about that in my GH stack podcast, which is in the past. And finally, Probot, you know, had existing documentation for how to deploy it on AWS Lambda. And this was very attractive to me, because I was when I was developing Probot, it was kind of one of these things where it\u0026rsquo;s like, okay, I want to build this thing. And then I want to forget about it and not have to worry about it ever again. And if I had to like, stand up a server, and then actually, you know, maintain the server over time, well, I\u0026rsquo;d have to take software upgrades and like, you know, kick the server when it goes down. Oh, don\u0026rsquo;t want to deal with that. But if it\u0026rsquo;s a Amazon Lambda, that\u0026rsquo;s great. And I don\u0026rsquo;t have to worry about it. Well, I mean, I do have to worry about it if Lambda like changes how their API works. But at least I don\u0026rsquo;t have to worry about doing a server that\u0026rsquo;s so vaunted serverless, which you know, a lot of people are like, actually, you know, we\u0026rsquo;ve gone too far, you know, serverless is not so great. But I think in this particular case, serverless was a really good call, because it just reduces the maintenance costs. And that really like gets me to the meta points, right? Like one of the like enduring goals with the with the design of PyTorch ProBot was that I wanted to have as little maintenance as possible on this. And so one one answer for that is to, you know, put this as a serverless deployment so that I don\u0026rsquo;t have to worry about administering the server. Another thing is that ProBot has no state, there\u0026rsquo;s no database, there\u0026rsquo;s no persistent state. The CC bot is an interesting thing, which is like, you know, we need to know what we\u0026rsquo;re going to see who we\u0026rsquo;re going to CC when a label is done. And the way ProBot actually does this is we have this GitHub issue, and the GitHub issues body contains the text of all the subscriptions. And so what ProBot just does is it loads up the GitHub issue on startup time. And then like, that\u0026rsquo;s, that\u0026rsquo;s how the state gets managed. And this is very, very simple. We can install a webhook for listening to issue updates so that when the issue gets updated, we know to redo it. And then when the Lambda instance dies, well, you know, the next time it spins up, we\u0026rsquo;ll just refetch it again from GitHub, no biggie. So we\u0026rsquo;ve like offloaded the state onto GitHub, which you know, is a big company and actually in the business of running a bunch of web servers and databases to maintain GitHub. And now we no longer have to maintain a database ourselves. I can\u0026rsquo;t stress how useful that is. And of course, we give up some stuff to do this, right? Like, for example, you can\u0026rsquo;t actually subscribe to labels on CCBot, unless you actually have right permissions on the PyTorch repository, because otherwise, you can\u0026rsquo;t edit this issue. But like, we hand those out like candy. So like, if you want to like, do one of those things, just ask, or you can just ask one of us to like add you to the CC list. And we can do that for you as well. And another thing is that, you know, we don\u0026rsquo;t want ProBot to be this thing where it can break in an unpredictable way, and then like go into an infinite loop, like repeatedly adding labels and everything. So ProBot is designed to be idempotent. So if I accidentally deliver the webhook again, or like I\u0026rsquo;m running multiple copies of ProBot, which is what I was actually doing at some period of time, ProBot can be deployed on Lambda, it can also be deployed on GitHub Actions. I tried deploying it on GitHub Actions. And at the time, GitHub Actions had a really long latency, like it took like up to a minute before the GitHub Action ran. And I really liked adding label and seeing it instantly show up, like, you know, less than a second later. So Lambda was the only way to do it. But I had both of these running at some time. And if the bot wasn\u0026rsquo;t idempotent, then you know, like bad things could have happened in this case. But like, if it\u0026rsquo;s idempotent, that does make the types of operations you\u0026rsquo;re allowed to do with a bot, you know, less complicated. But it also just, you know, makes it harder to have accidents with a framework in question. And finally, I talked a little bit about ProBot\u0026rsquo;s testing framework. So I was wondering if I would just do live testing and then call it a day. And I was like, in the end, no, I actually want to test this code, there is some non trivial parsing code associated with like CC bot. So I went and like, got the testing set up, I like figured out how to do testing in node JS, which like was kind of annoying, because I\u0026rsquo;m not really a node person, I don\u0026rsquo;t really know about like npm. But like, it\u0026rsquo;s all there. And that was really nice, because I doubt anyone else would have spent the time to add the testing framework. So it\u0026rsquo;s like making sure the initial infrastructure exists beforehand, is really helpful when you want to hand the project off to someone else. And they\u0026rsquo;re probably not actually going to like add tests unless there\u0026rsquo;s already a testing framework there. So I think that paid off. ProBot can use developers, for example, something that I\u0026rsquo;ve wanted to do for a really long time and haven\u0026rsquo;t done because I\u0026rsquo;ve just never gotten around to it is we have a CC bot, and it works for issue labels, but it doesn\u0026rsquo;t work for pull requests, because I just never set up listening for labels on pull requests. So that would be really nice feature to have so that people could also tag pull requests, and you could get CC\u0026rsquo;d on them in that case as well. All right, that\u0026rsquo;s everything I want to talk about today. Talk to you next time.\nEP45 Memory-layout Memory-layout Hello everyone, and welcome to the PyTorch Dev Podcast. Today, I want to talk about memory format in PyTorch. To answer what memory format is, I want to talk a little bit about how tensors are laid out in memory. Tensors are multi-dimensional qualities. You can have as many dimensions as you want in a tensor, and that distinguishes them from good old-fashioned vectors, which are one-dimensional, or even matrices, which are two-dimensional. And so, although there\u0026rsquo;s this dimensionality, although you can have cubicle data or just arbitrarily high-dimensional data, when it comes down to it, when you want to actually store this data in your memory, well, memory is linear, right? Standard CPUs let you address memory via a numeric pointer that is laid out entirely linearly, right? Like you\u0026rsquo;ve got address zero, then you\u0026rsquo;ve got address one, two, three, and so forth and so forth. So when you have a multi-dimensional tensor semantically, you need a way to linearize this into some actual concrete ordering in memory that doesn\u0026rsquo;t have any constant dimension. It\u0026rsquo;s strictly one-dimensional. And this linearization is the layout of a tensor in question. To give an example, it\u0026rsquo;s helpful to look at the two-dimensional case. So in a two-dimensional matrix, let\u0026rsquo;s imagine, for example, a matrix where reading left to right, top-down, I have one, two, new line, three, four. So this is a square matrix, one, two, three, four. I want to figure out how to lay this out in memory. And there are actually two reasonable ways you can go about doing this. So one is you can read out the rows and sort of paste out the rows side-by-side in memory. So when I read this to you left to right, top-down, right? And so I said one, two, and then three, four after the new line. And so you can actually just lay it out in this order. So in memory, you see one, two, three, four, like directly laid out in this way. And this is what we call C order or row major order because you first do the rows. And this is what like the layout you\u0026rsquo;ll get with PyTorch. And like with any sort of C programming language where you do a multi-dimensional array, this is exactly how it\u0026rsquo;s going to go. But there\u0026rsquo;s another choice, right? Instead of reading from left to right, top down, I could read top down first, and then left to right. I could do the columns first. And this gives you so-called column major layout. So when laid out, I would have one, three, two, four, right? Because the first column is one, three, and the second column is two, four. And so in this case, like the layout on disk is different. And in fact, if you were writing in Fortran, this is in fact the order that your arrays would be. Which one is better? Well, it depends. I mean, it depends on what kind of algorithm you want to do. And like either of these could be valid representations for your data in question. Of course, it\u0026rsquo;s often, there is often a convention, because when people write kernels, they usually want to make an assumption about how things are laid out. And so for example, in PyTorch, the conventionally is you just assume things are real major, unless, you know, like something specialist happened. Another example of layout, and this one is much more germane to deep learning is in image processing. So in images, what is the typical thing that you need to do? So an image consists of a bunch of pixels, of some height and some width. And typically images have multiple colors. So you need a channel dimension that represents, you know, this is the red color, this is the green color, this is the blue color. And of course, because you\u0026rsquo;re typically doing, you know, gradient descent on batches, you also have a batch dimension, so that you have a bunch of images sort of stacked up on top of each other. And so the standard representation for an image in PyTorch is what we call NCHW. So what that means is first, the first dimension is the batch dimension. The second dimension is the channel dimension. The third dimension is the height dimension. And the fourth dimension is the width dimension. So if you imagine, if you want to imagine what this looks like in memory, for a moment, let\u0026rsquo;s forget the batch dimension, the channel dimension comes first. And when the channel dimension comes first, or is the so called outermost dimension, that\u0026rsquo;s the thing that changes least frequently when you are going through the actual linearization in order. So just to like, you know, go back to the example, right, so you\u0026rsquo;re going to have a red and a green and a blue channel. So what is the image going to look like in memory if you have a CHW and CHW layout? Well, first, you\u0026rsquo;re going to see all the reds are right, like all the red pixel values, you know, for each row going going down on your tensor. And then you\u0026rsquo;re going to see all the greens, green, green, green, green, right, for once again, the rows going down the image, and then you\u0026rsquo;re gonna see the blues, right. So what you can imagine in memory is you\u0026rsquo;ve got this region for red region for green and region for blue. And why is it like that? Well, that\u0026rsquo;s because the channel is first. So it is the thing that changes least frequently. Of course, there\u0026rsquo;s another layout that is commonly used. And that layout is called NHWC. So instead of channels being first, channels are last. And so in this particular case, right, because the channels are the innermost dimension, it\u0026rsquo;s the the thing that changes the most frequently. So in fact, if I looked in memory, what I\u0026rsquo;d see is RGB, RGB, RGB, RGB. So like, you know, every time I\u0026rsquo;m, you know, handling a pixel, I\u0026rsquo;m going to put down all the values for each of the channels before moving on to the next pixel. So it looks like, you know, an actual like what an actual like, you know, LED screen would actually look like in this situation. And of course, which of these are better? Well, once again, it comes down to the algorithms. But it turns out that in, you know, some convolution algorithms, they\u0026rsquo;re just more efficiently implemented in NHWC. So that\u0026rsquo;s like sort of what a lot of people want to be able to use channels last layout to make their code run faster because of these special kernels. So we could just stop here and say, Okay, well, Ed, you know, I great, I know what NCHW is, and I know what NHWC is. So now, you know, whenever I get an image tensor, I just need to know if it\u0026rsquo;s one of these or the other and aren\u0026rsquo;t I done. And that is okay. But, um, you know, when we wanted to add support for both of these tensor types, we had a problem. And the problem we had was we didn\u0026rsquo;t want to actually force people to keep track of, you know, what layout their tensors were, right? Like they could do this, and they were doing this. But it was a pain in the ass to actually have to deal with this for all of our operators. Like just think about convolution for a second, right? Convolution needs to know what we\u0026rsquo;re actually going to, you know, do the convolution over with regards to the channels, and what we\u0026rsquo;re not going to do over. So if you have an NCHW tensor, you need the convolution to operate with the channels in the first position. And if you have an NHWC tensor, you need to have the convolution operate with channels in the last dimension. And these are different algorithms. And you need to actually tell convolution what type of tensor you actually pass in. And tensors are these very dumb, you know, like n-dimensional arrays, they don\u0026rsquo;t actually have any semantic content. So that\u0026rsquo;s something you\u0026rsquo;d have to keep track of externally from the tensor. And that\u0026rsquo;s a pain. And we didn\u0026rsquo;t want to have to do that. So what did we do? Well, to answer this question, I have to take another detour and talk a little bit about how we implement this linearization under the hood in PyTorch. And this is done using strides. So what are strides? So I said that, you know, layout, memory layout of a tensor, of an n-dimensional tensor, is all about taking your, you know, various elements, and then laying them out in a linear sequence of addresses in memory. Well, strides are a way of computing, given any given coordinate in the logical tensor, where does it physically lay in the actual linear memory address layout. So let\u0026rsquo;s just talk a little bit about, for example, C layout, which is what PyTorch does. So in C layout, right, the outermost dimension, the dimension that comes first, is the one that changes least frequently. Or in other words, like to get to the next element, the next slice in that dimension, you have to jump a bunch of elements further, right? That was the R, R, R, R, R, G, G, G, G, G, G, B, B, B, B, right? So you need to like jump four R\u0026rsquo;s to get to the G\u0026rsquo;s, and then another four to get to the B\u0026rsquo;s. But on the other hand, if you\u0026rsquo;re an innermost dimension, one on the very end, well, you just, you know, can look at the next element and see what the element is in that situation. And this is the concept that strides do. So stride says, for any given dimension position, how much do I have to advance the physical memory pointer to get to the next element corresponding to that dimension? So if your innermost dimension is fast moving, then the sorry, if the innermost dimension is the one that changes, you know, all the time contiguously, then I say the stride for that is one. Because if I like want to move to the next element, I just go to the next physical memory layout, it\u0026rsquo;s all laid out contiguously. Whereas if I\u0026rsquo;m on the outermost dimension, and I want to, you know, jump really far, then I might give it a stride of say four, if this was a, you know, size four tensor in the outer dimension. And that just means, hey, to get to the next element, you have to jump four elements ahead. So going back to our like original example, 1234, that square matrix, right, the strides for this in C layout would be 21. To get to the next element in a row, you only need to look at the next, next spot in your contiguous memory. But to get to the next, sorry, to get to the next value in the row, you just have to get to the next value in the column. To get to, you know, to move the row down, you have to jump past the entirety of the row. And so that\u0026rsquo;s why the stride is two, because the two is the size of the row that you have to skip across to get to the next thing. And of course, if you have Fortran layout, then your strides are simply one three, because when you want to see what the next column is, well, the columns are now laid out contiguously. So you just advance it by one. But if you want to see the next row, well, those are not set out contiguous, and you have to jump. And so the stride in that case is two, right? So seal out is two, one, the strides are in decreasing order. And Fortran layout is one, two, the strides are in increasing layout. And in fact, you can flip between these two strides just by using transpose and PyTorch, which doesn\u0026rsquo;t do a copy, it just, you know, fiddles around with the strides and then gives you a new tensor with those different strides. Okay, so what the heck does this have to do with memory layout? Well, we had a very clever idea to make memory layout work. So PyTorch originally only supported NCHW, and all of our convolution operations assumed that you would put the channels first when you call them. So what we said is, hey, let\u0026rsquo;s just double down on that. So the user visible API, the logical view on tensors, always requires channels to be in the first position right after batch. But if you want to use channels last layout, well, no one said that the NCHW logical layout had to correspond to NCHW physical layout, right? It could, and that would be the case when the strides are strictly decreasing. But it could also remap to a physical layout that actually holds things out in NHWC. I\u0026rsquo;m not going to tell you what the strides are in this case, because it\u0026rsquo;s not the obvious one. It\u0026rsquo;s not the permutation from NCHW to NHWC. It\u0026rsquo;s the reverse permutation because reasons. Try deriving that by yourself if you\u0026rsquo;re actually interested. And so by doing it this way, right, like the physical memory layout is what the kernel actually cares about, because like the kernel, like is going to run faster because of something that it\u0026rsquo;s doing regarding memory locality. But at the same time, we can still give the same user experience where like a convolution always takes an NCHW tensor, and it just might happen to be one of these weird transpose tensors that is represented differently in physical memory. Some things to know about internally how we implement this. So although we store strides, and in principle, you can calculate whether or not something is NCHW or NHWC from the strides, it\u0026rsquo;s kind of expensive to do this. So we actually have this giant bit filled on tensor that like has all the common memory layouts that you want to often test for, like when you\u0026rsquo;re doing convolution. And these are all just pre computed based off the strides to make access fast. I kind of hate this design, but it is very expedient. And it indeed does have performance benefits. There\u0026rsquo;s one last interesting thing about memory layouts done in this way that I want to tell you about. And this is the ambiguity problem. Let\u0026rsquo;s imagine that I have a one by one tensor. Well, the strides for this tensor are one one. Why is it one for rows? Well, because there are no rows. And even if there were rows, I would only have to go one to go to them because the size of the row is one. So like, you know, advancing it is easy. When I have strides that are like one one, where I have one of these one size dimensions, I cannot tell what the layout is, I cannot tell if this is row major or column major, because the strides just don\u0026rsquo;t have any information for me. And this is a problem. Because one of the things that we need to do when we are doing memory layouts is we need to propagate memory layouts, right? Like it\u0026rsquo;s no good if I feed in a NHWC tensor, expecting convolution to get it and use my efficient, you know, channels last kernel, if somewhere in the middle, I have an operator that takes in one of these tensors, and then just calls contiguous on it. And the meaning of contiguous is put it in NCHW format. So it\u0026rsquo;ll go ahead and do that. And then well, sucks to be you like you\u0026rsquo;ve just lost all the optimization opportunity. And so when you have tensors, which like lose this layout information, you might actually make the wrong choice and turn it back into an NCHW tensor. If you like expand the size, this has happened, we Natalia Gimelschein fixed a bunch of these cases, when we were originally trying to figure out how to do this. And like, most of the time, the way we resolved it was like, there was some extra data, there was some other tensor that we could rely on to get the information that we needed. There\u0026rsquo;s also some conventions you can do when you\u0026rsquo;re writing out the strides, because actually, you have a lot of degrees of freedom when a stride is for a size one or size zero tensor, right? Like, if your size, if your tensor is only size one, it doesn\u0026rsquo;t matter how big or small your stride is, because you\u0026rsquo;re never going to actually use it, you only ever multiply it with zero. And you know, you never multiply it with one, because that would imply there were two elements. I had a proposal for solving this problem called layout permutations, where the idea was, instead of only storing the strides, we also store a layout permutation that says exactly what the permutation is. This would also solve the ambiguity problem, because when I have strides one, one, I would also know via the permutation, if it was zero, one, or one, zero. But we never implemented this because it was kind of a lot of work. And we solved most of the most pressing problems by just manually fixing them. So that\u0026rsquo;s it about memory format. Memory format lets you, you know, move around your dimensions and get faster kernels. That\u0026rsquo;s everything I wanted to say for today. Talk to you next time.\nEP46 Reference-counting Reference-counting Hello everyone and welcome to the PyTorch Dev Podcast. Today I want to talk about how we do reference counting in PyTorch. You might think of reference counting as something that isn\u0026rsquo;t all that interesting, especially in C++ where there are plenty of classes like shared pointer that allow you to do reference counting without having to think very hard about it. Well, there\u0026rsquo;s actually a lot of subtleties doing reference counting in PyTorch and I want to talk about a few of the things that are going on here. So one of the very first things that you figure out when you look into reference counting in PyTorch is that we don\u0026rsquo;t actually use shared pointer for most things. Instead, we use this thing called intrusive pointer. Intrusive pointer is the term of art for reference counting schemes which store the reference count for an object directly on the object itself. So this is in conscious to shared pointers in C++ which work on any type of object and the way they do that is the reference count is stored in what\u0026rsquo;s called a control block which is allocated separately from the reference count in question. Of course, if you use make shared, the control block and the actual object in question will be done together in one allocation. But in general, when you have a shared pointer, it\u0026rsquo;s actually two pointers. One pointer to the control block and one pointer to the actual object in question. So that\u0026rsquo;s a little wasteful and it also makes it difficult to take a raw pointer and convert it into an owning pointer. So in PyTorch, we implement all of our reference counting using intrusive pointer. So the intrusive pointer stores the ref count on the object. You have to inherit from an intrusive pointer base which says, hey, here\u0026rsquo;s where the ref count is, here\u0026rsquo;s the memory layout that intrusive pointer expects. And then intrusive pointer is just, in fact, the actual, you know, smart pointer class that handles the reference count increment and decrement when things go in and out of scope. So the tensor type that you all know and love is exactly simply a wrapper on top of an intrusive pointer to the tensor impl, which actually contains the tensor data in question. And tensor impl has a very minimal API. And then tensor, the wrapper class actually has a ton of extra methods defined on it, which, you know, lets you do all the good old fashioned method calls that you want to do in PyTorch. So the ref count on intrusive pointers is atomic. So it means that PyTorch does work correctly in a multi-threaded setting. But it also means, like its shared pointer breadth in, atomic operations are actually quite expensive. And that also means that intrusive pointer bumps are also expensive. Why, by the way, are atomic ref count bumps expensive? Well, the reason is that when you do an atomic operation, your processor has to actually bounce the, you know, cache line, which, you know, previously could just directly operate it on back into main memory to make sure things get consistently seen by the other cores in question. And that communication is quite expensive. In contrast, Python does a lot of ref counting, and people don\u0026rsquo;t generally think of, you know, increasing or decreasing ref counts in Python as very expensive. And that\u0026rsquo;s because Python ref counts are actually non-atomic, and they\u0026rsquo;re protected by the global interpreter lock. So, you know, the interpreter only runs in a single-threaded fashion. And, you know, increments and decrements that are not atomic, that are not locked, those are very cheap to do. So because tensor ref count bumps are very expensive, we actually go through quite a lot of trouble to avoid actually doing ref count bumps when we can. And in fact, in PyTorch, when we write functions, like we write operators, typically the lifetime of tensors is very, very regular, right? In particular, is that, you know, when we call a function with a bunch of tensors, those tensors are going to stay live for the entirety of the function. Because, you know, what are these functions doing, they\u0026rsquo;re not storing things in data structures, they\u0026rsquo;re not destroying anything, right? They\u0026rsquo;re just reading in the tensors as input, and then doing things with those. So in fact, everywhere in PyTorch, where you know, you don\u0026rsquo;t actually want to steal a tensor in question, we just pass around const tensor ampersand, which is just a very convenient way of writing, hey, pass in this tensor, and don\u0026rsquo;t actually, you know, do a reference count bump when you pass it in in this way. Now, if you\u0026rsquo;re a veteran C++ programmer, you might be thinking to yourself, hey, why are you doing a const reference to a shared pointer type, which actually points to the object in question? Isn\u0026rsquo;t that a double indirection? Shouldn\u0026rsquo;t you just be passing a, you know, tensor impulse star, or some sort of direct pointer to the object in question in this situation? And really, the answer is, yes, you would be right. In an ideal world, this is what we would do. But remember that tensor is a type that has a lot of methods on it. And tensor impl is a very bare bones type. So, you know, when we were originally writing out the A10 library, we had this problem, which is that, well, you know, these tensors that people want to take in a non owning fashion, well, these people still want all of the methods, all of the, you know, useful, convenient stuff that\u0026rsquo;s only on tensor and not on tensor impl to be available in the situation. And if you pass a tensor impulse star, well, you\u0026rsquo;re not going to get any of that information. So, you know, at the very beginning, we were like, okay, well, we\u0026rsquo;re just going to const tensor ampersand. And, you know, that\u0026rsquo;ll be very easy and convenient to do. And you\u0026rsquo;ll get all the API that you had before. And then the rest is history. So like, everywhere you look in PyTorch, you\u0026rsquo;re going to see const tensor ampersand all around everywhere. There\u0026rsquo;s also a little bit of nuance here, which is that if you have a const tensor ampersand, you might be thinking to yourself, hey, you know, maybe I should just pass it by value. And that, you know, also, whenever I get to move into the tensor in question, doesn\u0026rsquo;t that, you know, save me a reference count bump in that situation anyway. And certainly, if you are dealing with a function that wants to take ownership of the tensor in question, this is certainly a good thing. But once again, most of the functions in PyTorch are borrowing from the tensor, they don\u0026rsquo;t actually take on ownership. And there\u0026rsquo;s this funny business with the itanium ABI, which says that if you have a non trivial class, an intrusive pointer is a non trivial class, because it has a destructor that\u0026rsquo;s responsible for decrementing the ref count, when it exits. If you have a non trivial class, you must put it on the stack so that I can take a pointer address to it. So I\u0026rsquo;m not allowed to pass in an intrusive pointer to a tensor impulse directly inside of a register, it always has to be on stack. It\u0026rsquo;s a kind of crappy thing about the ABI. It actually is one of the reasons why unique pointer is not a zero cost abstraction, you pay for using unique pointers instead of raw pointers that you just manually alloc and dealloc. But you know, basically, whenever you say constants or ampersand, that\u0026rsquo;s basically what you know, people were doing anyway, when they were forced to put their intrusive pointers on the stack. So it\u0026rsquo;s no worse, really. So taking stock where we are right now. So we\u0026rsquo;ve got tensor, tensor is a reference counted type. It internally is represented as an intrusive pointer to a tensor impel, which actually contains the actual data for the tensor in question. Reference count bumps in pytorch are atomic and therefore expensive. And in order to get around that, most people pass around tensors as const tensor ampersand. By the way, this const on the const tensor ampersand means that you\u0026rsquo;re not allowed to mutate the reference itself, right? So like if I had a tensor x, and I pass it into a const tensor ampersand, you wouldn\u0026rsquo;t be allowed to, you know, set x equal to y. And that would change what the binding was at the top level. What it does not mean and what something that is very easy to get confused about is it does not mean that the tensor itself is const, and we\u0026rsquo;re not allowed to mutate it, you\u0026rsquo;re allowed to mutate whatever you want. Const correctness on tensor is not actually a thing. And this is because when we say const tensor ampersand, we mean a const reference to a mutable tensor, not a reference to a const tensor, which in, you know, shared pointer parlance would have been shared pointer, open angle bracket, const tensor, closed angle bracket. That\u0026rsquo;s just sort of not representable. If you just say tensor, because tensor is already, you know, an intrusive pointer to a tensor impulse. So you\u0026rsquo;d have to like come up with a different type, like const tensor in that situation, which, you know, might not be a bad idea. And there\u0026rsquo;s an issue about this, and someone should go about and implement this at some point in time. A funny problem happens occasionally, when you\u0026rsquo;re working with this tensor type, which is that sometimes you have a tensor impulse star. Remember, one of the perks of doing intrusive pointers is you can pass around a bunch of raw pointers to the objects in question. And then you can always easily convert these into real, honestly, goodness shared pointers. You can\u0026rsquo;t easily do that with a shared pointer, because, well, you know, you need to somehow get at the control block. That\u0026rsquo;s why enable shared from this is a thing that, you know, is an extra bit of information that records where the control block is. So you can always get to it when you need it. So your problem is, you\u0026rsquo;ve got one of these raw tensor impuls, and you want to pass it to one of these constants or ampersand that I said is all over the code base in PyTorch. And here\u0026rsquo;s the problem to do this, you actually need an honest to goodness tensor class. Although the tensor class is, you know, representationally equivalent to a raw pointer, because at the end of the day, it contains a C10 intrusive pointer. And what is a C10 intrusive pointer? It\u0026rsquo;s just a raw pointer with a bunch of specialty structures. C++ does not allow you to actually interchangeably, you know, convert between these two representations. So like, you\u0026rsquo;re kind of stuck, right to actually pass a tensor impulse star to a const tensor ampersand, you have to somehow manufacture a tensor. But manufacturing a tensor, you know, ordinarily gives you a ref counted owning object that is obligated to destroy the tensor, you know, decrement the ref count when the tensor goes out of scope. So it seems kind of like you\u0026rsquo;re out of luck, right? Like you want to create a non-owning const tensor reference, but you can\u0026rsquo;t do it, because well, you know, you have to make a tensor and tensor is getting in the way. So Scott Walchuk had a really good observation about how to solve this problem, right? So remember that the problem is that if we created tensor, well, one is that, you know, ordinarily, you have to increment the ref count when you create a tensor, but you could imagine skipping that. But then when you destruct the tensor, the tensor will actually decrement the ref count, right? So you\u0026rsquo;ve got two ref counts, you need to somehow get rid of. But intrusive pointer actually has a condition in its deallocation. And the condition says that we only decrement the ref count, if the intrusive pointer actually is non-null. If the intrusive pointer is null, we skip the decrement altogether. And this behavior in the destructor gives us an out, right? What it says is that if I manually clear the intrusive pointer before the destructor of tensor runs, then the destructor of tensor will see that the pointer is null, and it\u0026rsquo;ll skip the decref. So all I need to do is be able to release an intrusive pointer without decrementing the ref count and nulling out the value on the inside. And I can get by scot-free. And this is the idea behind tensor ref. So how does tensor ref work? So tensor ref is a class, it contains a tensor as its member, but it\u0026rsquo;s intended to be a non-owning version of tensor. So you are able to construct these without incrementing ref count bumps. And when you destruct these, no ref count bumps happen. On construction, what you do is you take a tensor, and you take the raw pointer for that tensor, and you manufacture a new tensor object without actually incrementing the ref count. Intrusive pointer actually has an API for doing this. It\u0026rsquo;s like don\u0026rsquo;t increase ref count tag in the constructor. It used to be private, but you know, we made it a little less private so that we could do this particular thing for tensor refs. And then when we destruct the object, well, destructors for child classes run before parent classes. So in the child class destructor for tensor ref, what we do is we release the pointer. So what release does is it sets this intrusive pointer to null and skips the ref count bump. And now the parent destructor, which, you know, is going to process the members in the class in question, namely the tensor, will see that while it\u0026rsquo;s a null pointer, so there\u0026rsquo;s nothing to do. So you\u0026rsquo;ve bypassed the increment ref count and decrement ref count in both cases. And once again, what was the point of doing all of this? Well, now I have a way of given a tensor impulse star, I can create a tensor const tensor ampersand, right? I do that by creating one of these tensor refs, which internally contains a const tensor ampersand. And that\u0026rsquo;s the way that I can actually then call these functions without having to do any reference count bumps. So this is a pretty good, cool idea. And we actually never implemented it. And the reason we never implemented it was because, well, you know, tensor ref is an entirely new class, C++ doesn\u0026rsquo;t have dot overloading, that is to say, there\u0026rsquo;s no way to say, hey, given a class, here\u0026rsquo;s what the meaning of all dot foo operations means, because then I could just forward it to tensor. So actually, we\u0026rsquo;d have to code generate all of the same methods that used to live on tensor on tensor ref as well. That was kind of a pain. And so no one has gone around to doing it. However, Megan Lele has been working on a similar concept, optional tensor ref. So what is optional tensor ref? Well, optional tensor ref is for those situations where you want to optionally pass in a tensor to one of the kernels in PyTorch, or maybe there\u0026rsquo;s no tensor at all. Previously, we implemented these as a std optional tensor, but there\u0026rsquo;s a problem with this implementation. Do you see it? std optional tensor with no extra references or pointers or anything like that implies that you\u0026rsquo;re getting an owning reference to tensor. So in fact, to call a function like this, you have to do a reference count bump. That\u0026rsquo;s bad. And you know, we kind of mess this up. And we\u0026rsquo;re trying to fix it with structured kernels. So optional tensor ref doesn\u0026rsquo;t have this problem. It also is a little more efficient than optional tensor, because optional tensor, the optional class is obligated to store whether or not the tensor is full or not by a separate Boolean. But you know, we can actually just represent that as a null pointer tensor inside optional tensor ref. And finally, optional tensor ref doesn\u0026rsquo;t have the problem of the API, because well, you expect to have to use arrow notation whenever you\u0026rsquo;re accessing an optional object, because you don\u0026rsquo;t know if it\u0026rsquo;s null or not. So there\u0026rsquo;s a lot of stuff that goes into reference counting in PyTorch. And if there\u0026rsquo;s one thing that I want you to take away from this podcast, it\u0026rsquo;s that atomic ref counts are expensive. So avoid them whenever you can. That\u0026rsquo;s everything I wanted to say for today. Talk to you next time.\nEP47 torch.use_deterministic_algorithms torch.use_deterministic_algorithms Hello everyone, and welcome to the PyTorch Dev Podcast. Today, I want to talk about the determinism mode in PyTorch, which allows you to run PyTorch computations in a fully deterministic fashion, so that if you run PyTorch programs again, you\u0026rsquo;ll get the same result. What\u0026rsquo;s the point of determinism? Ask no one who ever had to debug a very hard-to-reproduce problem because the problem was non-deterministic. Deep learning programs are already very hard to debug because, well, there\u0026rsquo;s a lot of stuff that is going on that you don\u0026rsquo;t really directly have access to as an engineer, and so if you forget to add a constant in some area, you might not find out about this except that your network trains a little bit more slowly. And so if you\u0026rsquo;re doing a four-hour long training run and only into hour three do your gradients start exploding and you get lots of NANDs everywhere, well, it\u0026rsquo;s going to be very painful to debug this problem in most circumstances. So determinism makes at least part of the problem easier, which is that if your program is bit-for-bit deterministic, if it always produces exactly the same bits every time around, then, well, okay, maybe you don\u0026rsquo;t find out about the problem until three hours in, but at the very least, every time you try, you will get the same result. Because the only thing that\u0026rsquo;s worse than a problem that only happens three hours into your training is a problem that only happens once every 10 times three hours into your training. Ugh. So torch deterministic was a proposal that was made by Sam Gross a very long time ago, and we didn\u0026rsquo;t really do very much with it because there\u0026rsquo;s a very annoying thing you have to do to make this feature actually come into being, which is you actually have to audit all of the operations in PyTorch and look for ones that are not deterministic and then do something appropriate in the situation. So lots of kudos to Kurt Moeller who actually picked this up and, you know, got it, saw it all the way through to the end. He\u0026rsquo;s the one who made it all happen. So what\u0026rsquo;s the basic concept between torch dot deterministic? Well, there\u0026rsquo;s a few things to talk about what you want out of a way of running your programs deterministically. First off, you don\u0026rsquo;t want determinism to be on all the time. Why? Because being deterministic is actually quite expensive. There are a lot of algorithms where if you allow for a little bit of non-determinism, they can run much, much faster. And when you, you know, make things run deterministically, well, you\u0026rsquo;re going to get quite a bit of a slowdown. And so, you know, there\u0026rsquo;s actually a very delicate balancing game PyTorch does with regards to its defaults. Do we give people, you know, all the knives and run really fast and make it easy for them to do things that are wrong? Or do we actually, you know, try to prevent errors and, you know, try to make sure people, you know, don\u0026rsquo;t do the wrong thing. And sometimes we trade off performance for, you know, making it hard for people to do the wrong thing. But determinism is not one of those things. We do not give you determinism by default. You have to ask for it explicitly. Another question is, you know, one of the things about non-determinism in your network is you might not even know about it when you, when one of these things happens. So there\u0026rsquo;s sort of two parts to Torch Deterministic. So one is just letting you actually, you know, use deterministic algorithms whenever they are available. But second is just identifying when non-deterministic code is being run. So you can know, oh, yeah, this training run is using that function. That function is non-deterministic. Maybe it doesn\u0026rsquo;t even have a deterministic implementation, but at least I know about it. And I can, you know, ask my system error if I use something that doesn\u0026rsquo;t actually have a deterministic implementation. So the framework level implementation of Torch.deterministic is pretty simple. There is a context manager that, you know, you can use to turn on the, you know, warn or error on determinism. And then everywhere in our code base where we\u0026rsquo;re about to do a non-deterministic operation, there is a line that just says, hey, alert that, you know, this is non-deterministic. And then depending on the setting, if, you know, you\u0026rsquo;re supposed to error or if you\u0026rsquo;re supposed to warn, you\u0026rsquo;ll get one of these other things. And in some cases, if determinism is requested, we can route to a different algorithm. And there\u0026rsquo;s just an if statement that does that. Very, very simple. So really most of the juice is in deciding one, that this was a good idea, and two, actually following through and editing all the algorithms. There\u0026rsquo;s actually not that many different types of non-determinism in the library. So one of the most common ones is the backing libraries that we use, especially in CUDA, are often non-deterministic in these, in some situations. So the, like, classic example of this is convolution. CUDA-NN, you know, has a lot of algorithms, and some of its convolution algorithms are non-deterministic. And in fact, there\u0026rsquo;s even, prior to the, you know, generic determinism flag, there was a CUDA-NN deterministic flag, which was specifically about, you know, foregoing use of the faster algorithms so that, you know, you would use one of the, like, deterministic convolution algorithms. In other cases, you know, that are not library code, the most common reason for non-determinism is an atomic addition. So to explain why atomic additions can cause non-determinism, it\u0026rsquo;s important to know a little bit about floating point numbers. Floating point numbers are not commutative. Repeat after me, floating point numbers are not commutative. If you actually think about, you know, what it would take to actually write a floating point implementation, this kind of makes sense once you think about it. Because, like, let\u0026rsquo;s imagine that, you know, you have a, you know, very, very small quantity, and you keep adding it until eventually it becomes a larger quantity, like, you know, 0.1 plus 0.1 plus 0.1 plus 0.1 and so forth. And eventually, you know, you can get something fairly big until, you know, your precision falls off the cliff, and you don\u0026rsquo;t have enough precision to know every time when you\u0026rsquo;re adding plus 0.1, you, you know, actually can put it in. Because this is a limited number of bits, right? The whole point of floating point is you can change the amount of precision you have. So, like, if you\u0026rsquo;re close to zero, you get a lot more precision. If you\u0026rsquo;re a really big number, you have less precision. But if you\u0026rsquo;re adding, you know, 0.1 to, you know, a trillion, well, you\u0026rsquo;re probably not actually, you know, like, there\u0026rsquo;s just no space to represent it in the floating point representation. Well, if you, you know, do a bunch of these additions, and you get to a reasonably large number, and then you add it to a large number, you know, you can expect to see the contribution of all those incremental 0.1s. But if you start with a big number, and then you keep adding 0.1 to it, and each time you don\u0026rsquo;t actually take up the floating point number, because well, there\u0026rsquo;s, you know, no change, because you don\u0026rsquo;t have precision to represent it, well, you know, clearly, you\u0026rsquo;re gonna get a different result in these situations. By the way, there\u0026rsquo;s some kind of interesting ways to work around problems like this, even with limited precision. One interesting way is to sort of randomize whether or not you take up the result or not, depending on if, you know, it\u0026rsquo;s just too small for the precision. So like, say, you\u0026rsquo;re adding one with a million, and you don\u0026rsquo;t have enough precision to represent one, but you do have enough precision to represent 10. Well, maybe you would just increment only a tenth of the time, non-deterministically. Of course, this is really terrible for determinism. So let\u0026rsquo;s get back on track. So because floating point addition is not commutative, if you have any operations that are like, hey, run a bunch of stuff in parallel, and then atomically accumulate it into some buffer, which, you know, is doing some sort of reduction, well, that\u0026rsquo;s going to be non-deterministic if you do it the obvious way, which is with an atomic, you know, addition in the situation. So most cases of use of atomic add in CUDA, those are non-deterministic. And so it\u0026rsquo;s like, it\u0026rsquo;s like actually super, super simple, half the time to figure out if something is non-deterministic. Does it use atomic add? Oh, it uses atomic add? Well, it\u0026rsquo;s probably non-deterministic. And that\u0026rsquo;s really all there is to the feature, right? It\u0026rsquo;s, you know, a context manager that sets some global variable that triggers the behavior, and then a bunch of code everywhere that, you know, says whether or not something is deterministic or not. One of the things that we do, you know, would accept patches for, and in fact, in the last half, some work that some folks did was for some of our internal training workloads, they added support for a lot of deterministic versions of operations that didn\u0026rsquo;t previously exist. Kudos to them. Great work. Is yes, in some cases, we will just hard error if you ask for deterministic. And if you provide a deterministic version of the algorithm that we can use in place of it, even if it\u0026rsquo;s slower, that just, you know, makes the deterministic feature more generally applicable. I also think of torch.deterministic as a really good sort of API model for other types of things in PyTorch, which we might want to do. In particular, there is nothing intrinsically wrong with non-deterministic operations. It just happens that sometimes you want to know if you\u0026rsquo;re about to stumble into one of these things. So it would be nice to be able to just easily set some flag to then be told about all these situations, maybe as warnings to just find out about all the sites or errors if you like absolutely, absolutely cannot abide with a non-deterministic result. Well, there\u0026rsquo;s actually a lot of other behaviors in a framework like PyTorch, which have a very similar property. For example, Natalia Gimelshin is working on a version of this, but for CUDA synchronizations. What\u0026rsquo;s a CUDA synchronization? Well, that\u0026rsquo;s when you have some CUDA computation, and you\u0026rsquo;re like, hey, GPU, please finish all your compute, and then send me the result back to CPU, so I can go look at it. That\u0026rsquo;s, you know, something that will happen implicitly, sometimes in your PyTorch program, it can trash your CUDA performance. And it would be really nice to know when this has happened. And it\u0026rsquo;s as simple as just making sure everywhere these synchronizations can happen, we have a test. And so you can set a flag and then, you know, have it raise a warning or raise an error. So once again, super simple, but it goes a long way. So this is like, I don\u0026rsquo;t know, it\u0026rsquo;s not very glamorous, but I think it adds a lot of value to our users. And so I want to encourage people to work on this kind of feature, because hey, it really does pay off. Okay, that\u0026rsquo;s everything I wanted to say for today. Talk to next time.\nEP48 gradcheck gradcheck Hello everyone, and welcome to the PyTorchDev podcast. Today, I want to talk about GradCheck, a mechanism for automatically testing the correctness of derivatives written for functions. Where to start on this podcast? Well, I will tell you all about calculus and derivatives and finite differences, but before I get there, I want to talk a little bit about testing. When the word testing comes to mind, there\u0026rsquo;s a bunch of different possibilities for what you might mean in a situation like this. Perhaps the very first set of tests a enterprising programmer writes, or to put it more precisely, the first set of automated tests, because who hasn\u0026rsquo;t ever, you know, written some code and then just run it directly in the REPL to see if it worked or not. The first automated test one writes tends to be of the form, write a single test case with some input, and then, you know, test that the output is what you expect it to be. And for many types of programs, this works pretty well. And you know, if you are a proponent of say, test driven development, the model is that you\u0026rsquo;re not supposed to write any code before you start writing your test cases, and you go one by one by one until you get there. Of course, writing tests in this way manually can be a bit irritating. And it\u0026rsquo;s especially bad if you\u0026rsquo;re working on a numeric library like PyTorch, where typically the input is going to be some random number, and the output is also going to be some random number. And it is very non enlightening. If you write your tests, as you know, if I feed in a bunch of these floating point numbers, then I get these other floating point numbers, right? It\u0026rsquo;s just difficult to maintain. And it doesn\u0026rsquo;t even work that well. If for example, your precision is slightly off, or you make a change that changes some of the epsilons of it all. And now you have to go manually update all of your tests. Now, of course, in a previous podcast, I talked about expect testing, where the idea behind expect testing is that you can automatically update your test cases when things change. But in this particular podcast, we\u0026rsquo;re going to look at different form of testing, namely property based testing. What is property based testing? Well, property based testing is based on the idea that instead of specifying individual input output pairs, you say, hey, here is a property that I expect to hold for all inputs, or maybe conditional on some properties on the input being true, I expect to hold for all inputs. And so I will simply randomly generate inputs, and then see if this property is upheld by the function in question. And when we type when like in, you know, say undergrad, they teach you about property based testing, the like canonical example is reversing a list, right? So what are some properties you can test for reversing a list? Well, if I have a list, and I reverse it, and I reverse it again, that should give me back the original list, right? So reversing twice is idempotent. Of course, this is very boring. And a lot of people come out of this, and they think, you know, okay, well, property based testing isn\u0026rsquo;t that interesting. But I want to tell you today, that grad check is an example of property based testing. And it is really, really effective at testing programs in PyTorch. Okay, so how does grad check work? Well, the, the problem grad check is trying to solve is that in PyTorch, we are a library that implements automatic differentiation. And the problem with automatic differentiation is we basically have a bunch of mathematical functions, and we never write what their derivatives are. And in math, the derivative of function is a well defined concept, there is one correct answer, modulo subgradients and stuff like that. And of course, there\u0026rsquo;s always a possibility that when you write how to translate a primitive into its derivative, you wrote the translation down wrong. And so this kind of error is what the property based testing of grad check is trying to figure out. So for a moment, let\u0026rsquo;s remember what the definition of a derivative is, there are a number of ways to formulate this. For example, in your calculus class, you may remember some formula involving limits, and f of x plus dx minus f of x divided by dx, something like that. But another way that I like to think about derivatives is, you know, suppose I have some function, and, you know, it might be very wiggly, it might have very strange behavior. And at any given point, if I zoom in enough, the function starts looking more and more like a straight line, right? Like I keep zooming in, zooming in, until, you know, all the wiggles go away, right? I\u0026rsquo;m just looking at a single segment. And while it looks like a line. And so the derivative, what the derivative tells us is what the linear approximation of a function is at any given point on the function, right? So if I\u0026rsquo;m asking derivative at, you know, some point, then like, that\u0026rsquo;s going to give me, hey, you know, like, it\u0026rsquo;s flat, or it\u0026rsquo;s curved upwards, or it\u0026rsquo;s curved downwards, those are the various different derivatives you can have. So it stands to reason that you can calculate a derivative simply by zooming in sufficiently on the function, looking at two points, and then turning those points into a line. And that\u0026rsquo;s exactly what the method of finite differences does. It says, you can numerically compute a derivative by simply just taking two points on the line, the function that are very close to each other, dividing by the distance they are from each other. And that\u0026rsquo;ll give you, you know, the slope of the line at that point. And as I said, that\u0026rsquo;s a very good linear approximation, essentially, the smaller and smaller you make the delta. Of course, PyTorch doesn\u0026rsquo;t compute your derivatives this way, it would be heinously slow to do so and also not very accurate. Instead, you know, when we write derivative formulas, what we\u0026rsquo;re doing is what we\u0026rsquo;re writing down what\u0026rsquo;s called the analytic derivative, which is, you know, like in math, in calculus, right, you had a bunch of rules for, you know, given a bunch of functions, how to convert them into derivatives, and the analytic derivatives are just simply directly writing down those rules for automatic differentiation system. So one of the themes in property based testing is that if you have some way of implementing a function in two ways, or if you have some way of representing a property in two different ways, if you have two ways of doing the same thing, then a very easy to set up property in this situation is just to say, hey, when I do method A, and when I do method B, they need to give the same result. And if they do, well, that\u0026rsquo;s good for me. This sort of like sort of comparison against a reference implementation is very, very convenient to test because you don\u0026rsquo;t have to know anything about the outputs, you just need two implementations, and you can compare if they work together or not. And so in the situation of differentiation, and what Gratchek does is Gratchek says, hey, I have two ways of testing what a derivative is, I can do the numerical method where I just, you know, take finite differences and see what it looks like just by looking at the points. Or I can take the analytic solution, the one that I\u0026rsquo;m trying to test the system under test, and just directly compute it based on the symbolic formula in that case. And all I need to do is compare these two formulas on a bunch of random inputs. And if they always agree with each other up to some tolerance, then I know that I\u0026rsquo;ve implemented my derivative correctly. Of course, there\u0026rsquo;s a complication. And it\u0026rsquo;s easiest to explain this complication in two steps. First, when I describe this function to you, you were probably imagining a squiggly line in 2d space. And you know, the derivative was just some straight line that was tangent to the line at this point. But first, if we think about neural networks, neural networks aren\u0026rsquo;t, you know, there aren\u0026rsquo;t only two parameters in a neural network, that would be a very impressive neural network called linear regression. Instead, there are many, many dimensions for all the parameters. And they all, you know, do a lot of computation up until one point. And so a more accurate way to think about a neural network is that you have some sort of surface, some like high dimensional surface, but, you know, it\u0026rsquo;s easiest to visualize this in 3d space. And what you\u0026rsquo;re trying to do is you\u0026rsquo;re trying to find the gradient, which represents the, you know, orientation of a hyperplane, a the plane, which is tangent to the surface at some point. But that\u0026rsquo;s just complication one. Complication two is that when you look at the individual functions that are used inside a neural network, and these are the ones that we actually want to do grad checks on, remember, because they\u0026rsquo;re the ones we\u0026rsquo;re writing derivatives for. It\u0026rsquo;s not just a, you know, surface, because that might be some sort of function, which takes in some high dimensional space, and produces another high dimensional space. So really, to model what this transformation looks like, in a linear way at a very at a neighborhood, you need a thing called the Jacobian matrix. It\u0026rsquo;s kind of hard to describe what a Jacobian matrix does. But one of the explanations that I read on math overflow, that I quite like, is imagine you you have your vector space, and you\u0026rsquo;re looking at one point in the vector space, when you perform your operation on this, you map this point in the vector space to another point in the destination vector space. And furthermore, all the points in the neighborhood also get mapped at the same time when you do this. And you\u0026rsquo;re looking for a single matrix that describes how these points distort, move around, etc. And it\u0026rsquo;s a matrix because, you know, we\u0026rsquo;re talking about linear approximations of functions. Okay, so where where am I going with all this? Well, it turns out that even when you have a n dimensional input and an n dimensional output, you can compute the Jacobian. And the way you compute the Jacobian is, you can do it both analytically and numerically, I\u0026rsquo;m doing it numerically, you simply just, you know, take all your inputs, you change one of them to be perturbed slightly. And then you keep doing this for every single input until you eventually get to the end, right. And so every time you perturb a different input slightly, you\u0026rsquo;re getting another column of the Jacobian, I actually always forget whether or not it\u0026rsquo;s rows or columns, but I did look this up for the podcast. Similarly, when we do the symbolic derivative using our backwards formulas, all we need to do is for every output, because remember, this is a possibly n dimensional output, try saying, hey, what\u0026rsquo;s the derivative, what\u0026rsquo;s the gradient that affects this particular output, or the next one or the next one, and this one will help us reconstruct the Jacobian row by row. And so now, well, suppose you didn\u0026rsquo;t get all of that, right? At the end of the day, we\u0026rsquo;re setting up this property based test. And what are we doing? Well, we\u0026rsquo;re just, you know, taking our two implementations, which know how to compute the Jacobian, and then just checking if the Jacobian actually equals each other. Or at least, that\u0026rsquo;s what we used to do. So Alvin Desmaison and Jeffrey Wang came up with a pretty interesting technique for making this faster, because what\u0026rsquo;s basically happening is you\u0026rsquo;re repeatedly running your finite difference slash your backwards derivative on every single input slash output until you\u0026rsquo;ve read out every row slash column of the Jacobian. And then you\u0026rsquo;re just doing the check on the Jacobian. So this is like very precise, right? Like the Jacobian is fully specified by once you read out each thing, because it\u0026rsquo;s linear, right? And the magic of linear functions is they can be fully characterized as a matrix of the appropriate dimensionality. But remember, this is property based testing, right? We\u0026rsquo;re not even getting full verification that anything is right, we\u0026rsquo;re testing that the gradients line up on various points in the function space. And in fact, it\u0026rsquo;s sort of the least of our worries, whether or not any particular approximation of linear approximation is correct, like we don\u0026rsquo;t really need to check it in all that detail. Realistically, we will figure it out in the end. So this leads to the idea behind fast grad check, the idea behind fast grad check is, hey, we have this matrix, the this implicit Jacobian matrix. And previously, we were, you know, painstakingly reconstructing each of the rows slash columns, because that\u0026rsquo;s what our things gave us. But in fact, we don\u0026rsquo;t need to reconstruct everything. Instead, all we need to do is compute some sort of linear combination of this with some randomly sampled vector. And well, as long as these vectors are similar, then we know that the matrices are very likely to be similar. The reason why this works involves a bit of math. And I encourage you to look at the quoted resources, which talk about this in more detail. But essentially, what\u0026rsquo;s going on is we\u0026rsquo;re computing either a JVP or a VJP, depending on whether or not we\u0026rsquo;re doing a backwards formula, that\u0026rsquo;s the VJP, or we\u0026rsquo;re doing finite differences, that\u0026rsquo;s the JVP. And what you\u0026rsquo;ve got in this case is you\u0026rsquo;ve got the Jacobian multiplied with a vector, but on one side or the other, depending on the case you are. So you just multiply it by a different vector on the other side and make sure it\u0026rsquo;s consistent in both cases, then you end up with a VJU. And that is going to be a very small quantity and very easy to compare. Another analogy for this situation, which might be useful if you remember this from your probabilistic programming classes, is Freewald\u0026rsquo;s algorithm. So that\u0026rsquo;s given two matrices, and you want to multiply them together, you\u0026rsquo;ve got some result, and you want to see if this result is actually the correct one. So the naive way to do this is to actually just go ahead and do the matrix multiply between A and B, and then compare the elements point-wise, giving you C. But what you can do instead is you can just multiply the matrices by some vector, and then by the properties of associativity, you can multiply B by the vector first, and then multiply that vector by A, and that gives you a simple vector, whereas C multiplied with a vector directly. And then you don\u0026rsquo;t have to actually do a matrix multiply. You just are doing easier to do operations that are simply matrix vector different. So that\u0026rsquo;s the main idea behind grad check. If you are very interested in automatic differentiation, I highly recommend learning what Jacobians are, what JVPs and VJPs are. Unfortunately, a podcast is not a very good vehicle for mathematical understanding. So if you didn\u0026rsquo;t really understand all that, that\u0026rsquo;s okay, you\u0026rsquo;re gonna have to spend some time with the textbook. It\u0026rsquo;s just the name of the game. But a higher meta principle here is that property based testing is pretty cool. Yes, it can be hard to do correctly sometimes, right? Like you need to make sure you, you know, run your random samples deterministically, so that you always get the same result in CI. And you also need to make sure you design your properties and your random numbers really well. Because if you don\u0026rsquo;t, you\u0026rsquo;re just going to get nonsense. But grad check is a really example of a really elegant way of using math, where we\u0026rsquo;re basically like taking advantage of the fact that you know, there\u0026rsquo;s like this adjoint thing going on. And that\u0026rsquo;s some that relates the two different ways to do derivatives, and then using that to basically test all of our functions. So we basically don\u0026rsquo;t do tests for gradients by hand, we just rely on grad check to tell us if we got it right or not. Okay, that\u0026rsquo;s everything I wanted to say for today. Talk to you next time.\nEP49 Asynchronous-versus-synchronous-execution Asynchronous-versus-synchronous-execution Hello, everyone, and welcome to the PyTorch Dev Podcast. Today, I want to talk a little bit more about blocking versus unblocking APIs in PyTorch and its implications on various design questions in PyTorch. In the CUDA podcast that I gave a long time ago, I mentioned how CUDA is asynchronous. That is to say, when you do operations on CUDA tensors, they don\u0026rsquo;t execute immediately. The program will actually return from the function you call before it\u0026rsquo;s actually done doing the operation in question. Instead, in the background, your GP will be chugging away doing the actual computation in question, and your actual Python program is allowed to run ahead and figure out what the next thing that it needs to do in order to execute the next operation is. This is in contrast to CPU execution, which is synchronous. So when you ask for a CPU operation on something, well, you\u0026rsquo;re going to wait until the CPU operation is entirely done before moving on to the next thing in question. Asynchronous execution for CUDA is pretty nice because it means that we aren\u0026rsquo;t bottlenecked by the Python interpreter overhead so long as we queue enough work for the GPU to do. You just have to wait until you\u0026rsquo;ve started up actually doing computation, and then any further overhead from the Python side program can be covered up as long as, you know, you\u0026rsquo;ve got enough work to do because you\u0026rsquo;ll probably hit and queue the next piece of work before the GPU is actually ready to do it. On the flip side of the coin, it\u0026rsquo;s nice for CPU to be synchronous because, well, it means that, you know, once you actually have a CPU tensor, it\u0026rsquo;s actually got the honest-to-goodness data. So if you want to, say, FFI it out to some other system by passing on the raw data pointer, there\u0026rsquo;s nothing special you have to do. It just works. And, of course, it\u0026rsquo;s a lot easier to implement a synchronous API than an asynchronous one because then you have to decide all sorts of questions about, you know, how exactly you\u0026rsquo;re going to notify the threads that something is ready, how exactly you\u0026rsquo;re going to, like, queue things to execute, and, all in all, it just removes a bunch of implementation complexities that you have to deal with. By the way, CUDA, by default, pulls, but, you know, you can actually change out how exactly it does the synchronization between the thread that\u0026rsquo;s actually executing things and your native thread by toggling a special configuration in the CUDA API. So both of these paradigms make sense. When you operate exclusively in CPU or exclusively in CUDA, you know, there isn\u0026rsquo;t too much to worry about. But there are a bunch of places in our API where we interact between CPU and CUDA. And this is the point at which it actually is a little non-trivial to deal with the impedance matching between these two paradigms. So to look at one particular example, let\u0026rsquo;s look at the non-blocking argument on the to method on tensor. So what does this do? Well, it says when normally we have a conversion from a CPU tensor to a CUDA tensor or vice versa, we will wait until the conversion is completely done before returning from this function. And non-blocking says, actually, don\u0026rsquo;t bother waiting. Just go ahead and return immediately while the, you know, CUDA driver is doing this asynchronous update. And let me go ahead and do other things in the Python program. So it doesn\u0026rsquo;t take too long to realize why we don\u0026rsquo;t default to non-blocking execution by default. Let\u0026rsquo;s think about the CPU to CUDA case. So the CPU to CUDA case is not such a big deal, right? So you have some memory and you want to transfer it to CUDA. And, you know, like your CUDA kernels are already going to be asynchronously executed after this particular hosted device copy happens. So what\u0026rsquo;s the big deal? Well, there are two problems. One is that when CUDA does memory transfer, it needs to actually have the memory in some location so that the GPU hardware can actually direct memory access it out of the RAM in your actual CPU. And so to do that, you need some special memory called page locked memory. And the way you get that is using a pin memory allocator in PyTorch. That\u0026rsquo;s from the CUDA API. So you can\u0026rsquo;t do non-blocking CPU to CUDA or vice versa operations by default. You actually need your CPU tensors to live in pinned memory. And pinned memory isn\u0026rsquo;t free because, like, when you pin the memory, you\u0026rsquo;re saying to the operating system, you\u0026rsquo;re not allowed to, like, move it to swap. You\u0026rsquo;re not allowed to move it around. And so it reduces the amount of flexibility your operating system has to deal with your CPU memory. So by default, PyTorch doesn\u0026rsquo;t allocate pinned memory. By the way, Cafe2 did allocate pinned memory by default. But PyTorch doesn\u0026rsquo;t do that. And so you need to make sure you, like, ahead of time, actually pin things if you want to use non-blocking. But that\u0026rsquo;s not even the end of your troubles. So if you do a CPU to CUDA operation on pinned memory, you will have some, you know, thread in the CUDA runtime going ahead and copying the data from CPU to CUDA. What happens if someone goes ahead and mutates that CPU tensor while this transfer is taking place? Well, you\u0026rsquo;ll get nonsense in this situation, because it\u0026rsquo;s not like we went ahead and made a copy of the CPU buffer before we did the transfer, right? The whole point of non-blocking is to make things run faster. And, you know, the way it actually makes things not faster in this particular case with pin memory is we get to avoid actually having to do a copy into pin memory before we do the operation in question. So we\u0026rsquo;re reading directly out of the source tensor, zero copies. And that means you actually need to make sure that the tensor sticks around, doesn\u0026rsquo;t get deallocated, doesn\u0026rsquo;t get overridden until you\u0026rsquo;re done doing this memory transfer. And of course, ordinarily, it would be safe to override the CPU tensor immediately after the two operation returns, except remember, you said it was non-blocking, right? So it\u0026rsquo;s going to return immediately, regardless of whether or not the copy is finished or not. The reverse situation is even worse. So when you have CUDA going to CPU, ordinarily, you know, once again, this will block until everything has been copied into CPU. If you specify that to be non-blocking, then we will immediately return, we will have given you a CPU tensor, but the CPU tensor will be filled with garbage until some undeterminate time in the future when the device to host copy finishes. And in fact, the only way to properly wait for this transfer to finish is to either do a CUDA synchronize, which is just a blocking operation waiting for everything in, you know, CUDA to make its way back to CPU. Or if you want to be a little more fine grained about it because you\u0026rsquo;re running multiple streams or multiple other types of concurrency, you can set up an event on CUDA, which will trigger after this copy is done. So there are a lot of caveats here, and it is not easy to use these APIs correctly. But, you know, one of the philosophies of PyTorch is, right, like give a simple API, not a easy to use one necessarily. And so we give people all the tools they need. We have reasonably simple semantics. And in this case, you know, you\u0026rsquo;re kind of just up to your own to make sure you do everything correctly. And there is performance to be gained here. So people will use non-blocking to get that performance in the situation. There\u0026rsquo;s been a longstanding idea running around that no one has implemented yet to sort of make the situation a little better. And it\u0026rsquo;s called async CPU, right? So I talked about how CPU is synchronous. And one of the reasons why it\u0026rsquo;s synchronous is it\u0026rsquo;s just easier and more efficient to implement because you don\u0026rsquo;t need any blocking mechanisms. But there\u0026rsquo;s nothing stopping us from having a CUDA-like asynchronous execution model, except all the execution is happening on CPU. So we dubbed this async CPU. The idea behind async CPU is it would be a different device, distinct from the CPU device. You would share all the kernels that regular PyTorch uses. But when you do an operation, instead of immediately going ahead and running the CPU computation to the end, we would put this in some sort of queue for some worker thread to actually execute the actual operation on. And once again, the idea is, you know, if you have multiple threads and you have a lot of work to do, you may be able to successfully have the control thread run ahead and, you know, make up for the fixed overhead of doing all the synchronizes correctly in this multi-threaded concept context to avoid, you know, once again, cover up the latency from executing Python programs. An added benefit, which is, you know, sort of drawing from the discussion we just had, is if we had an async CPU tensor, we could give a user-friendly API for CUDA-to-CPU non-blocking copies, right? So what you would do is you would say CUDA-to-CPU doesn\u0026rsquo;t return a CPU tensor. It returns an async CPU tensor. And you can now just directly run operations on it and rest assured that those operations would only ever actually execute once the device-to-host copy had actually finished. So the async CPU idea has been around for a long time. And for the longest time, we never implemented. And there was a good reason why we didn\u0026rsquo;t implement it, right? Which is that adding a new device to PyTorch is a lot of work, right? We\u0026rsquo;ve got so many operators. And, you know, if you had a new device like async CPU, well, yes, you can reuse all of the kernels that you, you know, had for the CPU thing, but, you know, async somehow. But you still have to actually handle computing the metadata for the tensor you are going to return from the async CPU operation. To explain this in more detail, it\u0026rsquo;s useful thinking about what are blocking versus non-blocking operations on CUDA tensors. So we\u0026rsquo;ve already established that doing something like a device-to-host transfer, aka what would happen if you called, say, item on a CUDA tensor, is blocking, right? We have to wait until we get the actual data in a CPU before we can do anything with it. But there are a lot of also methods on tensors which are not blocking. For example, I can take a CUDA tensor and I can ask for what its size is. And this doesn\u0026rsquo;t actually cause us to synchronize with the GPU waiting for all the operations to finish. Why? Well, it\u0026rsquo;s because the size information is maintained on CPU, right? It\u0026rsquo;s not something that\u0026rsquo;s stored in CUDA. It\u0026rsquo;s stored on CPU. Many things are like this. In fact, you know, if I ask you a question like, hey, here are two CUDA tensors. Do they overlap in memory? Well, I don\u0026rsquo;t need to actually do a synchronize with CUDA because I have my CUDA data pointers and I can just look at those and the sizes and the strides and figure out if there\u0026rsquo;s a overlap or not. So the problem with async CPU, right, is that whenever you want to do an async backend, you need to actually say what the output like size and strides and everything else is without actually running the kernel in question. And that would have been a lot of work. You would have to do it for every operator. And so no one really wanted to do the work. And so async CPU never became a thing. Fortunately, there is a project called metatensors, which allows you to run the operations without doing the computation in question and figure out what the output tensors, size, dtype, everything like that looks like. So basically assuming that you have something that is like metatensors, you actually basically have most of the pieces you need for doing asynchronous CPU generation. And you just need to like stick a code gen on the problem to generate fast unboxed kernels that like put the arguments on the queue and ship them off wherever else to actually execute. So async CPU is a project that probably finally has gotten its time, but with metatensors. And, you know, it just needs someone to actually go ahead and work on it. Stuff gets really weird when you\u0026rsquo;re in the asynchronous world, though. So I want to give one more example of non-blocking making things very complicated. And that\u0026rsquo;s in the CUDA caching allocator. So the CUDA caching allocator is a way of, you know, allocating CUDA memory without actually hitting CUDA malloc, which in old versions of CUDA was very slow. So we maintain this big pile of CUDA memory. And, you know, when you ask for an allocation, we look in it, find a free spot that, you know, has enough space, and we give that to you. And similarly, if you give us back some memory, you free some memory, we just return it to the pool so someone else can use it. So the hazard in the CUDA caching allocator is what happens if someone returns some memory to the CUDA caching allocator, which, by the way, this is entirely CPU-side. There\u0026rsquo;s no synchronization involved. And then the CUDA caching allocator goes ahead and hands out the memory to someone else. But at the same time, you are still executing the asynchronous CUDA kernels that were expecting the CUDA memory to be live. So you\u0026rsquo;re in one of these very awkward situations where I can have some CUDA memory in the CUDA caching allocator. According to the state on CPU, it is free. But actually, we are still operating on that memory in a bunch of backlogged async CUDA kernels that are executing. Oof. Now, ordinarily, this doesn\u0026rsquo;t cause any problems. Because remember, CUDA is organized into these streams. So if you are only operating on a single stream, well, if you say, OK, now I\u0026rsquo;m going to reallocate this memory for someone else and trash it, that trashing operation happens in the stream and will happen after all the original CUDA kernels that were waiting to work on the original data before you get there. So, you know, the race is averted. But that\u0026rsquo;s only true if all those operations are on the same stream. And as I said, we support multiple streams in PyTorch. And so you can actually end up with the data showing up on a different stream, you know, and then there\u0026rsquo;s no guarantee of synchronization. So to handle that, we, you know, force people to also record stream information when they run their kernels. And this is how we insert the necessary events to make sure that we actually go ahead and wait for all those informations to be done before the caching allocator, you know, you can actually use this memory that you\u0026rsquo;ve gotten from the CUDA caching allocator. Okay, so that\u0026rsquo;s been a whirlwind tour of async and synchronous execution and how to put them together. That\u0026rsquo;s everything I wanted to say for today. Talk to you next time.\nEP50 Multithreading Multithreading Hello everyone and welcome to the PyTorch Dev Podcast. Today I want to talk about multi-threading in PyTorch. Threads are a mechanism for running multiple computations in parallel and it\u0026rsquo;s no accident that in PyTorch we make extensive use of threads to make computations run faster because, well, as you may know, the thing we\u0026rsquo;re doing most of the time is running lots and lots of very similar CPU computations and so it actually is typically embarrassingly parallel and we can often take advantage of multiple threads to make things run faster. That being said, threading is a surprisingly tricky and surprisingly subtle problem and in this podcast I just want to talk a little bit about some of the things to be aware about when working with multi-threaded code in PyTorch. To start off, I want to talk a little bit about how you as a user slash developer typically interact with multiple threads in PyTorch. There are, of course, APIs in PyTorch which implicitly use multiple threads in the course of their execution without any work from you at all. For example, when you run data parallel to run multiple computations in parallel over multiple GPUs on your device, when you run Autograd backwards on it, in fact, Autograd will automatically parallelize the backwards passes of each of your GPUs to run on separate threads because without doing that we would actually be unable to saturate your GPU devices. Of course, all the operators in PyTorch that you call may or may not use multiple threads and there is a mechanism in PyTorch called SetNumThreads that lets you help tell PyTorch how many threads to use when executing various operations in PyTorch, whether or not to use lots of threads or only to use one thread because maybe you\u0026rsquo;re using the threads for something else and you don\u0026rsquo;t want PyTorch using up all the cores on your system. As an operator implementer, the typical way you can parallelize code is using a handy-dandy function called Parallel4, and we\u0026rsquo;ll talk a little bit more about how that exactly is implemented in a bit. And there\u0026rsquo;s a few other bits and bobs for places where you interact with multiple threads. For example, there\u0026rsquo;s a little thread pool that has got a work queue attached to it in C10 that you can queue various things to run at some later point in time. Our RPC system makes use of this extensively. And there\u0026rsquo;s also fork join parallelism support in TorchScript where, you know, although Python, you know, doesn\u0026rsquo;t normally support multi-threaded execution, more on that in a moment as well, you can do fork joins and when run in the TorchScript interpreter, they\u0026rsquo;ll get run in parallel in that situation. So, in other words, there\u0026rsquo;s multi-threading all over the place in PyTorch, and oftentimes you don\u0026rsquo;t really have to think very hard about it because there\u0026rsquo;s usually some pattern or some pre-existing way of handling it that makes things work out. Except when you do, and then we get all these bug reports about how PyTorch is running slower, or PyTorch is using too many cores, or, you know, PyTorch isn\u0026rsquo;t respecting the number of threads people are asking about to give it. And don\u0026rsquo;t forget about, you know, just straight-up crashes and other mishandling from handling threads. There\u0026rsquo;s a lot to chew on on the subject of multi-threading. So, we\u0026rsquo;re going to just sort of walk through some of the things to be aware about in PyTorch. No discussion of multi-threading in PyTorch would be complete without a brief reminder that Python is not a multi-threaded, friendly programming language. Of course, there is a multi-threading module in Python, and you can, in fact, run your Python computations in multiple threads. You just won\u0026rsquo;t get any parallel speed-up from it because Python has this thing called the global interpreter lock, which means that at any given point in time, there may only be a single thread running instructions in the Python interpreter. So, say goodbye to your ideas of, you know, popping open multiple threads and then, you know, running your Python code in each of them to make things run faster. We only are able to get a parallel speed-up when we are not holding the global interpreter lock, which, fortunately, is most of the code in PyTorch written in C++. This is a very important thing to keep in mind because it also means that, in some cases, when we do need people to be able to write Python code that runs in parallel, we have to do very strange things to it. Of course, I\u0026rsquo;m not really going to talk about multiprocessing. I\u0026rsquo;m not going to talk about data loader. We\u0026rsquo;re just going to focus on multiple threads. But it\u0026rsquo;s good to have this idea about the gill in the back of your mind. So, one of the ways to taxonomize the uses of parallelism in a library like PyTorch is to distinguish between what we call inter-op parallelism, that is, running multiple ops in parallel, versus intra-op parallelism, where we have parallelism inside of an operator. Inter-op parallelism is kind of your good old-fashioned parallelism that you would imagine in a, say, web server or, you know, RPC service, where, you know, you\u0026rsquo;re getting a bunch of requests from the external world. These requests are all coming in concurrently, and you just need to have enough threads running to service all of these requests. And you don\u0026rsquo;t really want a single thread servicing every request because, well, you know, that\u0026rsquo;s not using up all the capabilities in your system because your system has multiple physical cores. So, you want to parallelize over the logical workload. So, inter-op parallelism refers to parallelism that sort of is external to PyTorch. It is sort of the parallelism that is over what models you\u0026rsquo;re running or how you\u0026rsquo;re running those models. There is some level of inter-op parallelism in PyTorch. When I talked about, say, for example, fork join parallelism in TorchScript, that counts as inter-op parallelism because, you know, TorchScript can run multiple TorchScript interpreters in parallel, each of them firing off various operators. Intra-op parallelism, on the other hand, is the kind of parallelism that I talked about at the beginning of this podcast where, you know, when we\u0026rsquo;re doing tensor operations, we have a lot of data we want to work over. And so, you know, when that data is sufficiently large enough, you want to split it up into various pieces of work and then just have multiple threads working on it. And that\u0026rsquo;s what APIs like parallel for are. They\u0026rsquo;re just a way of kernel writers to say, hey, you know, I\u0026rsquo;m writing this code and, you know, I think it\u0026rsquo;s pretty chunky. So, I think it would be useful if this main loop got parallelized and, you know, maybe it\u0026rsquo;s like a point-wise operation. So, it\u0026rsquo;s embarrassingly parallel and I can just have each of the threads working on their own little chunk of memory. No problem. So, we\u0026rsquo;ve got all of these APIs for working with threads. And so, how do we actually, you know, run this computation on threads? And to think about this question, we have to say, we have to ask a question that is basically, what are the thread pools in PyTorch? So, just to briefly talk about what a thread pool is, slash why they exist. A thread pool is just this concept of a number of threads that sort of are allocated once by the system and then hang around to, you know, deal with the work that you want to do. So, it\u0026rsquo;s called a thread pool because you\u0026rsquo;ve got this pool of threads available to do work for you. Why do thread pools exist? Well, they mostly exist because we don\u0026rsquo;t really trust the operating system to do a good job in efficiently allocating and deallocating the threads. Because, like, a very simple way, and in fact, you might do this in languages with better native support for threads, like in the language itself, is you might imagine just spinning up a new thread whenever you want to do a piece of parallel work and then just finishing it when you\u0026rsquo;re done. Unfortunately, you know, operating system threads are specified to have a minimum amount of stack, and of course, they have a bunch of operating system context, and so it\u0026rsquo;s actually pretty expensive to, like, spin up and spin down threads all the time. So, instead, we just have a pool of threads. We don\u0026rsquo;t. We spin them up once, and then we just reuse them as much as we need for the rest of the things we want to do. Some other conventional wisdom that comes from working with thread pools includes the idea that you want one thread per physical core in your system. Now, this conventional wisdom is a little bit of a mixed bag. So, first, let me tell you where this idea comes from. So, this idea comes from various applications where latency is a problem, and you don\u0026rsquo;t really trust the operating system thread scheduler to do a good job of making sure that your threads get scheduled in a prompt manner. There are a number of reasons why this mistrust is reasonable, but one of them is because the operating system doesn\u0026rsquo;t really know any specifics about the workload that your application is doing. And so, it does do preemptive, you know, threading when you have more threads than physical cores, and it\u0026rsquo;s actually reasonably efficient in throughput-heavy applications, but, you know, there is a quantum for when the operating system scheduler is willing to, you know, switch a thread to some other thread. And, you know, if you have an application where your latency requirements are smaller than that quantum, well, it sucks to be you. You better go ahead and implement your own thing. Similarly, operating system threads have some fixed cost for context switching. That\u0026rsquo;s why if you have too many threads in your system, that also causes the operating system to thrash because it\u0026rsquo;s spending all of its time context switching. And if you know something special about the workloads you\u0026rsquo;re doing, well, maybe you can do a little better than having to context switch in this situation. So, having a thread pool is just common sense when it comes to doing a multi-threaded application. Like, it\u0026rsquo;s the first, like, the cost of creating threads and destroying them is the first thing that will show up in a profile if you write a system in the naive way. And that leads to a problem. What\u0026rsquo;s the problem? Everyone and their dog has their own thread pool. So, let\u0026rsquo;s talk a little bit about all of the thread pools in PyTorch. So, there are a few ones that are sort of very classic. So, the classic thread pool that we use for a lot of things is the OpenMP thread pool. OpenMP is a compiler extension for conveniently writing parallel applications. You may have used it before with the Pragma OMP compiler Pragma. Although, in PyTorch, you shouldn\u0026rsquo;t do that. You should use a parallel for instead. It solves a number of problems that, you know, for example, actually using the number, right, correct number of OpenMP threads when you\u0026rsquo;re in a subthread in this situation. But, OpenMP is very common and we use it to do basic parallelism on all of our threads. And it\u0026rsquo;s very easy to get started. If you just look it up online, you can see that, you know, how to use this thing. And that\u0026rsquo;s one thread pool. I mentioned earlier that Autograd has its own thread pool, which we use to make sure we can saturate GPUs when we\u0026rsquo;re executing them. It wouldn\u0026rsquo;t really make sense to run these in the OpenMP thread pool. There\u0026rsquo;s no really way to, like, drive the OpenMP thread pool with the types of workloads that the Autograd threads have. And also, we also have some really crazy stuff implemented in the Autograd thread pool for dealing with re-entrant Autograd. That\u0026rsquo;s Autograd where we call into some custom Python function and then that function itself calls into the Autograd engine again. We have this problem where we need to preserve the C stack, but the C stack has limited space. And so if you keep calling into Autograd again and again, you\u0026rsquo;ll run out of stack space in this situation. And finally, there\u0026rsquo;s also a C10 thread pool. And this is what we use to do interop parallelism. It\u0026rsquo;s, you know, sort of our own implementation thread pool. You can put work onto it and the work gets processed by thread when it\u0026rsquo;s ready. The JIT uses it and also distributed uses it. Although distributed also happens to fire up a bunch of its own threads for various tasks that it needs to do. And, of course, we use a number of libraries to do various acceleration for many of our operators like mkldnn and nnpack. And all of these libraries also need a thread pool of some sort because, well, you know, being able to paralyze your operators is really, really helpful. For some libraries like mkl, they just used OpenMP. And so we actually just get to share that thread pool with our own uses of OpenMP. But there\u0026rsquo;s also some applications that have their own thread pools and some applications that, you know, to their credit, allow you to explicitly specify what thread pool you want to use. The fact that libraries come with their own thread pools that they want to use makes it difficult to change what the thread pool implementation is. So OpenMP is not the only game in town when it comes to, you know, sort of lightweight multi-threading inside of operators. There\u0026rsquo;s also another library by Intel called TBB, Thread Building Blocks, which is an alternate implementation of thread pools that has some nice properties. And TBB is cool, and we actually, Christian Perch, spent some time looking into whether or not we could use it in PyTorch. And in the end, we couldn\u0026rsquo;t because, well, mkl is compiled against OpenMP. And so, well, we are stuck using OpenMP because, well, you know, that\u0026rsquo;s just what we\u0026rsquo;ve got to do. So I hope this proliferation of thread pools explains to some small degree why when you ask PyTorch to set the number of threads to blah, it\u0026rsquo;s actually not so simple a thing to implement. Because it\u0026rsquo;s not just a matter of, like, going to the one place where the one true thread pool is set and changing the number of threads there. No, we have to go to every thread pool and modify them. And if we forget one or someone, you know, slips in a new thread pool when we aren\u0026rsquo;t looking, then this thing won\u0026rsquo;t be respected. And so we\u0026rsquo;ve had a lot of bugs over the years, you know, sort of fixing cases where the knobs for changing the number of threads doesn\u0026rsquo;t work. But I think it\u0026rsquo;s working right now in master, which is nice. Okay, so we\u0026rsquo;ve talked about how to use threads and when you queue parallel work, how it actually gets executed via thread pools and how many thread pools there are. So what else is there to worry about about multithreading? Well, there\u0026rsquo;s also just a ton of other random stuff. Let me just go through some of it before we finish up this podcast. So one is that PyTorch will occasionally fork itself. And the reasoning for this is because, as I said, Python, you know, doesn\u0026rsquo;t support multiple threads. And so people often use multiprocessing to deal with this. And on Linux systems, people often use fork multiprocessing to deal with this situation. What do I mean by forking? Well, when you have a process, when you fork it, the process turns into two processes, one that continues, you know, at the same point it was originally, and one that goes into a condition branch that, you know, it\u0026rsquo;s got exactly the same program state as before, but it\u0026rsquo;s executing another branch on the conditional. While almost exactly the same state. It just doesn\u0026rsquo;t have any of the threads that the original process had. And this is a big problem because what if those threads were doing something important? So fork based multiprocessing is fundamentally broken in the presence of threads, but that doesn\u0026rsquo;t stop people from accidentally trying to use it when they use multiprocessing. So that\u0026rsquo;s why we always tell people to, you know, try the other multiprocessing option, spawn, which actually creates a new project process from scratch, rather than trying to fork the original process. But, you know, people do it, and, you know, the CUDA runtime, in fact, internally makes use of threads. So if you fork while the CUDA runtime is initialized, it\u0026rsquo;ll just be completely broken. And we also have some logic explicitly checking for when this happens, so that we can give a better error message than just hanging on users when this happens. Some more fun stuff. So we really like thread local state in PyTorch. Thread local state is a very modular way of adding sort of, it\u0026rsquo;s basically a really convenient way of adding an argument you pass to every function without having to actually modify every function to add that argument. So, like, whenever we have things like automatic mixed precision, or other, like, modal type things, those are implemented using thread local state. Because if you did it with a global variable, well, then, you know, these things wouldn\u0026rsquo;t be thread safe. Because you couldn\u0026rsquo;t have multiple threads with different settings of AMP being turned on versus AMP being turned off. The problem with thread local state is thread local state is specific to a single thread. So what if you, say, fork off into another thread, or you have some work and you put it off into another thread because you want to run it under parallel four? Well, you\u0026rsquo;re not going to preserve the thread local state in that situation. And sometimes that\u0026rsquo;s the wrong thing to do, because morally, you actually wanted to preserve the thread local state in the situation. We\u0026rsquo;ve had a number of bugs over the years, where we, like, forgot to preserve one piece of thread local state or another. At this point, most state gets preserved by parallel four, but there\u0026rsquo;s some places where we don\u0026rsquo;t want to do it for performance reasons. There\u0026rsquo;s an issue tracking this. It\u0026rsquo;s kind of annoying. Something to be aware of when you\u0026rsquo;re relying on thread local state inside code that runs inside parallel blocks. One last thing, multi-threading is sort of the bane of every computer science student, because it\u0026rsquo;s really, really hard to write multi-threaded code correctly. Scratch, computer science student. Bane of any engineer, honestly. And in PyTorch, we don\u0026rsquo;t really, like, do very much with multi-threading. So if, say, for example, you looked at the tensor object, we don\u0026rsquo;t give any multi-threading guarantees on it, besides that reading from a tensor is okay for multiple threads. Reading and writing from multiple threads, no good. Writing, definitely no good. With a little caveat that if you\u0026rsquo;re writing into the actual data in the tensor, well, I suppose we can let that slide, even if you\u0026rsquo;re racing a bit. Because it\u0026rsquo;s just, you know, numbers, you know, who cares if it gets corrupted? It\u0026rsquo;s just, you know, stochastic gradient descent in that situation. So, multi-threading. It\u0026rsquo;s kind of complicated. There\u0026rsquo;s a lot of thread pools. There are a lot of ways to blow your foot off. We get a lot of bugs related to multi-threading. But if you\u0026rsquo;re writing any serious, you know, high-performance computing library, it\u0026rsquo;s something you have to know about. So, hopefully this podcast has given you a little taste of, you know, what some of the PyTorch world problems and multi-threading are. That\u0026rsquo;s everything I wanted to say for today. Talk to you next time.\nEP51 Multiple-dispatch-in-torch_function Multiple-dispatch-in-torch_function Hello everyone and welcome to the PyTorch Dev Podcast. Today I want to talk about multiple dispatch in Torch Function and how you can use it to make sure your Torch Function implementations play nicely with others. So if you don\u0026rsquo;t know what Torch Function is, I highly recommend go and listen to the Torch Function podcast that I did a few weeks ago. The short version is Torch Function is a way to overload the meaning of Torch Functions when you make your own custom subclasses in Python. And so when you\u0026rsquo;re writing a Torch Function, there is an interesting problem, which is what if you subclass tensor one way and you say, I want the behavior to be this, and someone else subclasses a tensor another way and says, I want the behavior to be some other thing, and I pass both of these tensors to the same operation, like say I add a logging tensor with a unit of measure tensor. What is supposed to happen in this situation? If we look at the behavior of Python in situations like this, on normal method overloading, we realize Python is a single dispatch language. And so traditionally, there is a distinguished argument, the self-argument, for which you actually do the implementation on. So let\u0026rsquo;s say that I have, I\u0026rsquo;m adding two objects together, a plus b, well, what will happen is I will call the add magic method on a because Python orients towards, you know, preferring the first argument in this situation. And a is responsible for checking if it actually understands how to deal with the second object in question. If b is, say, a subclass of a, chances are, a is going to just go ahead and treat b as if it were an a without using any of the extra behavior from b. Of course, this can be horribly inflexible sometimes. And so Python added another way to handle situations such as what if you said one plus some object instead of some object plus one? Well, clearly, you can\u0026rsquo;t override the underscore underscore add on the one literal. So what Python also has is the right side versions of match magic methods such as our add, which say, if the implement method isn\u0026rsquo;t implemented on the first object in question, try it again with the second object in looking for the other implementation are add instead of add. And so what will happen is when you say one plus some object, first Python will attempt to run the operation using the implementation from one, one is going to say, I don\u0026rsquo;t know how to add to this some object thing. So I\u0026rsquo;m just going to return not implemented. And then Python will try again with the second argument calling our add on that argument. And this time it will work. And you\u0026rsquo;ll actually get a successful dispatch in this situation. So to recap, in stock Python, most method dispatch is single dispatch. And if you have a normal method on a function, that\u0026rsquo;s what\u0026rsquo;s going to happen. But sometimes there is a need for multiple dispatch. And Python has this sort of convention, which is, you know, well, try the operation on all of the objects in question. And you know, if one of them says, I don\u0026rsquo;t know how to do it, try it on the other one. So binary ops, and you know, ops with many tensor arguments are galore in, you know, the torch library, right, like whatever we had to deal with addition in Python, well, we also can add two tensors together. And so when torch function was originally designed as array function in the NumPy ecosystem, it was designed with an extra mechanism for making sure multiple dispatch would work in this situation. Here\u0026rsquo;s how it works. And remember, it works very similarly how Python simulates multiple dispatch and certain magic methods. When you call an operator that is torch function overloaded, for the first thing we do is we collect up the classes of all the tensor arguments in it, because that\u0026rsquo;s all of the possible implementations of torch function that may be used in this situation. We look and see if any of these classes are subclasses of other classes. This is important because, well, let\u0026rsquo;s say that I have an A and I have a B that inherits from A, and I want to add A and B together, it\u0026rsquo;s better for me to try the B first, rather than the A first, because B might have some special handling that overrides the behavior of a stock A operation. Other than that, I pick some arbitrary order to run the torch functions on just subclasses first, and then I go ahead and run them one by one. And the first time one doesn\u0026rsquo;t return a not implemented error, and actually returns an actual result, that\u0026rsquo;s when I actually return that result for real. However, torch function implementations can say, I don\u0026rsquo;t know how to deal with this, and pass on the baton to some other class that might be able to handle it later in the implementation. Unlike stock Python, we don\u0026rsquo;t have special versions of torch function if you are in the first argument or the second argument or third argument. Torch function is a class method, so it can always be called no matter what, where the class in question lives in the argument list. So, you know, as an actual implementer of torch function, you\u0026rsquo;re responsible for going over the arguments, and making sure if they are actually your object in question, or if they\u0026rsquo;re a normal tensor, or God forbid, there\u0026rsquo;s some other class that you don\u0026rsquo;t know how to deal with. So let\u0026rsquo;s imagine that I\u0026rsquo;m writing a logging tensor. And a logging tensor is very simple, because it just prints something and then just wants to go ahead and run whatever the operation was before. So logging tensor is kind of universal, right? It works in any situation. And so we don\u0026rsquo;t need to be very restrictive about what kinds of other subclasses we can deal with. So logging tensor might go ahead, look through all the arguments, find the logging tensors that are in them, log what their values are, and then go ahead and unwrap them and call the function again, on the same arguments as before. Remember, calling the same function as before, make sure that if there are other subclasses involved, those can get a chance at it, the logging tensor just removes itself from the picture. Or let\u0026rsquo;s say you\u0026rsquo;re some very special tensor that is implemented like, as a back end into some accelerator, or some custom back end, well, you\u0026rsquo;re probably not going to be able to deal with arbitrary subclasses. So what you should do in the torch function is when you are processing it, you should go through all the types that were passed in and check that they are all exactly your type or maybe, you know, a tensor type. And if you see anything you don\u0026rsquo;t support, you should return not implemented instead of raising an error or anything like that. This is not super obvious to do when you\u0026rsquo;re just copy pasting code. But if you keep it in mind, it\u0026rsquo;s actually pretty simple. It\u0026rsquo;s just a little bit of extra error checking that you need to add to torch function and make it compose well with other implementations of torch function. And of course, it\u0026rsquo;s not a magic bullet, right? At the end of the day, someone needs to be able to handle all of the arguments in question. So if you know, you have a bunch of extensions, and none of them know how to deal with each other, then well, that\u0026rsquo;s fine, you\u0026rsquo;ll just get an error saying that there wasn\u0026rsquo;t any torch function that actually implemented this. The key thing about multiple dispatch is that you can retrofit new functionality onto the system that you may not have had before. So imagine that you know, someone\u0026rsquo;s gone ahead and built a torch function subclass that does some extra behavior. And then you\u0026rsquo;re a further extender and you\u0026rsquo;re like, Oh, this is a great idea. But I if only I had another class that I could customize the behavior even more, well, that class knows about the first torch function implementation, and it can write generic implementations that work in both cases. And in this way, you can post facto add more functionality onto the system that you know, perhaps the original implementer of some class didn\u0026rsquo;t anticipate. And this is one of the things that people like a lot about multiple dispatch. It\u0026rsquo;s this ability to solve the expression problem by just, you know, putting giving people a place to put the completion of how feature A interacts with feature B. So multiple dispatch in this way is kind of cool. And remember that I said that we we always run subclasses before their parent classes, because you know, they\u0026rsquo;re more specific, but otherwise, the order of the multiple dispatch is unspecified and PyTorch is allowed to pick whatever order it wants. But in general, most operations you\u0026rsquo;re going to do on a tensor aren\u0026rsquo;t commutative. And so it\u0026rsquo;s kind of, it\u0026rsquo;s a bit tricky if you know, you actually are going to run these in any arbitrary order, and you still want them to be well specified. So what really is going to happen most of the time is most of your operations that you know, don\u0026rsquo;t know about each other are just going to say not implemented when they see something they don\u0026rsquo;t support. And it\u0026rsquo;s only really the things that you know, know about each other, they\u0026rsquo;ll have a very specific ordering in mind. But there is a situation when you do want to be able to make custom subclasses of tensors, and you want them to be composable, and you want control over the order in which they run. And this is called functorch, aka jack style composable transformations on functions. One way to think about what functorch does is it creates a bunch of new subclasses like batch tensor and grad tensor, which, you know, imbue the meaning of operations with different things, right, like batch tensor takes in what used to be a single example series of operator calls, and turns them into batch versions. And a grad tensor takes what used to be a simple forward only secretaries of calls, and then also computes the backwards at the same time when you execute those calls. The composition of these passes matter, it matters if you do a vmap, and then a grad, which is traditional good old fashioned, you know, training over batch, versus a grad, and then a vmap, which is a more exotic type of training called per sample gradients, where you actually compute a gradient for every single sample, you don\u0026rsquo;t average all together in one big loss. And the whole pitch about functorch is that these transformations are composable. So you, you know, grad can work with vmap, vmap can work on grad, and you don\u0026rsquo;t want these to actually have to know about each other, right? Like you can specify these transformations individually, and then, you know, put them together in whatever order you like. So how the heck does this play well with a multiple dispatch system, like we just described before with torch function? Well, remember that I said that although the order we call methods is unspecified, there is one thing that is guaranteed, which is we are always guaranteed to run the subclass method before the parent method. So let\u0026rsquo;s say that I want to do some composition of operations, say a vmap first, and then a grad. Well, if I want to make sure that I handle the gradients before I do any v mapping, then all I need to do is make sure the gradient class subclasses the vmap class. And of course, I might do it the other way, right? I might want to have the vmap class subclass the gradient class. And so really, what I want to happen in this situation is I\u0026rsquo;m actually just going to dynamically create new classes for whatever sequence of compositions I want. So if I want to do a vmap and then grad and then a vmap, well, I\u0026rsquo;ll just, you know, have a vmap one that inherits from grad zero, that inherits from vmap zero, or, you know, like, whatever. Fortunately, Python is a very dynamic language. And so it\u0026rsquo;s pretty easy to allocate classes on the fly. So you\u0026rsquo;ll have some implementation of this class. But when a user wants to actually use it, they actually have to, you know, set up this inheritance hierarchy that says what order the transformations relate to each other. But you know, this is not something they have to write any code for, you can just do this for them on the fly by generating the classes. And the wonderful thing about this is it says, hey, you know, functorch is this cool thing. It\u0026rsquo;s got all these transformations, they\u0026rsquo;re composable with each other. And in fact, the torch function, multiple dispatch mechanism, or really the dispatch to Python dispatch mechanism, but they\u0026rsquo;re one in the same, they\u0026rsquo;re literally implemented using the same code. This mechanism is general enough to make this work. So we don\u0026rsquo;t actually have to add any extra level or stack or anything like that to make the multiple dispatch work out in the situation. That\u0026rsquo;s pretty cool. And something Richard and I didn\u0026rsquo;t expect when we\u0026rsquo;re trying to work out what to do in the situation. It also answers some questions we had, which is what should happen if you have, you know, some functional transforms that aren\u0026rsquo;t nested in each other and are leaking between each other. And this would correspond to a subclass A of some parent and a subclass B of some parent, but A and B aren\u0026rsquo;t related at all. And you know, remember what I said about torch function, what you\u0026rsquo;re supposed to do is check your types and make sure you actually understand everything that is in there. So if you get some type that isn\u0026rsquo;t related to your current class hierarchy, you\u0026rsquo;re supposed to return not implemented error. And so we\u0026rsquo;ll correctly get the correct error case in this situation, which is that well, this is not something that\u0026rsquo;s implemented, you haven\u0026rsquo;t said how these two passes interact with each other. So we\u0026rsquo;re not going to guess one way or another. So what\u0026rsquo;s the upshot? Well, Python doesn\u0026rsquo;t have native multiple dispatch, but torch function and torch dispatch dispatch to Python, both implement a form of multiple dispatch for handling what happens when you pass multiple different subclasses to a function. It\u0026rsquo;s pretty simple, but very powerful and good enough to express all sorts of things, including Jack style, composable transformations. That\u0026rsquo;s everything I wanted to say for today. Talk to you next time.\nEP52 Batching Batching Hello, everyone, and welcome to the PyTorch Dev Podcast. Today, I want to talk about batching, a fundamental concept in PyTorch and many other numeric computing libraries. Batching is one of those very fundamental characteristics in PyTorch. And if you\u0026rsquo;re listening to this podcast and have gotten this far, you probably know a thing or two about it. But let me just take some time to explain, you know, what is so important about batching. The concept behind batching is that when we do operations in PyTorch, like adding or subtracting or multiplying, we don\u0026rsquo;t do them on single numbers. Instead, we do them on batches, on arrays of numbers. And when we do an operation, we do it many times over. In a deep learning context, when we do a batch computation, we might be doing the same operation, doing the same series of matrix multiplies, convolutions, whatever, on multiple inputs all at once, called a batch of inputs, before, you know, computing a loss and doing stochastic gradient descent in the situation. Batching has a long history. The concept of computing on arrays or vectors comes from all the way back from this language called APL, where everything was an array and you sort of only could ever do operations on it. APL\u0026rsquo;s concept of defining operations that worked on arrays rather than single elements sort of paved the way for most modern numeric computing libraries, PyTorch included. The most important thing about batching is it lets you amortize the overhead of whatever interpreter loop or top-level programming language you\u0026rsquo;re using. Because when you ask for an addition or a multiply, you\u0026rsquo;re not just doing it on one element, you\u0026rsquo;re doing it on many, many elements. And so if your batch size is large enough, if your array is large enough, then, well, you\u0026rsquo;re going to spend most of your time in some sort of optimized C code that\u0026rsquo;s handling the actual processing for each element, rather than, you know, wasting all your time in the interpreter, you know, repeatedly looping over something. So, you know, like, basically, at a higher level of abstraction, if you write code that operates on, you know, arrays rather than on single elements, we can just do a lot better job at executing it eagerly. This characteristic of batching shows up all over the place. For example, in the automatic differentiation community, prior to the rise of deep learning, many AD systems would actually, you know, perform AD at the level of individual operations on single numbers. And, well, this would actually lead to quite a lot of performance problems, because, well, you know, you\u0026rsquo;re tracking these fine-grained, you know, autograd histories through every single element in, you know, maybe some sort of physics simulation. And so when we do autograd in PyTorch, we actually track automatic differentiation at the level of batched operations, not individual operations. And that reduces a lot of the record keeping we have to do, because, well, if you have a 10,000 size array, we still only record one piece of information for the autograd of operations involving that array. Given the importance of batching for running code efficiently, you might imagine that it would be easy to write batched code in PyTorch. And, well, you\u0026rsquo;d be half right. So in a previous podcast episode, I talked about a vmap, a new feature in PyTorch for taking code that\u0026rsquo;s written in a per-example way and converting it into its batched version without requiring any changes. Vmap is pretty cool. You should go listen to that podcast if you\u0026rsquo;re interested in it. But, you know, people were writing PyTorch models way before the days of vmap. And there were, you know, simpler ways of writing batched computations. Namely, you just took operations that knew how to handle batch operations and you strung them together. And so if you\u0026rsquo;re just doing simple operations like, you know, point-wise operations, this wasn\u0026rsquo;t too difficult because, well, if you add together a tensor of size 2 with another tensor of size 2, that\u0026rsquo;s the same thing if you turn it into a batch where you take a tensor of size n by 2 and add it to another tensor of size n by 2. Nothing changes in the way you write the operation in this situation. But it\u0026rsquo;s a little too much to ask for every PyTorch operator to work in the same way. And in fact, if we look at all sorts of operators and we try to classify what their batching behavior is, you\u0026rsquo;ll quickly find that there is a lot of variation. So there\u0026rsquo;s a few cases that are very regular. So one is this point-wise situation, right? And in fact, there\u0026rsquo;s a sort of more general case of this, which are just functions that take arbitrarily many batch dimensions and functions that are willing to broadcast. Broadcasting, by the way, is this thing where if you don\u0026rsquo;t provide enough elements compared to someone else\u0026rsquo;s batch, we will so-called broadcast the element. Namely, we\u0026rsquo;ll stamp out multiple copies to sort of match up the size in question. This is really useful, for example, if you have an array and you want to add 2 to it. Well, 2 is not the same size as a, you know, 10 by 10 array, but we\u0026rsquo;ll just broadcast 2 into a 10 by 10 array that just contains a lot of 2s and then add them together. So functions that, you know, take many batch dimensions and are willing to broadcast, these are typically just the point-wise functions. And these are very well-behaved and it\u0026rsquo;s very easy to, you know, do batch computations with them. Some functions, however, only take one batch dimension. And you\u0026rsquo;re going to have to actually kind of look at the documentation to figure out if this is the case or not. There really isn\u0026rsquo;t any rhyme or reason. A lot of this behavior is simply inherited from the old days in LuaTorch where, you know, like someone was writing in the kernel and it was for neural networks. And, you know, usually you only have one batch dimension in neural networks, just the batch of the inputs you\u0026rsquo;re processing over. So they didn\u0026rsquo;t really need more batch dimensions. So you\u0026rsquo;ll have some functions that only take one batch dimension. Some of these functions, you know, are even like they will take an optional batch dimension. So if you just leave that dimension off, they\u0026rsquo;ll just assume that you just wanted to operate on a batch size of one. And some functions are really weird. Like take, for example, torch.matmol. Depending on the dimension size of each of its inputs, it might do a matrix multiply, it might do a dot product, it might do a matrix vector product, or it might do some sort of batched computation. And there\u0026rsquo;s like a bullet list saying what happens in each of these situations. So it\u0026rsquo;s no surprise people really like using vmap, because, well, vmap just sort of abstracts all this information away. But, you know, we have to pay the piper somehow. And so the cost of implementing vmap is we actually have to write all of these batching rules to like figure out, you know, how exactly to put things together. And I talk a bit more about that in the vmap podcast. What I want to talk about today is I want to compare and contrast batching operations with how it\u0026rsquo;s done in NumPy. Because in NumPy, actually, over the years, NumPy has developed a little more structure on batching and broadcasting operations. And they call these the structure ufunks. And I just want to explain what a ufunk is, because it\u0026rsquo;s a pretty useful concept. And all of API concepts from PyTorch were taken from NumPy. We don\u0026rsquo;t actually have a direct concept of ufunk. But it\u0026rsquo;s one of the things we\u0026rsquo;re considering adding in the near future. So a ufunk, or universal function for short, is NumPy\u0026rsquo;s way of referring to any function that, you know, has a number of well-defined properties that make it work very regularly. And what do I mean by that? Well, ufunks are functions that have batching behavior. So that is to say you can add more dimensions to their beginning. And you can, you know, broadcast them if the dimensions don\u0026rsquo;t line up exactly. And they also support some amount of typecasting. So if you pass in some inputs that don\u0026rsquo;t have exactly the same types, a ufunk will know how to promote the type and, you know, like get some common type to do the computation in. So the concept behind a ufunk is really just, you know, taking some primitive operation like an add between two elements and then turning it into a vectorized operation that has, that can actually operate on as many dimensions as you want. And if this sounds familiar to you, it should because tensor iterator is basically an implementation of, you know, turning a C++ functions into what are basically ufunks in PyTorchic. So we don\u0026rsquo;t call them ufunks. And, you know, actually ufunks in NumPy have a bunch of other interesting properties. For one, they have a bunch of other variants. So when you talk about NumPy.add, there\u0026rsquo;s actually also a NumPy.add.reduce. And what that function does, it\u0026rsquo;s a function attribute on side of the NumPy.add function, is it takes, you know, your reduction dimension and reduces it using the operation that is the one from the ufunk, namely addition. So how come ufunks aren\u0026rsquo;t just an internal implementation detail in NumPy? I mean, you know, tensor iterator is something that you have to know about if you\u0026rsquo;re a PyTorch developer, but it\u0026rsquo;s not a user-visible concept. I asked Ralph Gommers, a NumPy maintainer and one of our collaborators at Quantsite, about this. And he gave me some very interesting information about ufunks. So ufunks are not that great for users because users find it a little strange to take a function and then take an attribute on it and then say np.add.reduce. That\u0026rsquo;s kind of weird. But because ufunks are introspectable and, you know, they have a very regular structure, they can be used in other libraries to do things that, you know, sort of wouldn\u0026rsquo;t be possible with just plain NumPy. So for example, scipy.special consists mostly of ufunks that are easy to define and they just reuse NumPy\u0026rsquo;s machinery to take these, you know, functions and then turn them into ufunks. In the same way that, you know, downstream users of PyTorch might want to use tensor iterator to, you know, make point-wise style operations. But another example of ufunks is Numba. So Numba is a optimizing compiler for Python that basically, you know, can take code that is just written in Python and then compile it to CPU or CUDA. And so when you write a NumPy operation and it is a ufunc operation, well, Numba can actually easily lower that into their IR because they know, hey, well, ufunks all operate the same way. So if it\u0026rsquo;s something that ufunks, it just needs to know, you know, what the single element operation is and then otherwise can use a common lowering behavior in the situation. One of the reasons why I personally have been thinking about NumPy ufunks recently is because we\u0026rsquo;re looking at how to rationalize our operators and sort of reduce the amount of boilerplates we will have to write in the situation. And, you know, actually exposing this concept of ufunks as a concept in our operating library is one way of saying, hey, all of these operators have the same behavior, so you can treat them in the same way. In fact, there\u0026rsquo;s an even more general concept than ufunks called a generalized ufunk. And these ufunks basically make it possible to define things that aren\u0026rsquo;t just element-wise operations like add or subtract, but things that actually do non-trivial transformations on dimensions like matrix multiply. And the concept is still kind of the same. You need to define what the, you know, sort of core operation is, right? Like a matrix multiply takes your dimensions and removes the inner dimensions and, you know, puts the outer two dimensions together. But then, once again, because we\u0026rsquo;re in a batched universe, you might want to actually batch this operation. And so the generalized ufunk says, okay, and then, you know, you can tack on as many batch dimensions as you want. And so once again, if something is a generalized ufunk, then you know at least that the batching is handled in a very regular way. So, you know, the combination of these two things means that, you know, it\u0026rsquo;s not as important to have something like vmap because, well, as long as your operators are one of these things, then, you know, you can rely on it acting the same way. Although, well, it\u0026rsquo;s still kind of nice having vmap because not everything is going to be a ufunk. Not everything is going to be a generalized ufunk. And so, you know, if you just don\u0026rsquo;t know, if you don\u0026rsquo;t have time to read the documentation, you know, vmap will just make it easy. You just don\u0026rsquo;t have to worry about it. So that\u0026rsquo;s it for batching. So batching is how we make PyTorch as an eager library efficient because we can amortize the overhead of Python over doing computations over many, many elements. PyTorch is not very regular about how batching is done on operators. It\u0026rsquo;s a very per operator thing. Some operators take many batch dimensions. Some operators only take one batch dimension. Some operators don\u0026rsquo;t take a batch dimension at all. But there is some structure to our operators. And one way to think about it is, is an operator implemented using tensor iterator or not? But some other ways of thinking about it, because PyTorch is very similar to NumPy, is, you know, what things are ufunks? What things are generalized ufunks? That\u0026rsquo;s everything I wanted to say for today. Talk to you next time.\nEP53 DataLoader-with-multiple-workers-leaks-memory DataLoader-with-multiple-workers-leaks-memory Hello everyone and welcome to the PyTorch Dev Podcast. Today I want to talk about a famous bug in PyTorch, bug 13246, aka data loader leaks memory when workers is greater than zero. This is my apology for not actually knowing how to do a data loader episode because the subject of data loaders is deep and vast and I should probably do an interview with Vitale FedUnion, our main developer working on data loader right now. So instead I\u0026rsquo;m going to just talk about this particular issue which is of interest to anyone who\u0026rsquo;s ever you know trained any models in PyTorch and talk about all the things you need to understand to know exactly what is going on with the issue, why the issue happens, and why the various fixes for it works. So at the end of this podcast hopefully you\u0026rsquo;ll know about all of these things. So to start I should explain what exactly this bug looks like from the perspective of a user. So imagine you\u0026rsquo;re trying to train some model in PyTorch. There\u0026rsquo;s a bunch of things that are important to training a model but in particular we want to look at how exactly you are getting data into the model in the first place right like your data is going to be some data set of images or audio files or whatever depending on your domain you need to somehow load it up into memory and then actually feed it into your model to do the training in question. And so that process of loading the data is done by the aptly named data loader which is responsible for you know getting this data from wherever it is doing some pre-processing on it and then formatting it into PyTorch tensors so that we can actually use it for actual you know processing. So the bug looks like this. So data loader has this feature called num workers which lets you parallelize the data loading process over multiple processes. This is pretty handy because sometimes you are CPU bound on the you know number of pre-processing steps you can do and so farming them out to a bunch of you know separate processes can help make sure that your actual model you know stays full of data because maybe your GPUs are actually running way faster than the process of loading your data. This is very easy to accidentally end up in and so like paralyzing the data loading process can help in this situation. So what you do is you\u0026rsquo;ve got your data loader and you say okay I want the number of workers to be you know 8 or 18 or you know however many you think you want your parallelism to be. You start running your model. It starts training. Everything looks okay. You know it\u0026rsquo;s using a lot of memory but it\u0026rsquo;s within the bounds of your CPU system and you know you start doing iterations one after another and at some point you out of memory and so you run it again and you look at the memory usage and you notice the memory usage is slowly going up as you are running your training process and you\u0026rsquo;re like huh there must be some sort of memory leak in the data loader and so the issue\u0026rsquo;s original name was data loader leaks memory when numworkers is greater than zero. You\u0026rsquo;ll also notice that if you don\u0026rsquo;t set the number of workers to you know something big then the leak quote-unquote doesn\u0026rsquo;t actually happen. So that\u0026rsquo;s what the bug looks like. Now to explain where this bug comes from because actually in fact it\u0026rsquo;s not a technically a PyTorch problem it\u0026rsquo;s a problem with CPython and it\u0026rsquo;s actually a very difficult problem to resolve at the CPython level. We have to talk about a lot of concepts. So one is I need to explain you know what exactly is going on with data loading and multiple workers and why we want to do it and how this is set up. Two we need to talk a little bit about how process creation works on Linux, what fork is, what copy and write page memory is, and finally we have to talk a little bit about the CPython runtime, namely what is reference counting and what is it all about. Eagle-eyed listeners, forgive my mixed metaphors here, may notice that in fact we have talked about many of these things in previous versions of the podcast but I\u0026rsquo;m going to just talk about them over again today because it\u0026rsquo;s important to understanding what is going on with this so-called memory leak in data loader. Okay so let\u0026rsquo;s first talk about data set and data loader. So as I mentioned data loading is very important for deep learning training and sometimes it\u0026rsquo;s hard to make sure that you you know have enough data to actually keep your model busy on it and so that\u0026rsquo;s why people often want to do parallelization. Now how exactly does parallelization work in PyTorch\u0026rsquo;s existing data loader design and this is important because the way we set things up here contributes to the likelihood you\u0026rsquo;ll run into this problem. So the first thing to remember is that data loader was originally designed to be something that just works in a single process. So people just you know looked at it and tried to make something that you know would be reasonably idiomatic and make sense if you wanted to load things from a single process. So the way things tend to work in the data set is well you\u0026rsquo;ve got some data set so you need to run some constructor for it which you know initializes some stuff about the data set more on this later and then depending on whether or not you\u0026rsquo;re doing one of these iterable style or map style data sets there\u0026rsquo;s some way of actually fetching data when you want to get it in the data set in question. So a very common separation say if you\u0026rsquo;re doing training on an image model is in the constructor for the data set you load up a list of file names say for all of the file images that you know might be in your data set and you know that\u0026rsquo;s helpful because it can tell you you know how long the data set is and you know what are all the possible like indexes that you can sample in this situation. And then when you actually index into the data set to get something that\u0026rsquo;s when we actually load data from the image in question. So what does this look like right? So you like have your constructor you say okay well load up all the file names store it as a method on the stored as a property on the object and then inside the iterator for the object read out that property do some stuff with it you know read out the actual image give it to the user. So this is like the obvious way you would go about writing a data set in a single process case and one of the things that data loader wanted to do was we wanted to like keep this same code working but just on multiple workers. So how exactly do we do that right? Because like we\u0026rsquo;re accessing this data that was constructed in the data set and you know like what\u0026rsquo;s going on with all the workers in question. Intuitively what\u0026rsquo;s going on is we actually are able to access these properties on the data set from each of the workers in question even though you know we only allocated them once in the parent process. So how exactly does that work? To answer that question we need to know a little bit about how multiprocessing works and in particular how multiprocessing with fork works. So fork is a core primitive in the Unix style operating systems and what it does is it takes some process and it makes a copy of it literally a copy. So that\u0026rsquo;s why we call it a fork because you know previously there was one process now there are two processes and they are exactly the same. Well except for the fact that you when you do the fork syscall one process gets zero the other process gets one that\u0026rsquo;s how you tell if you\u0026rsquo;re the parent or the child. Now this might sound kind of crazy pants right like why would you go through all the trouble of you know copying all the memory from the first process into the next process like what\u0026rsquo;s up with that? Well it\u0026rsquo;s kind of useful right because maybe there\u0026rsquo;s a bunch of memory that you set up beforehand and then the code after the fork wants to make use of it and so well you need it in the parent process and you need it in the child process. And in fact forking is very cheap in operating systems like Linux because of an optimization called copy on write. So remember when I talked about shared memory in PyTorch and I said hey you know normally each process has its own memory but in some circumstances you can share memory between processes and that\u0026rsquo;s how shared memory CPU tensors work and that\u0026rsquo;s also how shared libraries in your operating system works. Well like a single library is loaded up once into physical memory but then mapped into multiple processes via virtual memory mapping on your operating system. Well the same applies when you do a fork. So when you do a fork we don\u0026rsquo;t actually go ahead and eagerly clone all the physical pages we just make a copy that refers to the same physical page. Now of course each of the processes that the child and the parent could go ahead and start mutating these pages and the the like sort of semantic meaning of a fork is you actually did get a copy. So if we don\u0026rsquo;t actually make a copy when someone writes to it we have to then actually materialize the copy and that\u0026rsquo;s why it\u0026rsquo;s called copy and write. It\u0026rsquo;s free as long as you only read it and if you start writing into it well now we\u0026rsquo;re going to start doing copies on these pages. So going back to the data loader well you know so what\u0026rsquo;s happening when we have multiple workers is we just fork the python process. Every process still gets access to all the stuff that you initialized in the constructor for the data set and as long as you don\u0026rsquo;t write to it which you know like intuitively you\u0026rsquo;re not doing any writing to the you know like list of file names right you\u0026rsquo;re just reading from it then you know you should be able to share this memory without actually having any problems right right well there\u0026rsquo;s one last piece of the puzzle and that\u0026rsquo;s python reference counting. In python things that look like read-only operations like oh give me you know the field of this object and assign it to a variable these so-called read-only objects operations actually do writes under the hood to the memory in question and what are these writes for therefore reference counting. Reference counting is a way of ensuring that we know how many outstanding references there are to any given piece of data so that when the ref count goes to zero we know we can deallocate it. What this means is that if you you know read some field out of an object and assign it to a new variable that didn\u0026rsquo;t exist before we\u0026rsquo;re obligated to increase the reference count of the object in question and that\u0026rsquo;s a memory write. So hopefully you can see where this is going so putting all the pieces together. So why when we you know run the data loader initially there\u0026rsquo;s not very much memory used even though we\u0026rsquo;ve spawned off all these workers. Well that\u0026rsquo;s because of the fork copy on write optimization which says is that hey when you immediately fork the process we don\u0026rsquo;t need to use that much memory because we can just you know share the pages between the processes. Of course if we start writing to those pages and that\u0026rsquo;s what happens when python reference counting comes into play then you will start actually you know writing to the pages and forcing them to be materialized. And so as you go through your data set as you process more and more items you will start touching more and more reference counts causing more and more pages to get copied to your child processes until in the worst case scenario every child process is using as much memory as the parent process. And sure that\u0026rsquo;s not a big deal if your parent process was only using you know 10 megabytes of memory but it is a pretty big deal if your parent process was using 4 gigabytes of memory and you know 4 times 10 worker processes that\u0026rsquo;s 40 gigabytes you\u0026rsquo;re probably out of memory at that point. So what can you do about this situation? And actually we can just examine various you know things that led to this problem and each of them sort of suggest a way to solve this issue. So we might say hey the problem is that we\u0026rsquo;re doing this python reference counting and you know like if we had some way of sharing data between processes without requiring you to increase the reference count when you access them that would prevent us from paging this copy on write memory into you know copies in the child processes and save us from a lot of memory usage. Well that\u0026rsquo;s not so easy to do with pure python objects. but it\u0026rsquo;s easy enough to do with other types of objects like numpy arrays and py arrow arrays. These are objects they are reference counted per se but the data in question each individual integer that\u0026rsquo;s stored in a numpy array or you know as people were doing in you know workarounds for this issue storing strings in numpy array those things inside of the array themselves are not reference counted. So as long as you don\u0026rsquo;t actually like take out a new reference to your numpy array then you can just you know index out a subset of the numpy array and that will actually just you know be an operation you can do without incurring any reference count bump. Of course even if you actually cause a reference count bump on the numpy array you might still get lucky if say the actual data for the numpy array was allocated out of line and so you you know like they lived on different pages so you only cause one page to come in but not the rest of your data. Although I wouldn\u0026rsquo;t count on that just make sure you don\u0026rsquo;t increase the reference counts on the shared data you\u0026rsquo;re accessing. There\u0026rsquo;s a bunch of other things you can do right like you can use c types to allocate raw data. You can also use any other library that you know basically wraps around a raw c representation of the data in question that doesn\u0026rsquo;t involve real python objects. Another conceptual fix to this problem is to say hey um this you know accessing a shared memory is kind of you know bogus right like um the first rule of designing distributed systems is shared memory is bad right. You want explicit cues you want to be explicitly about saying what communication you do between processes. It\u0026rsquo;s a lot easier to debug it\u0026rsquo;s a lot more scalable it you know prevents problems like this. And so that\u0026rsquo;s what the sort of data loader rewrite that Vitaly Fedunin and Erjie Aguan have been working on and specifically the data pipes concept is that instead of having this monolithic data set object that like does everything that you want to do we\u0026rsquo;ll have a bunch of composable data pipes which you can hook up with queues. And that do various stats of processing the most important thing is it\u0026rsquo;s functional and so you don\u0026rsquo;t actually have any shared state right like when I want to feed something from one data pipe to another I have to do it via an explicit queue. And that would prevent this problem now there\u0026rsquo;s one more way of solving this problem which isn\u0026rsquo;t even mentioned on the issue in question but which I discovered recently thanks to some of my colleagues at Facebook. So another way you could solve this issue is you could literally go into CPython and say hey these objects I just don\u0026rsquo;t want you to increase the reference count anymore right. I want to somehow make these objects immortal and so you know in CPython if I you know acts as an immortal object I\u0026rsquo;m just going to skip the reference count entirely. If you can somehow do that right then you could actually use honesty goodness normal Python objects in the good old-fashioned data loader API and that\u0026rsquo;s you know kind of attractive because it is kind of a pain to go and pack all your strings in a numpy arrays. Well it turns out there is a fork of the CPython interpreter called Cinder developed by folks at Facebook. I can talk about this because Cinder is actually open source you can go download it and try it out and Cinder implements an API for immortalizing the entirety of your Python heap. So the way it works is at some point in time you can say hey I think everything on this heap is going to be live for the rest of eternity and Cinder will go ahead and you know mark all those objects as immortal and now you will no longer do reference counts on them which means that if you then fork and have workers access that memory they can access it willy-nilly without worrying about reference counts. So there you have it one of the most famous quote-unquote memory leaks in data loader. It\u0026rsquo;s probably affected everyone who\u0026rsquo;s done any non-trivial processing with data loaders in PyTorch. I\u0026rsquo;m not going to say that you know PyTorch exactly is blameless here although this is technically a CPython and fork and interaction problem we probably could have done a better job designing the core abstraction in PyTorch to make it harder to actually accidentally run into this case. But it\u0026rsquo;s a pretty interesting problem one that you know is likely to show up if you do any other sort of multiprocessing and I hope this was an interesting podcast and gave you a little bit of insight about some of the complexities and interesting internal workings of working with data loaders in PyTorch. That\u0026rsquo;s everything I wanted to say for today talk to you next time.\nEP54 Half-precision Half-precision Hello everyone and welcome to the PyTorch Dev Podcast. Today I want to talk about reduced precision floating point formats, namely float 16 aka half precision and bfloat 16 brain floating point. Float 16 and bfloat 16 are important alternative precisions for floating point numbers as opposed to the ordinary 32-bit floating point representation, which are often used in deep applications to speed up computation in cases where the extra precision afforded by 32-bit floating point numbers is not necessary. I\u0026rsquo;m not really going to have time to give you a complete lowdown about everything there is to know about IEEE floating point numbers or how these formats are set up, but I do want to give a little bit of a working knowledge about some of the important points of floating point formats and also how they affect how we write code inside PyTorch, because something that you very often have to do, for example, when you\u0026rsquo;re writing a kernel is you\u0026rsquo;ll write a normal implementation, the normal mathematical way for 32-bit floating point and for 64-bit floating point, but then for half precision, you need to do something special. And we\u0026rsquo;ll talk about why you often need to do something special in these situations and what kinds of things you have to pay attention to. So to start off, let\u0026rsquo;s talk a little bit about floating point numbers, what they are, to understand why half precision does something a little unusual with floating points in normal sense. So floating point numbers are a way of representing decimal numbers, because if you\u0026rsquo;re familiar with normal computation on computers, we love integers. We use integers to represent everything, but sometimes some things can\u0026rsquo;t really be represented as integers, right? Like numbers with decimals on them, for example. And so the float in floating point numbers refers to the fact that we change the precision by which we\u0026rsquo;re representing numbers, depending on how large the number is. To understand what I mean by this, let\u0026rsquo;s think about a situation where you don\u0026rsquo;t care about floating point precision, namely storing currency. So, you know, in US dollars, you have number of cents and you have number of dollars. So, you know, I may have $10.46. And there\u0026rsquo;s always some amount of cents associated with any number, no matter how big the quantity of dollars I\u0026rsquo;m talking about, like a million dollars or a billion dollars. There\u0026rsquo;s still only is ever two decimals of precision that I need to record the number of cents in question. There\u0026rsquo;s never like sub cent quantities in typical monetary transactions. So this is a prototypical example of a fixed point, fixed point number, where you want to fix the decimal point at two, you know, two digits of precision, no matter how big the number in question is. Of course, if you\u0026rsquo;re doing something like doing a measurement between how far you are between two cities, or for example, representing a neural activation in a neural network, your neural, if you\u0026rsquo;re if your quantity is in the millions, you don\u0026rsquo;t really care about those two digits of precision after the decimal point. So the idea behind floating point is that you don\u0026rsquo;t have to, you know, store significant digits based on where the decimal is, you let it float, and you just store a fixed number of significant digits. And just what those digits are depend on how big your number is, right? So if you\u0026rsquo;re talking about a million, then you might store significant digits for the millions, the hundred thousands, the ten thousands. But if you\u0026rsquo;re storing something like one, then you might store the, you know, first decimal place and the second decimal place, and the third decimal place, it floats along with you when you have the number in question. So given this basic specification of floating point numbers, there are basically two major parameters that you can vary when you\u0026rsquo;re defining a floating point representation, right? You can say how many bits you\u0026rsquo;re going to use on the significant, aka, the like significant digits that are in your in your number, and how many digits you\u0026rsquo;re going to devote to representing the exponent, which basically just says how big the number is, right? Are you talking about ones or thousands, or millions or billions? And so we can use this to sort of understand what\u0026rsquo;s going on with 32 bit floating point numbers, 16 bit floating point numbers, and also brain floating point, because they all actually have the same semantics, just different settings for these parameters. So the significant for 32 bit floating numbers is 24. So that\u0026rsquo;s a lot of digits of precision. And so one of the sort of observations that drives lower precision floating point is that, well, you don\u0026rsquo;t actually need all those significance. So 16 bit floating point numbers only have 11 bits of significant, and B float 16 only has seven bits of significant. If we talk about the exponent instead, well, 32 bit floating point numbers have eight bits of exponent, 16 bit floating point numbers reduce the amount of exponent you have. So you only have five bits of exponent. And B float 16 actually keeps the number of bits for the exponent the same as 32 bit floating point numbers. So another way to like think about the difference between float 16 and B float 16 is float 16 sort of was like, okay, well, we need to chop off 16 bits from our representation to, you know, reduce it in size by half, we\u0026rsquo;re going to chunk some of it off of this thing. If we can, we\u0026rsquo;re going to chunk some of it off of the exponent. And then you know, we have a nice balance. B float 16 was like, we want all of the exponent bits, we want the same what we call dynamic range, the same, you know, max and min values we can represent in floating point numbers, and we\u0026rsquo;re willing to chop off tons and tons of actual precision off of the actual, like, you know, digits in question, the significant to get it. So why use half precision, or B float 16 numbers? Well, as I mentioned before, they use half the space of memory that a 32 bit floating point number uses. So this has a number of implications, right? We need to store the values of tensors in memory. And so if you can store a number in half the space, well, you\u0026rsquo;ve basically doubled the number of parameters you can store in your model. And furthermore, you know, not just, you can store more numbers in your RAM. But when you\u0026rsquo;re actually like loading up this data into your processors to actually compute on it, well, that\u0026rsquo;s half as much memory bandwidth you need in this situation as well. And oftentimes, one of the primary costs of doing deep learning inference or training is just getting the freaking values out of memory in the first place. And of course, if you only need to compute on 16 bits of data, instead of 32 bits of data, that means less silicon. And you can, for example, vectorize more easily for the same amount of silicon. Now, it\u0026rsquo;s sometimes, you know, the memory benefits, I would say for half precision are the primary benefits. And the computation benefits do help sometimes, but also sometimes they happen not to matter. And we\u0026rsquo;ll see an example of this when we talk about CUDA support for half precision. So let\u0026rsquo;s talk specifically about half precision for a moment. So what are some things to know about when you are writing code that needs to operate in half precision? So one of the like things you first figure out that\u0026rsquo;s very, very obvious is you are way, way, way more likely to overflow your floating point number than if you were doing 32 bit floating point numbers. A float 32 can store values up to 10 to the 38. That\u0026rsquo;s 38. That\u0026rsquo;s 38 zeros after, you know, three quantity. I don\u0026rsquo;t even know what quantities can go that high that I normally deal with in a day to day basis. In contrast, a float 16 can only go up to 65,504. That\u0026rsquo;s it. If you go much higher than that, they\u0026rsquo;ll just go to infinity in float 16. So yeah, got to be super, super careful. Because the dynamic range of half precision floating point numbers is smaller, when you want to do training with networks, and you want to use half precision instead of float 32, you often need to tune your hyper parameters differently, because while you need to make sure you don\u0026rsquo;t actually go outside of the dynamic range supported by half precision. One of the most common ways people use half precision is in fact, not by making their entire network operate only in 60 bit floating point numbers, that\u0026rsquo;s often just too much, it\u0026rsquo;s like too little precision, and your dynamic range is just going to get messed up in a lot of situations. But instead, by using something called automatic mixed precision, which just says, well, there\u0026rsquo;s some operations that are very unlikely to go outside of the dynamic range you want, and will only cast to float 16 and make use of the benefit, the lower memory usage in those situations. It also helps that automatic mixed precision is super easy to use, you literally write your network as if you\u0026rsquo;re writing it for 32 bit floating point numbers, and then you just turn on a flip switch that like automatically switches it without you having to do anything. Half precision has been around for a while, and it\u0026rsquo;s been available in specially in Nvidia, CUDA. There\u0026rsquo;s actually really no silicon for doing half precision computations on Intel CPUs. And so you\u0026rsquo;re most likely to see use of half precision inside CUDA programs. But actually, there\u0026rsquo;s a little bit of nuance to this, which is that you might imagine that like, you know, you put your tensors in half precision, and then you do operations on them. And you\u0026rsquo;d expect to see, you know, actual like half precision silicon being used. But in fact, in PyTorch, we don\u0026rsquo;t use any of CUDA\u0026rsquo;s half precision intrinsics, which would let you actually use the half precision operations directly in the hardware. Instead, we convert everything into floating point numbers and do the do the computations at higher precision. Why do we do this? Well, it\u0026rsquo;s because for many of our operations that we implemented for half precision, they are in fact, not compute bound, they\u0026rsquo;re bandwidth bound, and we spend more time reading the data out from memory than we do actually doing the computation on it. And in these situations, it doesn\u0026rsquo;t matter if we waste time doing conversions to and back from floating point, because, you know, we\u0026rsquo;re still waiting on the next block of memory. And so we can just, you know, do things in higher precision. And so a lot of computations in CUDA operate at this higher precision internally, only converting back to float 16, when you need to write it out back out into memory. Remember, this is still a win, because you\u0026rsquo;re using half as much memory, using half as much memory bandwidth. So what you typically expect is for a computation on half precision to be twice as fast as a computation in 32 bit for precision. And that\u0026rsquo;s just because you\u0026rsquo;re literally reading out half as much memory. That being said, in some situations, you are somewhat compute bound. A good example of this is when you\u0026rsquo;re doing matrix multiplies. And so when you do matrix multiplies, in fact, there is this thing called TF32 that newer NVIDIA GPUs implement, where they do the multiplies and matrix multiplies at an internal format. And in fact, they don\u0026rsquo;t do it in half precision, they do it in a special precision that is 11 significant digits and eight exponents, sort of like a combo of float 16\u0026rsquo;s precision and B float 16\u0026rsquo;s dynamic range. And this happens entirely internally. So you don\u0026rsquo;t you don\u0026rsquo;t see it, you\u0026rsquo;re just feeding in float 32s and getting out float 32s. But it makes things run faster. And you know, you hope that the numerics don\u0026rsquo;t change too much. So to summarize, half precision, the dynamic range is way, way small. So you\u0026rsquo;re mostly likely to see people converting half precision at very, you know, localized spots in their code, where they know they don\u0026rsquo;t actually need that level dynamic range. And you mostly only ever see half precision in CUDA on NVIDIA GPUs. Okay, let\u0026rsquo;s talk a little bit about B float 16. So as I said, B float 16 is they just took their float 32, chopped off enough significant digits until they, you know, could fit in 16 bits, and they kept exactly the same dynamic range. So floats 16, B float 16 is actually very easy to emulate, right? Because you can use normal 32 bit floating point hardware to run it, you just, you know, sort of zero out all of the digits that are below the level of precision that B float 16 would have given you, and then just run the normal float 32. So people did a number of experiments with it, and showed, hey, you know, B float 16 is great, because, you know, we got rid of all of those, you know, pesky, like, you know, very fine detailed digits in the numbers. And turns out, it didn\u0026rsquo;t matter at all, like, you know, our model still converged, because we weren\u0026rsquo;t actually making use of that precision in any good way. And so B float 16 has shown up in a lot of places. True to its name, brain floating point, it was originally designed by folks at Google for use inside the TPU. But since then, it\u0026rsquo;s shown up in a lot of places, in particular, on the latest Intel CPUs, starting with Xeon, there\u0026rsquo;s actually silicon for doing B float 16. So unlike half precision, which only ever usually shows up in CUDA, B float 16 shows up in a lot of places, it shows up in TPUs, up on your CPU. So if you\u0026rsquo;re probably looking for some lower precision training, it\u0026rsquo;s probably going to be B float 16. In fact, Intel has been working with us to extend automatic mix precision to support B float 16. So originally AMP was something developed by NVIDIA for CUDA only for half precision, and Intel\u0026rsquo;s, you know, given us a patch that turns it on for CPU, and does exactly the same thing except using B float 16 instead of half precision. Unlike in the CUDA situation, where we were typically memory bound, we are often compute bound on CPU. And so sort of the silicon we\u0026rsquo;re using is in the AVX 512, you know, vector instruction set. See also my, you know, previous podcast about vectorization. And there\u0026rsquo;s just, you know, a lot of built in support for actually doing these computations fast. Okay, so I\u0026rsquo;ve told you a lot of facts about float 16 and b float 16. What does this matter if you\u0026rsquo;re doing code in PyTorch? Well, it mostly only matters if you\u0026rsquo;re writing kernels. And so when we write kernels in PyTorch, we typically try to write it in a generic way that works for any, any type in question, right? So typically, it\u0026rsquo;s templated so that you can do it in float, and you can do it in double. And for most use cases, float versus double doesn\u0026rsquo;t really matter. You can write the same algorithm in all of these cases. But when you have float 16, or b float 16, now you actually have to pay attention to how you\u0026rsquo;re doing your internal computations. And in fact, we have two concepts for like basically internal computation types, which are important when like, you know, using the low precision floating point would result in catastrophic loss of precision, and you\u0026rsquo;d basically get wrong results. So the first concept is the act type template, a ACC underscore type. What this does is it gives you an accumulator type corresponding to the number in question. So for example, if I had int 8, the act type of int 8 is int 64. Because if I\u0026rsquo;m, you know, summing together a bunch of 8-bit integers, I will very quickly overflow 8 bits. And so we do the accumulation in 64 bits so that we can actually, you know, get the real value in the situation. Similarly, similarly, when we do accumulations on half precision floating point numbers, we need to accumulate them in 32-bit floating point. Because as I said, you\u0026rsquo;re really likely to overflow 65,000 if you don\u0026rsquo;t actually do this at a higher precision. This is very, very common, right? Like I mentioned matrix multiply using this TF32 thing. They only do that for multiplies. The accumulate still happens in 32-bit floating points. So like the the the the idea of needing to accumulate at a higher precision is common all over the place. We don\u0026rsquo;t we don\u0026rsquo;t accumulate in double precision for float on CUDA because double hardware is really, really slow. But in fact, on CPU, we still we act type goes to double in this situation. The other concept we have is op math. And that just says what the internal computation type we\u0026rsquo;re going to do. And this takes advantage of the fact that on CUDA, we\u0026rsquo;re typically memory bound, not compute bound. So in fact, most of our internal operations happen in floating point precision. And this is good for precision purposes, because if you do all your internal computation in 32-bit floating, and only convert to 16-bit floating at the end, you\u0026rsquo;re not going to have as many like you sort of catastrophic cancelation or loss of precision events from every intermediate operation in question. Of course, if you\u0026rsquo;re running enough operations, you might still want to do them in half precision, because you might be compute bound in that situation. So that\u0026rsquo;s most of everything that I wanted to talk about with half precision. There\u0026rsquo;s one last thing that I want to put in your brain, which is that reducing the number of significant bits or exponent bits is not the only way to, you know, sort of reduce the amount of memory that your parameters use. There\u0026rsquo;s another way you can do it, which is you can represent your parameters as integers entirely. And that\u0026rsquo;s called quantization. And it\u0026rsquo;s another very interesting way to reduce the memory footprint and compute costs of your models. That\u0026rsquo;s everything I wanted to talk about today. Talk to you next time.\nEP55 Tensor-subclasses-and-Liskov-substitution-principle Tensor-subclasses-and-Liskov-substitution-principle Hello everyone and welcome to the PyTorch Dev Podcast. Today I want to talk about tensor subclasses and the Liskov substitution principle. If you haven\u0026rsquo;t seen it already, I recently posted the State of PyTorch Core September 2021 edition, which basically talked about all the things that were going on inside PyTorch Core right now. And one of the things that you may have picked up reading over this is that we actually got a lot of stuff going on related to tensor subclasses. That is to say, you know, subclasses of tensor that add more different kinds of behavior for any sorts of things you might want to do. And there\u0026rsquo;s a ton of things going on here, like linear operators, like debug tensors, like Funktorch. And I wanted to pull open the cover on one of the things that we\u0026rsquo;ve been thinking about when designing how this ecosystem should look like, and that\u0026rsquo;s the Liskov substitution principle, which says some things about when it is permissible to subclass some object and when it is not permissible to. Okay, so let\u0026rsquo;s just dive straight into it. So what is the Liskov substitution principle? So you may have learned this in, you know, your undergrad class about object-oriented programming. And the definition you heard probably sounds something like this. If S is a subtype of T, then any T may be replaced with S without altering any desirable properties of programs that were previously using T. That\u0026rsquo;s a bit of a mouthful. So let\u0026rsquo;s look at an example. Let\u0026rsquo;s suppose that we have some class that implements, say, bags. So bags are sort of owner-order collections of items. But unlike sets, you can have multiple copies of an item in a bag, right? So like I might have three apples and two oranges. And if I had a set, I could only say that I have an apple and an orange. But in a bag, I could say I have three apples and two oranges. Now, if I have an implementation of a bag, I can easily reuse this implementation to implement a set. All I have to do is subclass it and say, well, whenever I insert things into the bag, if I already have the thing in the bag, I\u0026rsquo;m just not going to insert it in that situation. No problem. So this subclassing works, I can use inheritance to implement sets in this way. And it violates the Liskov substitution principle. Why does it violate the Liskov substitution principle? Well, imagine that you\u0026rsquo;ve got an algorithm and you know, it wants to do some sort of counting of objects. And so it was using a bag inside its algorithm to like put things in and then at the very end, read out what the counts of things should be. If you replaced the bag with a set, which we were sort of thinking about is a set, a subtype of a bag, if we replaced a bag with a set, then when I ran this algorithm, I would only ever count up to one for any given item that I was looking for. And that probably isn\u0026rsquo;t what my algorithm wanted to do. Barbara Liskov gives another example, which is that in the old days, when people were sort of just figuring out this object oriented programming thing, people would make claims like queues and stacks are subtypes of each other. Why did they say that? Well, you know, a queue and a stack have a push operation and a pop operation. And so you know, the methods are the same. So well, you know, they\u0026rsquo;re structurally indistinguishable from each other, right? Like they just have the same methods. So you can use one or the other. And Barbara was like, well, but that doesn\u0026rsquo;t make any sense, right? Like if I had a program, and it\u0026rsquo;s using a stack, and then I replaced the stack with a queue, my program is going to do something totally different, like because you know, last and first out and first and first out are totally different ways of going out doing things. And probably my program wouldn\u0026rsquo;t work at all if I replaced my stack with a queue. So the moral of the story behind Liskov substitution principle, and why you know, like we love to teach it in the undergrad CS curriculum, is because it shows people that, hey, subclassing is not the same thing as subtyping or behavioral subtyping, as Liskov liked to call it in the later days, right? Like just because something has the same interface doesn\u0026rsquo;t mean they\u0026rsquo;re actually substitutable. You actually have to say something about what the behavior of the program is in these situations. So I remember learning about the Liskov substitution principle and thinking to myself, well, that doesn\u0026rsquo;t sound too complicated. You know, like, this seems like a very simple thing to abide by. You know, what\u0026rsquo;s the big deal? And well, maybe it is. But in fact, you know, I would say LSP has spawned a ton of debate all over the internet about like, what exactly is meant by this. And it\u0026rsquo;s not exactly straightforward to apply the principle in any every cases. In fact, there are some very embarrassing situations where very famous software projects have violated LSP and discovered it to their detriment later. Ralph Gammers relates to me a very fun story from NumPy\u0026rsquo;s history, which is that there\u0026rsquo;s this class in NumPy called NumPy.matrix. It\u0026rsquo;s a subclass of ndarray. So it was at least originally intended to be usable in any situation where an ndarray was. And it\u0026rsquo;s basically a specialization of ndarray for the matrix situations, right? 2d. And what they did was they were like, okay, well, because these are matrices, we\u0026rsquo;re going to make multiply, like just the normal asterisk operator, mean matrix multiply in the situation. Well, even though NumPy.matrix has the same API as NumPy ndarray, it totally violates LSP because, you know, anywhere I had some NumPy program that was originally expecting to have an ndarray and expecting the star operator to give me pointwise multiplication, if I sub in a NumPy.matrix, I will suddenly get matrix multiplication. And I\u0026rsquo;ll probably just get errors in this situation. And my program will not behave the same way. And like it will have none of the, you know, properties that I wanted to have. So as a result, like every, you know, like serious NumPy function in the ecosystem first casts everything to ndarrays, just so that, you know, they don\u0026rsquo;t have to worry about someone passing a NumPy matrix. You really shouldn\u0026rsquo;t use NumPy matrix if you can get away with it. So what I think makes LSP so controversial is that we said that you can replace any T with an S without altering desirable properties. But we didn\u0026rsquo;t really say what is meant by desirable property. Barbara, at least, meant what she meant by properties was that if you were only using the API defined by the supertype, you couldn\u0026rsquo;t see the difference between using a T versus using an S. And this is a very reasonable definition, especially in an academic context. But while in actual programming languages like Python and C++, there are a lot of ways you can interact with an object. So if you say every operation that was possible on the supertype needs to be preserved by the subtype, well, in practice, there is basically no change you\u0026rsquo;re allowed to make. Like, as a simple example, in Python, I can ask what the type of an object is. And if I subclass my type, then I will get a different subclass in this situation. And therefore, it is observable that there is a difference in the situation. And therefore, no subclass is a true subtype in this situation. And to take the flip side perspective, I could say, well, you know, programs are meaningless. It doesn\u0026rsquo;t matter what a program does. All I need is for it to be type safe, or for it to not raise exceptions. And so as long as it cracks like a duck, as long as it implements all of the methods that I expected on the original object, I have no obligation to you to make the subclass actually behave in any reasonable way. And so a lot of, you know, monkey patching and duck typing in Python sort of is based on this idea, right? There\u0026rsquo;s no spec, you just subclass plus the object, override a bunch of stuff and pray that something reasonable happens. So clearly, there is a solution to this problem. And the solution to this problem is that we shouldn\u0026rsquo;t use concrete implementations of objects, as the definitions of our super types. And let instead, we should use some sort of abstract specification, and use that as the basis for deciding what behavior is allowable or not. And this is definitely, in my opinion, what Liskov had in mind when she said, well, you know, the LSP is all about not being observably different when you talked about it in terms of the super type. But of course, this was in simpler times when, you know, we didn\u0026rsquo;t have tons tons of ways to break encapsulation on objects. But of course, defining an abstract specification for what a tensor is supposed to be is not so easy. Of course, it\u0026rsquo;s easier than defining an abstract specification for what a widget factory is supposed to be because, you know, tensor has its roots in mathematics. And one could say mathematics is, you know, very much in the business of sort of abstracting away, you know, differences between objects. But at least in PyTorch, you know, we don\u0026rsquo;t have anything written down. It\u0026rsquo;s all based on off of an informal understanding of how code tends to work with tensors in practice. And that means that you really are, you know, sort of rediscovering what it means to be a tensor every time you make a tensor subclass. Of course, there are some tensor subclasses where it\u0026rsquo;s not so hard to make a determination in this way, right? Like, for example, there are a lot of types of tensor subclasses like logging tensors, or finite tensors, or nan tensors, where it\u0026rsquo;s kind of easy to see that these obey LSP, because all they do is they do the same thing a normal tensor would have done, but then with a little extra behavior on top, like printing out what operators were called, or, you know, testing if all the elements in the tensor are finite. And so the spec here is that while everything that like is the tensory behavior, that\u0026rsquo;s part of the abstract specification, and all the other things like the logging behavior, or whether or not we throw exceptions or not, that\u0026rsquo;s sort of external to the tensor specification. And most code that you write is going to, you know, be indifferent to those extra things, the extra logging or the error reporting. It\u0026rsquo;s indifferent to the error reporting, by the way, because in Python, you can actually throw exceptions, unlike in languages like Go, where all exceptions have to be handled manually. If you had to handle exceptions manually, then throwing an error would not, in fact, be a, you know, easy to add piece of behavior on top. Then there are some types of objects which mostly obey the Liskov substitution principle. But if you poke hard enough at implementation details, maybe not. And a good example of this are the linear operators from GPytorch, authored by Max Bellendot. What are these things? Well, the basic concept is that tensors traditionally store all of the data corresponding to them. But sometimes there\u0026rsquo;s special linear algebra structure associated with the tensor. And so if you store only that, or you like store that there is in fact this structure at all, in the first place, a lot of linear algebra operations can be run faster. So a very simple example of this is if you have a diagonal matrix, you don\u0026rsquo;t need to store all the matrix, which is mostly zeros, you can just store the diagonal and you want to multiply a diagonal matrix with another matrix. That\u0026rsquo;s only linear, right? Because you just zip through the diagonal and you\u0026rsquo;re done. So these also sort of obey the Liskov substitution principle in a very, you know, tight way because, well, a diagonal matrix is still a matrix, which is still a tensor. So there\u0026rsquo;s still this is a relationship and mathematically, you know, anything you can do with a tensor, you can also do on a diagonal matrix. And even if you don\u0026rsquo;t have a kernel for it, what you can do, you can just materialize the diagonal matrix into a normal dense tensor, and then do the operation. But there\u0026rsquo;s still some stuff that doesn\u0026rsquo;t work, right? Like, you can\u0026rsquo;t get out a data pointer to the contents of a diagonal tensor, and then expect, you know, the first N elements to be zero, right? Like you\u0026rsquo;re going to get if I give you a data pointer, it\u0026rsquo;s going to be do this contiguous representation. And it\u0026rsquo;s not really going to, you know, behave the same way you would have expected with a normal strided tensor. And this is sort of okay, right? Like most code written in PyTorch and Python doesn\u0026rsquo;t involve poking at raw pointers. And so for the most part, you can generally assume that code is going to behave okay, in this situation, you might still have to audit your code if you know, like maybe you\u0026rsquo;re back ending to some external C kernel. And finally, there\u0026rsquo;s tensor types that don\u0026rsquo;t really obey LSP at all, like nested tensors, which want to change the type of size in tensor so that it doesn\u0026rsquo;t return just a tuple of integers, but it actually returns some nested structure, saying what the size of all the various dimensions in your tensor are. And so technically, facilities like torch function allow for this, you can define a torch function on an object that doesn\u0026rsquo;t subclass from tensor at all. So there isn\u0026rsquo;t even any subtype relation, besides the, you know, the Python duck typing relation that all objects participate in. But it\u0026rsquo;s still rough for a tensor like this, because you might still want to use like code that was written on normal PyTorch tensors in this situation. And so you\u0026rsquo;re appealing to an even smaller subset of the tensor language, an even, you know, more relaxed set of invariants and properties that like generalizes for both nested tensors and normal tensors. And it\u0026rsquo;s just generally hard to figure out what this is supposed to mean. Things get even hairier if you actually honest to goodness subclass from tensor, because from our C++ side, we have a actually we have a very strict contract about what fields in the C++ implementation have to be filled in, you know, with actual values. And there\u0026rsquo;s very specific concrete machine types associated with them. And anyone who subclasses from tensor is obligated to fill these in, in a reasonable way. And sometimes it\u0026rsquo;s not so easy to do. But because we want to be able to inline accessors to these fields on tensor, we have this very strict, you know, behavioral requirement, that sort of makes it a little difficult to create subclasses of tensor. That\u0026rsquo;s why you have to use underscore underscore new, instead of underscore underscore init, it\u0026rsquo;s because that, you know, underlying C++ tensor object has to be allocated all in one go. There are many other subtleties that I could talk about. But I do want to relate this discussion back to LSP for one particular aspect, which is what should be the behavior of custom tensor subclasses be when you mix two different subclasses together. Like, say, I have a debugging tensor, and I add it to a diagonal tensor, like what exactly should happen in the situation. Zachary DeVito had a good comment the other day about what it means to be compositional, what it means to be compositional is that you don\u0026rsquo;t need to look at the cross product of any interaction between classes to understand what things are going to do, right. So if you have to sit down and like manually write down what it means when you cross a debugging tensor with a diagonal tensor, you\u0026rsquo;re not compositional, right, you\u0026rsquo;re writing this monolithic thing, and you\u0026rsquo;ve manually worked out what the interactions between these two things are supposed to be. If we want to be compositional, this interaction has to be worked out automatically. But how could we actually do that? Because if I am adding these two tensors together, I probably have an implementation of adding a logging tensor to a normal tensor. And I probably also have an implementation of adding a diagonal tensor to a normal tensor. But you know, that doesn\u0026rsquo;t give me an implementation of diagonal tensor added to a logging tensor. And of course, LSP says that actually, I do have a way of getting an implementation of this, right. So when I have a logging tensor, I also have a normal tensor. And so I could use that tensor in place of the tensor in the implementation that takes a diagonal tensor and adds it to a normal tensor. And similarly, when I have a diagonal tensor, I also have a normal tensor. And I could just use that diagonal tensor as if it were a tensor into the implementation of a logging tensor plus a tensor. And so via LSP, if you actually believe in it, which it\u0026rsquo;s not entirely clear that you should, um, we can actually generate a implementation that works out of the box without having to like deal with these cases individually. But there\u0026rsquo;s a problem, right, which is there\u0026rsquo;s two possible ways I can implement it, and their behaviors are actually going to be divergent. And so in general, this is kind of hard to resolve. And in fact, the only way to really resolve it, um, in a reasonable way is to do the non compositional thing, and just explicitly say what the interactions of these two tensors should be, unless you\u0026rsquo;re functor. The lesson of functor is that if we define an ordering between these two operations, and we say, we phrase each of these tensor subclasses as a way of, you know, sort of desugaring a bunch of tensor operations into a bunch of lower level tensor operations that don\u0026rsquo;t make reference to your tensor subclass, like, you know, this diagonal tensor turns into a bunch of operations on not diagonal tensors. If you have the ordering, and you have the desugaring, then you can decompose these, and it\u0026rsquo;s in a unique way, and it\u0026rsquo;s compositional. So I\u0026rsquo;m not really sure what the right answer here is in general. But my hypothesis, and when I look at NumPy, I see that there are plenty of ND array subclasses, but they mostly don\u0026rsquo;t interact with each other, is that people are going to write tensor subclasses, they are generally not going to make them compositional. And if you do want them to be compositional, well, you need to fit them into a framework, like in functor, like, you know, JAXA\u0026rsquo;s functional transformations. So that\u0026rsquo;s pretty interesting. And I hope we can develop it in more detail and share it with you when we figure it all out. That\u0026rsquo;s everything I wanted to say for today. Talk to you next time.\nEP56 All-about-NVIDIA-GPUs All-about-NVIDIA-GPUs Hello, everyone, and welcome to the PyTorch Dev Podcast. Today, I have a special guest with me today, Natalia Gimalshin, who\u0026rsquo;s going to be here to talk to us about all the various GPU architectures. Natalia, do you want to introduce yourself real quick before we start? Hi, I am Natalia Gimalshin, and I am considered a GPU expert around Facebook, perhaps undeservedly, but anyway, GPUs are going to be the subject of our podcast today. All right, so when I was thinking about topics that I wanted to bring in other folks to talk about, one of the things was sort of just, hey, there\u0026rsquo;s a lot of different NVIDIA cards out there that do all sorts of different things, and sometimes when you\u0026rsquo;re new, you hear things like A100s and V100s and how they have different performance characteristics, and I just want to talk about this a bit and sort of get a sense about what\u0026rsquo;s actually important, because if you just like pull up, say, the Wikipedia page that says about all of the devices that NVIDIA has, there\u0026rsquo;s a ton and ton of different cards. Which ones matter? Which ones don\u0026rsquo;t? How do I actually understand them? You know, so that\u0026rsquo;s what I kind of want to dig into today with Natalia. So I guess we should first start off by talking about GPU architecture. So Natalia, what exactly is a GPU architecture, at least in NVIDIA\u0026rsquo;s terms? In NVIDIA\u0026rsquo;s terms, GPU architecture has pretty much the same meaning to it as a CPU architecture. It\u0026rsquo;s just a number of capabilities that this particular generation of the GPUs has. How fast can it process floating point numbers? How fast can it process low precision numbers? How fast can it do indexing computations? How fast can it read and write memory? How many register and shared memory does it have? So all those characteristics constitute a GPU architecture. And each next one, of course, is considered to be better than the previous one. So what\u0026rsquo;s an example of one of these GPU architectures that we\u0026rsquo;re talking about? The most recent GPU architecture is Ampere. So those A100 cards, or if we are in consumer lands, then 30-something cards. That is the latest architecture that boasts, obviously, the best performance known so far. The previous architecture would be Volta and Turing cards that are still excellent cards and still used a lot around many places. And we can go back to the previous generations, but I guess we\u0026rsquo;ll do it a bit later and in a forward order and not in a reverse. What\u0026rsquo;s the difference between Volta and Turing? The difference between Volta and Turing is that Volta is mostly a data center card, and it is the first card to introduce the tensor cores. Turing is the thing that allows NVIDIA to do very fast, low-precision matrix multiplications. Turing is kind of a consumer brother of this server card, and an additional capability that Turing has is fast integer computations that can be used for very fast quantized inference. Okay, so we\u0026rsquo;ve so far talked about three GPU architectures that NVIDIA has released, Ampere, Volta, and Turing. So Ampere is the latest and greatest. And you mentioned a bunch of different things that, you know, distinguish these characteristics. So let\u0026rsquo;s talk about Ampere and Volta for now, staying in the data center. So, like, what are the big differences between these architectures? So, the big difference between Ampere and Volta, the one that\u0026rsquo;s probably most important for us, is that Ampere has introduced a couple of new data types for Matris computations. One is BeFloat16, the data type that has been used for a long time on TPUs. That also occupies 16 bits in memory, but doesn\u0026rsquo;t suffer from the same problem as the older low-precision NVIDIA type, FP16, suffered from. Because FP16 has a very small dynamic range and is prone to underflowing and overflowing. So a lot of numerical tricks have to be applied to avoid this. BeFloat16 has fewer Matrisi bits, but as many Matrisi bits as regular FP32 type. So, if you are not over-underflowing with FP32, then BeFloat16 will probably provide you with more stable numerical characteristics. And the second type that I mentioned, TF32, TensorFlow32, is a weird thing that\u0026rsquo;s aimed at DL practitioners who can get the speedups right out of the box. So, NVIDIA\u0026rsquo;s claim here is that if you are using TF32, then you don\u0026rsquo;t really have to do anything to your existing FP32 program. If it works with FP32, it\u0026rsquo;s supposed to work with TF32, except it will be much faster. And the reason it will be faster is that when GPU is doing matrix multiplication, instead of reading all your Matrisi bits, it will read just a few of them and perform lower-precision matrix multiplication. That will be much faster, but you will still be left with your 32-bit container. You will still be left with all your dynamic range, and generally, you\u0026rsquo;ll get the same results faster. So, if I don\u0026rsquo;t use any of these new features, and I upgrade from a V100 Volta to a A100 Ampere, do I expect my code to run faster? Yes, you would expect your code to run faster because the peak performance for A100 is noticeably better than peak performance of V100. V100, and that is both for bandwidth bound code, memory bandwidth for A100 is higher, and for compute bound codes, because peak compute performance for A100 is always higher. And you should be using at least one of the low-precision data types if you are running on V100 and A100, because that is their main claim to fame. If you are just doing your plain FP32 computations, you are throwing away a lot of compute power that V100 and A100 allow you. All right, so that\u0026rsquo;s cool. So, here\u0026rsquo;s a question for those of us who don\u0026rsquo;t work at Facebook. If I wanted to play around with an A100 or V100, is there any way I could actually get my hands on these cards without having to buy it? So, one thing is, you generally should not be buying data center cards, because they don\u0026rsquo;t have active cooling. You cannot put them in your desktop rig, even if it\u0026rsquo;s a very good rig. So, don\u0026rsquo;t buy them, please. You won\u0026rsquo;t have any use for them at home. But if you want to play around with them, then AWS has instances for both V100 and A100. They are not the cheapest, but probably you can find someone who would help you with this. Or, if you want to just play with the consumer equivalents of those cards, then, yes, you could buy 30 series of the GPUs. Hopefully, you can buy them by now, because a few months ago, it was a big quest. They were sold out the moment they appeared, and it was incredibly hard to buy them. And, unfortunately, I don\u0026rsquo;t know the exact situation on the ground now, but hopefully it\u0026rsquo;s better. Just to confirm, so, what we\u0026rsquo;ve been talking about, the A100 and V100s, those are the data center GPUs. But, like, when I talk to, like, gamers or, like, you know, machine learning enthusiasts who just have a few GPUs in their basement, they\u0026rsquo;re going to be buying different things. They\u0026rsquo;ll still be Ampere and Volta, is that right? They\u0026rsquo;ll still be Ampere and Turing. Volta didn\u0026rsquo;t really have a consumer-grade card. They had something, but it\u0026rsquo;s hard to find and not necessary. Okay, so I want to change the topic a little. So, when I look at the PyTorch codebase, typically I don\u0026rsquo;t see any references to Ampere and Volta specifically, except maybe in comments here or there. Instead, I see a lot of references to SMs. So, like, for example, when we build PyTorch, we can specify what set of architectures we want to build for via, like, the Torch architecture list. And usually I have to list a bunch of, like, these SM51, blah, blah, blah, blah, things like that. SM is the core part of a code architecture. So, GPU is a massively parallel processor. And to enact this parallelism, each GPU consists of a few SMs. For the first GPU generations, the number of SMs was on the order of 10, say 10 to 20. For the recent generations that we were talking about, it\u0026rsquo;s closer to 100 SMs. And each SM, in turn, is handling about 2,000 threads. So, you can see the level of parallel execution that\u0026rsquo;s going on on the GPU. And SM pretty much has everything that is needed for the GPU to process the data very fast. It has a few compute cores that would be doing your integer low precision of floating-point computations. It has on-chip memory that can be used to very quickly access and write some intermediate results. And, of course, it has the schedulers that would tell the threads when it\u0026rsquo;s time to go execute something and when it\u0026rsquo;s time to wait. So, if I actually want to talk about the nuts and bolts about, like, you know, what actually we\u0026rsquo;re targeting, I don\u0026rsquo;t talk about Ampere or Volta. I just talk about, you know, what architecture these actual SM units support in the chip. Is that right? Yeah, but there is also more or less one-to-one mapping between those SM61 or 70 or 75 architectures that you specify when you are compiling the code and more human-readable Volta during Ampere that we are talking about here. What\u0026rsquo;s your favorite way to remember what the correspondence here is? I don\u0026rsquo;t, unfortunately, have a favorite way. It\u0026rsquo;s just, you know, when you see it enough times, then you remember. Or look it up on Google, I suppose. Yeah, and on the wiki pages, there is also usually something. So, there\u0026rsquo;s one more piece to the puzzle. In talking about the hardware, like the actual, you know, silicon you get for any of these GPUs, but there\u0026rsquo;s also another part which is important, which is the CUDA version that, you know, you\u0026rsquo;re using to actually, you know, run the software stack on top of these GPUs. How should I think about the relationship between CUDA versions and the various GPU hardware that I might be using? Actually, it\u0026rsquo;s not just CUDA version. There are two pieces of software that are required for you to use your GPUs for computation. One is a CUDA-capable driver that comes with its own version and system. And another one is a CUDA toolkit, which probably you refer to as a CUDA version. And so, all these three components, that is hardware driver version and CUDA version, have to be in sync for you to be able to use your GPU. Exactly in sync? No, not exactly in sync. And this relationship have been relaxed recently, so it\u0026rsquo;s even easier to get to the working configuration than it used to be, say, a year or two ago. But generally, if you have a card of some architecture, let\u0026rsquo;s say Ampere, there is the minimum version of the CUDA toolkit that you need to be able to compile code that will run on this architecture. And for Ampere, that would be a 100 architecture, that would be CUDA 11 to release. Then, your driver also should be at least the necessary version to run this hardware, and for Ampere cards, that again would be a driver corresponding to 11 or 11.2 toolkit. Now, I was saying that it doesn\u0026rsquo;t have to be exactly in sync. Previously, your driver version had to be newer or the same as your toolkit. Now, with enhanced driver compatibility, NVIDIA allows the older version of driver for the newer toolkit, just as long as the major version for both driver and toolkit match. And also, yet another way you can run your newer cards with older software, and you still need the driver to be able to run the code. But if the toolkit that you use to compile is too old and doesn\u0026rsquo;t support the newer hardware yet, you can still compile not to a binary, but to an intermediate thing called PTX. And then this PTX would be compiled to a binary by the driver itself, and even if you have some pretty old code compiled with the old CUDA toolkit, you could rely on the driver to JIT compile it and be able to run it within your card. This is, however, not recommended because, A, this JIT compilation will be pretty slow. If you want to, for example, run Pyroge in this mode, you will have to wait for half an hour to an hour for all kernels to be compiled. And then the performance will probably be pretty bad. So you will be able to run, but you won\u0026rsquo;t be happy that you did. Half an hour, that\u0026rsquo;s really long. Yes, that\u0026rsquo;s really long, and we\u0026rsquo;re actually having big discussions whether we should disable this thing and error out altogether or whether we should allow people to do it. And there are some companies that have a hard time redistributing newer versions of software that actually rely on this being able to run old software on the newer cards. So that\u0026rsquo;s why we cannot disable it outright, even though for, in most cases, I think people would be happier at just erring out and not seeing how long it takes to do something. All right, so changing topics again. So we talked a bunch about A100s and V100s, the data center versions of the cards, because we work with, you know, a bunch of people who are running their deep learning models on big research clusters that, like, you know, have lots of GPUs of this kind. But GPU usage outside in the wild is very wide and heterogeneous. Are there other models that are worth knowing about in this market? We already talked about the consumer-grade 30 series. Anything else people should know about? Well, if they cannot get 30 series for some reason, or if they want something cheaper, then touring cards, that is, 75 series, are still an excellent thing. And they are good for gaming, not as good for gaming, not as good as 30 series, as Nvidia says. But still, they were the first one to introduce the ray tracing technology, and they still provide a pretty good performance for the compute workloads. I guess if someone wants to use GPU for their small projects, the biggest consideration is probably the amount of memory that the particular GPU has, because in most cases, your workloads would be limited by how much you can put on your GPU. Just get some recent video card, or Ampere series, make sure that it has, I don\u0026rsquo;t know, 8 gigabytes memory, at least. And at least for some small experiments, that should be enough. They used to give you free Turing GPUs, but recently, all my Colab instances that I was able to get were just K80s, which is a Kepler GPU that\u0026rsquo;s pretty old, was introduced to in 2014, if I\u0026rsquo;m not mistaken. Wow, that\u0026rsquo;s really old. Yeah, that\u0026rsquo;s really old. But I guess, as PyTurge developers, that means we do have to ship Kepler compatibility by default, don\u0026rsquo;t we? Yes, I guess since both Colab and AWS still have those K80 instances, we do have to support them. Okay, so that\u0026rsquo;s everything that I had on my topic list for today. Natalia, are there any final closing thoughts you want to give us before we close out? I do appreciate that we talked a lot about consumer-grade cards, because this is what most beginning researchers are working with, and that\u0026rsquo;s their introduction to CUDA. And I\u0026rsquo;m very happy that when NVIDIA started CUDA, they made this decision that absolutely every GPU is going to support CUDA. Not only, like, not only, like, higher-level models, but pretty much everything. And fun fact, the first ImageNet competition that Alex Kraszewski won with his AlexNet, it was trained on a couple of consumer-grade cards. So it shows you that consumer-grade is all you need, basically. That\u0026rsquo;s pretty cool. All right, well, that\u0026rsquo;s all we had to say for today. Talk to you all next time.\nEP57 Torch-vs-ATen-APIs Torch-vs-ATen-APIs Hello everyone and welcome to the PyTorch Dev Podcast. Today I want to talk about the A10 and the Torch APIs in the PyTorch library and how they affect how we think about API design as well as our intermediate representations that we send to graph mode compilers. Now you may not realize it but PyTorch actually has two APIs. The first API is the API that I\u0026rsquo;m going to call the Torch API and it\u0026rsquo;s the one you know and love. It\u0026rsquo;s the Python API that you know you use when you interact with our library as a normal PyTorch developer user. So it\u0026rsquo;s a documented Python API that uses all of the regular idioms that you\u0026rsquo;d expect from Python. And in fact we have a limited amount of programmability for this API as well via Torch function which if you don\u0026rsquo;t know what that is you should go listen to my podcast about Torch function. But basically we can override the meanings of these Python API functions including functions that are entirely written in Python that is to say they\u0026rsquo;re just plain Python and they have a little check at the front that says you know if the if any of the inputs are tensor subclasses then defer to them to figure out how to implement this function. All of these things constitute what we call the PyTorch Torch API and it\u0026rsquo;s you can also get a list of all of these methods via the Torch overrides module which gives you a bunch of overwritable functions and methods that you can actually change the behavior of when you subclass a tensor. So this is the normal API that everyone knows and loves and you might be thinking hmm if I want to work on PyTorch internals then clearly I\u0026rsquo;m going to expect to see a lot of functions that have that reflect the Torch API. Well that\u0026rsquo;s not quite right. When you\u0026rsquo;re working inside PyTorch\u0026rsquo;s internals you\u0026rsquo;re more likely to work with a different API that I\u0026rsquo;m going to call the A10 API. A10 in this case stands for the A tensor library which is a internal C++ library that you know sort of PyTorch\u0026rsquo;s Python frontend is built on top of. So the A10 API is a more limited API in the sense that instead of being the entirety of the Python language anything that is supported in Python is supported in the API. The A10 API operates on a restricted set of types called the JIT schema. This restricted set of types originated from the fact that we were working on a TorchScript compiler frontend and we didn\u0026rsquo;t want to support every single type in Python. So the JIT schema says what types that the JIT API supports but these types map both to Python as well as to C++ and they\u0026rsquo;re selected to be some limited subset that\u0026rsquo;s tractable for us to map to all of these languages. So from the start on the functions that make up the A10 API has limited set so you won\u0026rsquo;t see a function for example like a map in the A10 API. Map in the PyTorch API it\u0026rsquo;s a very obscure function but it takes a function a Python callable and runs it on every element in your tensor slowly but you know it\u0026rsquo;s something you sometimes want to do. We can\u0026rsquo;t do that in the A10 API because we don\u0026rsquo;t have a concept of a function that\u0026rsquo;s portable across languages. So there\u0026rsquo;s similar limitations like this. So the A10 API has limited type system and in fact basically every function that you can think can think of in the PyTorch API maps to one or more operations in the A10 API. Sometimes this mapping is quite obscure for example prior to Joel Schlosser refactoring our convolution implementation we had maybe 30 different internal A10 convolution operations whereas you know in the public PyTorch API there was you know one or three depending on if you count conv 1d 2d and 3d as being separate things. So the A10 API is exhaustively enumerated inside native functions.yaml and we it\u0026rsquo;s not documented like if you squint most of these will be similar to the PyTorch Python API but some of them will be different and you\u0026rsquo;ll sort of have to read the code to find out what the difference is. But the difference then is that because the A10 API is what we actually operate on in C++ most of our internal subsystems for example Autograd are written in terms of the A10 API. So for example if you wanted to look up a derivative formula for something in PyTorch you wouldn\u0026rsquo;t find a derivative formula for a function directly in the Torch API the Python API. Instead you would have to find what A10 function it mapped to and then look up the derivative formula for that A10 function. hopefully pretty obvious most of the time sometimes not so obvious. Now although we said that the Python API is overwritable via Torch function the A10 API is also overwritable by tensor subclasses but you use a different API for doing this namely Torch Dispatch and Torch Dispatch sort of also interposes at this lower level where all of the subsystems are already finished running. So it\u0026rsquo;s more appropriate for that situation when you know you want PyTorch to have done most of the work and now you just want to do a little bit of extra information in this case. Although the A10 API is primarily oriented at you know existing at the C++ level and being the you know library implementation that the PyTorch Python API is implemented on top of we also do expose A10 operations directly via the torch.ops module. The torch.ops module essentially has a sub module for every namespace of operators and the A10 operators are put in the A10 namespace. So for example if you wanted to call the native add you would say torch.ops.aten.add and that would go through a different code path than the traditional PyTorch API. You usually don\u0026rsquo;t want to use this API directly. It\u0026rsquo;s mostly intended for people who are one programming Torch Dispatch where when you get called in Torch Dispatch you\u0026rsquo;re given one of these torch.ops.aten functions to tell you hey you know this is not a regular Python Torch API this is an A10 API or perhaps when you use our custom operator registration API the torch.ops gives automatic Python bindings whereas most of the Python bindings in the traditional PyTorch API are automatically generated. So if you think about PyTorch as just an eager library it\u0026rsquo;s not too hard to understand torch versus A10 so torch is the front end it\u0026rsquo;s the Python API and internally it backends to A10 which is a lower level C++ API it\u0026rsquo;s a little more factored but it might have some more internal functions for various things we need to do and depending on what level of interposition you want in PyTorch\u0026rsquo;s internals you might use Torch or you might use A10. But there\u0026rsquo;s another way to think about these APIs and they are that way is to think of them as intermediate representation dialects. When we have PyTorch you know eager mode matters a lot but graph mode also matters and PyTorch also allows people to target you know take PyTorch programs turn them into graphs of operations and then send them to various backends and now because we have these two APIs you also have two ways you can end up with your IR you can end up with an IR that has the Torch API or you can end up with an IR that targets the A10 API and depending on your trace acquisition mechanism you\u0026rsquo;ll get one or the other. So how do you end up with the Torch API aka the Python the public API? well if you use Torch Fx tracer on that tracer operates at the Python level it actually doesn\u0026rsquo;t even go and execute any of the internal operations and it will collect up a Fx graph that contains all references to public Torch API functions. This is by the way one of the reasons why Fx is such a popular graph representation for PyTorch. It\u0026rsquo;s because you know when you look at these graphs they look exactly like what you\u0026rsquo;d expect to see you know based on what you know about PyTorch\u0026rsquo;s Python frontend. However there is a downside to this because Fx tracing operates purely at the Python level without interacting with any of PyTorch\u0026rsquo;s internal subsystems there\u0026rsquo;s some there\u0026rsquo;s some basic functionality that you don\u0026rsquo;t get when you\u0026rsquo;re working with the Fx tracer. For example, if you want to take a graph and look at the backwards for it, there\u0026rsquo;s no easy way to do this with the basic Fx tracer. And so there\u0026rsquo;s another tracer called the AOT autograd tracer which can take a Fx graph and retrace it through the C++ implementation using Torch dispatch to get out a backwards graph. But this backwards graph won\u0026rsquo;t be for the PyTorch Python API, it will instead be for the A10 API. So you\u0026rsquo;ll get it actually also uses Fx. So Fx IR can be thought of as a container format, which can have several dialects in it. And so in in this case, when you use AOT autograd, you get out a Fx graph that contains A10 operations. More concretely, when you look at the various, you know, function calls in the graph, instead of being calls to torch.add and torch.sub, they\u0026rsquo;ll be calls to torch.ops.a10.add and torch.ops.a10.sub. Actually, technically, you\u0026rsquo;ll also even know which overload you had. This A10 ops IR, Fx IR is closely maps to torch script IR, which also operates on the level of A10 operations. And then depending on your backend, you will have some backends that expect Fx IR in the torch form, and some backends that expect Fx IR in the A10 form. For example, if you have a Fx graph mode pass, for example, like the quantization path, that pass that\u0026rsquo;s going to expect code that is in the targeting the torch IR. But if you have, for example, some pass that was previously targeted torch script, for example, NV fuser, it\u0026rsquo;ll be easier to get there using the A10 IR. And so when we have these two IR dialects, we can start to think about, you know, can we transition from one to the other? Now, clearly, we can go from torch to A10, because that\u0026rsquo;s basically the process that happens when we execute our eager code. We take a bunch of user calls to the torch API, and then, you know, do some infrastructure to get it down into lower level A10 calls. And we can trace through those using any trace mechanism that operates at the A10 operator level, whether it\u0026rsquo;s torch dispatch tracing, or, you know, lazy tensor tracing, for example. But what about going from A10 to torch? Well, hypothetically, this should be possible because the A10 API is a well defined API, and the torch API is a well defined API. So you should be able to implement the A10 API in terms of the torch API. Unfortunately, no one has actually gone around and done this, but we think it would be a useful capability and we want to add it to PyTorch at some point in the near future. Another consideration is how how dynamic or static the IR produced by the various tracing mechanisms are. When you do FX tracing, the graphs you get are very, very symbolic. For example, when you call dot size on a FX proxy, you don\u0026rsquo;t get back an actual tuple of numbers. You get a symbolic proxy object that represents the sizes, but in fact, it\u0026rsquo;s just going to record your subsequent uses of the sizes. And the fact that everything in FX tracing is symbolic is one of the reasons why sometimes you can\u0026rsquo;t easily trace models because while they\u0026rsquo;re relying on actually knowing something concrete and FX isn\u0026rsquo;t willing to give that information to you. In contrast, essentially all APIs that go through PyTorch\u0026rsquo;s internal subsystems in C++ all require very concrete values for all the sizes, strides, dtypes that are involved. That\u0026rsquo;s because in our C++ implementation, we literally have, you know, lists of N64s floating around and how are you going to replace that with some sort of proxy object. Work by Nick Karaviko is working on extending our internal representation to allow for symbolic integers so that we can trace some level of dynamic shapes. This work is in early stages, but we\u0026rsquo;re hoping to get it done this year. There\u0026rsquo;s one more teaser that I want to leave you with, which is that we are looking at adding a third API. So you might be thinking, wow, why do you want so many APIs? So one reason is that the ATEN API, despite, you know, being more lower level than the Torch API is still essentially intended to be basically the the same thing as the Torch API. So for example, torch.ops.atend.ad, that\u0026rsquo;s still a broadcasting type promoting operation. And for some backends, that\u0026rsquo;s still a bit too implicit. You might want to have your type promotion and broadcasting be explicit so that the backend can easily say, oh, I see this is a this is a non-broadcasting ad or oh, I see this is a broadcasting ad. So the prim ops API is a concept where we have a even smaller, even more simple layer of operations under ATEN. Now, obviously decomposing operations like ad into their constituent type conversions and broadcasts is not good for eager mode performance. So the prim ops formulation is not intended for regular usage in PyTorch, but instead for use with compilers, which can recover performance, even if you\u0026rsquo;ve atomized a, you know, point wise operation into a lot of itty bitty small parts. And also for symbolic analysis applications where you would like to, you know, target a simpler, um, set of operations that is more, um, that\u0026rsquo;s more factored, um, and easier to understand and then have it sort of, um, take your complicated surface PyTorch program and de-sugar it into a bunch of small operations that are individually easy to analyze. So that\u0026rsquo;s a lot of the stuff that\u0026rsquo;s going on right now. And that\u0026rsquo;s everything I I wanted to tell you about today. Talk to you next time.\nEP58 Python-exceptions Python-exceptions Hello, everyone, and welcome to the PyTorch Dev Podcast. Today, I want to talk about exceptional handling in PyTorch, specifically how we handle the boundary between Python and C++ in PyTorch. So where to start? Well, let\u0026rsquo;s start off by talking a little bit about C++ exceptions. Love them, hate them, they\u0026rsquo;re kind of a very interesting language feature. So C++ exceptions are based off of the idea that, hey, we want a mechanism for doing error handling in the C++ language, which doesn\u0026rsquo;t cost anything when there is no exception. And as a result, exceptions have a very interesting performance characteristic, which is that when your code goes well, the exception handling logic doesn\u0026rsquo;t really cost you anything besides binary size. But when you do raise an exception, then things go very, very slowly. There is a very slow stack unwinding process that uses some look aside tables to figure out how far you need to go you to actually use this table, you need to take out a lock, it\u0026rsquo;s very, very slow. And because of this, and also because of the binary size bloat that\u0026rsquo;s associated with exception handling, a lot of environments, e.g. mobile, don\u0026rsquo;t really want to compile with exceptions turned on. And so, you know, you don\u0026rsquo;t really want to use exceptions most of the time when you\u0026rsquo;re writing normal C++ code. But of course, there are some situations where exceptions are appropriate. And I think PyTorch\u0026rsquo;s use of exceptions is quite appropriate. So PyTorch specifically uses exceptions whenever there\u0026rsquo;s some sort of, I\u0026rsquo;d say user error. So you know, you add two tensors together, but their shapes mismatch, we need to raise an error to the user, we do an exception in the situation, it would be a really big pain to try to manually pipe back the error status through all of our code. In this sort of exceptional situation. Now, if you\u0026rsquo;re a goal language developer, that\u0026rsquo;s the sort of thing that you\u0026rsquo;re used to doing, right? Like, hey, you know, explicit is better than implicit, but these really are edge cases. And most of the time, you\u0026rsquo;re not going to hit them. And it wouldn\u0026rsquo;t be a good thing in our code to actually have to explicitly deal with all the error handling all the time. And plus, it wouldn\u0026rsquo;t look very Pythonic. And as I\u0026rsquo;ve mentioned in earlier podcasts, you know, we\u0026rsquo;re all about writing C++ code, that looks a lot like the Python code you want to do. So these exceptions, they don\u0026rsquo;t happen normally, please don\u0026rsquo;t write code that raises exceptions and expects to catch them, right? The point of the exception is just so that we can bubble it up to Python, turn it into a regular old Python exception. And you know, usually this will fail a user\u0026rsquo;s program. But if, you know, there\u0026rsquo;s something that they actually want to do with the exception, like say, they\u0026rsquo;re in a REPL, and so you can just bring back control to the user, well, we want to give the user the ability to do that in that situation. This does sometimes cause some problems. So for example, we had a bunch of linear algebra operations that when the matrices were ill conditioned, they raised an exception. And some people, you know, caught those exceptions, because they knew that they could use some other algorithm in these situations. And this was very, very slow. And we actually added extra API\u0026rsquo;s for getting back the error status in those cases, as a Boolean, so not raising an exception in this case. So exceptions, therefore, really exceptional things, don\u0026rsquo;t use them for, you know, things that you expect to happen when your code is running normally. All right, so we\u0026rsquo;re using C++ exceptions to handle things inside, you know, the bowels of the C++ and PyTorch. But remember that once we hit the Python C++ language, boundary, we actually need them to be treated as Python exceptions. And now CPython, the Python implementation that most people use, is not implemented in C++, it\u0026rsquo;s implemented in C. And as such, it actually has no idea what is going on with C++ exceptions. So you actually have to do some conversion. So the convention in Python for handling exceptions, and because it\u0026rsquo;s C, you do have to do it for everything explicitly. And in CPython source code, it does handle everything explicitly, is you are obligated to check the return types of all functions you call. And normally, these functions will return py object pointers. But if a error was set, if some sort of Python exception was set, the object that will be returned returned is in fact, a null pointer. And there is some extra state, you know, off to the side of some global state, which gets populated with the exception info in this situation. Global error reporting state is very, you know, 90s error, no style reporting. But remember, Python has a global interpreter lock. So you\u0026rsquo;re not really at risk of some other, you know, thread stomping over your exception state in this situation. So if you return a null pointer, that means an error has happened. And there\u0026rsquo;s, you know, you\u0026rsquo;re supposed to go ahead and propagate this null pointer up until some point where exception handling can actually happen. So to interoperate between C++ exceptions and Python exceptions, it seems fairly simple, what we need to do is we need to catch the C++ exception. Before we go to the Python boundary, then we need to go ahead and, you know, take out this exception, look at it, convert it into a Python exception that we can also, you know, save to the global state saying that there\u0026rsquo;s a Python exception. And then we just need to return null pointer in that situation. Seems easy enough, right? Well, you have to actually remember to call the macro that actually does this. So in a kind of poorly named set of macros, we probably should rename these macros. They\u0026rsquo;re called handle th error and and handle th error. So when you\u0026rsquo;re writing Python binding code, you need to make sure that you, you know, start off with a handle th error, which will set up this try catch block, and then an end handle th error, which will, you know, sort of handle the end of the try catch block, including the catching exception, turning it into a Python error, and then returning null pointer. So c Python knows what\u0026rsquo;s up. But wait, there\u0026rsquo;s more. So we also use pybind 11 to do some Python binding inside of our source code. And pybind 11 has a different convention than c Python. c Python says return a null pointer, and we\u0026rsquo;ll handle it. Pybind 11 says, Oh, we\u0026rsquo;re a C++ library, we like exceptions, too. And so in fact, pybind 11 knows how to deal with exceptions. And in fact, we install a handler, handler, thanks, Peter Bell for adding this, which will know how to automatically convert exceptions into into the form that is expected by the c Python interpreter. So you don\u0026rsquo;t have to use handle th error when you\u0026rsquo;re doing pybind bindings, question mark, actually answer is no, you do, you still have to use them. But that\u0026rsquo;s another story, which we will talk about in the second part of this podcast. But yeah, so pybind 11 has a different convention. And if you\u0026rsquo;ve actually gone ahead and set the Python error state already, there is a special exception in pybind 11 that says error already set. And that\u0026rsquo;s the one that you can throw to have pybind 11 say, Oh, I see, you\u0026rsquo;ve already set the info. So I\u0026rsquo;m not going to do anything, return a null pointer in that situation. Now, there, it\u0026rsquo;s not obvious that c++ exceptions should map to Python exceptions. But we have a bunch of sort of precant exceptions, they\u0026rsquo;re all defined in c10 exceptions dot h, like not implemented error, and similar things like type error. And so if you want your c++ exception to turn into particular Python error handling class, just make sure you use the correct, you know, error class or other number macros that also let you, you know, specialize what type you get in that situation. All right, so if that was everything that handle th error did, I\u0026rsquo;d be done with this podcast in eight minutes, but it\u0026rsquo;s not there\u0026rsquo;s actually more. So, um, so exceptions are pretty nice. And you know, we like using them a lot to handle error cases. And there\u0026rsquo;s something else that\u0026rsquo;s pretty nice, which is warnings. We love warnings, uh, probably a little too much. We probably, uh, PyTorch has just, you know, sort of grown warnings over time and like people have stopped reading them and it\u0026rsquo;s bad and we should get the warnings to be less chatty. That\u0026rsquo;s a topic for another time. So, uh, warnings are pretty useful because, hey, sometimes people are doing things that are kind of bad and we don\u0026rsquo;t want to error on them, but we do want to let people know that, you know, something bad is up. Like for example, using a function that we\u0026rsquo;ve deprecated and plan to remove in the future. And a lot of this code only actually gets exercised in C++. So we want some way of reporting warnings. Now it\u0026rsquo;s easy enough to, um, you know, write a C++ warning function that just prints some stuff out to standard error, but similar to how exceptions have their own handling in Python, right? With the, you know, good old fashioned Python exceptions, warnings also have handling in Python. There\u0026rsquo;s a warnings module. There\u0026rsquo;s a concept of warnings filters and warnings handlers. And it would be nice if the warnings raised by PyTorch interoperated with his framework. And they do. So what we have is we have a way of mapping C++ warnings into Python warnings. So when you use the torch warn macro, which is the way of, you know, basically raising a warning from C++ code, what it will actually do is it will convert it into a Python warning and, you know, send it off so that you can, for example, ignore it, uh, as, as these things typically do, um, when you are actually dealing with it in your Python code. Now it used to be implemented such that, um, we would take out the global interpreter lock because remember when we\u0026rsquo;re in C++ code, we\u0026rsquo;ve released the global interpreter lock so that other threads can keep going. And, uh, so we would have to reacquire it and then, you know, fiddle around with Python state to actually raise the warning, but this sometimes caused deadlocks. So Albin, um, a few years ago submitted a patch to make this better. Um, and the idea is that, well, there isn\u0026rsquo;t really any point in reporting the warnings to user land until we actually, you know, get back to the Python interpreter. So we can basically defer all of the warnings we want to raise until we, um, you know, go back to Python. In fact, the CPython API has a dedicated function for doing this sort of thing. It\u0026rsquo;s basically at a callback, which when the next time the gill is acquired, uh, we\u0026rsquo;ll do these callbacks. And, and this thing is protected by its own very tiny lock. So you can take it out, uh, without fear of deadlocking the gill. But, um, we didn\u0026rsquo;t use that for this particular mechanism. Instead, we have our own little buffer, um, that warnings get rid into, and then we have some way of propagating to Python when we return. And how does this work? Well, we piggybacked on top of the existing handle th error macro. So how do you get some code to run when you\u0026rsquo;re exiting a code block? Well, in C++, the way to do that is RAII. So you allocate an object on the stack. And then when, you know, you\u0026rsquo;re exiting the scope by returning or by raising exception, then the destructor for this object will get called all happy, right? Well, no. So I mentioned that, uh, uh, when we have exceptions in C++, um, we turn them into Python exceptions. And so at the point in time, when we\u0026rsquo;re handling the warnings, basically feeding them into the Python interpreter, uh, we might have an active exception at this point in time. And now there\u0026rsquo;s a problem. When you print a warning to, when you, when you put a warning into Python\u0026rsquo;s warning system, you actually might be running arbitrary code. Why? Well, you need to actually construct the warning object. And there\u0026rsquo;s also some handlers, which, you know, might actually just go ahead and process the warning immediately, when you do it. And all of this code can raise errors. And so what do you do if you are raising an exception and the unwinding code also tries to raise an exception at the same time? Well, C++ has an answer for this. It\u0026rsquo;s, uh, you know, abort your program term, uh, immediately, um, you know, unceremoniously killing everything that\u0026rsquo;s going on. Well, that\u0026rsquo;s kind of bad. And we don\u0026rsquo;t really want to do that, right? We want to make sure we always get to Python in this case. So if that happens, then you have to basically not run the warning handlers, if there\u0026rsquo;s an exception being risen, because, you know, you are not going to be able to deal with another exception being raised at that point in time. And so the way we do this is just, if that happens, uh, we don\u0026rsquo;t actually give you the warnings in Python. We\u0026rsquo;ll just print them to std error in C++ and they get, and they vanish into the either. Well, you can still see them in standard error, but, um, they won\u0026rsquo;t be available in Python. And that\u0026rsquo;s pretty reasonable because this only happens when you are raising an error anyway. And remember those are exceptional situations. And so, you know, you really shouldn\u0026rsquo;t be doing that. Well, there is one subtle point though, which is that, uh, remember how I said that, uh, if you set a Python error, you know, the global, uh, flags inside, uh, um, Python, and then you return null pointer, Python will know what to do with that. That technically worked, um, before, even if you were using the handle teach error macros, but now you\u0026rsquo;re not allowed to do that because, uh, if you, um, if you are just returning a null pointer, then the warnings handler will run and it won\u0026rsquo;t know if there is a Python error or not. And it might accidentally try to raise an error again. And that\u0026rsquo;s, that\u0026rsquo;s bad. Okay. So that\u0026rsquo;s it for error handling. So if you don\u0026rsquo;t remember anything from this podcast, remember to put your handle th errors and and handle teach errors around your bindings. Otherwise your exceptions won\u0026rsquo;t work correctly or use pybine 11. But if you\u0026rsquo;re using pybine 11, you still probably want to use these macros or the nifty, um, you know, wrap, uh, warning a handler, uh, uh, function, which I will post in the podcast liner notes. If you need to look it up, um, which is just a nicer way of doing the same thing, uh, without using macros, make sure you do that because otherwise, if you raise warnings in your code, um, those won\u0026rsquo;t work either. And yes, this is probably too hard to remember and we probably should have a lint about this and we don\u0026rsquo;t really have a good linting framework. That\u0026rsquo;s a good topic for another time. All right. That\u0026rsquo;s everything I wanted to say for today. Uh, talk to you next time.\nEP59 New-CI New-CI Hello, everyone, and welcome to the PyTorch Dev Podcast. Today, I have Eli Urigas with me to talk about our new continuous integration system, which we migrated over from CircleCI to GitHub Actions. Eli, do you want to introduce yourself? Hey, everybody. I\u0026rsquo;m Eli Urigas. I work on the PyTorch DevInfor team over here at Meta, and I\u0026rsquo;ve been working on the team for probably about two years. Excited about all the CI options that we\u0026rsquo;ve been able to provide the PyTorch organization over the past couple of years. All right, so let\u0026rsquo;s get started. So the first question I have for you is, so we used to have a CI system that was on CircleCI, so why did we move to GitHub Actions? Like, if it\u0026rsquo;s working, why break it? Yeah, so that\u0026rsquo;s a great question. So this project kind of started at mid-2020, I think. We kind of identified, there\u0026rsquo;s a big cost motivation in terms of moving from CircleCI to GitHub Actions. One of the things that\u0026rsquo;s great about CircleCI is that it comes in, it\u0026rsquo;s a fully featured kind of CI system, but one of the negatives of that is that there is kind of a cost implication that comes with that, and kind of one that we foresaw growing as the PyTorch organization grew. But as well, outside of that, outside of the cost motivations, we kind of felt that there was a platform out there, like GitHub Actions, that provided a little bit more flexibility and extensibility that CircleCI just didn\u0026rsquo;t have at the time. And I think one of the biggest things for us, GitHub provides, is the API and webhook architecture that it has, allows us to do a lot of things with automation that are just not possible with other CI vendors. We talked about this with Jenkins, we talked about this with CircleCI, we had a lot of different vendors that we had talked to, and outside of all of them, GitHub Actions just hands down provided one of the best models to be able to extend their product and be able to provide automation and tooling that no other provider had. As well, one of the bigger things about GitHub Actions is the shareability of Actions just in general. We saw a great opportunity to be able to use off-the-shelf Actions as well as be able to build our own Actions and share them amongst different projects. And we see that as a big opportunity for the future for ourselves. If I recall correctly, CircleCI did have this Orbs functionality for sharing Actions. I guess it didn\u0026rsquo;t work too well for our use cases? Yeah, I tried out the CircleCI Orbs feature. I really wanted to be a big proponent of what Orbs were trying to do. Having that feature just seemed like a good idea. But unfortunately, I just was never able to get it to work correctly. I\u0026rsquo;m sure it\u0026rsquo;s a great feature for the people that use it, but unfortunately for our use case, it just didn\u0026rsquo;t provide the level of accessibility that we needed in terms of our shareability. All right. So my familiarity with the CI system is way back in the day when we initially had it on Jenkins and then when we ported it to CircleCI. So I actually know very little about how the GitHub Actions version of the CI system is set up. Can you just briefly walk me through what the major components of the CI is? Yeah, so our CI is a little bit interesting. If you worked with Jenkins, it might be a little more familiar now than it was with CircleCI, at least for the infrastructure portion. So right now, our CI is run on infrastructure that runs on AWS. It auto-scales to our needs. So if a workflow is fired off, it sends a request to our auto-scaling system, which creates a node, and then that node gets connected to GitHub, and then their provisioner or whatever actually schedules a job to be run on it. So that part should feel familiar. In terms of the way our CI is set up today with workflows, initially we used a Jinja templating type approach, which is kind of similar to what we did with the domains previously. So if you are familiar with Torch Vision\u0026rsquo;s CI configuration right now, that\u0026rsquo;s kind of an approach that we took earlier. The evolution of that is kind of related to GitHub\u0026rsquo;s feature set for actions just in general. So the reason why we chose Jinja templating first and foremost was an initial lack of reusable workflows. So when we started the migration, GitHub Actions lacked the idea of reusable workflows, lacked the idea of being able to use consolidated actions with regular actions, meaning that when we tried to do the thing about shareability, we weren\u0026rsquo;t necessarily able to do that initially. Also, there was initial lack of rerun workflow from failed, which is a feature from CircleCI that a lot of people love to use. And that led to the need to generate multiple workflow files just in case a singular workflow failed and was flaky. It didn\u0026rsquo;t affect all of the rest of the workflows from actually passing and running and reruns just in general. So right now, it should feel still a little bit familiar. We tried to use a lot of the same scripts that we had used before. So everything in the .Jenkins folder is probably still used. A lot of the scripts from the .CircleCI folder are still used. One of the things that we wanted to make sure when we did this migration was that we kept a lot of the things familiar in terms of the test scripts. We didn\u0026rsquo;t want to change those in particular because we wanted to de-risk the migration when we actually did it. Well, we probably shouldn\u0026rsquo;t be storing our build scripts in .Jenkins anymore. Yeah, that\u0026rsquo;s an item for us to fix later on. It hasn\u0026rsquo;t been a higher priority for us. We\u0026rsquo;ve been working on other features, but it is actually one of the things that we want to do work out later on. And just in general, we\u0026rsquo;re trying to make it so that our CI scripts are not vendor specific. Ideally, we\u0026rsquo;d want to move it to a .CI folder, but that work hasn\u0026rsquo;t begun yet. So I want to just make sure I understood correctly. So basically, in the bad old days when we were on CircleCI, there wasn\u0026rsquo;t a way to create a parametrized job. Actually, there was a way to create a parametrized job, right? But we needed to use templating to basically instantiate all the different versions that we wanted to do. So we\u0026rsquo;re not using templating anymore? We don\u0026rsquo;t need to do that? Right. So Michael Suho, who is an engineer who helps us out with the PyTorch DevInfra work every now and then, has done a great effort to consolidate a lot of our workflows. So to give you more of a full story, reusable workflows was introduced as a feature a couple months ago, and it\u0026rsquo;s finally matured to the point where we can actually use it inside of our CI workflows. And so Michael Suho went through, did a bunch of work to consolidate our workflows, and now we\u0026rsquo;ve moved our pull request workflow into a singular file, our trunk workflow into a singular file, our nightly workflow into a singular file. So it\u0026rsquo;s easy to kind of know where these things are coming from. And the added benefit is that if other people wanted to use the workflows that we use on PyTorch PyTorch, they actually can now. Okay, so previously, we generated a lot of little workflow files, one for every job we were doing. So these would be like things like, you know, Xenial, you know, Python 3.6, CPU, something like that. And so now they\u0026rsquo;re all put together in one giant workflow. Okay, well, that certainly reduces the number of entries in the GitHub CI status screen that I have to scroll through. It definitely does. I think at one point, we got all the way up to like 182 checks, which is a little bit ridiculous. And I think everybody kind of recognized how ridiculous that was. That\u0026rsquo;s a lot of checks. One of the other pieces of infrastructure that I noticed that changed is the HUD. So I actually, I wrote the first version of the HUD. It was like this crappy React application that we did in a few days. And we have a new HUD. What\u0026rsquo;s up with that? So the new HUD is based around the idea of being super fast, having a data set that we can make queries on really quickly. It\u0026rsquo;s based on a technology called Rockset, which we picked out. The old HUD, after you had got done with it, was based on RDS, which we identified as kind of a slower version of Rockset, so we moved on to Rockset. Tell me more about what Rockset does. So Rockset is just a database where we can make queries on. We send all of our webhook data to Rockset so that we have a set of GitHub data that we can do a lot of different things on. And so, yeah, one of the cool things about HUD right now is that it\u0026rsquo;s super fast. It works really well. Mini HUD is a new feature that we\u0026rsquo;ve added that allows you to be able to, instead of having a timeline view, you can have this mini view that just tells you what are the failing tests, what are the failing jobs, instead of having to go through a full view. And we just wanted to make sure that when we created the new HUD, we wanted to improve our error reporting experience. we understood that the GitHub Actions log view wasn\u0026rsquo;t necessarily the best way to view logs sometimes because of how slow it loads. So we moved logging into that, into HUD, and just made it a little bit easier to kind of surface areas in general, or surface errors in general. I heard we\u0026rsquo;re also using Rockset to do some new features, like apparently we can search for flaky tests in the corpus now? Yes, actually. So this is a really cool feature that was recently added by Carrie and Jane on the PyTorchadev Infra team as well. Basically, we have, now have, thanks to Rockset, a flaky test view. So we can actually view the history of a particular test over the past 14 days, and we can disable tests from that view within HUD as well. On top of that, there\u0026rsquo;s a bot that will actually go through and automatically disable tests if you reach a certain flakiness threshold. So we\u0026rsquo;re kind of doing a lot of work to kind of ensure that PyTorch CI in general is green without having to have active thought be put into what type of test to disable. What happens to the tests after they get disabled? We\u0026rsquo;ll ping the on call, and Jane did a lot of great work putting together a list of POCs for each of the test files. So we\u0026rsquo;ll contact the POC and the on call for that particular test file to notify them that their test was disabled and to hopefully, ideally, have them go through and fix it. Okay, so up until now, we\u0026rsquo;ve spent a bunch of time talking about how the internal architecture changed, which it sounds to me like it didn\u0026rsquo;t change too much, but we now have Rockset, which we\u0026rsquo;re using to aggregate our data, and we\u0026rsquo;ve also consolidated our workflows using GitHub\u0026rsquo;s actions. So if I\u0026rsquo;m just a plain old end user of the CI, do I care? Are there other things that are nice about being in this new CI universe? I think one of the things that is really nice is just having a singular view, like having a singular place that you go to to make your PR, have your PR tested, and you don\u0026rsquo;t have to leave GitHub in order to be able to do your own work. I think one of the biggest things that I kind of disliked about CircleCI is that I had to click through a bunch of different things in order to be able to view all of the CI that I had at a single point in time, and having that all on GitHub I think is a good experience for all of the developers out there to be able to just have a singular place to be at. Now GitHub Actions is a relatively new offering, and I know that we went through some growing pains where they didn\u0026rsquo;t support various features like parameterized workflows. Is there anything else that we\u0026rsquo;re still waiting for from GitHub? I think a lot of the features that we\u0026rsquo;ve asked for have been completed and done. One of the bigger features that we\u0026rsquo;re asking for from GitHub right now is kind of having a view of what our self-hosted runners are doing at that moment. That would be a really great feature for us on the infrastructure side so that we can understand what our runners are being used for at what times, what percentages, to be able to make better decisions on maybe we need to increase the amount of Linux runners that we have. Maybe we need to increase the amount of Windows runners that we have. Maybe there\u0026rsquo;s a workflow that is particularly greedy that we need to disable, and we don\u0026rsquo;t have that data right now. And having a webhook event that would provide that data for us I think would be amazing. I mean, but to speak about it, our GitHub partnership has actually been very, very good. We have regular meetings with the GitHub team, the GitHub Actions team, just to make sure that we can provide, we have a forum to provide feedback, and they\u0026rsquo;ve been really, really good. And they\u0026rsquo;ve implemented a lot of the features that we\u0026rsquo;ve asked for in the past. For example, parameterized builds, anything else? The rerun workflow from failed feature was a feature gap that was from CircleCI to GitHub Actions that we put as one of our highest priority items. This is actually what enabled the consolidation of workflows to actually work correctly. So remember when we had talked about the Jinja templating that we did, the little workflow files that we did, and a lot of that was due to having this system where we couldn\u0026rsquo;t rerun workflow from failed. So that was a feature that we\u0026rsquo;ve been asking for for a long time that GitHub was able to provide for us recently, which was a really awesome feature to see go GA. Okay, so if I want to make a change to something in the CI, like I want to add a new configuration, where should I look in that situation? So right now you can look in, I think it\u0026rsquo;s .github slash workflow slash pool.yaml that should provide a really good baseline view of what our CI workflows and how our CI workflows work just in general. Michael Suo has done a really great job, again, I want to give him a shout out of making reusable workflows kind of at the forefront of our CI offering. So in essence, it really should be as simple as copy-pasting one of the workflows that are already inside of there and just kind of molding that into the thing that you want it to be. In the old system, I remember I had to make new Docker images sometimes for configurations. Is that still necessary? Yeah, that\u0026rsquo;s one of the things that\u0026rsquo;s still a necessary evil of our CI system. I call it an evil because it\u0026rsquo;s one of those things that not a lot of people understand, but yeah, it is still one of the things that you do have to do, unfortunately. So what\u0026rsquo;s next for the CI system? What should I be hoping to see in the future? So there is a big effort going on. One of the things that we\u0026rsquo;re looking for in H2 is we have a big project called Project Nova, and it\u0026rsquo;s going to be around the idea of standardizing our tooling with reusable workflows and consolidated actions and rolling out that tooling to all of our domain libraries and ecosystem libraries as well. As part of Project Nova as well, we\u0026rsquo;re going to be giving more and better access to GPU runners across PyTorch projects. Right now we identified a need for PyTorch projects to have GPU runners just in general. This is like a baseline requirement and we understand that our infrastructure right now doesn\u0026rsquo;t provide that great of an experience. So what we\u0026rsquo;re trying to do is we\u0026rsquo;re trying to make it a little bit easier to maintain the infrastructure and then trying to make it so that low traffic repositories still get the same level of access that PyTorch PyTorch gets to GPU runners. So does that mean that if I want to spin up a little project that I don\u0026rsquo;t want in PyTorch PyTorch, I can easily get CI for that project now? Yeah, that\u0026rsquo;s part of the idea. Yeah, Project Nova is about that. One of the key results that we want to see out of the project is a bootstrapping process of a new project that is less than a week of engineering time. That\u0026rsquo;s the goal. We want to be able to create a runbook, we want to be able to create the tooling and give the access to the infrastructure to kind of make it simple. A lot of researchers aren\u0026rsquo;t super competent when it comes to doing CI work, and we want them to be able to focus on their core competency while we focus on our core competency. And that\u0026rsquo;s kind of what the idea of providing this tooling is about. Okay, well, thanks a lot for joining me today, Eli, and thank you for being here. Thank you for having me.\nEP60 Dispatcher-questions-with-Sherlock Dispatcher-questions-with-Sherlock All right, hello everyone. Today I\u0026rsquo;m here with Sherlock Huang, who is newly joined here at Meta, and this is an interesting new format that I wanted to do. Basically, Sherlock is going to ask me questions about things in PyTorch from a sort of newcomer\u0026rsquo;s eye. Although, Sherlock, you\u0026rsquo;re not really a newbie because you\u0026rsquo;ve been working on Onyx Runtime for quite some time before coming over here. And I\u0026rsquo;m going to answer them, and we\u0026rsquo;ll see how this goes. And today\u0026rsquo;s episode also has a video component with it because there are some diagrams that we\u0026rsquo;re going to reference as we\u0026rsquo;re going. All right, Sherlock, so do you want to get us started? Yeah, thank you, Edward, for inviting me here. Yeah, so today\u0026rsquo;s probably going to focus on the dispatcher component. So as I was reading your blog, so I come to this impression that originally dispatcher was designed to handle mostly just, you know, the device type and data type dispatching. So over time, it grew into this big magnetic and central place for many, many features. So can you tell me a little bit about the history of how we come to this stage and a little bit of the history about the dispatcher? Sure. So your guess about where the dispatcher used to be implemented is right. So in the beginning, well, in the way, way, way beginning, we had Torch. It was for Lua Torch. It was written entirely in C. And essentially, all we had was we had, like, separate copy-pasted files, one for CPU float tensor, one for CPU double tensor, one for CUDA float tensor, one for CUDA double tensor. And then there was just some bindings to the Lua programming language that actually, like, figured out where you would go so you didn\u0026rsquo;t have to, like, write individually which operation you wanted to do. So this got ported to PyTorch. And so the first version in PyTorch, there was some binding layer in Python that, once again, basically knew how to get to the right implementation. And when Zachary DeVito rewrote our bindings so that we had a C++ library intermediating between Python and the Torch C libraries, before it was, like, directly to C and was very hard to understand. The original thing that we needed was simply, yeah, to dispatch on the device type, and then to dispatch on the D type. So there was a virtual method that we used to do the device dispatch, and there were a bunch of macros for basically letting you stamp out multiple copies of each implementation for D types. So since then, we\u0026rsquo;ve added tons and tons of more features to this dispatcher. And I do have a, there\u0026rsquo;s a more recent diagram talking about dispatch keys. And the model we have now in C++ is that there is an order of various operations that we can do in the dispatcher. And we want to, we basically run things in order depending on whether or not they\u0026rsquo;re applicable to some computation or not. So like, in this example, Autograd is in red. And that\u0026rsquo;s because Autograd is a very common layer people want to do. So you hit Autograd, and then you do CPU. So Autograd was like one of the first layers to get added afterwards. And then all of these other ones sort of got added over time. Did that answer the question? I guess I didn\u0026rsquo;t answer this question you had over here, which is how to like, think about VMAP. But at least Autograd, that\u0026rsquo;s where it lives. It lives here. By the way, Torch Dispatch, it\u0026rsquo;s like a back-end. So there\u0026rsquo;s just a Torch, a Python key over here. And that\u0026rsquo;s how Torch Dispatch gets handled in the back-end section of the dispatch. Yeah, that answers the question. So it seems to me that the order of the dispatch key is extremely important. So as we add so many features, how do we determine the order of the dispatch key? Is there any principle behind deciding the order? This is a great question. So the order is indeed important. And in fact, there is not a single well-defined order necessarily in all cases. For example, Functorch, which is the new library for doing jack-style transforms on PyTorch, it provides two levels of functionality, VMAP and Autograd. And you can actually have them ordered one way or the other. And these correspond to different but both useful operations. One of them computes per sample gradients, whereas the other one is like normal. You have a batch computation inside of Autograd. So that\u0026rsquo;s kind of troublesome. And some of the more recent work has been about getting us away from this fixed set. But the order that is in C++, and this order is kind of important because it\u0026rsquo;s the one we can efficiently implement. This order is basically sort of worked out based on what the average use case in PyTorch is. So for example, there\u0026rsquo;s a question about tracing versus Autograd. So why is the tracer before or after Autograd? Well, actually, the tracer key is this interesting legacy concept for TorchScript tracing. And we\u0026rsquo;ve been talking about this new thing called AOT Autograd, which knows how to trace Autograd. And that\u0026rsquo;s implemented using the Python key. The Python key is after Autograd. So indeed, you get the traced forward and backwards in this situation. Another example is Autocast in Autograd. Autocast is before Autograd. Why is that the case? Well, it\u0026rsquo;s because if you do a bunch of casting, you need to also know how to differentiate through a cast to lower or higher precision. So Autocast doesn\u0026rsquo;t handle that. It just, you know, inserts the new operations and then Autograd handles it. So there\u0026rsquo;s a lot of thinking about like what you want the semantics to be. And the ordering of the dispatch keys is like our best guess about what you want in a situation. And hopefully it is useful, but sometimes it\u0026rsquo;s not. Yeah, this is great. So I also heard about like in terms of dispatch key, there are two categories. There are like backend related dispatch key and then there\u0026rsquo;s another feature keys. And backend keys is always the end destination of the dispatching, right? That\u0026rsquo;s right. Well, almost because the Python key, which we treat as a backend, can in fact start executing other PyTorch code, which will go through the dispatcher again. It\u0026rsquo;s a sort of re-entrant mode of execution. But most normal backend keys don\u0026rsquo;t do that. They just actually do the compute. So let\u0026rsquo;s say like user want to specifically override the dispatching order. Is there any way that a user currently can do that? Uh, that\u0026rsquo;s also a good question. So in the C++ dispatcher, there\u0026rsquo;s a fixed order and that\u0026rsquo;s it. You, you\u0026rsquo;re, you\u0026rsquo;re out of luck if you want to reorder things. So how does Funktorch do it? How does Funktorch? Let\u0026rsquo;s see. Do I have batch? Yeah. I had in, in, in this picture batched is, um, before autograd. So that\u0026rsquo;s the order you get, um, when you use PyTorch only. So how does Funktorch let users reorder it? So the basic idea is that, um, you, you, you have, you have an inner tensor and you have an outer tensor and each of these tensors gets its own copy of the dispatch tree. And then what you do is you say, okay, on the outer tensor, skip batched and go to autograd. Cool. You do your autograd stuff. And then you go to the backend key. It\u0026rsquo;s going to be the Python key. Or, um, in, in Funktorch\u0026rsquo;s case, there\u0026rsquo;s a special key that they\u0026rsquo;ve got for like going back to the front. And this goes to the inner tensor. It goes back to the beginning. And, uh, then you, this time hit batched. And then finally you get to the true backend. So you basically like, if, if it\u0026rsquo;s not in the right order, you just stack as many of these as you need, um, sort of nulling out all the things you don\u0026rsquo;t care about. And all of these, you know, layers are optional. You don\u0026rsquo;t have to do them if the functionality isn\u0026rsquo;t relevant. Yeah. This is the same idea, similar idea as the tensor subclass in private. So when you wrap tensor over tensor over tensor, you automatically get, uh, multiple, uh, stacks of this dispatch key and then you can compose them in any way that you wish. Uh, that\u0026rsquo;s right. So with tensor subclasses, you, so Funktorch\u0026rsquo;s implementation is done in C++, but with tensor subclasses, you can do it entirely in Python simply by having a tensor subclass that contains another tensor on the inside. And we actually have an example of how to do it this way in subclass zoo. Um, it\u0026rsquo;s in the Funktorch.py file. Yeah. So I want to dive a little bit into this backend, um, select key. So, um, so in, in, in the, in the, in the diagram, it seems that, okay, we do a bitwise for, um, all the, uh, hardware keys. Uh, but end up, uh, the one that ended up being selected is the left most of the, uh, dispatch key, right? Uh, uh, but in reality, like a single operation can only run on a single device. So, uh, basically the multi-dispatch behavior doesn\u0026rsquo;t apply to the backend select. Is that the right understanding? Uh, sort of. Okay. So there are a few things going on here. So, so this multi-select is for handling, uh, a multiple dispatch where you like have a CPU tensor added to a CUDA tensor. And, um, this figures out, Hey, you want to go to the CUDA key, not the CPU key. In that case, backend select is for a different situation, which is when you don\u0026rsquo;t have any dispatch keys in the inputs in question. So backend select is used primarily for factory functions, which don\u0026rsquo;t take any tensors as inputs. And because they don\u0026rsquo;t take tensors as inputs, there\u0026rsquo;s no tensor input dispatch keys to get you to the CPU or CUDA factory function that you need. Indeed, you would instead have to look at the device argument to figure out which one you want, but we didn\u0026rsquo;t write any like special logic, which is like, Oh, if your argument is a device, then I know how to extract a dispatch key from it. And so we just said, well, we\u0026rsquo;ll just put you in this backend select kernel. It will go and look at the arguments, figure out what to do, and then eventually take you to the correct kernel. I see. So for, let\u0026rsquo;s say there is a binary op and one input has a set of dispatch keys say, batching and tracing, but on the other input, it can have another set of dispatch behavior. So when we plug these two inputs into this binary op, so you\u0026rsquo;re saying that both, it will take a union of this dispatch key and invoke every single feature that both of them have, right? Yeah, that\u0026rsquo;s right. And then the implementations of the feature, like batched or autograd would be responsible for knowing how to deal with a tensor input that quote unquote, wasn\u0026rsquo;t batched or wasn\u0026rsquo;t autograded. In JAX, we call this lifting. You have to lift a tensor that doesn\u0026rsquo;t have that functionality into the world of autograd or batching. So it seems to me that like, okay, the destination is always on this backend device selection part. So has there been any consideration on, for example, breaking this one joint dispatcher into multiple ones, especially, for example, for the backend ones, because it\u0026rsquo;s always the destination. It doesn\u0026rsquo;t seem to be mixing with other orders. So Brian Hirsch recently landed a PR that gives us a lot more dispatch key space. And the way that he does it is by treating backend specially. So they don\u0026rsquo;t, you can sort of have something that is both autocast and autograd and XLA, but you can\u0026rsquo;t have something that\u0026rsquo;s both XLA and CUDA. So he encodes those differently. It\u0026rsquo;s still one in 64. So we didn\u0026rsquo;t actually separate them. And the dispatch table is still sort of set up as one table. But morally, now we are treating backends differently than the layers in question. I see. Is there any other category concept in this dispatch key? For example, we have this multiple autograd dispatch key. We have this view and conjugate and negative view. So it seems to me like the dispatch key is not completely flattened. There are structure within it, but somehow it\u0026rsquo;s all just appears to be flattened. So is there any consideration to put them in a more structural way? Oh, that\u0026rsquo;s a good question. So for the layers, I actually don\u0026rsquo;t think there is more structure, except in the sense that you might want to reorder them arbitrarily, which is what Functor is about. So my general way of thinking about every layer key, so backend keys are terminal, right? They don\u0026rsquo;t ever call anything else. Layer keys can call into other layer keys. And we sort of normally go down the dispatch key chain as we get there. So the way I think about a layer key is it\u0026rsquo;s basically a rewrite of some Torch API operation, some ATEN operation, into some smaller ATEN operations, right? And you continuously, like, desugar the ATEN ops into more and more ATEN ops until you finally got in the backend, and then those are the actual operations that you need to run. So in that sense, there isn\u0026rsquo;t really any meaningful grouping, right? And any transformation from a single ATEN op into several ATEN ops is a valid transformation. And we might group them up because some of the transformations do similar things, are implemented in a similar way, like conjugate and negative. Those are very similar. But, you know, like, you don\u0026rsquo;t need to bunch them up from the perspective of dispatch because they do want to be ordered in this way. Because that tells you what order the transformations happen. I guess there is an interesting point here to be made, which is whether or not sometimes transformations are commutative with each other. But we don\u0026rsquo;t encode this logic in any way right now. So another mystery that seems to me is, like, all this new feature end up landed in the dispatcher. So is it by design or is it just part of the constraint that we end up in the dispatcher? So, like, if you look back into all this feature that was added to the dispatcher, is there any particular ones that could have lived outside the dispatcher or done in a different way that eventually somehow still got into the dispatcher? Oh, that\u0026rsquo;s a good question. So let\u0026rsquo;s be a little concrete for a moment. So let\u0026rsquo;s say, for example, let\u0026rsquo;s talk about Autocast for a moment. So Autocast was an interesting feature. It wasn\u0026rsquo;t even developed by folks at Facebook. Michael Carilli over at NVIDIA had implemented Autocast as some, I don\u0026rsquo;t even remember how he, I think he was, like, monkey patching the PyTorch source code and basically doing a lot of work to, like, basically automatically insert casts to lower precision, you know, without having to modify your program. And so I think we were at the PyTorch dev day and I heard what Michael was doing and I was like, hey, you know, you don\u0026rsquo;t have to do it this way. We\u0026rsquo;ve got this thing called the dispatcher. And in particular, something you can do in the dispatcher is you can write what\u0026rsquo;s called a fallback kernel. So this is a single polymorphic kernel that, like, will operate on all of your operators. So you don\u0026rsquo;t, you can just write one of these fallback kernels. Actually, I think, I think for Autocast, it\u0026rsquo;s a fall through kernel. It just ignores the execution if you, you know, if it\u0026rsquo;s not an operator that knows how to do casting. And other than that, it was just a very convenient way to insert functionality into PyTorch, interpose it, you know, without having to, like, do stuff like monkey patching. Now, today in 2022, we\u0026rsquo;ve been adding a lot of new features that let stuff happen in Python user land. For example, I\u0026rsquo;ve got a PR, a Torch function mode, which I\u0026rsquo;ll be landing soon, which basically lets you do this kind of interposition at the Python API level. And this can\u0026rsquo;t happen in C++, because once you\u0026rsquo;ve got into C++, we have only the, like, narrow C++ set of types. So all Python objects have gone away. We\u0026rsquo;ve translated them into C++. But sometimes you want to, like, do some operations in Python. And that\u0026rsquo;s why Torch function modes are kind of like dispatcher layers, except they\u0026rsquo;re living at the Python level. And then you probably could have implemented autocast in Python, and you would only do it in C++ because you had a speed concern, or you needed to work with the C++ front end, or something like that. So, yeah, it\u0026rsquo;s a really good question. And so I guess historically the answer is there wasn\u0026rsquo;t a good way to do things other than in the dispatcher. But we are now adding more hooks, like Python Dispatch and, like, Torch function mode, which lets you do these in user land. Yeah, yeah, thank you, Edward. That\u0026rsquo;s all the questions I have for today. Okay, thank you, Sherlock, for asking some great questions. Talk to you next time. Thank you. Bye-bye.\nEP61 AOTAutograd AOTAutograd Hello everyone and welcome to the PyTorch Dev Podcast. Today I have Haris He with me who is going to come and talk about AOT Autograd, a system that is in Functorch which lets you capture both the forward and backward traces of PyTorch operations. And then you can send them to a compiler and then get back a compiled kernel and then stick them back in your PyTorch program just like any other old function that\u0026rsquo;s available. So Haris, can you tell me a little more about what AOT Autograd is? Yeah, so AOT Autograd is kind of just as you said is essentially a compiler integration point for PyTorch, yet another one. So kind of the main premise behind AOT Autograd is that we want something that makes integrating compilers into PyTorch training easy. So we have other APIs like Torch.fx or TorchScript but one of the things that makes integrating against training difficult is that they kind of have these very specific APIs in the case of TorchScript or they just don\u0026rsquo;t support Autograd in the case of FX. So the premise behind AOT Autograd is that we want to provide an integration point that makes integrating compilers into training seamless and basically as easy as integrating compilers for the purposes of inference. And the fundamental reason why this should be doable right is that the operations during training are just tensor operations and these are exactly the same tensor operations that occur during inference. The shape may be a little bit different but the actual operation should be the same. So as long as we can represent the operations that occur during the backwards pass So to actually achieve this AOT Autograd actually has to do a bunch of things which I guess normally we would think of as separable components but AOT Autograd just puts them all together in one package. So for example, you mentioned the other tracing mechanisms like TorchScript or FX. So AOT Autograd does come with a tracer. Is that right? Correct. So one of the aspects of AOT Autograd is it uses this tracer built on top of a new mechanism called Torch Dispatch. And Torch Dispatch is kind of this new extensibility point that probably could do with its own Posca episode. But kind of what Torch Dispatch does is it\u0026rsquo;s a multiple dispatch integration point that sits below the dispatcher. So unlike something like Torch.fx that sits at the Python level or something like jit.trace which sits above Autograd, Torch Dispatch sits below all of that and therefore allows you to capture Autograd. So that\u0026rsquo;s kind of the tracing mechanism that AOT Autograd leverages to capture the forwards pass and backwards pass. That\u0026rsquo;s right. So we are basically able to run both the forwards and backwards and trace all of that, including the code that normally is running in C++. But once you\u0026rsquo;re done tracing all of that, there\u0026rsquo;s still more stuff AOT Autograd does, is that right? That\u0026rsquo;s correct. So one of the tricky things about, so one of the things that AOT Autograd, or like one of the premises here, is that tracing is fundamentally a pretty good way of capturing machine learning models. And the reason for this is that tracing, most users code ends up being fairly lacking in dynamic control flow and things like that that make tracing difficult. And a lot of what you need to do with tracing, a lot of what tracing is able to do is it\u0026rsquo;s able to eliminate the Python data structures or like weird ways that users write code, like they might use lambdas, they might use other data structures, and it captures that. So that\u0026rsquo;s kind of what tracing is good for. But the problem is that oftentimes, there\u0026rsquo;s a lot of things that break tracing. So for example, users might want to log their tensors, they might want to branch on user input, they might want to branch on the loss and do different things. And so what we want to do with AOT Autograd is we want to allow you to apply a compiler to an arbitrarily small subsection of your program. And this is not, like this is not naturally fitting into the tracing paradigm. Because like, if you capture a sub part of your model, the forwards pass and backwards pass do not actually run at the same time. So it\u0026rsquo;s not like there\u0026rsquo;s like a single function that you can trace. And so what we need to do is we need to capture the forwards and backwards pass simultaneously by pretending it\u0026rsquo;s a single function. And then we need to do something else to be able to split the forwards and backwards pass into two separate graphs that we then run at different times. So just to emphasize here, normally, you think of tracing the entire model. But with AOT Autograd, you\u0026rsquo;re just tracing a little piece of it, or the entire thing, if you can manage it. But more frequently, it\u0026rsquo;s just going to be a little fragment of it. And that bit needs to be its own microcosm. Getting it\u0026rsquo;s forward and backwards. And then I guess AOT Autograd, the name Autograd in it comes because, in fact, the main thing it does is it creates a custom Autograd function that wraps up the forward and backward that can interoperate with the rest of your ego code. That\u0026rsquo;s correct. Yeah. You mentioned this cut thing. What\u0026rsquo;s that? So once you\u0026rsquo;ve traced your joined forwards and backwards graph, you now need to convert this like single joint graph into two graphs, like one that runs in your forwards pass and one that runs in your backwards pass. And it might be clear that actually there\u0026rsquo;s actually some leeway in how you\u0026rsquo;re willing to do this. So there\u0026rsquo;s some strict dependencies such as operations that need to be in the forwards pass or that need to be in the backwards pass. But there\u0026rsquo;s other operations where you have a choice of of whether you want to put it in your forwards pass or backwards pass. And so this choice actually ends up mattering in certain cases. So you might imagine that if you if you put an operation in the forwards pass or backwards pass, this might expose more fusion opportunities or other things like that. And so one of the things we\u0026rsquo;ve kind of figured out is that one of the most important optimizations you can do here is something called rematerialization, also often known as gradient checkpointing. So what we so we\u0026rsquo;ve kind of come up with an approach that minimizes the memory transfer between your forwards pass and backwards pass. And this is kind of done using a mincut algorithm that, you know, allows using a mincut algorithm. And kind of one of the neat things about this approach is that in combination with a fusing compiler, this allows us to improve both the runtime as well as the memory usage of the function. So if I understand you correctly, what happens is we trace out the forwards, we trace out the backwards. Actually, when I run traditional PyTorch Autograd, there is a choice made, right, about what I compute in the forwards and what I compute in the backwards. But that choice is fixed. It\u0026rsquo;s whatever my derivative formulas were implemented. But then with AOT Autograd, once I\u0026rsquo;ve got this trace, I\u0026rsquo;ve got both the backwards and the forwards, I can, I can basically renegotiate the boundary in that case. Is there a really good example of some place where this is really profitable? Yeah. So I think a pretty natural example is let\u0026rsquo;s imagine you have a sequence of operations like a cosine. So you\u0026rsquo;re just calling, you know, cosine on a tensor, you know, five or 10 times. So if you think about the autograd formula for cosine, it requires saving the, saving like the output tensor of your cosine operation. And so if you call cosine 10 times and you\u0026rsquo;re going to save the output tensor, you\u0026rsquo;re going to save 10 different output tensors, right? Because like, you know, the way autograd works is you apply the autograd formula to each operation individually. And then you, you know, like multiply them together and apply the chain rule. So if you just run PyTorch Eager autograd, you\u0026rsquo;ll end up saving 10 different tensors between the force pass and the backwards pass. But instead often what you should instead do is you should just not save any operations in your force pass. And so therefore, like you just get like a straight line graph in your force pass that doesn\u0026rsquo;t save anything. And then your backwards pass, you should simply recompute your force pass in your backwards pass. And so this allows you to reduce, instead of saving 10 tensors for your force pass, you only save one. And instead of reading 10 tensors for your backwards pass, you only read two tensors. And because the these cosine operations are what\u0026rsquo;s known as bandwidth band operations, fusers can fuse them and make them like you\u0026rsquo;re kind of dominated by the memory you\u0026rsquo;re writing and not the actual computation. So because we\u0026rsquo;re reducing the memory usage or like the memory reads and writes, we can actually improve both the runtime as well as the memory usage. And AOT Autograd does this today? Yeah, that\u0026rsquo;s actually pretty surprising to me, because I remember when I first read about the mincut algorithm, I just imagined this was, you know, sort of changing the sort of what tensors we say, like, there would be some intermediates, and we would choose to save some, but not the other. And then you just move the boundary around. But fundamentally, the computation wouldn\u0026rsquo;t be changed. So I guess that\u0026rsquo;s not exactly what you\u0026rsquo;re doing here, right? There\u0026rsquo;s also rematerialization going on. How does the algorithm figure out if something should be rematerialized? Right. So I think the the way I think of this, the way I think of this is that basically, the value we really care about is the gradient of the input, right? So like you\u0026rsquo;re the only reason you\u0026rsquo;re doing your force pass is to compute the gradient. So perhaps like the right way to think about this is that give it, let\u0026rsquo;s say, like, you know, you\u0026rsquo;re given both the inputs to your force pass and the inputs to your backwards pass. And you\u0026rsquo;re allowed to save any arbitrary values, such that computing, like your the gradient input is the easiest. So for example, you can sit, you can, you know, compute it from your. Let me just cut in for a moment. You said gradient inputs twice, but actually, you\u0026rsquo;re given the inputs and the grad outputs. And we want to compute the grad inputs. All right. Yeah, sorry. That\u0026rsquo;s correct. And so like one strategy you might do, right, is you might take both, like you might compute your the gradient of your inputs by taking the input to your forward pass, as well as your grad outputs. And so this corresponds to basically rematerial or like recomputing the entirety of your forwards pass during your backwards pass. But you can imagine that other strategies, for example, perhaps you\u0026rsquo;re doing a matmul on your forwards pass, you might want to start compute, you might want to start computing later down in your graph, and you know, skip the extra matmul during your backwards pass. So like this is kind of, you\u0026rsquo;re doing like a mint cut, not exactly to partition the two graphs, but kind of to decide what computation you\u0026rsquo;re going to perform in your backwards pass. In other words, the forwards computation actually always is the same no matter what. So we\u0026rsquo;re not really partitioning the graph in that sense. Or no, so the forwards pass is kind of the way I think of it is, is that it\u0026rsquo;s implicitly defined by what you choose to save for your backwards pass. And because like the thing we\u0026rsquo;re trying to minimize here is a memory, memory bandwidth costs. And any each input you need to save in your forwards pass corresponds to one input you need to read in your backwards pass. So luckily, minimizing, minimizing memory bandwidth actually ends up being symmetric for both your forwards pass and your backwards pass. So that\u0026rsquo;s kind of like one non obvious thing that makes this easier. Uh, so I misunderstood. So, so we are going to change what the forward passes, but we are going to sort of move stuff over into the backwards pass if we think it will be profitable. That\u0026rsquo;s correct. Yeah. Okay. Um, so we\u0026rsquo;ve talked about how AOT Autograd, you know, traces through our code, gets the backward traces. And then we\u0026rsquo;d also talked about how we split them up and then eventually put them in a Autograd function so that they work with eager mode. But of course we need to actually run a compiler on these traces to, um, you know, do something useful with them. So can you tell me a little bit more about, um, you know, how, how AOT Autograd works with compilers? Right. So, um, so what AOT Autograd does, right? So we trace things out and, uh, we actually trace things out into this kind of, you know, like standard, uh, PyTorch graph format, uh, called like, uh, called torch.fx. Um, and then we simply take this fx graph and we pass it to a, uh, arbitrary compiler. Uh, so for example, one thing we might do is we might take this fx graph, we might torch script, and then we might pass it to like a torch script, uh, fuser such as, um, um, mvfuser or NNC. Um, so one of the like complexities here, uh, so, you know, like the kind of pitch here, right, is that if you have a compiler that works for, uh, inference, um, you, this like AOT Autograd allows you to also apply that compiler, uh, in your backwards pass. And one of the, uh, that\u0026rsquo;s right. Because we don\u0026rsquo;t actually, um, pass it, we don\u0026rsquo;t expect the compiler to do differentiation. We just pass it the forward and the backward separately. Right. Um, but like one of the, uh, one, so that\u0026rsquo;s kind of the pitch, but one of the areas where that\u0026rsquo;s not quite true, right, is that there are certain operations, uh, that only occur in your backwards pass, uh, that never occur in your forwards pass. So for example, uh, PyTorch has operations like 10H underscore backward, um, that, uh, you know, are used to compute like, you know, backwards formula for 10H, but oftentimes compilers won\u0026rsquo;t have support of this operator because, uh, it never shows up in your forwards pass. Um, so, uh, one of the things we kind of do as part of AOT Autograd is we\u0026rsquo;ve written a bunch of these, uh, decompositions, uh, to basically rewrite these, uh, operator, like, you know, 10H backward in terms of other, uh, more common operators, uh, that compilers can fuse. So now to sum it up, AOT Autograd is a tracing mechanism. It\u0026rsquo;s a min cut, uh, algorithm, and it\u0026rsquo;s also a number of decompositions. That\u0026rsquo;s a lot of things packed into one box. Right. So, I mean, like one way to, you know, view AOT Autograd is like, it\u0026rsquo;s the specific kind of product where, you know, providing to, you know, and trying to improve performance. But I think another way of viewing AOT Autograd is it\u0026rsquo;s kind of a, uh, meant to be a, like, extensibility point for PyTorch. So there\u0026rsquo;s like many things that are, uh, there\u0026rsquo;s many things in PyTorch that like are hard to do because we\u0026rsquo;re in eager mode. And AOT Autograd is basically like an easy way of, you know, getting a graph, uh, for your forest pass and your backwards pass that allows you to do arbitrary things, you know, like rewrite them or, you know, reinterpret them in other ways. Um, yeah. So for example, you know, like it\u0026rsquo;s really easy to like change, uh, this remit, like this min cut rematerialization approach with a different algorithm. Uh, so for example, you might want to like save more memory at the cost of doing more compute or things like that. And that\u0026rsquo;s kind of like one of the things that we\u0026rsquo;re trying to support. So that\u0026rsquo;s a lot of cool stuff. And if I understand correctly, there\u0026rsquo;s a lot of things that, um, we also want to do with AOT Autograd in the future. Can you tell me about some of them? Um, right. So one of the, like the most, uh, I guess obvious things, right, is like, we\u0026rsquo;ve kind of implemented a couple of, uh, optimizations such as rematerialization, um, as well as, you know, hooking it up with the operator fusers. Um, but you know, there\u0026rsquo;s way more optimizations, uh, like still left that we haven\u0026rsquo;t really even like touched the surface on. So for example, uh, one of the things that you might want to do is kind of layout planning. Um, like you, you might want to, you know, change the layout of your operators so that like, you know, the mammal or the cov has like a more favorable, uh, performance. Um, and kind of what, uh, or, you know, other things you might do, you want to do like memory planning. And one of the kind of interesting things here about AOT Autograd is that, uh, uh, the setting that we\u0026rsquo;re operating in is kind of often fundamentally different from what, uh, many, like what, uh, a lot of the people in, you know, the literature or have kind of, you know, historically looked at, um, in that a lot of people usually kind of assume that they just get like the entire graph, um, in a single, uh, or like they get the entire model, you know, forwards and backwards in a single graph. Uh, but kind of the, what we believe here is that, uh, this will early, you know, like what we kind of believe in PyTorch, right? Is that like, this is not really true. And a lot of times our users want, you know, the flexibility of PyTorch and they want control flow and things like that. Um, so a lot of, you know, things like layout planning or, um, like, you know, memory planning become trickier, uh, when it comes to like operating in this setting. So that\u0026rsquo;s kind of one of the things we\u0026rsquo;re thinking about. Um, another one of the things we\u0026rsquo;re thinking about is that, uh, in some sense, like AOT Autograd, uh, is pretty inspired by, you know, JAX\u0026rsquo;s, uh, like JIT and, you know, how it\u0026rsquo;s composable, uh, with, uh, JAX. So, you know, you can apply JAX.JIT in an arbitrary location and, you know, it composes with Autograd or it composes with VMAP and things like that. And, uh, you know, currently we also have things like VMAP and, uh, uh, AOT Autograd, unfortunately, currently does not compose with those. So, um, although you can use, uh, AOT Autograd to compile, uh, you know, things like VMAP and, you know, we\u0026rsquo;ve used that for, you know, compiling things like Jacobians or Hessians, um, it does not allow you to compose in the other direction. So you can do, uh, AOT Autograd of VMAP, but you can\u0026rsquo;t do VMAP over AOT Autograd. Uh, so, you know, kind of figuring out composability, uh, in that manner is kind of one of the other things we\u0026rsquo;re currently thinking about. The way I think about this problem of, um, you know, running a VMAP over an AOT Autograd is, uh, in some sense, it\u0026rsquo;s not AOT Autograd anymore, but it\u0026rsquo;s AOT everything that PyTorch supports. So, you know, that includes Autograd, which we do support right now, but it also includes batching and functionalization and all of the other fun user transforms going on. Horace, do you agree with this point of view? Uh, yeah. So AOT Autograd is kind of like the current name, but, uh, in the future, yeah, perhaps they\u0026rsquo;ll need to be named something more generic. All right. Well, um, that\u0026rsquo;s it for our time today. Uh, thank you very much for joining. Uh, thanks for having me on. Cheers.\nEP62 Strides Strides Hello everyone, and welcome to the PyTorch Dev Podcast. Today, I want to talk about strides in PyTorch. This topic I have blogged about before, and I\u0026rsquo;ve also written a little bit about it, but given that Mike Rubery has recently raised a proposal for stride-agnostic PrimTorch semantics, I thought it would be a good time to talk about what is meant by strides and some of the interesting characteristics that matter when you\u0026rsquo;re dealing with this concept. Okay, so what is a stride? Well, a stride, as its name suggests, simply says how much you need to go to find the next element in some memory. So remember, when we represent tensor data, and these are these multidimensional arrays, it\u0026rsquo;s actually not fully specified how exactly you map a coordinate, aka a set of indices, to an actual location in memory. Now, with a one-dimensional array, you might imagine that, you know, you represent it in a very normal way, which is that, well, you have your first element at the location of the start of your array, and then you find the next element by going to the next slot, and so forth and so forth. And we would call that having a stride of one, because you just go to the next position, one over, in that situation. But you\u0026rsquo;re not limited to only being able to do that. For example, let\u0026rsquo;s suppose we have a two-dimensional array, say it\u0026rsquo;s a five by five array. Well, to go to the next element in a row, you would still go forward one in memory. But what if you want to go to the next row? Well, in that case, you wouldn\u0026rsquo;t be able to find the next element one over, you would have to go five elements over, skipping past all of the elements that were stored for the first row to get to the first element of the second row. So going by column, it\u0026rsquo;s stride one, but going by row, it\u0026rsquo;s stride five. This is a lot easier to visualize if you\u0026rsquo;ve actually got a diagram in front of you. So I highly recommend checking out one of my blog posts, you can find it inside the podcast description to see a little bit more about you know, how exactly this works. But essentially, all the stride is, is it\u0026rsquo;s a specification for any given dimension, your tensor, how far you have to go in memory to find the next element there. And so typically, you know, the innermost dimension, the one on the right, when you\u0026rsquo;re talking about like the size, that\u0026rsquo;s going to have a very low stride like one. And then the outermost dimension, the one on the leftmost side, like the first dimension, it will tend to have the biggest stride because well, you\u0026rsquo;ve got to get past all of that other data over to the right hand side, before you can actually get to the next, whatever it is in your dimension. Okay, so that\u0026rsquo;s what strides are. And mathematically, when you think about how to index into tensor, it turns out your indexing formula is very simple, you just take, you know, the index zero and multiply by the stride for zero, plus the index for one multiplied by the stride for one, and so forth and so forth, so forth. So it\u0026rsquo;s a very simple formula. And it\u0026rsquo;s pretty easy to implement, well, unless you have a arbitrary dimensionality tensor. And, you know, that\u0026rsquo;s sort of where we come from with PyTorch. Strides have been with PyTorch, since before PyTorch was even a thing. Torch, the library that PyTorch is derived from, also had strides. Strides are pretty useful. And there are two primary reasons why PyTorch, and also NumPy, NumPy also has a concept of strides, support it. And those two main reasons are views. This is the original reason we had strides. And the second reason, memory formats, which was added on at a later point in time during PyTorch\u0026rsquo;s history. Let\u0026rsquo;s unpack these two use cases. So what do I mean by a view? Well, notice that I was talking about how to find the next piece of data, right? And I said, well, you know, if I\u0026rsquo;m looking for the next element in the row, I just go by one. And for the next element in the column, I go by five. And so when you are talking about tensor data, sometimes you want to talk about a subset of the tensor data, and treat it as a tensor in its own right. For example, you have a two by two, sorry, a 2d matrix, and you want to extract out a row from that matrix. Well, extracting out rows is pretty easy, because all you need to do is you just take whatever your offset is, where the row starts. So if it\u0026rsquo;s the zeroth row, you\u0026rsquo;ll start at beginning. But if it\u0026rsquo;s the, you know, fifth row, you\u0026rsquo;ll start at, you know, memory location 25, say, and you just, you know, adjust the length so that you only see that row. And so if even if you don\u0026rsquo;t have any concept of strides, it\u0026rsquo;s very easy to represent, you know, subset rows, sub rows of a tensor in this way. And so if you were like doing stuff with C++ vectors, for example, there\u0026rsquo;s a very handy utility class we have in PyTorch called array ref, which is a non owning view onto vectors. And you can you can do this, you can have it take out an array ref, to a arbitrary row in a, you know, virtual 2d vector, where you just have everything stored contiguously, instead of having a vector of vectors. Okay, so that\u0026rsquo;s very easy. But what if you want to say, for example, return a tensor that represents a column of your, of your tensor. Now that is not going to be so easy. Because if you look at each of the individual elements, they\u0026rsquo;re going to be laid out in memory differently than on the row, the row, all the elements were one after another, you know, move one, find the next one, move one, find the next one. But for the column, we said, well, you have to move five to get to the next one. So they\u0026rsquo;re not together, they\u0026rsquo;re so called non contiguous. And so in this situation, if you only had the ability to say, well, here\u0026rsquo;s where you should start reading. And then you should just read out a contiguous chunks of data, that\u0026rsquo;s n long, you would have no way of representing a column without actually copying out the data so that it\u0026rsquo;s contiguous. And so when you have a tensor representation that supports strides natively, and that\u0026rsquo;s what PyTorch has, and that\u0026rsquo;s what NumPy has, we have the ability to represent column tensors without doing any copies, because all we do is we say, okay, well, let\u0026rsquo;s have a one dimensional tensor, we\u0026rsquo;ll start it at the beginning of the tensor. But instead of having the stride be one, which would be the normal situation with a 1d tensor, we would have the stride be five, say, so that I know, okay, to find the next one, I skip over five, and then go forth, and so on. And now, of course, many operations in PyTorch only know how to handle contiguous inputs. So when you do a, you know, strided tensor like this, sometimes there won\u0026rsquo;t be any profit, you\u0026rsquo;re simply just delaying the inevitable. Once you actually do a computation on it, we\u0026rsquo;ll go ahead and, you know, zip through the data, gathering it together into contiguous tensor, and then running the operation. So it\u0026rsquo;s the same as if you had done the copy ahead of time. But there are two important differences in PyTorch. So one is that because the view of the tensor shares storage, right, we didn\u0026rsquo;t do a copy initially, the copy only happens lazily, when an actual kernel needs to be run. If I mutate the original tensor, or if I say mutate the view, it will show up in the other place, either the view or the original tensor, depending on what you did. So that\u0026rsquo;s really handy. Because one of the things that you know, is really nice about working with PyTorch is you can, you know, go and explicitly mutate tensors, mutate views, and use that to sort of initialize your tensor, if you need to. Now, it\u0026rsquo;s not recommended because it doesn\u0026rsquo;t work well with Autograd, but it works, and you can do it. And that\u0026rsquo;s very useful. The other important thing about having views being represented in this way is that sometimes we are able to handle a non-contiguous input without, without having to do a copy. So in that case, we\u0026rsquo;ve saved ourself from having to do a bunch of data movement. And instead, we, you know, we\u0026rsquo;re able to fuse the gather operation directly into the kernel instead. So one way that I like to think about views is they\u0026rsquo;re a very limited form of lazy evaluation, right? Instead of doing the gather immediately, instead of doing the collection of all the columns elements into contiguous tensor, we defer it, and we wait until the actual kernel needs to get run. And then that\u0026rsquo;s when, you know, we figure out, oh, okay, the kernel actually supports the situation. Hooray. Otherwise, oh, maybe the kernel doesn\u0026rsquo;t support the situation. Oops. By the way, so this is a little side note. But when you\u0026rsquo;re writing kernels in PyTorch, you do have to think about whether or not your kernel is going to know how to deal with non-contiguous inputs or not. And this is actually kind of a pain because a lot of kernels don\u0026rsquo;t really know how to deal with discontiguous inputs. It\u0026rsquo;s a lot of work to actually handle strides. And it makes your indexing formulas more complicated. And it makes your kernel slower, because if your kernel can assume that everything is contiguous, then it doesn\u0026rsquo;t need to do all of this indexing arithmetic, you know, all the multiplying strides by sizes to actually figure out where the location it\u0026rsquo;s going to get out data from is. So a lot of kernels just want to assume a, you know, I\u0026rsquo;ve just got a contiguous thing. And so historically, if you wanted to write code like that by hand, what you would have to do to be strictly correct is you would have to go and contiguous. So okay, so there\u0026rsquo;s two situations. So one is if you\u0026rsquo;re writing a functional kernel, you would need to go and check if the input was contiguous. And if it was not, you would have to run contiguous on it to get the contiguous input. There\u0026rsquo;s some helper functions like expect contiguous, which help you do this without incurring a ref count bump when there\u0026rsquo;s no contiguity in this case. And the other thing that, um, they let you do is they let you, um, uh, let\u0026rsquo;s say that you have a kernel that takes an out parameter. Well, if the out parameter is strided, you\u0026rsquo;re expected to be able to go ahead and directly, um, you know, respect that striding because, you know, Hey, maybe it\u0026rsquo;s some view and that view is aliased with some other base tensor. And the user actually did wanted the output of the, uh, uh, computation to get scattered in this way. So to actually do this, we have to first allocate a, uh, contiguous tensor, which is going to be our output. Um, go ahead and run our kernel writing the data into the contiguous tensor, and then finally scatter it out into all of the actual user requested output tensor in the situation. And as you can imagine, this is very easy to forget to do. And there\u0026rsquo;s a lot of kernels that don\u0026rsquo;t do this correctly. Fortunately, um, if you\u0026rsquo;re writing structured kernels, um, there\u0026rsquo;s a very nice, uh, new piece of functionality by Yukio Sirachi, where basically you can say, Hey, um, my kernels, uh, can\u0026rsquo;t handle, uh, strided, uh, outputs. Um, they can only handle contiguous outputs and you say set output contiguous, and this will go ahead and handle all of the ensuring that the output is in fact contiguous under the hood for you and do the copy out to the real output if necessary. So you can just write your kernel without worrying about this stuff. So it\u0026rsquo;s pretty handy. You should use it if you\u0026rsquo;re in that situation. All right. So that\u0026rsquo;s it about what, uh, strides are good for with a views. Now there\u0026rsquo;s another thing that I said they\u0026rsquo;re good for, and that is a memory format. What do I mean by memory format? Well, memory format refers to, um, some conventions about where exactly physically we put data when we are talking about them. So for example, um, you may have heard of the terms channels first and channels last. What exactly is meant by these terms? Well, they refer to a very common, uh, uh, layout, a question you have to decide when dealing with, um, image data, which is specifically when you represent the image data, do you represent it as a, as a 2d matrix, uh, representing the image data? And, um, you have a copy for the red values, a copy for the green values and a copy for the blue values. So just imagine, you know, three distinct images, monochrome images, each one of them representing their respective color, but the images themselves being contiguous, or do you represent them in a, um, sort of bundled manner where you have the channels, uh, you, you have the value RGB for the first pixel, then RGB for the second pixel and so on and so on. And the difference between these two formats corresponds precisely to channels first or channels last channels first being CHW where the height and the width, um, pixels have the lowest stride. And then to get to the next channel, you have to do a big jump and channels last AKA HWC where the channel has the smallest stride. So to get to the next channel, you only have to do a single step. Now, depending on your situation, whether or not you\u0026rsquo;re on CPU or CUDA and so forth, um, whether or not you want to lay out the memory in channels first or channels last order, um, differs. And it also depends on what operations you\u0026rsquo;re doing. Sometimes operations are faster. You have channels first, and sometimes they\u0026rsquo;re faster if you have channels last. Historically PyTorch\u0026rsquo;s memory format convention is that we always do channels first. So whenever you have any APIs that taken data that is supposed to represent images, um, we\u0026rsquo;ll always take them as NCHW and being the batch dimension, C being the channels and then the height and the width. So what if you wanted to actually run some code that actually handled them with a channels last memory format? Well, to do that strides come to the rescue. So the NCHW layout is what we think of as logical layout. It just says, you know, when you\u0026rsquo;re accessing the tensor from the user program, you know, the H and the W dimensions are the second and the third, uh, well, it\u0026rsquo;s two index and the three index. And then the channels, the one index dimension. So if you want to actually change the physical layout in memory, so that the, you know, channels layout is index three, all you need to do is set your strides appropriately. So you can keep the same logical layout. And so even when you\u0026rsquo;re doing channels last or channels first, um, you always see a NCHW on tensor as far as you\u0026rsquo;re concerned from a user, but by modifying the strides underneath the hood, we can change the physical layout so that it\u0026rsquo;s actually laid out with channels first or channels last. So this is how we support memory formats in PyTorch. You, we don\u0026rsquo;t have, um, the thing that TensorFlow does where there\u0026rsquo;s an extra flag you have to pass to say convolution saying, Oh, channels are, you know, in the, um, beginning position or they\u0026rsquo;re in the end position. Instead, we always assume channels are in the channels first position. And if you want a different memory format, you just modify the strides to get there. Cool. Okay. So what is going on then given all of this information, what is going on with, uh, Mike\u0026rsquo;s proposal for stride agnostic semantics? Well, this comes down to the fact that although strides are very mathematically simple to express, that is to say, you just multiply the size by the stride, and then you do that for every dimension in your tensor, this actually leads to a little bit of, um, uh, too much leeway in the representation for strides. In particular, let\u0026rsquo;s suppose that I have a tensor and I have a, uh, I have a dimension whose size is zero or its size is one. When a size, a dimension size is zero. Um, you can see that the stride in this case doesn\u0026rsquo;t matter. Why doesn\u0026rsquo;t it matter? Well, you have no elements in the tensor at all. So you\u0026rsquo;re never going to try to ask for the next element because there\u0026rsquo;s no elements at all to ask for. So I can put whatever I want in the stride of a size zero tensor. In fact, I can do whatever I want for any of the operations because I will never ever get called out in this situation. Now, although you can put whatever you want, um, as far as the indexing is specified, you may not want to do that for memory format, because remember our memory format is telling us whether or not this is a channels for a last or a channels first tensor. So, you know, having the stride set up correctly for this case, it does matter sort of, I guess a similar situation shows up when you have a size one tensor, right? So once again, there is an element this time, so that\u0026rsquo;s great, but you\u0026rsquo;re never going to ask for the next element because there\u0026rsquo;s only one element. So you, you know, can have whatever you want in the stride once again, because you\u0026rsquo;re never ever going to observe it in the situation because the strides are over-specified in this way. Um, we do have a convention for what the contiguous strides of a tensor are supposed to be even the zero and the one case. But when you ask if a tensor is contiguous, we actually accept all of the possibilities for zero and one. So basically, um, we know that there\u0026rsquo;s flexibility here and we don\u0026rsquo;t, um, hold it against you if you pick something else in that situation. But that means that there\u0026rsquo;s a lot of flexibility for what kernels choose in this case. And they often just do whatever the heck they want in these situations. So that\u0026rsquo;s kind of annoying. And when we are doing stuff like prim torch, where we\u0026rsquo;re trying to re-implement all of PyTorch directly in Python, um, this is a pain because the way we do testing is we go ahead and, you know, run the original implementation and run our new implementation and check if the strides match up. And while lo and behold, sometimes they don\u0026rsquo;t because, you know, there\u0026rsquo;s these degrees of freedom and they let the strides, you know, wobble in a way that doesn\u0026rsquo;t actually matter. Okay. So I told you that the strides are, uh, over-specified in some situations because of these degrees of freedom from size zero or in size one, but wait, it gets worse. So remember what I told you about memory format, right? So you have these NCHW and NHWC tensors and, um, you know, depending on having one or the other, um, your kernels might run faster or not. So one of the things that we need in this situation is if you want to actually be able to run your network in NHWC, for example, that\u0026rsquo;s the non-default situation. You need operators that actually propagate this NHWC format through the entire network. So if I do an addition on a tensor and it\u0026rsquo;s NHWC, it better stay NHWC because there might be a convolution coming up afterwards that actually, you know, could have benefited from that other ordering. Now there\u0026rsquo;s a problem though, which is that when we write up the rules for how strides should propagate, we have a very complicated situation. What if a user passes us a tensor that is NCHW and another tensor that\u0026rsquo;s NHWC, that is to say their memory formats disagree. What do you do in this case? Who knows? Um, we have some algorithm for determining what exactly we should do in this situation, but it\u0026rsquo;s, you know, kind of complicated and hard to describe. And most people just don\u0026rsquo;t close their eyes and, you know, hope something reasonable happens in this situation. And so once again, you know, in Mike\u0026rsquo;s proposal for stride agnostic, he\u0026rsquo;s basically saying, Hey, you know, it\u0026rsquo;s a lot of work to mimic this stride behavior. And, you know, what are we even getting out of it? So since this is my podcast, I get to, you know, harp on what I think that what we should do in this situation. So I agree that it\u0026rsquo;s probably not a great idea to spend a lot of time worrying about what exactly happens when you have a tensor that is, uh, you, you add to like a channels first tensor and channels last tensor together. That just means you\u0026rsquo;ve done something wrong and you know, that\u0026rsquo;s fine. And we shouldn\u0026rsquo;t force ourselves, uh, to make sure that the semantics exactly match in this case. But as I\u0026rsquo;ve mentioned before, we do use strides for two very important use cases, views and memory formats. And so, although, you know, maybe strides in their full glory, it\u0026rsquo;s just too much for our puny, uh, little brains to deal with. We should make sure that we do have a good story for at least the two use cases we care about. So that\u0026rsquo;s everything I wanted to say about strides today. Um, see you all next time.\nEP63 Weak-references Weak-references Hello everyone and welcome to the PyTorch Dev podcast. Today I want to talk about weak references. Some useful background for today\u0026rsquo;s podcast. So we have a podcast about reference counting from way, way back then. Still relevant. If you haven\u0026rsquo;t listened to it, give it a listen. I\u0026rsquo;m not going to go over reference counting basics. And you might also be interested in the Python resurrection podcast. Also just check the links in the podcast. That one\u0026rsquo;s not, that one\u0026rsquo;s optional. You don\u0026rsquo;t have to listen to that one, but it\u0026rsquo;s some useful context as well for discussion about weak references. Okay. So weak references, what are they and what are they good for? So a weak reference is a reference to an object that doesn\u0026rsquo;t keep the object live. So let\u0026rsquo;s imagine that you\u0026rsquo;ve got a tensor and it\u0026rsquo;s got a lot of data in it. And, you know, you want to be able to store a reference to it because you\u0026rsquo;re keeping it in a cache or something like that, but you don\u0026rsquo;t actually want to hold on to it because maybe, um, uh, you\u0026rsquo;re just caching it, right? So if everything is done using it, then your cache is, you know, never going to actually let the tensor get freed in that situation. But the cache is purely advisory. If no one\u0026rsquo;s using the object anymore, then you would like the cache to automatically free it in that situation. You don\u0026rsquo;t want the cache\u0026rsquo;s reference to the object to be strong. You\u0026rsquo;d like it to be weak. Another common situation that this sort of thing shows up in is let\u0026rsquo;s say that you have a cache that is keyed by a tensor. So you\u0026rsquo;re mapping a tensor to another tensor. So let\u0026rsquo;s think about the key tensor in this situation. This key is basically in the hash map so that we know how to correspond, uh, the, um, you know, input tensor to whatever the cached output is. But once again, we don\u0026rsquo;t want to keep this input live. If all the references to the input are dead, then there\u0026rsquo;s no way I can ever actually pull out that tensor from my cache. So I really don\u0026rsquo;t want the cache to keep it live in that situation. One last example for weak references. So, uh, in, uh, in Python, um, uh, object manipulation is very flexible. So you\u0026rsquo;ve got all these objects lying around and you can basically mutate them willy nilly, however you like, right? You can like add extra fields, do whatever you want. Um, unless the object, you know, doesn\u0026rsquo;t support underscore underscore dict, uh, Python supports adding arbitrary attributes to objects, but there\u0026rsquo;s a problem to this, right? The attributes on the object form a sort of global namespace. So if you, you\u0026rsquo;re, you know, using, you know, one name, like, uh, say name, for example, for your own nefarious private purposes, and someone else wants to also put something else on, uh, the same field name, well, that\u0026rsquo;s going to be a conflict and your code is just not going to work. And so because of this situation, uh, it\u0026rsquo;s not really safe to just arbitrarily write random attributes onto tensors. You\u0026rsquo;d kind of like them to be, you know, some private in some way. Now, of course you can mangle the name of the attribute to make them private, but there\u0026rsquo;s another way you can also implement this. And that is once again, using a weak map, you just have a weak map mapping, uh, you know, any given tensor to the attribute you want to store for them. And as we, uh, said earlier, we do want the entries in this map to get garbage collected if the tensor goes dead. And, you know, that\u0026rsquo;s what a weak map exactly would do. And similarly, uh, because we have separate maps for all of our various users that want to store metadata, then you actually, you know, don\u0026rsquo;t ever have a possibility of conflict because each map is its own heap allocated object. And, you know, they\u0026rsquo;re not being addressed by some string name. Something else that\u0026rsquo;s really good about doing it this way is that you can also just delete the entire weak map when you\u0026rsquo;re done. And then all of those attributes go away. So you don\u0026rsquo;t have to like worry about, well, you know, I\u0026rsquo;m done with all of my private attributes. How do I get rid of them at some later point in time? You know, you just use a weak map to do that. Okay. So weak references, Hey, they\u0026rsquo;re kind of useful. So we do support them in PyTorch, um, in several ways. One is, um, in C++, obviously, if you use, um, the shared pointer, uh, smart pointer type, uh, shared pointer comes with built-in support for weak references. Also our, uh, intrusive pointer, um, see our previous podcast that also supports weak references. And of course, Python, um, with, uh, Python objects, they also support weak references. So there\u0026rsquo;s actually two weak reference mechanisms, um, either a C++ mechanism or the Python side mechanism. And you can use either one if you have an object that\u0026rsquo;s bound from both places. So what I want to do is I want to explain a little bit of how, how these are implemented and then some consequences of these implementation decisions. Let\u0026rsquo;s get down to it. So how are C++ weak references implemented? Well, um, when we talked about reference counting, we said reference counts were a field on an object saying how many references there were to the object so that when the, you know, field was still, uh, you know, positive, that meant the object was live. And when that count goes down to zero, now we know the object is dead. So weak references are just a, um, you know, extension to this where not only do we keep a strong reference count, we also keep a weak reference count on the object. So the weak reference count, as its name suggests, counts how many weak references there are into the object. Now that, uh, do note that, uh, when I have a weak reference, um, uh, count, it\u0026rsquo;s actually not only weak references. There\u0026rsquo;s actually one extra weak reference and that\u0026rsquo;s, um, for the strong reference count on the object. So the invariant here is as long as the strong reference count is greater than zero, then my weak reference count has one, uh, is at least one where that one is from the strong reference count. And then you can have as many extra weak references to the object as you like. So how do these two reference count fields interplay with each other? Well, the algorithm looks like this. So long as the strong reference count of the object is greater than, uh, zero, um, the object is live. Um, and when the strong reference count goes to zero and you know, this zero, this testing, if the strong reference count has gone to zero is an atomic instruction. When it goes to zero, that is when the object becomes dead. So no matter how many weak references you have to an object, it doesn\u0026rsquo;t matter, right? Weak references don\u0026rsquo;t keep an object live. Only strong references keep an object live. So when all the weak reference, uh, so when all the strong references are gone, then we kill the object and we say, okay, um, we are done with this object. However, ordinarily, when we want to deallocate an object, we would just go ahead and free the memory associated with this object, but that\u0026rsquo;s not okay. We\u0026rsquo;ve got a bunch of weak references to the object that are pointing to this memory. And I just go ahead and free that memory. There\u0026rsquo;s no way for those weak references to know, Hey, um, you know, there\u0026rsquo;s no object here anymore. I can\u0026rsquo;t actually, um, give you a strong reference, by the way, um, when you have a weak reference to a still live object and you say, Hey, I would like a strong reference from this week reference. You, I\u0026rsquo;d like to de-reference the weak reference. All we do is we attempt to atomically exchange the strong reference count with one greater than the strong reference count. And, um, that will succeed. So long as the strong reference count wasn\u0026rsquo;t zero. And if it was zero, then we\u0026rsquo;ll just say, Hey, there\u0026rsquo;s no element available in this situation. So we\u0026rsquo;ve got these weak reference counts, but they need to be able to access the, you know, reference count fields that are stored on the object. Remember, this is an intrusive, uh, reference count in our case or the control block in the case of a shared pointer. And so if I just go ahead and deallocate that, then that\u0026rsquo;s no good, right? I don\u0026rsquo;t actually have the data anymore. It would just be in a sand violation in that situation. So what I do is I actually keep the object live. Now, wait, you might be saying that sounds very silly. If I keep the object live, then what\u0026rsquo;s the point of having the weak reference distinction? Aren\u0026rsquo;t I supposed to deallocate the object in this case? And indeed, uh, for, you know, objects that are sort of stored, all the data is stored in line, weak reference are kind of useless in this situation. And so with shared pointers, um, the way this is dealt with is actually the reference counts are stored in an extra object called the control block. And the control block is the only thing that gets stays live. You actually deallocate the object in that case, as long as you didn\u0026rsquo;t use make shared, that is to say, which allocates the object and the control block together. But for an object like tensor, um, we have something else we can do, right? Because the tensor object itself doesn\u0026rsquo;t contain all that much data. It is, it is a kind of fat object and it\u0026rsquo;s got a lot of fields on it, but really most of the data usage of a tensor is coming from the, uh, the data, the actual tensor floating point data that is associated with the array in question. So all I need to do is I just need to deallocate that. And then I\u0026rsquo;ll have a little stubby, you know, tensor data structure left, which, you know, is not, uh, which is taking up some space, but it\u0026rsquo;s taking up far less space than the actual tensor data in question. And so we\u0026rsquo;ve got a method on our tensor objects that does this. It\u0026rsquo;s called release resources. So just to go over the algorithm first, we, uh, you know, have the strong ref count is greater than zero. We do a bunch of stuff. When the strong reference count goes to zero, we go ahead and release resources. If there are still, um, uh, sorry, we go ahead and release resources, right? Because those are the resources that are not being used anymore. And then as soon as the weak reference count goes to zero, oh, by the way, when the strong reference count goes to zero, we also decrement the weak reference count by one, right? Because remember there was one weak reference count, uh, associated with the strong reference count. So when the weak reference count goes to zero, then we know there really are no pointers into the data, uh, into the object in question. And now we can actually free it from the heat. All right. So that\u0026rsquo;s cool. So that\u0026rsquo;s how C plus plus side support for weak references are implemented. You have to allocate an extra field for maintaining the weak reference count. And then there\u0026rsquo;s a bunch of extra stuff that happens, um, at the allocation time in the common case, uh, when you, uh, deallocate a tensor, there aren\u0026rsquo;t any weak references to it. So the strong reference count goes to zero that causes the weak reference count to go to zero. And then we immediately delete the object in that situation. We are, we actually have an optimization for this, uh, courtesy of Scott Walchalk, where we don\u0026rsquo;t have to do the, uh, atomic compare and exchange anymore. You, you just do a relaxed, uh, load on the week count and check if it\u0026rsquo;s one. And if it is, you just go ahead and delete it in that case. Okay. So what about Python? So Python also implements weak references, but it actually does them in a quite different manner. And, um, Python\u0026rsquo;s, uh, implementation works because, um, remember Python has a global interpreter lock. So it actually doesn\u0026rsquo;t need to work in a multi-threaded setting. I talked a lot about of atomics in the straw, uh, in the C plus plus side of things. And really C plus plus\u0026rsquo;s implementation is, uh, by and large, uh, you know, sort of, it has to look this way because it\u0026rsquo;s supposed to work in a multi-threaded setting. So how exactly do weak references work in Python? Well, it\u0026rsquo;s pretty simple. Every object that is able to be refer, uh, referenced as a weak reference has an extra field called the weak reference list. What exactly is the weak reference list? Well, it\u0026rsquo;s literally a list of all the weak references that point to this object. So weak references in Python is a special object. And so whenever you create one of these to point to an object, we actually just go ahead and put that object on this list. And you know, that would be hella unsafe in a multi-threaded environment, but in Python, it\u0026rsquo;s fine because there\u0026rsquo;s a global interpreter lock. So whatever. And so now, um, these weak references don\u0026rsquo;t actually, uh, don\u0026rsquo;t actually, um, increase the true Python ref count. So Python ref count does the normal thing when it goes to zero as part of the deallocation process, we go ahead and go through all the weak references pointing to this object and say, okay, well, you are no longer valid. So you can\u0026rsquo;t, uh, you can\u0026rsquo;t use this weak reference to go ahead and, uh, run this object. And because, um, you know, we know what all the weak references to this object are. We can also go ahead and run finalizers. So that\u0026rsquo;s, that\u0026rsquo;s also when finalizers get run in Python. We just iterate through all the weak references. Those weak references can have finalizers attached to them. And that\u0026rsquo;s just some code we execute when we do it, by the way, uh, the fact that finalization can resurrect an object because, you know, finalization is just arbitrary Python code. Maybe when you\u0026rsquo;re done finalizing, the reference count has gone back to one or greater. That is exactly what we\u0026rsquo;re using to implement, uh, you know, uh, tensor py object resurrection, which we talked about in a previous podcast. Okay. So that\u0026rsquo;s about how Python side weak references work. So let\u0026rsquo;s talk a little bit about some consequences of these implementations. So one thing to know about is that, um, when you use weak references in Python to specifically do tensors, you have an extra, uh, uh, because of Python object resurrection, there\u0026rsquo;s a little extra work you have to do. So the work you have to do is there\u0026rsquo;s a private method on tensor called fix weak ref. And what it does is it, um, makes sure that the sort of ownership pointer between the Python object and the tensor object looks the correct way. Let me explain why this is needed. So I mentioned that we\u0026rsquo;ve got this thing called Python object resurrection, which says that when a Python tensor object would have died, we check if the C++ object for it is still live. If it is, we go ahead and flip the ownership pointer so that the C++ object owns the Python pointer. And whenever we take out a new Python reference to the, uh, py object, making it live again, we go ahead and flip the reference back. Well, the problem with weak references in Python is they constitute another way of accessing the Python object, um, that it might be ostensibly, uh, sorry, a Python object that isn\u0026rsquo;t a normal, um, you know, sort of give me a tensor from the Python API bindings. And most importantly, this way of referencing the Python object is not interposable by us. So we have no way of seeing when this sort of thing happens and then going ahead and flipping the ownership pointer if it\u0026rsquo;s necessary. So you have to tell us, um, this yourself. So this is something to be aware of if you\u0026rsquo;re working with weak references. And if you\u0026rsquo;re working with weak references in Python, you probably want to do them with tensors. So this is something you need to know about. Like it\u0026rsquo;s very, very important to do. Um, another consequence of, um, this design is, um, so I mentioned that release resources is about releasing resources that, you know, sort of take up a lot of space when the strong reference count goes to zero, but maybe there\u0026rsquo;s still a weak reference count. Release resources is a virtual method because, um, there are maybe multiple tensor subclasses and they might have different resources that need to be deallocated. So it\u0026rsquo;s actually, um, and this was discovered by Scott wall chalk. Um, it\u0026rsquo;s actually quite a performance, uh, problem to always be, uh, to always be, uh, running the release resources, uh, method, um, whenever a strong reference count goes to zero, because, um, most of the time there aren\u0026rsquo;t any weak references. So you can just go ahead and delete the object entirely and like, that would be fine, right? That would also, uh, do the same thing. And in particular, the delete, uh, um, method would not actually, well, okay. It\u0026rsquo;s also virtual, but you\u0026rsquo;re safe. You\u0026rsquo;re going from two virtual calls to one virtual call. So Scott has a patch that basically makes the call to release resources optional. It only gets called if we\u0026rsquo;re actually in the situation where we\u0026rsquo;re trying to keep the object live for weak references, but we know that all the strong references are dead and we want to delete the data. Um, so, you know, there\u0026rsquo;s a lot of this kind of optimization that goes into making a smart pointer, uh, implementation. And so it\u0026rsquo;s, it\u0026rsquo;s quite tricky actually. Like the basic algorithm is not too hard, but then you want to like reduce the number of atomics and, uh, you know, get it as efficient as possible. And that\u0026rsquo;s when things get pretty complicated. In fact, it\u0026rsquo;s, it\u0026rsquo;s so complicated that, um, Scott\u0026rsquo;s original version of PR has a bug in it. And the bug in it is essentially related to, um, how we maintain the reference counts. And when we have the, um, when we\u0026rsquo;re running release resources, because release resources is actually, it\u0026rsquo;s a pretty much an arbitrary piece of code that gets run at the end of the object. It\u0026rsquo;s basically like a finalizer. And so because release resources can trigger arbitrary, other destructors to run. One of the things that it can do is it can actually cause a weak reference to the tensor you\u0026rsquo;re currently deallocating to be dead. So you need to make sure that while you\u0026rsquo;re running release resources, you don\u0026rsquo;t accidentally deallocate the object you\u0026rsquo;re working on while you\u0026rsquo;re doing it, right? Because, um, you\u0026rsquo;re ostensibly running release resources because it\u0026rsquo;s being kept live by a weak reference. But if that weak reference dies while you\u0026rsquo;re releasing the resources, you need to keep the object live until you\u0026rsquo;re done running release resources, and then you can delete it. So, you know, just the kind of thing to be worried about. Okay, that\u0026rsquo;s everything I wanted to talk about today. Uh, see you again next time.\nEP64 Learning-rate-schedulers Learning-rate-schedulers Hello, everyone, and welcome to the PyTorchDev podcast. Today, I want to talk about learning rate schedulers on request of Nelson Alhaj. What is a learning rate? Well, remember, deep learning is all about optimization, and optimization is all about starting off at some point in your very hyperdimensional parameter space and then slowly making your way to a set of parameters which does better. And so every step we do is based off of the gradient we compute for computation. And so the learning rate simply says, once I\u0026rsquo;m at some point in my parameter space and I figure out where I want to go, my gradient, how far do I go in that direction before I stop and reassess the landscape and compute my gradient again and go further? So that is the learning rate. So why does the learning rate matter? Well, you can think about the situation and if you have a very, you know, spiky landscape where there\u0026rsquo;s a lot of different changes to the gradient, then if you do a very large learning rate and you make a very large step when you\u0026rsquo;re doing an optimization, you may have been locally improving the loss for a very small amount of the step, but then the landscape changed and now you\u0026rsquo;re climbing back up the hill and you just went too far and you overshot the place you wanted to go. A very common diagram, and sorry, this is a podcast so I can\u0026rsquo;t show you a picture. A very common diagram is imagine you have some sort of valley where the valley sort of is slowly going down until you get to the global optimum. In this case, we\u0026rsquo;ll have the global optimum be something that\u0026rsquo;s low because this graph is representing our loss. So the lower the losses, the better. So if you start your ball, the ball being, you know, the point we are at on the parameter space on the side of the valley, then if you go too far, you will bypass the sort of the bottom, the deepest point of the valley and hop to the other side of the valley. And then you\u0026rsquo;ll sort of zigzag back and forth until eventually you get to the final destination. But you\u0026rsquo;ll do a lot of wasted steps along the way. So, you know, a lot of sort of optimization techniques and a lot of playing around with learning rate, it\u0026rsquo;s all about sort of trying to get to your final destination, you know, more directly without, you know, overshooting every single time. That being said, you don\u0026rsquo;t want your learning rate to be too small either because, well, if it\u0026rsquo;s a really small learning rate, then you\u0026rsquo;re just not making very much progress at any given step in time. So, you know, if you don\u0026rsquo;t make very much progress, you might just never actually get to convergence on your network in this situation. Okay. So learning rates are kind of important. And, you know, certainly when you\u0026rsquo;re writing a Python model, you\u0026rsquo;ll have some sort of optimizer. And your optimizer is going to make some decisions about how exactly it\u0026rsquo;s going to go about exploring the state space. But most optimizers have a hyperparameter called the learning rate, which is just a global number you can toggle to say sort of how, you know, far or close you should go. There are some optimizers that automatically determine a good learning rate, but there are also optimizers which don\u0026rsquo;t. And so that\u0026rsquo;s just a parameter you have to do. So a learning rate scheduler is a way to sort of automatically modify this parameter on your, this hyperparameter on your optimizers in some way that\u0026rsquo;s sort of non-standard, right? Because there\u0026rsquo;s a lot of things you might want to do. Maybe while you are warming up, you know, while you\u0026rsquo;re doing the initial, you know, few steps of your commutation, you don\u0026rsquo;t want to, you know, go too far. So you want to sort of just slowly explore your local space, ramping up until you actually hit your final learning rate. Or maybe, you know, as time goes on, you want your learning rate to decay and get smaller and smaller so that, you know, you\u0026rsquo;re, you know, after you\u0026rsquo;ve done all the major learning at the beginning, you\u0026rsquo;re going to finally get closer and closer to your, the final optimum. And now you want to make smaller and smaller steps so you are careful not to overshoot in the situation. So there are tons and tons of, you know, different learning rate schedulers. Honestly, there aren\u0026rsquo;t that many. So if you like look at torch.optim, that\u0026rsquo;s the directory that has all of our optimizers. We have tons and tons of optimizers because there are lots of, you know, ways to go about doing optimizations. Our learning rate schedulers, they fit in a single Python file. So, you know, it\u0026rsquo;s really not, there\u0026rsquo;s not that much stuff going on there. But it\u0026rsquo;s something that, you know, people do care about. And that\u0026rsquo;s what I want to talk about in the podcast today. So we have to, so where I want to go next is how exactly does the learning rate scheduler API in PyTorch works? It\u0026rsquo;s kind of surprising. And we basically haven\u0026rsquo;t changed it since, you know, the very beginning. I think it was like 0.1. Someone submitted a pull request to add learning rate schedulers to PyTorch. And we were like, okay, we\u0026rsquo;ll add it. And we have basically not changed the API since then. A lot of APIs in PyTorch\u0026rsquo;s neural network, you know, library in Python have not changed. So this can result in some weirdness in the API that, you know, things we learned over time and we haven\u0026rsquo;t been able to fix them. Well, let\u0026rsquo;s talk a little bit about what this learning rate scheduler API looks like. So the learning rate scheduler API is sort of based off of two things. The first thing it\u0026rsquo;s based off of is it\u0026rsquo;s based off the optimizer API. Why is this important? Well, optimizers in PyTorch are stateful. So the standard model for, you know, a PyTorch program is, you know, you\u0026rsquo;re off doing your optimization and the way things work is you go ahead, you run your computation, you run your forwards and backwards, you compute the gradients, and then you ask the optimizer to do a step. And the step, you know, is a method on the optimizer. It\u0026rsquo;s a stateful method. It has side effects. And what it does is it goes ahead and reads out all the gradients, updates the optimizer\u0026rsquo;s internal state, and updates all the parameters to actually, you know, make them all go well. So, you know, the model that people have is, you know, they\u0026rsquo;re looping through their, you know, batches of inputs and every batch they do, they call the optimizer step at the end. So our intrepid contributor back in the day looked at this API and they were like, okay, well, let\u0026rsquo;s do something similar for learning rate scheduling. So what they did was they said, okay, we\u0026rsquo;ll have an API for, you know, modifying the learning rate. It\u0026rsquo;s going to be a learning rate scheduler object and you will call step on it to modify the learning rate. Unlike optimizers, you know, optimization needs to happen every mini batch, right? Because, you know, every batch you do, you want to actually update the parameters with what you did. Typically for a learning rate, you only want to do that for an entire epoch. You don\u0026rsquo;t want to modify the learning rate until you\u0026rsquo;ve actually finished processing the entirety of your input data set. So learning rate has to have its own step function, but okay, fine. So, you know, you have your optimizer step, you have your learning rate step, and, you know, you just call them when appropriate, either at the end of your training iteration or at the end of your training epoch. That being said, actually, in the beginning, learning rate schedulers were implemented in a kind of funny way, and whether or not you call them before the optimizer step or after the optimizer step was something that sort of wasn\u0026rsquo;t well specified. So we had to make a BC breaking change to sort of fix it so that, you know, the behavior was uniform. You always call it after the optimizer step. It was pretty confusing because, you know, CFL APIs are confusing. It\u0026rsquo;s hard to, you know, make sure that they do exactly the right thing. And, you know, you don\u0026rsquo;t even notice half the time, right? Because the learning rate is just this hyperparameter and obviously your optimization is still going to work even if you stay stuck on your old learning rate, you know, one epoch more than you expected. So the kind of people who notice this sort of thing is like if they\u0026rsquo;re like real, they have some network that\u0026rsquo;s super sensitive to the initial conditions or maybe they\u0026rsquo;re trying to reproduce a paper and they\u0026rsquo;re like, huh, how come the learning rate is not the same thing as, you know, on what I saw in the paper? And, well, that\u0026rsquo;s because, you know, we messed up the stateful API. That simple. Okay. So I mentioned that the learning rate API was based off of this optimizer stateful API, right? So it\u0026rsquo;s like you say, okay, you know, when I\u0026rsquo;m done, I run step and that will update the learning rates everywhere. But the second thing, and this is important, is that PyTorch\u0026rsquo;s learning rate schedulers were essentially cribbed from Keras. So, you know, Keras was, you know, existed back then. Keras has been around for a while and Keras had a learning rate scheduler API and basically, besides, you know, statefulizing up the API, because Keras' learning rate API is sort of based on a sort of callback model where, you know, the optimizer calls into the learning rate callback to figure out what to do. basically grafted into the stateful API but using the same algorithms that Keras was using to determine learning rate. And in particular, the way Keras computes learning rate is you are at some point in your computation and you are, you know, you basically are at some epoch, you know, epoch 10, epoch 20, whatever, and you have a formula which says, given this epoch, what should my learning rate be? So this is a closed form formula. It, you know, takes in the epoch, produces the learning rate and that\u0026rsquo;s what you set everything to. So, okay, so we\u0026rsquo;ve got the stateful API but what the stateful API is doing under the hood is it\u0026rsquo;s just going ahead and running this closed form compute to figure out what the next step should be. So actually, there is no, there\u0026rsquo;s no statefulness beyond the fact that, you know, you just call step and this internal state mutates. Actually, this is why the step function for the longest time accepted an epoch parameter and you could use this to sort of, you know, time travel your learning rate schedulers into the future, right? Epoch 1, Epoch 2, Epoch 100, whatever, you know, that\u0026rsquo;s fine. It\u0026rsquo;s going to work. Why does it work? It\u0026rsquo;s because there\u0026rsquo;s a closed form formula, right? And we can just zoom straight to that spot and, you know, that seemed reasonable-ish. The problem with giving people stateful APIs is they start looking at what the stateful APIs do and they start expecting them to actually be stateful. So pretty early in PyTorch\u0026rsquo;s life, we got a feature request. And the feature request was, I\u0026rsquo;d like to have so-called chainable or, as I like to say, composable learning rate schedulers. So the ask here was, you know, sometimes people want to combine various learning rate strategies, right? They might have a learning rate strategy where they are doing, you know, they\u0026rsquo;re doing some sort of decay as the training run goes on and on and on, but they want some special behavior at the very beginning. And so they\u0026rsquo;ll have an extra learning rate scheduler just for handling that sort of situation. And it\u0026rsquo;s not really obvious how to mash together two learning rate schedulers. Certainly, if they\u0026rsquo;re using the closed form solutions, they\u0026rsquo;re just not compositional at all because let\u0026rsquo;s say that you have one learning rate scheduler and you call it and it figures out, oh, hey, the learning rate should be five at this epoch and it sets all the learning rates to five. And then the next learning rate scheduler, you know, says, okay, well, this is the current epoch and the learning schedule should be eight. And then it sets eight to everyone. And actually, you know, like they basically don\u0026rsquo;t communicate with each other at all, right? The closed form solutions are actually not compositional in this way. So people were like, hey, you know, it would be cool if, you know, actually if I, you know, did a learning rate schedule step and then another learning rate schedule step, it actually did what the API suggested. That is to say, you know, we\u0026rsquo;ve got the stateful API, a step should, you know, transform the learning rate to the next learning rate. And like, you know, that\u0026rsquo;s what I expected to do. And through the efforts of Chandler Zoho and then later Vincent Quineville Blair, we actually did exactly that. We took all of the closed form formulas that were previously in the learning rate scheduler and we essentially figured out how to turn them into the single step functions that would give you the same result as the closed form solution. So now you could actually compose these things because you would say, okay, well, first I apply the step implied by the first learning rate schedule and then I apply the step up implied by the second learning rate schedule. And now you actually have compositional learning rate schedulers. Woohoo! Well, actually the first time we tried to land this, it broke because remember, you\u0026rsquo;ve got time traveling epochs and if you\u0026rsquo;re going to do time traveling epochs, I don\u0026rsquo;t know how you\u0026rsquo;re going to do the stepping thing because like, how does that even work? You don\u0026rsquo;t have a closed form solution anymore and so basically you only have a choice of either going ahead and playing out the epochs one by one if you have a time traveling epoch or you can do what we actually did which is we\u0026rsquo;ve got a closed form solution in our back pocket. it\u0026rsquo;s like an underscore method closed form and we just call that and we\u0026rsquo;re like, it\u0026rsquo;s not going to be compositional if you\u0026rsquo;re time traveling. All right. So, the reason why I did this podcast episode is because essentially Nelson came to me and was like, hey, what the heck is going on with these learning rate schedulers? It feels like someone had a dare to make it as stateful as possible and they followed through with the dare and the answer to that is yes, that is basically what happened, right? we started off with a stateful API wrapping over a functional closed form computation of learning rate schedules but people were like, hey, you know, stateful API, I\u0026rsquo;m expecting it to be stateful and so we turned the insights into the stateful version. Was this the right decision? I have no idea. I managed to trick several people into, you know, making this possible so if it was a bad decision I suppose it wasn\u0026rsquo;t obviously a bad decision but with the benefit of hindsight I\u0026rsquo;m not really sure I would have gone about doing it the same way. Probably the distinction that we probably should have made is there are some learning rate schedules that are compensational and some that are not, right? So like if I\u0026rsquo;m going to do a exponential learning rate and then I want to compose this with something that sort of fiddles around with the initial conditions of my learning rate what I\u0026rsquo;m probably expecting to happen is I start off with my exponential rate exactly as is and then I\u0026rsquo;m just going to do a transformation on that learning rate afterwards on a thing and so probably people weren\u0026rsquo;t expecting to like arbitrarily compose an exponential learning rate with a step LR learning rate all sorts of random compositions probably that\u0026rsquo;s not actually what people want to do they probably only want a set of compositional ones but then the basic learning rate schedules those probably just want to be closed for maybe I have no idea one of the things about learning rate schedules in PyTorch is as I said it is very simple the API is not so simple sorry we\u0026rsquo;re kind of stuck with the stateful API but it\u0026rsquo;s very easy to write your own learning rate scheduler and so you know with a lot of things in PyTorch\u0026rsquo;s you know core library sometimes they\u0026rsquo;re just not very well put together and it\u0026rsquo;s been okay it\u0026rsquo;s because people can just write their own you know schedulers and do their own thing and that\u0026rsquo;s always been you know one of the things about PyTorch it\u0026rsquo;s that you know hey if there\u0026rsquo;s some piece you don\u0026rsquo;t like well this is just a library you don\u0026rsquo;t have to use us you can you know write your own thing and really all the learning rate scheduler is doing is it\u0026rsquo;s going into the optimizers and just updating their internal learning rates so you can absolutely as I said it\u0026rsquo;s just one file you can go ahead and do your own thing and people have gone ahead and done their own thing I know for the very least like ClassyVision had their own you know implementation of learning rate learning rate schedules learning rate schedulers I mean I talked about what learning rates are and what really is going on here I think is just a question about PyTorch\u0026rsquo;s API design right one of the things that made PyTorch really really successful was that we let people work with NN modules in a imperative mutable way it\u0026rsquo;s just very very natural for people if you look in the Jax world people are trying to discover how to make neural networks work with a functional API where you don\u0026rsquo;t have stateful operations it\u0026rsquo;s you know pretty interesting I think they\u0026rsquo;ve come up with some pretty good stuff but it\u0026rsquo;s also non-obvious what exactly that API should be because it\u0026rsquo;s just like just less natural for people and so as a result there\u0026rsquo;s lots of libraries exploring different corners of the design space I actually think they will probably figure out a really good design in the end but it\u0026rsquo;s going to take you know a dozen libraries to get there or I guess we can be in the PyTorch world where it\u0026rsquo;s like hey mutation everywhere hooray and you know it also is very very complicated it\u0026rsquo;s probably more complicated than the functional API but I mean people seem to like it so who am I to quibble with them this is me right a formerly rabid purely functional programmer I used to work on the GHC Haskell compiler and now I\u0026rsquo;m like hey you know mutation is great I just use all of my functional programming tricks to help reason about what the code is supposed to do in the end all right that\u0026rsquo;s everything I wanted to talk about today talk to you next\nEP65 History-of-functorch History-of-functorch Hello, everyone, and welcome to the PyTorch Dev Podcast. Today, I\u0026rsquo;m joined by Richard Zhou, who is going to come talk to us about the history of Funktorch. Hi, Richard. Hi, Ed. Thanks for having me. All right, Richard. So before we get started, let\u0026rsquo;s just briefly talk about what is Funktorch and what does it let you do? Cool. Yeah, so Funktorch was inspired by Google\u0026rsquo;s JAX framework, which was released in either like 2018 or 2019. I don\u0026rsquo;t remember at this point. The novel thing that JAX brought to the community was the notion of composable function transforms, and that\u0026rsquo;s what Funktorch provides as well. So let\u0026rsquo;s unpack those three words a bit. There\u0026rsquo;s a lot of meaning behind them. So a function transform is an API that takes in a function and returns you a new function that does something else. It transforms your function to do something else. JAX offers a grad transform. You pass it a function and it returns you a new function that computes gradients. It offers a JIT function. You pass a function. It returns you a new function that makes your code run fast. And they also provide this new thing called VMAP. And you pass VMAP a function and it will return you a new function that accepts tensors or arrays with an additional input. So VMAP is like doing a for loop of your function over your data. And instead of actually doing a for loop, it\u0026rsquo;s making your code faster. So I remember when JAX came out, you know, way back in 2018. And it was like this cool kid on the block. And, you know, there\u0026rsquo;s a lot of, you know, buzz on Twitter about it. And, you know, some of the things that they were doing seemed really legitimately like useful. Like, for example, VMAP was just a total, like, game changer in, like, how, you know, you should go about doing batching in your computations. How did that turn into, oh, I guess we should, you know, build our own thing, Functorch, that was inspired by it? Right. A lot of us on, like, the PyTorch team, we tried out JAX and we thought it was really cool. JAX has its, like, core library of function transforms that can compose with each other to provide, like, various different other quantities. And so people were actually asking us for similar features in PyTorch. They\u0026rsquo;ve been wanting, like, efficient Jacobian computation. They\u0026rsquo;ve been wanting fast per sample gradient computation, like, all of which is easy to do with JAX. And JAX showed that we could get these through, like, the compositions of their transforms. One of the motivations for Functorch was instead of, like, us designing a different subsystem to compute, like, per sample gradients, you could instead compose two things, like, vmap and grad to produce per sample gradients. Instead of designing a separate subsystem to compute Jacobians, you can compose, like, vmap and grad in some order to also just give you Jacobians out of the box. Furthermore, we thought this would be more future-proof. We could see people wanting, like, other, like, crazy quantities in the future. And if we provided these function transforms in PyTorch, then the rationale was people could just compose them to do whatever they needed in the future. So now you\u0026rsquo;re making it sound like we just, you know, started off and thought, hey, we\u0026rsquo;re going to, like, you know, we want to build these new features and we\u0026rsquo;re going to do them with function transforms. But that\u0026rsquo;s not actually how it worked out, right? I\u0026rsquo;m jumping around a bit. Because didn\u0026rsquo;t we do a version of per sample gradients entirely directly in PyTorch itself initially? Yeah. So initially, we actually did try to prototype vmap in PyTorch using the regular PyTorch dispatcher. And what happened there was it could compute some quantities, but not, like, all quantities. So you could compose, like, the vmap we had prototyped in PyTorch with PyTorch autograd to give you Jacobians. But we had this problem that it couldn\u0026rsquo;t actually compute per sample gradients. Oh, yes. I remembered wrong. It wasn\u0026rsquo;t that we implemented per sample gradients, but that we did vmap in PyTorch, but it had a fixed ordering. So you could, yeah, you could, what is it, vmap over grad, but not grad over vmap? You can grad over vmap. You can grad over vmap, but you couldn\u0026rsquo;t have vmap over grad. This is one of the things people find, you know, it\u0026rsquo;s one of the, like, mind-bending things about function composition is that you can do them in whatever order you want in Jaxx. And PyTorch\u0026rsquo;s traditional API has always been about, you know, well, there\u0026rsquo;s a fixed order. And it just turns out that\u0026rsquo;s not enough sometimes. Yep. So how did we actually decide that, no, we actually need the full generality of function transforms? Yeah, so, um, we, so, okay, so in addition to, like, figuring out if we needed a full generality of function transforms, like, we also took a deeper look at Jaxx. And we\u0026rsquo;re like, hey, um, Jaxx is, like, very functional. And we weren\u0026rsquo;t, so, like, one of the things we could do is just, like, copy paste Jaxx into PyTorch. And that\u0026rsquo;s not, like, a good idea at first glance, because, like, Jaxx has all these limitations on its code. Like, Jaxx is purely functional. Uh, there\u0026rsquo;s no mutations, but PyTorch, in PyTorch, people actually use a lot of mutations and, like, views in their programs. And that actually matters. So at the beginning of around 2021, Horace Ho and I, uh, we started to take a step back, um, to take a more holistic view at, um, this idea of bringing function transforms, uh, to PyTorch. And so, like, we wanted to answer questions, like, do researchers, like, actually want to use transforms in PyTorch? Um, did this composability matter to them? And, uh, were users willing to give away some flexibility on what they could do with PyTorch, like, to limit themselves to this world where composable function transforms actually works well? And so Horace and I, we ended up talking to around, um, 10 to 20 researchers, we did some user studies, and we, we basically just code emailed some of them, uh, we, we messaged some on Slack asking if they wanted to talk, to talk. And also asked publicly for interest. And we got, from, from those user conversations, like, we got yes. The answer was yes. Like, people did want to use transforms. Like, they wanted to use VMAP. Um, they wanted, like, their code to go faster. Who doesn\u0026rsquo;t want their code to go faster? Like, you ask, you ask people on this, and they can\u0026rsquo;t say no. And then, uh, like, users were, we saw that users were indeed actually, like, as we thought before, like, looking at, like, wanting different compositions of, like, VMAP and grad. So people, like, they wanted to compute per-timeable gradients in PyTorch, like, fast Jacobians. We talked to some meta-learning researchers, and that\u0026rsquo;s, like, a, a bit of a crazier composition of VMAP and grad. Like, you can do that to easily be able to express some, uh, models and model-agnostic meta-learning. And we also saw, like, some scientific computing use cases where people just had really overhead bound code, and they just wanted their code to go faster. That\u0026rsquo;s a lot of different users. Um, I suppose that, like, if we were just reading the tweets about Jax, um, you could have figured all those out. But I do remember, like, the fact that you guys did these user studies was really useful. Do you, do you remember, um, you know, what were some of the outcomes from the user studies? Yeah. So we, we didn\u0026rsquo;t just talk to users while doing user studies. So, before all of this, like, um, Zach DeVito, who you guys might know, um, he sent us a slide, of course, a nice slide deck on design thinking from the Stanford D School on, like, just how to design things for users. And they were, like, the TLDR is that there are, like, four steps, um, to designing something. You want to do some brainstorming, uh, some prototyping, some talking to users. Uh, you give the user some bad prototypes, and then you iterate on these prototypes until they, um, become, like, less bad prototypes. And you don\u0026rsquo;t have to do this, like, in, like, that order. Like, they can be done in any order. After the, like, we started talking to users, we actually started giving people some of these prototypes. And we, like, worked with these users on these prototypes and just saw which ones they liked better, which ones they didn\u0026rsquo;t really like, and kept on iterating until we got something better. If I recall correctly, you ended up with two prototypes in the end that you were considering what to do with. Is that right? Yes. So, we had, like, two prototypes in the end. One of them was based off of the PyTorch dispatcher. And so, like, PyTorch Autograd is written in C++. Uh, we had, like, a old prototype of vMap in C++ as well. And there was a layering problem, as we discussed before, where you could layer, um, you could do grad of vMap, but you couldn\u0026rsquo;t do vMap of grad. And to solve that layering problem, we just introduced a new mechanism inside the PyTorch dispatcher that lets you just flip the layers. Um, we call that the dynamic layer stack. What was missing from this prototype was a way to do compilation. Um, and that\u0026rsquo;s prototype number two. In prototype number two, um, we, given a user function, we just captured it using this mechanism called Torch shot FX that produces a graph. And then, using that graph, we could lower it to a backend, like, NNC. Now, the problem is, uh, we couldn\u0026rsquo;t actually do things like vMap over the function or, like, call grad over the function because, um, our vMap and autograd were written in Python. Um, this Torch shot FX tracing only worked in Python. And so, and Jack\u0026rsquo;s, like, all of these things really work together. So, like, our vMap and grad were written in C++, Torch shot FX was written in Python, um, and Jack\u0026rsquo;s, like, you have all these things written in Python and, like, you just, you do the vMap and grad transformations and then you can trace underneath all of that. But we found a solution to this problem, right? Yeah, we ended up finding a solution to this problem. It was, like, very inspired from things you, you, you told us, actually. Like, using the Jack\u0026rsquo;s, uh, design as an inspiration, we were like, okay, why don\u0026rsquo;t we just, like, somehow get Torch shot FX into the dispatcher and, like, just trace out everything. And so, like, that\u0026rsquo;s what we ended up doing. Uh, Horace, like, ended up prototyping a basic version of this, um, that he called Python key. And then, Ed, you implemented a more general version of this called Torch dispatch. And so, what that let us do was actually trace in, like, what the dispatcher was doing using Torch shot FX. So, we ended up smashing all of these prototypes together. And, um, we had, um, vMap implemented in C++, Autograd implemented in C++, this mechanism that let us, like, toggle, like, switch between those two, uh, which one comes first. And then we had, um, Torch dispatch plus Torch FX, which actually let us trace out what we were doing and compile it. And during this time you were giving these to users, was this, was this the winner, um, for, for the users you were working with? Yeah. So, the users actually, they liked both of the prototypes initially. So, they liked both of the prototypes initially. Um, in the Python-only Torch Trap X prototype, we actually re-implemented Autograd and vMap a little bit. Um, so it wasn\u0026rsquo;t more that the users liked this prototype. It was more that the users liked both of the two existing prototypes. And we wanted some way, like, some technical way to not repeat ourselves. Okay. So, at this point in time, if I recall correctly, um, you guys had been greenlit to, like, work on Funktorch properly and make it into a real thing. Um, what, what happened at that point? Yeah, so we took this final prototype, uh, we created the Funktorch repository, uh, on GitHub, and that\u0026rsquo;s where we replaced the prototype. And we were interested in getting even more users to actually install the prototype and use it. Previously, the prototype was just on a branch of PyTorch, but by putting it into the repository and separating out from PyTorch, we were able to release it, like, do separate releases of it, um, and actually get it out to more users than we could before. So, there were, um, two general work streams, um, that we were going for. So, like, one was, uh, compilation. And with the compilation, what we had was an API equivalent to Jax.jit. We had this thing called NNCJit, which basically trace your code, lowered it down to NNC. But Horace was curious to see if, um, we could actually use that with regular PyTorch, existing PyTorch models without changing the existing PyTorch models too much. And that\u0026rsquo;s how AOT Autograd was born. Um, AOT Autograd is this mechanism that lets you trace both the forward and backward paths of a function or a model, and then lower that to a compiler. On the transform side, just this vmap grad, et cetera, uh, side, we did some significant hardening work. Um, you\u0026rsquo;ll notice that I\u0026rsquo;ve been talking about vmap and grad, but there\u0026rsquo;s, like, a, another transform called JVP for forward mode AD. And, um, we didn\u0026rsquo;t have it at that point, Jax did, and we were like, some folks were asking us for it. We ended up leveraging a bunch of existing work streams in PyTorch to really harden the function transforms. On the forward mode AD side, um, Alvin and Jeffrey from the PyTorch core team were already working on a forward mode AD implementation in PyTorch, and we ended up, like, reusing that for Funktorch\u0026rsquo;s JVP transform. Um, we also want to do some significant testing, like, of our operator coverage. And, uh, one thing that was just beginning to brew, um, in PyTorch, uh, back then, was, um, this thing called OpInfos. Uh, so Mike, Ruberi, and Natalia, um, they were, they made this database of operators with sample inputs. And you could test things using, by just querying the database, you could test an operator by querying a database for an operator, some sample inputs, and then just, like, running the operator with the sample inputs. And so we leveraged that to actually do, like, full-on, like, operator testing for VMAP, um, and for, like, all the other Funktorch transforms. And so we basically went along these two routes, and we kept on hardening, uh, Funktorch until, like, our beta release in March of 2022. So it\u0026rsquo;s been, uh, more than half a year, uh, I guess half a year? More than half a year since then. Yeah. What have you been up to since then? Yeah, so the beta release was fairly recent. There was, there\u0026rsquo;s, like, two different stories of what\u0026rsquo;s happening to, um, the transform workstream and to the compilation workstream. But, like, in general, like, our design philosophy has been to, um, try to, in, like, the past half year is, has been to try to make sure that Funktorch fits well into PyTorch. Um, so, in particular, we\u0026rsquo;ve been trying to move Funktorch, um, into PyTorch and just upstream everything. In fact, we, we\u0026rsquo;ve done it, right? Like, Funktorch now lives in the PyTorch repository. In fact, we have upstreamed everything, but there is still some API work left to be done. On the eager transform side, um, we\u0026rsquo;ve just been trying to make sure that, like, the Funktorch transforms, um, compose well with existing PyTorch constructs, uh, and vice versa. So that\u0026rsquo;s been number one. And number two is, um, a lot of the Funktorch APIs sort of exist in PyTorch, but just as, like, not, they, they don\u0026rsquo;t work as well. So, like, there\u0026rsquo;s this older Torch vmap in PyTorch, uh, Funktorch\u0026rsquo;s vmap supersedes that. Um, then, PyTorch has an NN functional module API. Funktorch also has that. And so we\u0026rsquo;re working on trying to, like, pick one to just, like, go with in the future and deprecate the other one. Um, PyTorch already has ways to compute, like, Jacobus and Hessians in the Torch autograd functional library. However, you cannot actually vmap over those. Or you can vmap in some cases, but it doesn\u0026rsquo;t work in all cases. And users have tried this. And so, like, on the transform side, we have been hardening, uh, Funktorch, um, in order to be able to wholesale replace parts of, uh, existing PyTorch APIs. On the compilation side, um, this thing called Torch Dynamo came along. So, Torch Dynamo is this, uh, Python bytecode tracing JIT. What it really means for users is that you can rely on Torch Dynamo to capture PyTorch code and not have to, like, constrain yourself to limitations of tracing. Like, one thing we did hear from users was that sometimes it was annoying to just write code that could be traced by Jaxx.JIT. Um, Torch Dynamo sort of lets you completely avoid this. You can write whatever you want and it will capture the pieces that are actually, um, traceable. And so, like, this gives you a better UX. Um, there\u0026rsquo;s no need to worry about the constraints of tracing. So, we felt like Torch Dynamo was a better user story, um, for the PyTorch compilation story. Um, however, AOT Autograd is still alive somewhere. Torch Dynamo works at the Python level. It gives you a Python program in order to do things like, sorry, it gives you a trace of a Python program in order to do things like, uh, compile through model training where you need to compute gradients. Then you need something to read into, like, what the C++ Autograd code is doing. And that\u0026rsquo;s where AOT Autograd comes in. In fact, I wouldn\u0026rsquo;t say it\u0026rsquo;s just alive. I\u0026rsquo;d say it\u0026rsquo;s, like, an integral part of using Dynamo to optimize training code. It just would not work without AOT Autograd at all. Right. All right. So, what\u0026rsquo;s coming next for us from Functorch? Yeah. So, we started Functorch, um, Horst and I started Functorch because we were in all of what you could do with Jaxx and there\u0026rsquo;s still a long way to go. I\u0026rsquo;ll just give you two items. I don\u0026rsquo;t want to promise too much. Um, so, the first one is compilation and performance. So, we haven\u0026rsquo;t really kept an eye on numbers. Like, from working with our users, we found things like, um, like, your code computation is something like 5x faster than what it used to be for some, what it could have been for some use cases. But we, although we know that Functorch is faster at actually computing these quantities than, like, naive ways to do them, uh, we don\u0026rsquo;t actually know what the baseline is. So, definitely want to focus on just finding out if we can, like, get additional performance out and we want to make sure that Functorch actually works well with the compilation pipeline. So, that\u0026rsquo;s number one. Uh, number two is we\u0026rsquo;d like to improve the set of PyTorch-like functions or functions written in PyTorch that can be transformed over using Functorch. And in particular, we don\u0026rsquo;t support some existing PyTorch constructs yet. Users have really asked us for, uh, uh, autograd.function support. And so, that\u0026rsquo;s, like, one of them. Um, some other things that users have asked us for are things like data-dependent control flow, uh, where you do things like write an if statement that is conditional on, like, the data of a tensor. So, if x squared and zero, do something else, do something else. Or, like, while loops, that we\u0026rsquo;re at a while loop condition is conditional on a tensor. And so, people want to actually write data-dependent control flow and do things like vmap over them. And that\u0026rsquo;s something you can do in JAX. You cannot do that in Functorch yet. All right. Well, thanks a lot for coming to tell us about the history of Functorch, Richard. Cool. Thanks for having me, Ed. Goodbye.\nEP66 PyTorch-2.0 PyTorch-2.0 Hello, everyone, and welcome to the PyTorch Dev Podcast. So you may have seen in the news that we have announced the release of PyTorch 2.0. If you haven\u0026rsquo;t seen it already, Sumith has a keynote talk from the PyTorch Dev Conference, which you can go check out to see a, you know, sort of very quick overview of all the concepts behind PyTorch 2. Today\u0026rsquo;s podcast is going to be the beginning of a series of podcasts diving deep into all aspects of PyTorch 2.0. Today\u0026rsquo;s podcast, I just want to talk a little bit about the high-level constraints behind PyTorch 2, sort of just do a little bit of an elaboration over Sumith\u0026rsquo;s talk, you know, go into a little bit more of the details about, you know, what we were thinking about and, you know, what you should expect when you start digging into the components of PyTorch 2. Accompanying the release of this podcast are two docs that we wrote about half a year ago, sort of setting the goaling for PyTorch 2. It\u0026rsquo;s the PyTorch 2 manifesto and the PyTorch 2 architecture documents. I went through them and didn\u0026rsquo;t have to edit them very much. So we did a pretty good job of setting up what we wanted to do half a year ago. And if you are more of a fan of the written text, you can go check those out. And they\u0026rsquo;ll also talk about the things we\u0026rsquo;re going to talk about here. Okay, so PyTorch 2, what is it? Well, you know, if we think about the user experience, what we\u0026rsquo;ve got is we\u0026rsquo;ve got a new function called torch.compile. And when you put it on your models, things go faster. So that\u0026rsquo;s basically like at a very, very high level, what to expect. But of course, this is the PyTorch dev podcast. So we want to look a little deeper. So the question we have here is what exactly is torch.compile doing when you actually do this? What the heck is going on with all the components? Why is this different from the various different compilation methods like torch script and fx that we\u0026rsquo;ve done before in PyTorch 2? All right, well, let\u0026rsquo;s try to unpack this. So at the top level, when you look at PyTorch 2, there are a few important components. So first, there\u0026rsquo;s a graph acquisition mechanism. That\u0026rsquo;s torch dynamo, where you essentially have a symbolic evaluator for Python bytecode. It goes ahead and looks at your Python code. It tries to understand as much as it can. Whatever it can understand, it, you know, sort of steps through it, bytecode, bytecode, bytecode, and gives you a graph representing the tensor operations that happen during that segment. If it doesn\u0026rsquo;t understand something, then it says, oh, well, whatever, and goes ahead and uses the Python interpreter, the regular Python interpreter, as a backup mechanism. So you have Dynamo. When you have Dynamo, you\u0026rsquo;ve got a bunch of these graphs. And what you need to do is you need to actually, you know, incorporate these graphs into a, you know, Python program that might have a bunch of regular Eager kernels in it. Because we said that this is not a full graph capture mechanism. It\u0026rsquo;s a partial graph capture mechanism. And so to do that, well, we need some sort of mechanism with integrating with the traditional Eager Mode automatic differentiation system. And that mechanism is called AOT Autograd. It takes a graph and turns it into a custom Autograd function that knows both how to run forwards and backwards. And of course, these forwards and backwards are also represented as graphs. And then what we do is we go ahead and send those on to a compiler. And the compiler that we\u0026rsquo;ve been, you know, advertising the most with the most recent release is Torch Inductor, which is what we call a define by run compiler built on top of Trident, which, you know, just actually knows how to go ahead and compile a bunch of code. So three big components, right? So you\u0026rsquo;ve got the graph acquisition, then you\u0026rsquo;ve got graph lowering, and then you\u0026rsquo;ve got actual backend compiler. And, you know, if you aren\u0026rsquo;t paying too close attention, this might sound like, you know, the regular story that you\u0026rsquo;ve heard over and over again about all sorts of things, you know, when you want to compile deep learning models. So what makes PyTorch 2 different? Like, why did we not do this, you know, five years ago when we embarked on building TorchScript? How come we couldn\u0026rsquo;t use TorchScript to do these things? You know, what is peculiar about the system that we\u0026rsquo;ve set up here? So there are a bunch of things that I want to call your attention to. But the first and foremost one is that PyTorch 2.0 is a partial graph mechanism. Now, I\u0026rsquo;ve already mentioned the word partial graphs. And to just unpack the definition of partial graphs a moment, what I mean by partial graphs is that when I\u0026rsquo;m running my compiler, I don\u0026rsquo;t expect to actually necessarily be able to compile my entire program. Now, if I can compile my entire program, that\u0026rsquo;s great. I\u0026rsquo;m not going to, like, purposely stop myself from compiling the entire program. But it\u0026rsquo;s a non-goal to get it all the time. And this is, you know, in deep contrast with lots of other sort of mechanisms, like, you know, if you think about TensorFlow or you think about TorchScript, these are all, you know, predicated on sort of whole graph acquisition mechanisms where you want to get the entirety of your program into some format. And indeed, there are some good reasons to want to get the entire graph. For example, if you want to ship a model to mobile, something that PyTorch does support but, you know, is not as first class a citizen as, you know, if you were, for example, programming on top of TF Lite, to ship a model to mobile, you would need to actually have the entire model, right? You couldn\u0026rsquo;t actually, you know, have an interspersed mix of, you know, a bunch of operators that you\u0026rsquo;ve compiled from partial graphs and then a bunch of Python code. That wouldn\u0026rsquo;t work. Well, unless you were, you know, going to ship a Python interpreter to your mobile phone, which, you know, maybe is a good idea. But, you know, let\u0026rsquo;s set that aside for a moment. So, you know, there are a bunch of use cases where you just don\u0026rsquo;t want to have a Python interpreter and so you naturally gravitate in towards, you know, having a, you know, full graph export mechanism or, you know, you might try and say, okay, well, I want my entire programming language to be differentiable and I\u0026rsquo;m going to build my deep learning compiler on top of an entire programming language that I can understand. But hey, we\u0026rsquo;re PyTorch, we\u0026rsquo;re built on top of Python, we have a lot of users using Python, they don\u0026rsquo;t necessarily need to export their graphs to a runtime that doesn\u0026rsquo;t run Python at all. And in return, what we get for saying, okay, well, sometimes we just don\u0026rsquo;t understand your Python code, and we\u0026rsquo;re going to fall back to the Python interpreter. What we get in return for making this assumption is we don\u0026rsquo;t have to do the sort of mind-crushing coverage problem that is, well, now you need to understand the entirety of the Python ecosystem. Whenever there is something in your program that we don\u0026rsquo;t understand, whether or not it\u0026rsquo;s a Python language feature, a call to an external library, or even an operator that, you know, is kind of very weird and unconventional. And then you can see that in the same way that we don\u0026rsquo;t understand the size of the input. And so we can just say, well, okay, fine, we\u0026rsquo;re going to stop compiling here. And then we\u0026rsquo;re going to go ahead, and we\u0026rsquo;re going to go and, you know, go back to the Python interpreter. And sure, you just got a partial graph, but that\u0026rsquo;s fine. As long as your partial graphs are big enough, you know, you\u0026rsquo;re going to get most of the benefits from compilation. Why is that? Well, you know, to think about this, we have to think about, you know, why was PyTorch eager mode viable at all in the first place? And the reason why PyTorch eager mode was viable in the first place, because, you know, naively, you might expect that, hey, you know, you\u0026rsquo;re writing Python all the time, you know, isn\u0026rsquo;t that going to be really slow? Aren\u0026rsquo;t you going to have a lot of framework overhead? The answer is yes, there is a lot of framework overhead in PyTorch. And in fact, PyTorch is not a very good match today, well, prior to PyTorch 2, for handling overhead-bound models. But what it turned out was that, you know, with lots of operations that people wanted to do, you do a single, say, matrix multiply call, and that actually needs to do a lot of flops. And so actually, the operation takes a lot of time on the GPU. And as long as you can keep the GPU busy, right, you don\u0026rsquo;t have to outrun the bear, you just have to outrun, you know, the next lowest person, in this case, the, you know, actual GPU processing. So as long as you can run your Python code faster than the GPU can actually crunch the numbers, then you\u0026rsquo;re fine. It doesn\u0026rsquo;t actually matter how long or how much overhead your framework has, because you can just go ahead and hide it, because you\u0026rsquo;re waiting on the GPU anyway. And so this was true for PyTorch for a very long time. And it turns out that GPUs get faster and faster over time. And this is one of the reasons why, you know, we knew strategically it was really important. It was an existential problem for PyTorch. If we didn\u0026rsquo;t get our act together and figure out a way of running, you know, bigger chunks of code so that we weren\u0026rsquo;t overhead bound, whenever people upgraded to V100s and then to A100s and then to H100s, the GPUs get faster and faster. And then suddenly, you know, you\u0026rsquo;re at this point where previously you could cover it up, waiting for the GPU to come back. But now the GPU is so fast, you can\u0026rsquo;t cover up the framework latency at all. So, you know, we\u0026rsquo;re saying, hey, okay, GPUs are getting faster and faster. And so dispatching kernels one by one, as you wrote in eager mode, is just not cutting it anymore. But if we can take a bunch of kernels, and it doesn\u0026rsquo;t have to be the entire program, right? It just has to be enough kernels so that we can bundle them all up and run them all at once. And now, once again, the GPU compute is now taking a long time. If we can do that, then you\u0026rsquo;re fine. And you don\u0026rsquo;t, you\u0026rsquo;re once again back in the regime where you\u0026rsquo;re, you know, bound by the GPU. And you\u0026rsquo;re, you know, you\u0026rsquo;re happy because, you know, you didn\u0026rsquo;t require a whole graph export mechanism. So, you know, we can fall back to Python, whenever there\u0026rsquo;s something that doesn\u0026rsquo;t work very well. And you don\u0026rsquo;t rely on a whole graph mechanism, because, hey, you can fall back to Python whenever you need to. But at the same time, you\u0026rsquo;re getting large enough partial graphs, so that you can cover up the overhead of actually dispatching to the GPU. And that\u0026rsquo;s perfect, because we\u0026rsquo;re actually hitting this new sweet spot where we\u0026rsquo;ve pushed the Pareto frontier. Previously, you had a, you know, make a trade off between, oh, you know, nice user friendly Python native experience, versus, you know, not so user friendly, but compiler experience. And so now we have a new point in the trade off space, where we can still, you know, get the nice ear mode UX that everyone knows and loves about PyTorch. But at the same time, we\u0026rsquo;re actually compiling things. Now, of course, we do have to give up some stuff to get here. And, you know, one of the big things we have to give up here is the stack is kind of complicated. And, you know, Dynamo, right, is a symbolic Python bytecode interpreter. What does that mean? It means that, you know, when you run Python programs, your Python interpreter turns your Python source code into a bunch of bytecodes. And then there\u0026rsquo;s an interpreter that goes over the bytecodes one by one and actually executes them. So we needed to reimplement this interpreter so that we could, you know, go ahead and look for tensor operations and handle them specially, right? That\u0026rsquo;s basically the entirety of which Torch Dynamo does. And we had to do it. And, you know, that\u0026rsquo;s a new piece of code, which is sort of complicated and can have bugs in it. And, oh, yes, we do have bugs in Torch Dynamo. And then, of course, we need, you know, the rest of the stack, such as AOT Autograd for actually performing differentiation and then, you know, Inductor for actually compiling code. So there\u0026rsquo;s a lot more stuff going on in PyTorch right now. And so you might also have the question, which is, is it worth it, right? Like when you write traditional e-remote programs, you know, it\u0026rsquo;s sort of very simple. You know, you call a function, you execute the code in the function, and then you\u0026rsquo;re done. And that\u0026rsquo;s it. Nothing else to do. Whereas in this new stack, you know, there\u0026rsquo;s all of these different moving parts, you know, like how can you even tell what\u0026rsquo;s going on? And so this leads us to a second thing, which I think is really, really important for PyTorch 2, which is that all of the important code in PyTorch 2 all lives in Python. So what do I mean by that? Well, Torch Dynamo is a, you know, symbolic bytecode interpreter. Traditionally in CPython, you would, of course, want a bytecode interpreter to live in C because, hey, it\u0026rsquo;s kind of important, right? It needs to run fast. Well, we have plenty of caching, right? Once we have processed a given frame and, you know, evaluated all this bytecode, we don\u0026rsquo;t need to do this evaluation again. We\u0026rsquo;re just going to, you know, jump straight to the actual, you know, graph that we extracted and compiled before. So we can actually run Torch Dynamo in Python and it is implemented entirely in Python. You can set PDB breaks in it. You can, you know, single step through it. It\u0026rsquo;s actually a really nice way of understanding, you know, what is going on. And it\u0026rsquo;s fine. Like, I actually was worried a lot about the performance overhead of, you know, running Torch Dynamo in Python. But it turns out it doesn\u0026rsquo;t matter. Like, there\u0026rsquo;s plenty of other parts of the system that are slow. And similarly, Torch Inductor is a back-end compiler. And, you know, traditionally, back-end compilers are written in C++ or some sort of similar compile language. When we wrote the first version of TorchScript, we actually, we wrote it in C++ specifically because we wanted static types. Knock on wood. But Torch Inductor is written entirely in Python as well. So, you know, once again, if you are so inclined, you can go and check out all the different pieces of it. Now, it does back-end to Trident, which is written in C++. But there\u0026rsquo;s a sort of very clear abstraction boundary. There\u0026rsquo;s, you know, a Trident front-end language that\u0026rsquo;s written in Python that we generate. And so, you know, sure, Trident can have bugs. And, you know, Trident also has bugs. But you don\u0026rsquo;t have to use it in this situation. Because, but the parts that are actually generating the Trident code, the parts in Torch Inductor, those are entirely in Python. And now, I lied a little because AOT Autograd isn\u0026rsquo;t entirely written in Python. It\u0026rsquo;s got a lot of stuff in C++. But the stuff that AOT Autograd runs in C++ is sort of just pre-existing components of PyTorch. And this is another important constraint when we were thinking about what to do with PyTorch 2, which is that, you know, we had this shiny new bytecode interpreter in Dynamo. And if we wanted some sort of automatic differentiation system, one way you could go about doing it is just by re-implementing our AD system in Python, so you could Dynamo trace through it. But we decided not to do that. Now, whether or not this was the right call or not, it certainly saved us a bunch of time in terms of implementation. Our choice was to instead reuse the pre-existing C++ Autograd engine. You know, and as a benefit from that, we get all of the edge case handling, all of the sort of battle-tested work that we\u0026rsquo;ve put into the engine over the years. All of that transfers over to PyTorch 2. So you don\u0026rsquo;t have to worry about Autograd working differently when you run into PyTorch 2. All we\u0026rsquo;re doing is we\u0026rsquo;re just going ahead and tracing the set of operations that the original Autograd engine would have done, and then, you know, using that as the basis for a compiled program. Now, one downside to that is we had to work pretty hard to get dynamic shapes to work in this situation. So that\u0026rsquo;s why it\u0026rsquo;s not entirely clear to me if it was a win. You know, we traded off, you know, having to do some fairly major surgery to the internals of PyTorch to, like, make it support propagating dynamic shapes throughout. But, you know, like, we have a system that, you know, just really is reusing most pre-existing components of PyTorch. So, you know, in this sense, it really is additive. We\u0026rsquo;re not like, you know, the truly new parts like Dynamo and Inductor have no pre-existing analogs in PyTorch. And the parts that do have overlap with PyTorch where we\u0026rsquo;re actually using the same code in these cases. This is not entirely true. So in some cases, you know, we have operator implementations, and we opted to just go ahead and re-implement them in Python. But that\u0026rsquo;s a very small part of the system. And sort of the core subsystems are all shared in this case. Okay, so what have we talked about? So we\u0026rsquo;ve talked about, you know, what is PyTorch 2, right? So PyTorch 2 is a way to make your programs go faster. And the way it does that is by, you know, allowing us to compile fragments of PyTorch code, but without the constraint that you have to compile the entirety of your program. And what that means is that, you know, unlike TorchScript, where you have to actually go and, you know, modify your programs so that they are TorchScriptable, in PyTorch 2, you know, you can generally just slap a Torch.compile on any function, and it will generally work. Now, you might not get good performance. If there\u0026rsquo;s too many graph breaks, then, you know, you might not see any benefit at all. But, you know, it\u0026rsquo;ll always work. Or, you know, if it doesn\u0026rsquo;t work, you should send us a bug report. And, you know, if there\u0026rsquo;s anything weird, you know, we will be able to handle it without having to do special workaround code. And it turns out that this is good enough. We get speedups, pretty good speedups, in fact, without having to capture the entirety of the model and without having to give up the nice ear mode UX. And the rest is execution details. Coming up in the future, what we\u0026rsquo;re going to try to do is I\u0026rsquo;m going to try to walk through all of the components in PyTorch 2. You know, if you\u0026rsquo;re wondering how it works or, you know, you\u0026rsquo;re just trying to get involved in the process. You know, there\u0026rsquo;s a lot of different pieces. And, yeah, I\u0026rsquo;m looking forward to sharing a lot more about PyTorch 2 with you in the future. That\u0026rsquo;s all for today. Talk to you next time.\nEP67 torchdynamo torchdynamo Hello, everyone, and welcome to the PyTorch Dev Podcast. Today, I want to talk about the very first part of the PyTorch 2 stack, namely Torch Dynamo. This is the component that you interact with when you, for example, use Torch.compile. That just means that you\u0026rsquo;re turning on Dynamo. Dynamo is going to collect up graphs and then pass it on to a compiler. So there\u0026rsquo;s a lot of things that go on, but the very first thing that we have to do is actually get the graphs from your Eager program. And this is where, you know, Dynamo does something a little different. As we\u0026rsquo;ve mentioned in many different places, the idea behind Dynamo is we are going to go ahead and take your Python program as is, do a analysis on the bytecodes in your Python bytecode, and use this to figure out what the actual tensor operations on a given piece of Python code are. So what I want to do in this podcast is I want to go in a little more detail about what exactly that means and what that also, what the implications of setting up a graph capture mechanism in this way are. Because there are some questions that you might have about whether or not Torch Dynamo will work on a given program or not. And those questions can often easily be answered by just knowing a little bit about how Dynamo is supposed to work. And in particular, knowing how Dynamo is supposed to work can also help answer a question, which is, you know, does this not work just because there\u0026rsquo;s a bug in PyTorch or does it not work because there\u0026rsquo;s some deep fundamental reason? And so I just want to pull back the covers a little in this podcast to help you, you know, make assessments like that about whether or not Dynamo is correct for a given situation or not. All right. So let\u0026rsquo;s talk a little bit about the high-level UX behind Dynamo, and then we\u0026rsquo;ll dive a little bit into the big design concepts here. So the UX behind Dynamo, right, is it\u0026rsquo;s the Torch.compile UI. So you have this method called Torch.compile. You can decorate a function with it. And what Torch.compile does is somehow makes your program go faster. And the way it makes your program go faster is you have a Python program. This Python program normally does some stuff, right? It does some tensor computation. It might also, you know, print some lines out. It might also, you know, go ahead and modify some Python data structures. And Dynamo\u0026rsquo;s job is to take this Python program, this stream of Python bytecode instructions, and turn it into two pieces. One is a graph of tensor operations. This graph of tensor operations is what we\u0026rsquo;ll actually pass on to the compiler and hopefully compile into some form that can run more quickly. The other thing Dynamo does is it takes your Python program and rewrites it into what I call a residual Python bytecode program, which simply goes ahead and calls that graph, that graph of tensor operations that you saw before, as well as does whatever extra Python operations that were necessary because, you know, those were the Python operations that your program did. So somewhat unusually, for example, if you had a function and it, for example, added a number to some global variable in your program, normally you\u0026rsquo;d think, well, that\u0026rsquo;s kind of weird, right? Like, that\u0026rsquo;s not something that I want to show up in my tensor program. It\u0026rsquo;s just good old Python code. Surely Dynamo can\u0026rsquo;t handle that. Well, the answer is no. In fact, Dynamo can. Dynamo sees that there is this operation going on when it\u0026rsquo;s analyzing the bytecode and it makes sure to replicate it when it extracts out your program. So just because you go ahead and increment a counter in the middle of your program doesn\u0026rsquo;t mean that we do what\u0026rsquo;s called a graph break. That is, say, Dynamo throws up his hands and is like, well, I don\u0026rsquo;t know what\u0026rsquo;s going on, so I\u0026rsquo;m just going to bail out and ask Python to do the thing. Dynamo actually understands a lot of operations in Python code. And this is important because in order to get good compilation results, we need to be able to capture enough of our program. And to capture enough of our program, well, we need to not be breaking on every little thing. I also want to point out that there\u0026rsquo;s an important philosophical consideration behind this, which is that we always have the Python interpreter available. So although Torch Dynamo does a lot of work to understand as many constructs in the Python language as possible, it also has permission to not understand things. If something is too complicated or, you know, too annoying for us to implement, maybe you\u0026rsquo;re calling some, you know, giant third-party library and it\u0026rsquo;s, you know, doing web requests or something like that. If there\u0026rsquo;s something too strange, too unusual, Dynamo has the ability to say, okay, fine, I am just going to run your code as is, as in Python. And, you know, we\u0026rsquo;re not going to actually see the rest of your program. So we hope to capture as much as we can, but we are not forced to catch everything. And this was really important, you know, when we were working on Dynamo, well, when Jason Ansell was, you know, developing the very early versions of Dynamo, because in fact, there were a lot of features in the Python language you need to implement to get a lot of benchmarks going. But he didn\u0026rsquo;t need all of them implemented all at once at the beginning. He could start off with, you know, just a subset of the features he needed, and some models would work well, and some models would have lots of graph breaks. And then as we improved Dynamo, there would be less and less graph breaks in your programs. So, you know, that\u0026rsquo;s also kind of the situation you should expect here, which is that, hey, you know, maybe you run your program through Dynamo, and you get a single graph. Hooray, nothing left to do. But maybe you run your program through Dynamo, and you get a lot of graph breaks. Well, don\u0026rsquo;t despair, right? Maybe in the next version of PyTorch, or, you know, maybe even before the stable release, there might be work done to actually understand the things that are tripping you up. And then, you know, you can figure it out that way. There\u0026rsquo;s actually a configuration flag in Dynamo that you can turn on to give warnings whenever there are graph breaks. And, you know, if you think you\u0026rsquo;ve got a reasonable model that, you know, should work and is graph breaking, send us a bug report. And, you know, we\u0026rsquo;ll look into it, because, you know, we\u0026rsquo;re definitely interested in helping Dynamo understand more things. Okay, so what do we say so far? So we\u0026rsquo;ve got Dynamo, right? It understands your Python program and converts it into a series of tensor operations and a series of residual Python operations. And I also want to talk a little bit about what kind of graph you get from Dynamo, okay? So, you know, if you have tried playing around with a custom backend, Torch.compile makes it really easy to play around with a custom backend, because you can just pass in a function to be your compiler, and you\u0026rsquo;ll just get an FX graph, which represents the computation in question. So, you know, FX graphs, and you can see my podcast on FX graphs if you\u0026rsquo;re curious more about them. An FX graph is just this very simple, you know, data structure representing Python programs. So, you know, it\u0026rsquo;s got a list of nodes, you can iterate through the nodes, and there\u0026rsquo;s various calls on the nodes to various Python functions. And it\u0026rsquo;s very, very flexible. It\u0026rsquo;s really just a container format. It\u0026rsquo;s not a true IR, because all of the function calls inside an FX graph are just actual callables that, you know, are the ones that you actually call in Python. So what exactly does an FX graph that Dynamo gives you look like? Now, if we weren\u0026rsquo;t, you know, doing Dynamo at all, right, I could just have my program be represented as a single function call in my Dynamo graph, which called into whatever the original user code was. But Dynamo doesn\u0026rsquo;t do that, right? Because one of the things that Dynamo is doing is it is understanding what exactly your Python code is doing and making sure that it produces a graph that is always valid to use in some later iteration. So if I just have some black box Python function that is the only thing in my IR that, you know, I can file. Well, for one, how the heck am I going to compile that? And the answer is, well, with tracing, but, you know, like having a single function is not all that useful. But also it\u0026rsquo;s because Dynamo needs to keep track of, you know, whether or not I, whatever, you know, this graph is valid in the future. And to do that, it actually needs to look into things. So what you\u0026rsquo;ll actually expect to get is you\u0026rsquo;ll actually expect to get a bunch of PyTorch operations. If you had a bunch of calls to user functions, you should expect those to be inline into Dynamo. So you\u0026rsquo;re not going to see a bunch of recursive function calls. You\u0026rsquo;re just going to see a straight line program that has all the operations you need. You\u0026rsquo;re not going to expect to see loops in the graph because, in fact, you know, FXIR does not natively support loops. All your loops will be unrolled. All your conditionals will be flattened. You won\u0026rsquo;t see conditionals in your Dynamo graph. You\u0026rsquo;ll basically have a straight line program of a bunch of Python calls. Now, this is very nice and normalized, but it\u0026rsquo;s actually not that normalized. So here are some things that you\u0026rsquo;re not going to get directly from Dynamo. So one thing you\u0026rsquo;re not going to get is you\u0026rsquo;re not going to get a backwards graph. To get the backwards graph, you need another component, AOT Autograd, which I\u0026rsquo;ve had a podcast about with Horace before. But we\u0026rsquo;re going to do another podcast about, you know, the PyTorch 2 specific implications of AOT Autograd. Suffice to say, you aren\u0026rsquo;t going to get the backwards. So you want to use AOT Autograd to do that. In fact, there\u0026rsquo;s an API change winding its way, which we\u0026rsquo;re probably going to change the default behavior of Torch.compile. If you feed it a function, we\u0026rsquo;re not going to give you all the Torch functions anymore. We\u0026rsquo;re going to give you it. We\u0026rsquo;re going to call you once for forwards and call you again for backwards. That probably is more likely what users want to see. So, you know, stay tuned for the API change. This doesn\u0026rsquo;t affect you if you\u0026rsquo;re using just the inductor backend. But for all you compiler backend writers out there, this is probably a change. And before this change actually lands, you probably do want to be using AOT Autograd because, you know, you actually do want Autograd support for your compiler. You also get some other things. So some other things you don\u0026rsquo;t get from this graph. So it\u0026rsquo;s going to be calls to the Torch API. It\u0026rsquo;s going to be the calls to the Python API. It\u0026rsquo;s going to look very, very similar to the actual function calls that were in your original program. Now, we actually can normalize this IR a bit, right? So these Torch function calls have all of the weirdnesses of the Python API. For example, you can call reshape on a tensor and you can pass to reshape either a tuple of sizes you want, or you can just, you know, get rid of the tuple and just pass them in one by one as positional arguments. This doesn\u0026rsquo;t get normalized at all. You\u0026rsquo;ll see exactly what the user saw in that question. To get this normalization and to also just, you know, you know, tease apart some of these high-level operations, you might want to lower to Aten operations. Once again, this is something, this is not something that Dynamo does built in. This is something that AOT Autograd, now it\u0026rsquo;s a little, AOT Autograd is actually doing a lot of lifting. It\u0026rsquo;s not just doing Autograd, but it\u0026rsquo;s also lowering things to Aten. AOT Autograd is responsible for lowering from Torch Ops API into Aten API. So, you know, you\u0026rsquo;re not going to get that by default. You need to opt into AOT Autograd to get that. One more thing that you\u0026rsquo;re not going to see in the IR is you\u0026rsquo;re not going to see, sorry, actually, what\u0026rsquo;s something you\u0026rsquo;ll see in the IR and maybe you don\u0026rsquo;t want to see is if the Python program had views or it had mutation, all of that is going to also be captured faithfully. So, really, all Dynamo is doing is, you know, it\u0026rsquo;s inlining away and removing all the Python constructs from your program, but you\u0026rsquo;re really just getting like a forward-only, you know, very idiomatic PyTorch program. And that\u0026rsquo;s sort of easy to understand, but it\u0026rsquo;s actually not so easy for compilers to deal with. In fact, compilers have a lot of headache dealing with mutation and views. Just ask, for example, XLA, where, you know, their HLO IR does not actually have a concept of mutation or of views. So, in order to also get rid of those, once again, you can probably guess where I\u0026rsquo;m going with this, AOT Autograd is responsible for what\u0026rsquo;s called functionalizing away those operations so that, you know, you get a very nice, functional, clean IR that\u0026rsquo;s good for compilers. So, you know, what is Dynamo doing, right? All Dynamo is doing is it\u0026rsquo;s understanding the Python code. It\u0026rsquo;s figuring out how to remove Python constructs. So, you\u0026rsquo;re never going to see a Python class or a Python or even a Python named tuple inside of the Dynamo graphs. All of that gets flattened away. You know, you\u0026rsquo;re just getting a bunch of tensors and doing operations on those tensors and then returning a bunch more tensors. But beyond that, beyond what Dynamo can understand at a superficial level by just looking at the Python code, looking at the Python byte code, you don\u0026rsquo;t get any normalization beyond that. That\u0026rsquo;s all AOT Autograd\u0026rsquo;s job. Okay, so with this understanding about, you know, what Dynamo actually does and doesn\u0026rsquo;t do, we can also, you know, think about, you know, what kinds of problems are likely to show up due to Dynamo itself as opposed to other parts of the stack. So, for example, if you are, you know, seeing that, you know, you\u0026rsquo;ve got a graph and it doesn\u0026rsquo;t look quite right, like, you know, maybe there are some operations in it that, you know, don\u0026rsquo;t look quite correct. And this is before you\u0026rsquo;ve gone ahead and sent it to AOT Autograd. So this is like, for example, if you just, you know, pass in a simple backend compiler that prints the FX graph in question, well, that means that it is a problem in Dynamo. And this is one of the reasons why Torch.compile has a backend. It\u0026rsquo;s called Eager. It\u0026rsquo;s a very pointless backend. All it does is it takes the FX graph and then runs it directly as is. But it\u0026rsquo;s really useful for figuring out if you have a Dynamo bug at all, right? So you\u0026rsquo;ve got your program, you\u0026rsquo;re trying to run it, it\u0026rsquo;s doing something weird. So you replace the backend with Eager and now, you know, we are not doing anything interesting except running Dynamo. And if it still fails in that case, well, you know, you found a Dynamo bug. Similarly, if you are, you know, running Dynamo and you\u0026rsquo;re like, well, this is kind of weird. Some of my Python state doesn\u0026rsquo;t look quite right after running Dynamo. Well, that\u0026rsquo;s also likely to be a Dynamo bug. And once again, you can figure out if that\u0026rsquo;s the case by switching Torch.compile to Eager. So Torch.compile Eager says use Dynamo, but don\u0026rsquo;t actually run any of the compiler. Don\u0026rsquo;t even run AOT Autograd. Because AOT Autograd is its own sort of complicated component in its own right. It also has bugs. And so sometimes, you know, you want to like run AOT Autograd and Dynamo, but not anything else. That\u0026rsquo;s the backend called AOT Eager. And so by, you know, sort of varying your backends, you can sort of use this to sort of figure out which part of the compiler stack is, you know, breaking. And this is really useful. I use this all the time when I\u0026rsquo;m working on PyTorch to figure things out. Okay. So we\u0026rsquo;ve talked a little bit about Dynamo, right? What is Dynamo? It, you know, processes Python bytecode to get you the tensor graph and a bunch of residual Python operations. What do you get as an output? You get a graph. The graph has a bunch of tensor operations in it. It doesn\u0026rsquo;t have any Python types in it. It doesn\u0026rsquo;t have any Python control flow or loops, but it isn\u0026rsquo;t lowered. And so if you want to do the lowering, you have to go to AOT Autograd. And so this, you know, this description of Dynamo is a pretty good, I think it\u0026rsquo;s a pretty good, you know, like black box description of what Dynamo does. And so you should be able to think about this and think to yourself, you know, is Dynamo useful for my situation or is it not? So to wrap up this podcast, I just want to compare Dynamo to a few of the other graph capture mechanisms we\u0026rsquo;ve built in PyTorch. And we can just use this sort of bird\u0026rsquo;s eye view to like, you know, talk about the pros and cons of different approaches. So one very obvious comparison point that people want to make with Dynamo is with TorchScript, right? So TorchScript is the original PyTorch just-in-time compiler. You know, what does it look like? Well, you know, you\u0026rsquo;ve also got a decorator. You can decorate your functions. But unlike Dynamo, you have to, you know, make sure all of your program is what\u0026rsquo;s quote-unquote called TorchScriptable. And what do we mean by TorchScriptable? Well, because TorchScript is a subset of Python that our compiler understands. And so there are some Python features you\u0026rsquo;re allowed to use, some features that you\u0026rsquo;re not allowed to use. And so depending on whether or not you use those features or not, you know, your program may be TorchScriptable or not. So let\u0026rsquo;s do a little bit of a comparison here. So what does Dynamo do? So I said Dynamo understands your PyTorch program at the bytecode level. So Dynamo processes the, you know, bytecode stream that your Python interpreter compiled you to. TorchScript, on the other hand, processes Python ASTs. So it actually takes your Python program, you know, produces an AST for it using, you know, for example, a standard Python AST parser and then attempts to map that into its own internal intermediate representation that can represent all the things that are in a normal Python program. So this is where this is like a major philosophy difference, right? When Dynamo gives you a graph, this graph is completely inline. There are no loops. There are no data structures. In TorchScript, all of those constructs are preserved, right? So if you have a loop that is TorchScriptable, then you will get that loop inside TorchScript. And so that makes TorchScript really good for, well, okay, of debatable goodness. But one of the things that TorchScript really got used for a lot early in its lifetime was for sort of export situations where, you know, you were doing a beam search and you wanted to loop over various elements. And then you wanted to capture that loop as is and then ship it to some other environment. TorchScript can do that for you because it understands loops. It has an understanding of many different Python data types like mutable lists. So, you know, if you stay in that subset, you know, it\u0026rsquo;s basically like a tiny scripting language that happens to be runnable in C++ without the gill. And, you know, that is beneficial in a lot of situations. The downside to doing it this way is that TorchScript programs are a lot more difficult to compile, right? Because you\u0026rsquo;ve got these random Python lists running around. You\u0026rsquo;ve got, you know, all sorts of weird data structures running around. You basically, you know, can\u0026rsquo;t really compile a TorchScript program as is. You have to sort of extract out the, you know, functional graph bits first and then you have you can actually compile those. And like, you know, that\u0026rsquo;s a bit of a step. And, you know, like oftentimes, you know, maybe there is a list data structure, but it\u0026rsquo;s always static. And so if you had just unrolled it, then you would have gotten a nice, easy to compile sequence of tensor operations. But no, you know, you couldn\u0026rsquo;t, you couldn\u0026rsquo;t do that, right? Because TorchScript didn\u0026rsquo;t know that that was the case. Compare that with Dynamo, right? Dynamo is operating byte codes. And, you know, all it\u0026rsquo;s doing is it\u0026rsquo;s inlining and, you know, getting rid of all that stuff. So the graph you get is a lot easier to compile because it\u0026rsquo;s basically straight line code and, you know, like just in time compilers really like compiling straight line code because it, you know, it\u0026rsquo;s a lot easier to not have to deal with control flow. And, you know, the, you know, the downside of that is, right, we it\u0026rsquo;s less likely that your code will be valid because what if the, you know, number of loop iterations changes? What if some conditional changes and so Dynamo has a lot of machinery for making sure that, you know, it knows exactly what conditions have to be upheld in this situation. And then you can actually, you know, you can specialize on all those things and, you know, breathe safe that, hey, you know, next time around, if, you know, a conditional had changed or if a loop counter had changed, I\u0026rsquo;m not going to attempt to reuse the stale graph. By the way, that\u0026rsquo;s another one of the things that, you know, if you\u0026rsquo;re thinking about ways Dynamo can go wrong, the guard infrastructure, the infrastructure which tells us whether or not we can safely reuse a graph or not, that\u0026rsquo;s the other thing that can cause problems. And I hope to talk a little bit about some of the debugging tools we have for diagnosing if that\u0026rsquo;s one of the situations or not. Okay, so, you know, Dynamo, simple graphs, all inline, cool, TorchScript, complicated graphs, lots of support for Python features, you know, less easy to compile, but, you know, you can express more programs in it. Another comparison people often want to ask us about is FX symbolic trace, right? So FX was a new graph representation we wrote, we did it in Python, doing it in Python, by the way, was a really good idea. And, you know, Dynamo is written in Python, and that makes it a lot easier to debug and deal with, right? TorchScript is written entirely in C++. It\u0026rsquo;s very difficult for, you know, an external person to, you know, get their hands on it and make changes. It\u0026rsquo;s very easy to tweak Dynamo, you know, change things around and see what happens. So FX, you know, introduced the Python IR format that we still use in Dynamo, but it also introduced this thing called FX symbolic tracing. And what symbolic tracing is basically is it\u0026rsquo;s a Python level tracer using, you know, Python\u0026rsquo;s ability to do operator overloading to capture the things that are going on. So, like, say you have a model, and you want to figure out what operations are in it, then you pass in, you call it with symbolic trace. Symbolic trace, instead of passing in tensors, presses in these things called proxies, and then, you know, it looks and sees what operations get called on these proxies and records that to the FX graph. So, once again, what\u0026rsquo;s the difference between this and Dynamo? Well, you know, Dynamo is sort of morally doing the same thing, but it\u0026rsquo;s operating at a different level. FX has to operate at the level of whatever Python\u0026rsquo;s operator overloading supports. So, for example, if there is a conditional and, you know, someone is trying to figure out what the heck, you know, sorry, if there\u0026rsquo;s a conditional, you know, FX doesn\u0026rsquo;t actually have a opportunity to see what the conditional is or do anything special. But because Dynamo is, like, executing bytecode by bytecode, it actually can see, oh, there\u0026rsquo;s a jump condition here and do all sorts of things. So, Dynamo, you know, is sort of morally doing the same thing as FX tracer, but because it\u0026rsquo;s doing it at a lower level, it has a lot more flexibility and ability to put in safety guards that FX can\u0026rsquo;t do. Actually, FX symbolic trace is very, very limited in some sense, which is because it doesn\u0026rsquo;t actually, it doesn\u0026rsquo;t even support querying shapes on tensors because it just replaces these things with proxies and it just says, well, I don\u0026rsquo;t know what these are. So, this is not a fundamental limitation and, in fact, the what AOT and there\u0026rsquo;s a different mechanism that AOT Autograd uses called proxy tensor tracing where we actually maintain fully fledged proxy tensors. And, you know, this is also very similar to symbolic tracing, but now you can actually query for the size of a tensor and get that out. But the fact remains, right, that, like, when you run Dynamo, if you, like, call into some external library, Dynamo can notice it because it\u0026rsquo;s processing each of the bytecode instructions and say, oh, I\u0026rsquo;m calling a function into matplotlib. That doesn\u0026rsquo;t sound good. I should graph break here. Any sort of Python operator overlaying mechanism cannot get that level of insight into what is executing in your program. You\u0026rsquo;re just going to go ahead and execute, you know, operations. And only if, you know, you\u0026rsquo;re dealing with your proxies, do you actually get the callback and get to record things. So, if there\u0026rsquo;s other stuff going on in the Python program, you have no idea what\u0026rsquo;s going on. So, Dynamo, by hooking into the bytecode, can get all that information. So, hopefully, I\u0026rsquo;ve given you a little bit more sort of the high-level information about, you know, what Dynamo does at a high level and how it compares to other systems. There\u0026rsquo;s plenty of other things to talk about, and I will talk about them in later podcasts. Thank you very much for your time. See you next time.\nEP68 Zero-one-specialization Zero-one-specialization Hello, everyone, and welcome to the PyTorch Dev Podcast. Today, I\u0026rsquo;m here with Mikey Daggettsies, who is going to help me sort of explain a little bit more about, you know, the PyTorch 2 model. And so our goal coming into this conversation, as Mikey was telling me, was we were talking about 0.1 specialization at the most recent Composability meeting, which, by the way, we have a recording for on Twitch and hopefully on YouTube soon if you want to go check that out. And so Mikey is sort of newer with the PyTorch 2 project. And so he was wondering, well, you know, what exactly does this all mean? Like, what\u0026rsquo;s going on here? Mikey, did I describe that correctly? Yeah, that sounds good. Okay. So to start off with, what we were discussing before I decided, hey, let\u0026rsquo;s record a podcast for this conversation, was the very concept of, you know, why are we talking about 0.1 specialization? Why is it a problem for experts? Like, what does this all mean? My response to that was to say, well, hey, to understand this, we first need to know a little bit about PyTorch 2\u0026rsquo;s compilation model, you know, in its entirety, right? Like, so we first need to know, like, what exactly is a guard? Why does this matter? And so the idea we\u0026rsquo;re talking about here is that when we are running models through PyTorch 2, we run them through the compilation stack, and we get out some compiled artifact, and that artifact may or may not be valid for certain inputs, right? We may have done sort of specializations for certain input sizes to, you know, allow us to hard code in these constants and make things run faster. And so when we want to actually run these on new user inputs, we need to check whether or not all of those things are valid. And so if you listen to my podcast about Torch Dynamo and guards, the way we find out whether or not those things are valid are guards. Okay. So let me give you a chance, Mikey. So that\u0026rsquo;s where we were so far. And so what was the next question you had on your mind? So let\u0026rsquo;s, let\u0026rsquo;s do something simple. What is the advantage of specializing on, on input sizes, zero or one? That\u0026rsquo;s a great question. Okay. So you, because this is a policy decision. So, um, what, when we, yeah, so, okay. Rewinding a sec. So, so in general, you can end up with a compiled artifact and we\u0026rsquo;ll have some guards saying when it\u0026rsquo;s valid, when it\u0026rsquo;s not. And in fact, in PyTorch 2, we have some upfront decisions we make. We say, if you have any input whose input size is zero or one, um, and by the way, um, this all applies under dynamic shapes because under static shapes, we just specialize on all the shapes and there\u0026rsquo;s, there\u0026rsquo;s nothing to be dynamically varying. Like if it has to, if it was 20, 2048 on the original run, it has to be 2048 again, but with dynamic shapes, we try to make things, uh, be able to vary, but we still specialize if we ever see any input that is zero and one. So the question here is why is that a good idea? And so the reason why this is a good idea, um, has less to do with a sort of, um, uh, like what, um, what\u0026rsquo;s the word in principle, we could choose not to upfront specialize on things being zero or one and just run all of our infrastructure in PyTorch, which is responsible for figuring out whether or not, um, expressions, uh, you know, needed to be guarded on or not. And we would get a better result, a better result in the sense that you would have less guards than if we had eagerly specialized on zero or one. So, you know, it\u0026rsquo;s a very valid question to ask, well, why do we eagerly specialize on zero one? So there\u0026rsquo;s two answers to this question. So the first answer is this is for performance reasons, because when you, uh, you know, make something symbolic, um, we have to do a lot more reasoning about it. It\u0026rsquo;s, it\u0026rsquo;s kept as a simpi variable. It can build expressions. These expressions might, you know, involve lots of additions and things like that. And we can\u0026rsquo;t ever simplify it down to, oh, it\u0026rsquo;s five, um, or, oh, it\u0026rsquo;s eight because, you know, we didn\u0026rsquo;t specialize. We\u0026rsquo;re trying to find out. And if it turns out that way later in your program, you end up specializing because there is a guard, there\u0026rsquo;s a condition on it or something like that. It actually is one or zero, then you\u0026rsquo;ve wasted all that time doing all the symbolic reasoning, um, ahead of time when you could have just like specialized it to be one or zero. And then your tracing would have gone a lot faster because you just do constant propagation. You\u0026rsquo;re not doing any symbolic reasoning. So we\u0026rsquo;ve, we\u0026rsquo;ve measured this and empirically zero one specialization buys you a lot in terms of trace time because, well, for one, it\u0026rsquo;s not that common to show up in inputs. And two, and this leads to the second reason is we do a lot of conditional testing on whether or not sizes are one or zero. For example, let\u0026rsquo;s suppose that you have a, um, tensor, uh, you\u0026rsquo;re creating a new tensor and, um, you\u0026rsquo;re creating it with, uh, some, uh, um, um, okay. You\u0026rsquo;re creating it with both sizes and strides explicitly. And so the question at hand is, um, is this tensor being created with some set of sizes and strides, is it contiguous? And so there\u0026rsquo;s a very complicated algorithm you can do to like figure out if it\u0026rsquo;s contiguous, which involves like looking at, you know, the ordering of the strides and then making sure they like multiply together in the way you expect so that everything is densely packed together. But there\u0026rsquo;s a simple way, um, for something to be contiguous. And that is if any of the, uh, input, uh, input sizes is zero, then the tensor is contiguous. Why? Because there\u0026rsquo;s no elements in the tensor. So like it\u0026rsquo;s, it\u0026rsquo;s contiguous because there\u0026rsquo;s just nothing to be discontiguous. Similarly, if the number of total elements in the tensor is one, that\u0026rsquo;s also contiguous because while, you know, like there\u0026rsquo;s only one element, you know, it can\u0026rsquo;t be discontiguous with anything else. So the code in our framework, which is generating guards, um, is doing all of these checks. And so if you don\u0026rsquo;t actually, if you\u0026rsquo;re not able to do things like say, well, this is, uh, you know, definitely not zero, then you end up having a lot more sort of reasoning to go through, uh, where you could have just been like, oh yeah, definitely all the sizes are not zero. I\u0026rsquo;m not going to worry about the case when sizes could be zero and that\u0026rsquo;s fine. The other classical example of this is broadcasting. So when you broadcast, when you add two tensors together, um, you, you normally need to check that their sizes are equal at equivalent dimensions. But if one of the dimensions is size one, we\u0026rsquo;re willing to broadcast it into, you know, the size of the other dim. How do we test that? Well, we have to look and see if it\u0026rsquo;s actually equal to one. So like, you know, we end up doing lots of guards on oneness and zerowness. And so that\u0026rsquo;s why zero one specialization is useful. It\u0026rsquo;s because we\u0026rsquo;re probably going to guard on it anyway. So we can just get back the performance if we just assume, oh, if it\u0026rsquo;s zero, it\u0026rsquo;s going to be zero and we\u0026rsquo;re not going to try to generalize. So where are these, uh, checks for zero or oneness applied? Are they checked at all at layers all throughout a, uh, PyTorch program, or are they only checked on the initial inputs? Yeah, that\u0026rsquo;s a good question as well. So, uh, so there\u0026rsquo;s two parts to this question. So one is when we talk about guards, when are guards checked? And the answer to that question is simple. It\u0026rsquo;s, we only check guards at the very beginning of a compiled block, right? Because the compiled block is this opaque blob of code. Once we start executing it, that\u0026rsquo;s it. We\u0026rsquo;ve got to execute it all the way to the end. We\u0026rsquo;re not like other, like a JavaScript JIT where you have a bailout midway through and then you like reconstruct your stack state and then go back to the slope path interpreter. We can\u0026rsquo;t do that. So we need to have everything in line when we go in. So all the guards for everything that happened during the computation are there. The other half of your question is, um, like when are, when are these zero one tests happening most of the time? And the answer to that is they happen mostly one, when you\u0026rsquo;re constructing tensors, because when you construct a tensor, we need to do a bunch of tests to figure out various things about contiguity to it happens when you do point wise operations, because that\u0026rsquo;s when you test for broadcasting. And three, um, they happen, uh, uh, sort of on a very ad hoc basis on a lot of kernels, um, that are complicated that involve algorithm selection. So think convolution batch norm. And this is for a sort of different reason. It\u0026rsquo;s because a lot of libraries don\u0026rsquo;t handle, for example, zero size inputs. So you have to check, oh, is the number of elements zero? If it is, well then, you know, short circuit and don\u0026rsquo;t do anything because, because a zero batch convolution is very easy to do because there\u0026rsquo;s no work to do. You\u0026rsquo;ve got no data and otherwise call it a decoudian convolution. And this, this, this last class of things shouldn\u0026rsquo;t matter, but it does because, you know, which algorithm you select changes what strides you actually end up with in the end. And this is one of the reasons why like stride agnostic PyTorch is kind of relevant to like the discussions we\u0026rsquo;ve been having about zero one specialization. Okay. So, um, to summarize part of that, every place we have a graph break, we are checking guards because we have to know, um, which graph to enter. To be clear when, when you, when you enter a graph, because you can enter a graph without there being a graph break. Like when, when you start a torch compile region, that\u0026rsquo;s not going to be a graph break. And we do zero specialization, zero one specialization at every, um, at every graph entry. Yeah. So when you enter the graph, we allocate symbolic sizes for all of your inputs. And if any of those inputs happen to be zero one, we say, okay, fine. Uh, this is just literally zero, or this is just literally one. And we specialize it on the, on the spot. Okay. And do we, and, um, you\u0026rsquo;ve, I think you made the case that this is generally a good idea. Um, do you, would you say that this is always a good idea or are, do you think there are times where you wouldn\u0026rsquo;t want to have this specialization? No, it\u0026rsquo;s not always a good idea. And so that leads us to the conversation we were having at composability sync, which is for export zero one specialization is bad. Well, zero specialization, uh, we, we had some examples. So actually zero specialization is probably bad, but one specialization is really obvious. If you\u0026rsquo;re tracing a program and you want a dynamically varying batch size, you really don\u0026rsquo;t want your program to not work for batch size one. That\u0026rsquo;s probably like the, like most likely batch size you\u0026rsquo;re going to run in any situation where you can\u0026rsquo;t actually pull up inputs and then, you know, do batch inference over them. Okay. Um, so it\u0026rsquo;s not, it\u0026rsquo;s not, um, it\u0026rsquo;s not clear to me, like why having a specialization for, for batch size one would be incorrect. I mean, is, is my understanding was that, um, you would still do the specialization and you would still produce a correct graph for an input of that size. Um, under what circumstance would you be generating a graph that would be incorrect? Um, that\u0026rsquo;s correct. So in fact, we are generating a correct graph under certain conditions. So if I trace my program with batch size equals one, I will get a program, which is correct. Whenever batch size is equal to one. However, if batch size is equal to four, this trace is not necessarily correct. And one of the things that, you know, people want in export is they want to only have a single graph, which handles all of the possible cases, right? I don\u0026rsquo;t want to batch size one graph and a batch size two graph and a batch size three graph and a batch size four graph. Like that\u0026rsquo;s, that\u0026rsquo;s dumb, right? Like probably they\u0026rsquo;re all the same graph. And I would just want one graph in that case. So when I zero one specialize, uh, even if I pass in a batch size two, I will get a graph, but it is only valid when my size is not one and not zero. So it\u0026rsquo;ll be valid for two, three, four, five, six, seven, so forth, but not for one and zero. Because when we zero one specialize, that also means that if you do a test and you test, Hey, is this equal to one? I can say no, when the batch size is two. Okay. So, um, again, I think like what I\u0026rsquo;m, what I\u0026rsquo;m missing here is it\u0026rsquo;s like, why in export, would you not want multiple graphs? Um, like if, if, um, if you could have a graph that\u0026rsquo;s good for one, and you can have a graph that\u0026rsquo;s good for numbers, uh, greater than one, um, and those graphs are distinct, like what would be the harm of exporting two different graphs that can be used for different input sizes? Obviously you wouldn\u0026rsquo;t want to graph size two, size three, size four. Like, um, there\u0026rsquo;s, uh, there, you know, if those graphs are the same, you would want to have, you, you would want to collapse them. But if you do have a distinct graph for size equals one, like why not, why not use it? That\u0026rsquo;s a good question. And indeed, uh, in regular eager mode, this is fine, right? We\u0026rsquo;ll have a graph for one, we\u0026rsquo;ll have a graph for N, and we\u0026rsquo;ll switch between them depending on what users give us. And what, what\u0026rsquo;s the, what\u0026rsquo;s the cost? The user just had to wait a little bit longer for two graphs to compile as opposed to one. But on export, this is bad because, um, we\u0026rsquo;re talking about export to these like mobile devices. They have very little memory. They like really, you know, want, uh, like a, a single model that has a small footprint that they can put on some smartwatch. And you\u0026rsquo;re going to, are you going to tell them, Hey, actually, uh, you know, we need to give you two graphs, one for the batch size, one case, one for the batch size and case. And, but wait, it gets worse because say you have two dynamic dimensions. So you have sequence length and you have batch size. I need four models this time, one for the batch size, one sequence length, one, one N, and one and N, right. It\u0026rsquo;s a combinatorial explosion of possibilities. So like, yes, in principle, you could do this. And in situations where it doesn\u0026rsquo;t cost a lot to have multiple graphs flying around, this might be fine, but it just, it\u0026rsquo;s worse. And like, it doesn\u0026rsquo;t surprise me that people have a preference for having only one graph in this situation. Okay. That, that, that, that makes a ton of sense to me. Um, I think the, the question that remains then is, is so if we\u0026rsquo;re checking for this on export and we, we identify that there are multiple graphs and we want to flag that, what can we, what, what can we really do there? Because the, the graph that is not specialized for size one, um, may not be valid for size one. Is, is that, is that correct? Uh, a graph that is, um, for a batch size greater than one is not necessarily valid for one. Yes. So what can we do other than saying, uh, sorry, you\u0026rsquo;re out of luck. Um, like, do we have to go back and, um, just have like a, like rerun the trace with, uh, just saying disable zero one specialization. Like what\u0026rsquo;s the, what\u0026rsquo;s our recourse here? Yeah. So we went through a bunch of possible solutions in composability sync, and I believe the solution the room converged on looks like this. So first you turn off zero one specialization, right? So I just spent a while saying, Hey, zero one specialization is great. Um, you know, like we, we really like it for trace time performance and stuff. And I\u0026rsquo;m like, no, no, no. Like get rid of it. Right. Like we\u0026rsquo;re, we\u0026rsquo;re not going to eagerly specialize on things. So under the assumption that you don\u0026rsquo;t generate any guards on the batch dimension, as long as you don\u0026rsquo;t upfront zero one specialize, you will in fact, get a graph that works for any selection of the sizing question. Right. That\u0026rsquo;s a big if though, if you don\u0026rsquo;t have any guards. So in practice, um, you will have guards, right? For all the reasons that I described so far. And so now what do you do? Well, you essentially, and this is what I\u0026rsquo;ve been doing when working on Unbacked Simmons, you basically rewrite all the library code in PyTorch to not unnecessarily guard on ones and zeros if it doesn\u0026rsquo;t have to. And it\u0026rsquo;s actually, you have to change a lot of spots in the code, but it is surprisingly tractable. For example, um, like let\u0026rsquo;s talk about tensor construction, right? So I said, well, to configure out if something is contiguous, you have to like, look at the sizes, right? Because if it\u0026rsquo;s zero, then it\u0026rsquo;s definitely contiguous. Otherwise, you know, who knows, but if you call torch.empty directly, not empty strided and you don\u0026rsquo;t pass in strides, you know, that it\u0026rsquo;s contiguous, obviously it\u0026rsquo;s contiguous because like, you know, there\u0026rsquo;s no way to allocate a non-contiguous output when from torch.empty. So that\u0026rsquo;s fine. There\u0026rsquo;s another annoying thing, which is, um, uh, in stock PyTorch will compute this thing called channels last contiguity, which essentially says, well, maybe it\u0026rsquo;s not contiguous, but if you, if you move the channels dimension to the last position, would it be contiguous in that case? And this is, this is, this is not easy to answer because with torch.empty, the answer is no, it\u0026rsquo;s not channels last contiguous unless the number of elements is one, in which case it is contiguous because like, you know, everything is contiguous when you only have a one element tensor. But in this particular case, it doesn\u0026rsquo;t matter because no one actually ever like in, in models we\u0026rsquo;re exporting, no one actually asks, Hey, is it channels last contiguous? And so if you can just delay it, you can say, Oh, well, I don\u0026rsquo;t know if it\u0026rsquo;s going to be channels last contiguous or not, but as long as you don\u0026rsquo;t call me out on it, then we can avoid the guard and everything\u0026rsquo;s peachy. And so there\u0026rsquo;s a lot of like near misses like this, where you like sort of fiddle around with things and then you get it so that, Oh, actually we don\u0026rsquo;t have to do the guard. Um, I recently got resnet running without any, um, guards on, uh, batch size being one. Uh, there were a lot of things, but I was able to get to the end and like, you know, I was able to resolve all of them. There\u0026rsquo;s one thing though, that I wasn\u0026rsquo;t able to resolve, which is that sometimes to get the exactly correct output stride for a given operation, I actually really needed to do a equals equals one test. Um, but this is also something we discussed in the meeting. And the thinking is that, well, you know, strides are this, like, you know, they\u0026rsquo;re this advisory thing. They\u0026rsquo;re not supposed to change the semantics of programs. So it should be okay for us to, you know, slightly change what the stride output is if we\u0026rsquo;re, you know, tracing one of these things versus not. And that\u0026rsquo;s kind of like not entirely true, but like, you know, the whole point of the stride agnostic PyTorch work stream, which Mikey is working on, um, is to like, make that more true in your PyTorch. Great. Um, the, uh, so yeah, so that, that, um, that\u0026rsquo;s probably a topic for another day. Um, cause I know that you, you do think that it may be controversial to, um, extend stride agnosticism to the outputs of programs. Right. Um, so I\u0026rsquo;m curious, I\u0026rsquo;m curious, uh, uh, where we\u0026rsquo;re going to land on that controversy in the longterm. Yeah. So just to, just to repeat on this question, right. One of the things that originally, um, you know, spurred this question about stride agnostic PyTorch was, you know, we\u0026rsquo;re working on this cool compiler. And sometimes the compiler is like, Hey, like I see that you\u0026rsquo;re outputting a tensor with this striding in the original year program. And I don\u0026rsquo;t like that. I want to, I want to give you a different one. Cause I can give that one to you faster. I can give you a channels last tensor much faster than I can give you a contiguous tensor. And we\u0026rsquo;re not allowed to do this today because it can break user code. And so like, if we want to change this, if we want to be allowed to do this, we need to make it so that user code can\u0026rsquo;t be broken in that case. And that\u0026rsquo;s, that\u0026rsquo;s what stride agnostic PyTorch is about. Or part of it, at least we can, I mean, we can break those assumptions in the middle of the program. Right. But it\u0026rsquo;s the, uh, when it goes into eager mode. And this is also why export is sort of like an easier version of stride agnostic PyTorch because we do have the assumption that we can, you know, trace through the entirety of the program. And so I do think it is okay to assume PyTorch is already stride agnostic, um, when you\u0026rsquo;re doing an export style workflow where you have the entire program and maybe PyTorch isn\u0026rsquo;t, but you can still do easy tests. Like you can just make sure the user model isn\u0026rsquo;t calling as strided or make sure the user model isn\u0026rsquo;t like trying to mutate through a reshape call. Um, these are all relatively simple things to test. If you can assume you\u0026rsquo;ve got the entire model. All right. Well, um, I think you\u0026rsquo;ve answered all my questions for today. Uh, thanks a lot, Ed. Okay. Thanks for agreeing to, uh, be recorded on the podcast. I hope, uh, listeners out there also found that useful. Okay. Talk to you all next time. Yeah. I hope to see you again soon.\nEP69 Unbacked-SymInts Unbacked-SymInts Hello, everyone, and welcome to the PyTorch Dev Podcast. This podcast is a little bit of out of order from the previous podcast about 01 specialization. So if you haven\u0026rsquo;t listened to the 01 specialization podcast, try listening to this one, which is going to be about unbacked cements in general for PyTorch 2 in both eGremote and export. So this podcast is coming because we\u0026rsquo;ve been talking more about 01 specialization and also about the stack of PRs that I\u0026rsquo;ve been working on regarding unbacked symbolic integers. And there\u0026rsquo;s been a lot of questions about what the heck are unbacked cements? What exactly is going on with them? You know, what are the consequences of adding this feature? And so I wanted to record this podcast to talk a little bit about, you know, what exactly is going on here and answer some of these questions. Gregory Chanan, who isn\u0026rsquo;t joining me, but sent me a list of questions that he had regarding the feature. And I\u0026rsquo;m going to use these to sort of drive the discussion in this podcast. Okay, so let\u0026rsquo;s start off with the basics. So what is an unbacked cements? So to answer that question, I first need to mention what a backed cements is. So a backed cements refers to our symbolic shapes that we\u0026rsquo;re passing through our program. You know, we have a bunch of input tensors. Instead of statically specializing on these tensors, we give them symbolic sizes, which just say, hey, you\u0026rsquo;re going to do the symbolic execution on these sizes and you\u0026rsquo;re not actually going to burn in any particular size. So if you do a view operation based on the size of something else, we\u0026rsquo;ll pull out the symbolic size for that particular tensor that I\u0026rsquo;m reading out the shape from and pass it on to the view without burning in whatever the actual value was. So if that value changes in the future, then I can actually, you know, just reuse the same graph in this situation. Now, the thing about having symbolic integers like this is if someone writes some Python code and they say, if x is equal to two, then do something else, do something else. There really isn\u0026rsquo;t any way to keep things symbolic in this case, because, you know, we need to actually know which branch we\u0026rsquo;re actually going to go down. Now, of course, there are some program analysis techniques that will allow you to sort of keep those, keep, you know, trace through both branches and do some sort of fancy stuff in that situation. But we\u0026rsquo;re generally talking about straight line traces in PyTorch 2, and we don\u0026rsquo;t have anything that fancy. So we need to actually have an answer in this case. And so when you have a condition on a symbolic integer, we do what\u0026rsquo;s called a guard. So we look at what the actual value is, the sort of backing value. And this is where the term backed versus unbacked comes from. We look at the backing value. This is also referred to as a hint inside of our code base, because the hint basically says what kind of size we might expect this tensor to be in practice. We look at the backing value, the hint of the tensor, and then we do the condition based on, you know, the actual value that we have in the backing value. And then we go ahead and we say, okay, well, if it\u0026rsquo;s true, then I\u0026rsquo;m going to go down the true path. Otherwise, I\u0026rsquo;m going to go down the false path. And importantly, I will add a guard, a guard that is executed at the beginning of the graph, which just says whether or not I\u0026rsquo;ve actually fulfilled this condition. So the next time that I run my graph, will I actually go through the same conditional branch or not? And these conditional branches can happen anywhere in PyTorch code. It can happen in user code, where, you know, a user does some condition on, you know, what the shape of a tensor is. And it can also happen in library code, where inside of the PyTorch library, you know, we\u0026rsquo;re looking at sizes and we\u0026rsquo;re making decisions based on, you know, whether or not the sizes are big or not to do one thing or another. For example, when you\u0026rsquo;re running convolution, we will look at the size of your input tensor, decide which particular convolution algorithm we\u0026rsquo;re going to do. Okay, so to summarize, you know, we have symbolic integers, but they have backing values, hints. And if we do a condition on them, then we look, we peek at the backing value and use that to resolve what the condition is, inserting a guard in that situation. So what is an unbacked simon? Well, an unbacked simon is simply when you just don\u0026rsquo;t have a backing value. And there are two reasons why you might not want to have a backing value. So one is, you might just not have a backing value at all. For example, say you have a tensor that was produced by a non-zero call. What the actual value size of this tensor is going to be is not known to you unless you actually, you know, run the operation because it\u0026rsquo;s data dependent. So we don\u0026rsquo;t know what the value is. We have no idea what it could be. And so we have no choice but to give you an unbacked simon in this case because we don\u0026rsquo;t have a backing value. We don\u0026rsquo;t know what it is. The other example of when they might be useful is when you want to intentionally prevent guards from occurring on a variable. Let\u0026rsquo;s say you\u0026rsquo;re doing export. And so with export, you might want to produce a graph that can work for any batch size. So if you\u0026rsquo;re going to make a graph that works for any batch size, then you would like to say, okay, well, I don\u0026rsquo;t want you to be able to guard on a batch size being zero or one. I just want to, you know, like say, hey, you know, you did no conditional jumps on the value of batch. So my entire program is indifferent to whatever the batch size was. And so you might just intentionally feed in an unbacked simon for the dimension for your batch dimension, just so that you could make sure that you error out if, you know, some code, either user code or library code, attempts to actually do a guard on it in that question. So one question that people often ask me, because a lot of our discussion has been revolving around export, because that\u0026rsquo;s sort of what\u0026rsquo;s been driving, you know, working on unbacked simon\u0026rsquo;s, you know, recently is, are unbacked simon\u0026rsquo;s only for export? And the answer is no, because you can also use them to, you know, implement. You can also use them right for the non-zero case if you are actually going to compile in that case. And you might also use them to just like, you know, say, hey, I want to compile this model for eager mode, but I really, really don\u0026rsquo;t want to, you know, have any guards on this value, because I really only want to compile one graph in this case. And unbacked simon\u0026rsquo;s would be useful in this case. That being said, primarily, we are working on unbacked simon\u0026rsquo;s right now, because we are trying to do something with export. So most of the discussion that\u0026rsquo;s happening right now is all about export, because that\u0026rsquo;s what we\u0026rsquo;re spending most of the time thinking about. I was in a discussion with Sam Gross, and Sam was asking me, well, you know, about this non-zero compilation case, you know, is that a real use case? Because you might want to just, you know, graph break, and then, you know, you run the non-zero, and then you run the graph afterwards, and isn\u0026rsquo;t that good enough? And the answer is, well, yes, that is mostly good enough. But there are some situations where you will miss optimization opportunities for this. And in particular, if you have some sort of data-dependent operation, say, non-zero, or more realistically, a packing operation, where you have some padded tensors, and you pack them into a, you know, small tensor that doesn\u0026rsquo;t have any of the padding values. And by the way, the output of this packing operation is dynamic, because, you know, what you pack depends on how much padding there was inside the original tensors, and that\u0026rsquo;s a data-dependent concept. So after you pack, you might want to run some point-wise operations after it. And here, it would be profitable to fuse in those point-wise operations into the packing operation, which is getting the data in this place. And this happens with jagged slash nested tensors, where, you know, often you have a bunch of input tensors, and you want to pack them into, you know, a smaller, you know, with no padding tensor, and then do the operation on it. So this is a profitable optimization. It\u0026rsquo;s something that I\u0026rsquo;ve been told by the folks working with jagged tensors that they want. And, you know, it\u0026rsquo;s one of the reasons why you might want to support this. But as I said, like most of the discussion that\u0026rsquo;s happening right now in PyTorch development is all about export. So, you know, that\u0026rsquo;s what we\u0026rsquo;re doing. So then, okay, so we got Unbacked Simmons. And so a lot of our discussion with Unbacked Simmons is Unbacked Simmons work a lot like Simmons, but all your guards fail, right? So when you try to actually use them, you end up with a pretty common situation, which is you try to feed in Unbacked Simmons into your model, and they don\u0026rsquo;t work because there\u0026rsquo;s a guard. And now you\u0026rsquo;re like, well, why is there a guard on my code? You know, and you look into the bunch of the cases, and there are all sorts of different scenarios. And I actually talked through a bunch of these scenarios inside the Dynamic Shapes manual, so you can check that out for more details. But one of the examples that has been causing folks quite a bit of trouble, you know, sort of like, do we want to do Unbacked Simmons in this way, is the so-called broadcasting example. So let\u0026rsquo;s unpack the broadcasting example for a moment. The broadcasting example says, hey, you have got a tensor, and let\u0026rsquo;s say it\u0026rsquo;s got an Unbacked Simmons, and you want to add some other tensor to it. And let\u0026rsquo;s say maybe it\u0026rsquo;s also got an Unbacked Simmons in it. And it just so happens that the sizes of the two tensors are equal, so they will add together no problem. So we happen to know out of band that everything is going to be okay. But when you run this code, what PyTorch in the library code is going to do is it\u0026rsquo;s going to attempt to test for broadcasting. Namely, it\u0026rsquo;s going to check and see if any given size on the left-hand tensor is one, because if so, it can broadcast to the right-hand side. And we\u0026rsquo;ll test if the right-hand side is one, and if so, it can broadcast to the left-hand side. Broadcasting being, you know, just replicating the one size dim as many times as necessary to fill in the other size. So, if you just run the library code as is without any changes, what we will do is we will test if the input tensor size is one, and then we will test if the right-hand side tensor size is one, and then we will test if their sizes are equal. But I just told you that I was passing in a tensor that was unbacked. And so if I do a condition on it, if I actually say, hey, tell me if the tensor size is one, if that size is unbacked, then that will just immediately fail, saying, hey, you tried to guard on an unbacked simon. But actually, you know, in this particular case, the guard was completely unnecessary, because the sizes would have ended up being the same on both sides, and you just would have been fine. You didn\u0026rsquo;t need to broadcast because they were just equal. So, like, this is the sort of situation where, you know, you end up with a, hey, unbacked simon caused a guard failure, and now I need to go modify PyTorch library code. Now, when I told people this, you know, there were a few questions about, like, is this a real problem? Because, well, like, how, this seems like a dumb issue to have, because obviously, the broadcasting code is, you know, going to be fine, and, you know, like, surely there\u0026rsquo;s some simple solution to solve this problem. And, one question that people had was, you know, why am I looking into the broadcasting code at all? Naively, I would expect the export graph to just be a list of eight and ops strung together. So, so why do I have to recurse into the point-wise operation to actually, you know, where, you know, I actually run all this broadcasting logic, right? Because, because when I look at my graph, all I\u0026rsquo;m getting is an add operation. And so, you know, like, there\u0026rsquo;s no broadcasting to be seen. So, so why does this matter for tracing? And to answer this question, I have to say, well, the reason why you\u0026rsquo;re, you know, going into this code is because when you run the add operation, you get out some result tensor, and that result tensor has sizes on it. What are those sizes going to be? Well, to figure out what those sizes are going to be, you have to run the shape propagation rules for addition. And those shape propagation rules are what actually do the broadcasting. So, so, you know, to do the shape propagation, that\u0026rsquo;s when you actually do the broadcasting checks and that\u0026rsquo;s when you do the one check and that triggers the guard. So guards aren\u0026rsquo;t just, you know, remember, executing on user code, they\u0026rsquo;re also executing on library code. And in particular, they\u0026rsquo;re executing in the shape propagation code, even if that shape propagation code is completely invisible in the final exported program you get. So then you might be like, well, okay, Ed, I can see that, you know, to compute what the output size is going to be, I have to run this operation. But what if I was, you know, what if I said, hey, I just don\u0026rsquo;t want to actually, you know, like do any of this because I don\u0026rsquo;t need to know what the output shapes are. Maybe I just, I don\u0026rsquo;t care. I\u0026rsquo;m going to, you know, sum over them or do something very simple to them in the end. And I don\u0026rsquo;t need a very, very fine grained, you know, expression that tells me exactly how to compute the size of this in terms of the inputs. And so for one, yes, this is a thing you could do. Two, you typically don\u0026rsquo;t want to do this in eager mode because if you were to guard on the output size, because remember, the user can do whatever they want. And in particular, they can pass it to another operation where, you know, that size needs to be checked its equality against something else. So if you want to guard on it, then you actually need to be able to express the guard in terms of the input sizes. So you need to know how to actually do the computation from the graph inputs to the end. It\u0026rsquo;s not like a traditional JIT system where, you know, when you realize that you violated some constraint for your trace, you can bail out. We have to like, you know, move all of these bailout checks to the beginning of the graph when we compile them. But hey, we\u0026rsquo;re export. We\u0026rsquo;re not going to like, you know, really poke on these with guards. Would that be fine as well? And then is, yeah, sort of. So what we can do is we can say, okay, we don\u0026rsquo;t know anything about the output sizes of this tensor. We just want to say, hey, it\u0026rsquo;s something. And as long as you don\u0026rsquo;t look at it too hard, if you don\u0026rsquo;t try to do any reasoning about it, it\u0026rsquo;s fine. And we can do this. And in fact, I do this for a, for the non-overlapping and dense tech check on tensors. So when we, when you make a tensor, one of the Boolean fields we pre-compute is, is this tensor non-overlapping and dense? Sometimes this is obvious, but if you pass in a bunch of strides, it\u0026rsquo;s very non-obvious. You have to sort the strides and then like, look and make sure they all like line up exactly correctly. And it\u0026rsquo;s very complicated and causes a lot of guards. So what I do instead is I just return a, hey, you know, this is just an opaque thing. is non-overlapping or dense function. It takes in all the sizes and strides for the tensor and that\u0026rsquo;s it. You don\u0026rsquo;t get to know anything else about what this quantity is. And so the point is that as long as you never actually try to touch this quantity in any meaningful way, like you never try to condition on it, you never try to test it for equality with anything else, that\u0026rsquo;s fine. And this works perfectly okay. and so it only blows up if you actually, if you actually try to do something with it. And it\u0026rsquo;ll probably blow up if you actually try to do something with it because you said, well, I don\u0026rsquo;t know anything about this, so there\u0026rsquo;s no way to do any reasoning about this. And this is one of the reasons why, you know, when Horace looked at this situation, he\u0026rsquo;s like, well, this seems kind of bad because you\u0026rsquo;re just, you know, pushing off the problem until later. And the answer is yes, I\u0026rsquo;m pushing off the problem to later. It pays to be lazy if you end up not having to do the work at all. Another question, and we\u0026rsquo;re going to relate this to the 0-1 specialization episode is, you know, how does 0-1 specialization fit into all of this? You know, we might want to 0-1 specialize in a dynamic shape regime, but like, does that actually seem to matter for export? And the answer is yes. so 0-1 specialization is kind of, kind of mixing up a few topics here. So one thing that I mentioned about 0-1 specialization, it is a trace time optimization, right? You don\u0026rsquo;t have to up front 0-1 specialized tensors when they feed into your program. You can just say, well, I\u0026rsquo;m not going to assume that, you know, this 0-size tensor is always going to be 0. I\u0026rsquo;m going to try to run my program anyway. The reason why 0-1 specialization is so useful for PyTorch though is a sort of empirical observation, which is that there\u0026rsquo;s a lot of code in PyTorch which does all sorts of 0-1 tests. So, you know, basically, you\u0026rsquo;re going to specialize on 0-1 anyway when they do the test and guard on the quantity as well, so might as well do it earlier on in the program. But, you know, if you just say, well, I\u0026rsquo;m not going to do it up front, well, you\u0026rsquo;ll just collect up a bunch of places where you actually do 0-1 specialization later. so it\u0026rsquo;s sort of irrelevant. For export, you just turn off 0-1 specialization and you pass in an unbacked cement and then you just, you know, deal with the guards one by one, at least in, you know, my proposal for how to do unbacked simmons. Okay, one last thing that I want to talk about here, which is why has the unbacked cement stack of PRs been kind of controversial? So what you find this stack of PRs doing is it\u0026rsquo;s saying, hey, you know, I had some model. I had like ResNet and I wanted to run it with an unbacked cement for batch size. So I put in one of these unbacked simmons, I ran it, and whenever there was a guard failure, I went and tweaked PyTorch library code until it no longer had this problem. And so people look at these diffs and they say, hey, well, like, does this mean that I have to, you know, write my PyTorch library code in this funny way in the future? That sure sounds like, you know, having to tortscript my code and, well, tortscripting my code was very painful and I don\u0026rsquo;t want to have to do this again for another thing. So I don\u0026rsquo;t know exactly how to argue this one way or another, but my general thinking is that yes, you have to modify your code, but I don\u0026rsquo;t think it is as bad as tortscript. So there are a few reasons I don\u0026rsquo;t, I think this is not as bad as tortscript. So one is that really all of the really complicated cases have been inside, you know, PyTorch library code, very low level operations like empty, like reshape, and like is contiguous. And so, you know, one of the ideas that, you know, I was hoping would be true with my patch set is I fix these like low level problems and then, you know, most code is not written in a branchy way, right? Like, you know, you don\u0026rsquo;t have people re-implementing broadcast everywhere. They usually just call an operation that broadcasts and, you know, if that broadcast implementation knows how to like, you know, tiptoe around unbacked Simmons, then that\u0026rsquo;s fine. So the hope is that like the sort of fat, there\u0026rsquo;s a fat tale of very complicated operators that we have to handle internally and the rest kind of will just work out because most people aren\u0026rsquo;t writing their models trying to, you know, like condition on what your batch size is going to be. The other thing that I think is a little different is that in TorchScript, you, it was an all or nothing deal, right? You had to get all of your code end-to-end towards Scriptable to actually get something useful. With unbacked Simmons, you don\u0026rsquo;t have to actually get everything going, right? Like if you\u0026rsquo;re not doing export or you\u0026rsquo;re not like, you know, saying, hey, I must compile all of my program in a single traced block from head to toe, then you\u0026rsquo;re allowed to not, you know, not use unbacked Simmons all the way through. In fact, I would not recommend using an unbacked Simmons. In this case, you can just say, okay, well, this is fine. Like, I\u0026rsquo;m going to make sure that it works for sizes that are greater than two. And if you happen to send me a batch size one, I\u0026rsquo;m just going to go ahead and recompile my program for the batch size equals one case. No problem. What\u0026rsquo;s the big deal, right? Like it\u0026rsquo;s just a 2x cost in number of compiled graphs. And I still have one that, you know, can handle all of the variable cases. So really, the only time you need to like squeeze into this regime is if you are trying to export and it is a dynamically sized model, so you want the varying batch size and you\u0026rsquo;re in a situation where you can\u0026rsquo;t ship multiple graphs, you have to ship one graph. And to that, I say, well, you know, what did you expect, right? You\u0026rsquo;re going to have to write your code so that it doesn\u0026rsquo;t actually like do any branching on the batch size. And there\u0026rsquo;s sort of just some sort of irreducible complexity, at least in my opinion. Okay, so this is an ongoing conversation. I recorded this to help information share. We might have an updated recording later once we have some more alignment. So I\u0026rsquo;ll also link that in the podcast if that actually happens. All right, thank you very much for listening. See you all next time.\nEP70 Dynamo\u0026mdash;VariableTracker Dynamo\u0026mdash;VariableTracker Hello, everyone, and welcome to the PyTorch Dev Podcast. Today, I want to talk about variable trackers in Dynamo. What is a variable tracker? Well, to explain the concept, we first have to think about what it is that Dynamo is trying to do. Dynamo is trying to take your Python program, and without actually running it, it wants to simulate the execution of every single operation that happened in your program so they can find where all the Torch operations happen, put them in a graph, and then send them off to the rest of the PyTorch compiler so that we can compile them into Efficient code. So in order to do this, we need to run the code, but without actually running it. And depending on how complicated your program is, that may require us to do a lot of stuff, right? Let\u0026rsquo;s say that within your model, you\u0026rsquo;re creating a dictionary, you\u0026rsquo;re putting things into the dictionary, you\u0026rsquo;re taking things out. In order for us to step through every line of code in this situation, we have to actually model this dictionary in some way. But we can\u0026rsquo;t use an actual dictionary. Well, actually, we can, but sometimes these data structures have side effects. They actually do things. Print to your terminal, write to other things. So we can\u0026rsquo;t actually use the actual data structures in a lot of situations. What we actually need to do is we need to be able to maintain some parallel universe in Dynamo, which is sort of like the dynamified universe of all the state in your Python heap, which we can go ahead and do operations on, right? For example, if you have a global dictionary and inside your model code, you\u0026rsquo;re writing, you\u0026rsquo;re incrementing a counter on it. When we symbolically evaluate it to extract out of Dynamo graph, we can\u0026rsquo;t actually mutate that global dictionary. We have to do it in our sort of local universe that is our simulation and only when we are done, have some actual code which replays this effect onto the real thing. So variable trackers are essentially our way of representing the Python heap in a way that Dynamo can work with it, can do analysis on it without actually having to touch the real Python values. So if you\u0026rsquo;re working on the Dynamo code base and you\u0026rsquo;re thinking, hey, you know, how, where exactly should I implement some logic for, you know, how I should be able to update some state when I do some operation or, you know, how do I model some data structure that someone has written that I need to do some special handling for? Chances are it\u0026rsquo;s going to live in the variable tracker one way or another. So variable trackers have a lot of purposes, right? So they encapsulate a state that we can\u0026rsquo;t actually get our hands at by just directly looking at bytecode. So most frequently, that\u0026rsquo;s because the structure is implemented in C. Like, you know, if you\u0026rsquo;re talking about something like a dictionary or a set from C Python, there is no quote unquote Python implementation, right? It\u0026rsquo;s natively provided by C Python. So anything that is natively provided is not an actual Python bytecode. We have to implement by hand inside Dynamo so that, you know, we can basically replicate the logic that is living inside the C code because we\u0026rsquo;re not tracing through the C code. The C code is opaque. There\u0026rsquo;s no way we can look at it. If you have a user-defined class and that class is written entirely in Python, chances are we don\u0026rsquo;t have to write a variable tracker for that because in that case, the variable tracker, in that case, it\u0026rsquo;s built out of some simpler primitives, which we have written variable trackers for. But then all the operations, the constructor, you know, accessors, those are all Python bytecode, and we can just step through them in a normal way to actually process them. Okay, so that\u0026rsquo;s what variable trackers are in a nutshell. And what are some things that are useful to know about them? So we\u0026rsquo;ve actually made some changes recently to the variable tracker. We\u0026rsquo;ve got the PyTorch 2 paper coming soon in ASPLOS, but it\u0026rsquo;s got the old version of how variable trackers work. And so I think this podcast is going to be one of the first places where we actually say, besides in GitHub issues, what has changed. So there are two big things that I want to convey. So the first is that variable trackers are in charge of doing guards. Remember, a guard is how we tell, hey, you know, this execution, this symbolic evaluation that we did, requires certain aspects of the Python heap to be some way, right? Like if I do a conditional on a Boolean, and it goes down one path the first time, well, the next time I go, I need to make sure that Boolean is the same way. Because if it changes, if it goes false, I\u0026rsquo;ll go down this different path. But that\u0026rsquo;s not the path I trace down. I\u0026rsquo;m not parsing the program. I\u0026rsquo;m just looking at a particular sequence of execution. So variable trackers are very important for keeping track of guards because we have all these values floating around. If we actually poke at these values, it actually matters what the value is. And so we typically need to do a guard. But we don\u0026rsquo;t want to immediately say, well, the exact state of every single object in the Python heap has to be exactly this way when we, you know, start our analysis. Because chances are, we\u0026rsquo;re not going to touch most of these things, we\u0026rsquo;re not going to touch most of the variables that we actually model in Dynamo. So we only want to actually do guards on things when they actually matter for the execution in hand. And so the old version of Dynamo, the way it worked was essentially any variable tracker had a set of guards on it, basically saying, if you use this variable tracker in a non-trivial way, here are the guards that you need to use, you need to put into maybe the global guard state that\u0026rsquo;s actually getting installed, or maybe some other variable tracker, which was derived off of the original variable tracker, so that, you know, all the things you looked at on the variable tracker are valid in the same way. It turns out there were two problems with this. So one is that it was a lot of pain to, like, do all this propagation logic, because every time you did something with a variable tracker, you needed to, you know, make sure you didn\u0026rsquo;t forget to collect off all the guards off each of them, you know, plomb them together into one giant set, and then put that on your new variable tracker. Very easy to forget, you know, very hard to test, because to actually test that, you know, you\u0026rsquo;ve actually done this right, you have to, you know, set up some program, and then change the thing that would have been guarded, and make sure things actually get rewritten. And, you know, most of the time, people are just writing tests that are just testing, we can actually get through some code one way to another. So, like, writing very good tests, that test that we are guarding enough, it actually takes a lot of care, and so, you know, it\u0026rsquo;s pretty difficult. The second problem is that maintaining these sets of all these guards is actually really expensive. Like, you know, Python is not a spring chicken, and then if you have these giant sets with tons and tons of objects, that, you know, need to be hashed every time you\u0026rsquo;re putting them in the set, it actually was materially making an impact on how quickly you could Dynamo trace through things. And so, on our, like, open source benchmark suite, our, you know, tracing times are not so bad, but we\u0026rsquo;ve been using PyTorch 2 on a lot of internal workloads, and these workloads have tons and tons of Python code, and there\u0026rsquo;s, like, sometimes it would take hours for Dynamo, just Dynamo, not even the compiler, not even Inductor or Trident, just Dynamo to get through all of that code. And part of it was we were just, you know, shunting all these guards around, you know, kind of difficult to deal with. So, Jason Ansell did a patch to make us not have to do this, and the new world order is this. When you have a variable tracker, we have guarded on it. That\u0026rsquo;s it, right? So, if you have your variable tracker in your hands, there\u0026rsquo;s all sorts of things you can access on it, and we\u0026rsquo;re just going to assume that we have already guarded on everything needed on the variable tracker in that case. So, there\u0026rsquo;s no propagation needed, right? Once you\u0026rsquo;ve got the variable tracker, we can assume that we already have the guards in question. Now, sometimes this lazy behavior that we had before is good, right? Like, say I have a bunch of arguments to your function. I don\u0026rsquo;t want to actually guard on all of them exactly. So, there\u0026rsquo;s some amount of laziness that we have for some variable trackers, which is that you can have a variable tracker which doesn\u0026rsquo;t actually exist yet. We haven\u0026rsquo;t actually populated it into an honest-to-goodness variable tracker. The first time you poke at it and you\u0026rsquo;re like, hey, you know, tell me what this attribute is. Tell me, you know, what the value of the boolean is. Then we actually populated it into a real variable tracker and installed all the guards. So, there\u0026rsquo;s, like, you know, specific laziness in various parts of the code base. The new structure, I think, is very nice. It reduced a lot of the administrative burden we had to do, and it made stuff a lot faster. So, hooray. So, variable trackers, right? If you\u0026rsquo;ve got a variable tracker, the obvious thing, which is that you can access it however you like, that will work. And when you create a new variable tracker, you\u0026rsquo;re responsible for making sure, at that point in time, you install all the guards you need. Okay. So, I talked about how guards work with variable trackers. There\u0026rsquo;s actually another update, which is pretty nice, and this is from Michael Lazos, landed in December, and this is what we\u0026rsquo;re calling mutable variable tracker. So, another thing that you may not have realized about variable tracker in the old days is that variable trackers were actually implemented as immutable data structures. The Haskler in me is like, hooray! Why were they implemented as immutable data structures? Well, the motivating reason for making them immutable was to support this checkpointing thing that we do in Dynamo. So, let me explain what\u0026rsquo;s going on with checkpointing. So, with checkpointing, the reason why we need a checkpoint in Dynamo is that sometimes we will be symbolically executing some code, and we will be like, oh, no, we messed up. We need to rewind the state of our execution back to some point, some earlier point in time, where we can actually go ahead and insert a graph break. And the canonical example of this is if you\u0026rsquo;re inlining a function, right? So, if I\u0026rsquo;ve got some code in Dynamo, and I\u0026rsquo;m tracing through it happily, I hit a function call, I start inlining the function call, and then inside that function call, I have a graph break. What do I do in this situation? Well, if I had some sort of fancy multi-call frame reconstruction logic, the way I could deal with this is just by, like, doing a graph break right then and there. But we actually don\u0026rsquo;t have this logic. Someone should implement it, by the way. This would be great. So, because we don\u0026rsquo;t have this logic, what I have to do is I have to rewind execution back to when I was about to call into the function I inlined. And at that point in time, I do the graph break. So, how can I do this rewinding? Well, if I have a checkpointing mechanism, whenever I start an inline function call, I can just checkpoint the state of all the variables in my Dynamo program, and then, you know, just throw out anything else, throw out the new state, and reuse my checkpointing state. And so, immutable variable trackers make this easier to do in this situation. But there is a cost, right? The cost of this is that, you know, we actually have to do these as immutable data structures, and that means that simple operations, like, let\u0026rsquo;s say you have a list and you\u0026rsquo;re appending to it, normally these appends are O of 1. But if you have an immutable variable tracker, then I have to create a new copy of the list every time, and so this ends up being an N squared operation to insert N elements onto the list. Now, of course, you know, once again, with my Haskell hat on, why don\u0026rsquo;t you just use a more efficient functional data structure? And the answer is, yes, you could, but, you know, CPython doesn\u0026rsquo;t have very good support for this sort of thing, because most people in Python are just doing mutable lists, like, whatever. Like, that\u0026rsquo;s the normal thing to do. So you would be in this situation where if you just wanted to make this go faster, you would have to write a big library full of all sorts of immutable data structures. And also, it\u0026rsquo;s kind of like a bad idea in a reference counted language like Python, because, you know, every time you generate garbage, you generate, you know, these new copies of nodes that you then throw out immediately, because you\u0026rsquo;re, you know, just, you\u0026rsquo;re continuously revving this immutable data structure. You have to spend all this time, you know, incrementing and decrementing the ref counts. It\u0026rsquo;s not like in a garbage collected language where the more garbage you make, the faster your garbage collector runs, because remember, a garbage collector only needs to traverse the live routes of your object. So what do we do? So we said, okay, fine. Checkpointing is cool, but we actually don\u0026rsquo;t need it. And the reason we don\u0026rsquo;t need checkpointing is, remember this thing, right? Dyno is working in this alternate universe. It is, you know, symbolically evaluating your program without modifying the original program state. So we have an ultimate checkpoint, which is at the very beginning of your program. That basically tells you what all the state is, and we haven\u0026rsquo;t touched that at all. So we don\u0026rsquo;t have to, like, actually checkpoint midway through. If we need to rerun Dyno, we can just rewind all the way back to the beginning and then run again from the start. And so we don\u0026rsquo;t need a mutable value circle. So Michael Lauzos got rid of mutable variable trackers. They are now mutable. You can mutate them in the normal way you expect. And, you know, life is good. And this also made some of our internal tracing a lot faster. Okay. So I told you about why variable trackers exist and some of the changes that went on. One more thing I want to say is how to find your way around variable trackers in the Dynamo codebase. So there are a lot of variable trackers. And sometimes it can be a bit bewildering to try to figure out, like, which variable tracker should I use? And we don\u0026rsquo;t have really that good of an organization for the variable trackers, but there is some logic to it, right? So in particular, we\u0026rsquo;re trying to organize basically the chunks of C code that we are simulating in Dynamo into sort of, you know, various logical things instead of just blobbing them into one giant thing. So if you think about it that way, this will tell you about sort of where things are. So in particular, if you have a completely immutable state in Python, like, you know, if it\u0026rsquo;s a literal, like an integer or a float, we typically model these as constant variables, right? There\u0026rsquo;s also an enum variable for doing enum specifically. If you have some state which is immutable, so you can\u0026rsquo;t actually modify it, then we tend to organize the variable tracker subclass based on where it comes from, right? If it comes from PyTorch, then it\u0026rsquo;s a torch variable. If it comes from CPython, it\u0026rsquo;s a built-in variable. If it comes from NumPy, it\u0026rsquo;s a NumPy variable, right? Like, we basically say, where does the code live? And then we just go ahead and put the code in those locations. Now, these are giant variables, right? Because, you know, like, think about torch variable, right? Like, we have tons and tons of stateless C code because every single function in the PyTorch API counts as, you know, something we need to model in variables. So these classes tend to be very big. But, you know, at a high level, the organization is based on, you know, where you can find it. Similarly, if you have a, some state, sorry, something in C, and it is stateful, then that\u0026rsquo;s the situation in time where you get the normal thing where you have a dedicated variable per object. So we have a tensor variable, we have a list variable, we have a set variable, we have a dict variable. If you need to introduce anything else like that, you\u0026rsquo;re probably going to make a new variable subclass. Because state needs to be handled specially, you need to, you know, write logic for how to replay changes to the state back to the original variables, stuff like that. And then finally, for things that are implemented in Python, and we can inline into them, we have a big pile of, you know, user blah variables, like user function variable, user defined class variable, user defined object variable. These tend to be actually relatively simple, because we don\u0026rsquo;t need any special smarts, right, we just are going to plan to inline into the bytecode for them to actually implement them. So that\u0026rsquo;s everything I wanted to say about variable tracker today. See you next time.\nEP71 Inductor\u0026mdash;IR Inductor\u0026mdash;IR Hello, everyone, and welcome to the PyTorch Dev Podcast. Today, I want to talk about Inductor IR. Inductor IR is an intermediate representation that lies after the A10 graph, but before the actual Trident code generation. So if you think about the overall PyTorch 2 stack, once we are done capturing the graph with Dynamo, we have a bunch of FX nodes referring to A10 operations. And then in order for Inductor to actually compile this code into Trident code, it will take that A10 graph and do a bunch of transformations on it, first converting it into Inductor IR, and then scheduling that IR, and then actually finally generating the code from the scheduled nodes. And that\u0026rsquo;s how you actually get the good Trident goodness, as well as the wrapper Python or C++ code that strings it all together. So as you might imagine, the Inductor IR is a pretty important thing to know how to work with if you are planning to work on the compiler in Inductor at all. Now, I don\u0026rsquo;t have a good organization for all of the things that I want to talk about here. So this is going to be a bit of a grab bag of things. But to start off, I first want to talk about sort of some of the motivating design considerations behind Inductor IR. Also, a disclaimer, I did not write Inductor IR, and I am still learning how to figure things out, as most of us are over in PyTorch Core. So another thing is that there are some things that are not ideal about the current state of Inductor IR, and probably could use some refactoring. So I may make some errors in this podcast. It\u0026rsquo;s also rapidly changing. This podcast may become out of date. I recommend sort of like, you know, continuing to join the conversation on PyTorch GitHub if you are interested in contributing. Okay. So let\u0026rsquo;s talk about some of the design considerations behind Inductor IR. So first off, we might ask the question, why do we need an intermediate representation between ATEN operators and the actual Triton code gen, right? Why can\u0026rsquo;t we just go ahead and directly generate Triton code from each ATEN operation? So there are a few reasons why you don\u0026rsquo;t want to do this, right? So one is that, you know, we don\u0026rsquo;t actually want to generate a Triton kernel per ATEN operation. We want to do fusion, right? So we need some way of saying, hey, we have a bunch of ATEN operations which can be fused together, say a sequence of point-wise operations, and we need to be able to represent the result of doing this fusion. And of course, ATEN graphs are pretty simple in that they only call a sequence of operations and are done, which means that they don\u0026rsquo;t really have any concept of, well, this is a fused node that contains a bunch of operations that are grouped together. Now, if you\u0026rsquo;re familiar with old-school TORTScript, we sort of just did the obvious thing in that case, right? So a fusion group is simply a big operator that inside itself contains another graph which contains a bunch of little operators that are expected to be fused together. So you can certainly design your compiler this way, but we did it a little differently with Inductor IR for some other reasons. What exactly did we do in Inductor IR? There are a few ways to understand this. I\u0026rsquo;m going to go through a few different ways of thinking about it. So one way to go about looking up at this is to just look at the code and see, well, what exactly are the classes and data structures that we define for IR? So if you look in Inductor IR.py, you\u0026rsquo;ll see that there is a class called IRNode and there are a number of subclasses of it. And these subclasses have names such as loops, base view, layout, buffer, mutable box. And you might be wondering, well, what exactly is the relationship for all of these things? Actually, these are all different types of things. IRNode is sort of just this grab bag class that puts everything together. In fact, each of these is its own distinct concept. And so it\u0026rsquo;s best to not imagine that all IRNodes are interchangeable. It\u0026rsquo;s not an abstract data type with these various things that you can have as various options. What\u0026rsquo;s another way to understand how IRNode works? Well, another way is when we write lowerings. So a lowering in Inductor is the code that takes a particular AT\u0026amp;OP and then produces a bunch of IR nodes, a bunch of Inductor IR representing that operation. What exactly does that look like? And so a lowering very closely follows the format that you expect for the function signature in question. So for example, if I\u0026rsquo;m lowering an addition between two operations, I\u0026rsquo;m going to get something that\u0026rsquo;s like a tensor, but in Inductor IR, and something that\u0026rsquo;s like a tensor in Inductor IR. So those are my two arguments to my addition. Now, what exactly do I get? Well, I mentioned this thing called mutable box. What I actually get is this thing called tensor box, which represents a tensor in Inductor IR. Now, what exactly is inside a tensor box? Why do I have a box on its name? Well, let\u0026rsquo;s think about it. So when I do operations in PyTorch, I can have mutating operations, right? So for example, if I have an AT\u0026amp;T graph and I say add underscore, I\u0026rsquo;m going to mutate a tensor in place. Now, this makes sense and is fine if I\u0026rsquo;m actually trying to do the operation for real on actual data, but what if I\u0026rsquo;m trying to actually go ahead and do some sort of lowering? What if I\u0026rsquo;m trying to generate some Inductor IR for this situation? Well, I can\u0026rsquo;t just do the mutation, but if anyone else references my tensor at some later point in time in some other lowering, what I need to have happen is I needed to reference the result after having done the mutation, not the result before having done it. So the IR is immutable in some sense, right? When I do a in-place addition, I want to create a node that represents, hey, I did this mutation. But now when I do operations afterwards, I want everyone to make reference to the thing afterwards. So TensorFlow box basically says, okay, I\u0026rsquo;m going to contain some IR inside, which represents the whatever it is that I want to, whatever it is produces the output for the tensor in question. But whenever I do a mutation on it, I will mutably swap out the IR that it is pointing to to whatever the new IR is that represents the result having after done the mutation in question. Now, that\u0026rsquo;s not to say that inductor IR doesn\u0026rsquo;t have mutation. It does. But, you know, we are sort of, we\u0026rsquo;re not processing the IR in a traditional compiler sense where, you know, you just have some sort of graph representation and we\u0026rsquo;re writing things into the graph. Instead, we\u0026rsquo;re sort of maintaining a big pile of tensors which represent, you know, pointers into various parts of the IR. In fact, the IR isn\u0026rsquo;t even ordered at this point. We just have a bunch of IR fragments floating around that have some dependencies between each other because we\u0026rsquo;re going to actually figure out what order we actually want to run them in later when we do scheduling. Okay, so a tensor box contains a, you know, something. Actually, the tensor box doesn\u0026rsquo;t actually contain the thing that you would mutate if you\u0026rsquo;re doing a data mutation because the tensor box contains a pointer to a storage box and the storage box represents the actual backing data score. And this is useful because we can have multiple tensors referencing the same storage. And so, you know, we may have multiple tensor boxes referencing the same storage box. And the storage box is what actually references a buffer that actually, you know, represents the data that is living in the tensor at that point in time. So one thing to notice here is that, you know, inductor IR actually faithfully models PyTorch semantics, right? If you like think you understand how PyTorch eager mode works and then you go look at how inductor IR works and more importantly, how inductor IR evolves while you\u0026rsquo;re doing the lowering, it really matches what you\u0026rsquo;d expect to see if you were just running a traditional PyTorch program. So you have eager mutation, you can have views. In fact, not only can you have views, you can have arbitrary indexing, you know, arithmetic, depending on what the view in question is. This is something that Jason Ansell was very important to him when he was designing Inductor because a lot of compilers, you know, don\u0026rsquo;t like, don\u0026rsquo;t buy into PyTorch\u0026rsquo;s idea of views and strides. And as a result, they have to do a lot of work to, you know, sort of deal with strange patterns that show up when people write PyTorch programs in practice. So Inductor is all about, you know, being able to compile PyTorch as it is and it builds in all of these concepts that are very important to PyTorch and so we\u0026rsquo;re willing to deal with them as well. One consequence, for example, of, you know, being able to do strided indexing is we have this entire mechanism for making complicated indexers where, you know, I\u0026rsquo;m doing a kernel, I\u0026rsquo;m accessing data on a tensor, but the data may not be contiguous. There might be some strange stride pattern that I need to do. Inductor can generate arbitrary indexing expressions to fetch out the correct data in this case and, you know, we need to be able to do simplifications and things on this indexing and this is one of the reasons why we use SymPy in Inductor, for example. Okay, so Inductor faithfully models PyTorch semantics. We have this tensor storage distinction between the box. Eventually, you get to a buffer which actually represents the data in question. You can views on the buffers, all that sort of stuff. You can have mutation on the IR, but when you do mutation, all we\u0026rsquo;re doing is we\u0026rsquo;re swapping out the contents of a storage box with a new buffer that represents what happened after the mutation. And finally, we get to the buffer itself, which, you know, somehow represents how we computed the data in question. And the most interesting buffer that, you know, you will usually see when you\u0026rsquo;re looking at Inductor IR is the so-called computed buffer, which says, hey, we did some sort of computation like a point-wise operation or reduction that actually produced the data in question. And inside these computed buffers, you actually finally have the IR nodes like point-wise and reduction that represent the actual, you know, computation that we\u0026rsquo;re doing in PyTorch. Now, there\u0026rsquo;s something kind of interesting here, which is that these nodes, you know, you would expect them naively to contain FX graphs representing the various operations that are being fused together in a point-wise operation or reduction. But we don\u0026rsquo;t actually define them this way in Inductor. They\u0026rsquo;re instead defined by this thing called define by run. If you are a PL nerd, this is actually another way of referring to what we call higher order abstract syntax. The main idea behind define by run is that instead of maintaining an explicit graph representation, we instead maintain a graph as a function. So it\u0026rsquo;s very high order. You have a function, which takes in a bunch of arguments representing all the arguments that the actual, you know, IR graph would have represented. And then on the inside calls all the operations that represent the define by run operation in question. So this can be conveniently done in Inductor because our loops, our loop bodies are control flow free. We don\u0026rsquo;t have any sort of control flow. So we can just use a regular Python interpreter to step through them. And the big consequence of doing it this way is that you get to write really compact definitions. For example, let\u0026rsquo;s say that you have two point-wise bodies and you want to compose them together into a single point-wise body. In a normal graph transformation, you have to take the two graphs, you know, sort of muck around with the inputs, rename nodes so that you manage to get, you know, the outputs lined up with the inputs, so forth, and, you know, do a lot of administrative work to get things together. In a define by run IR, you just have two functions, right? One function takes in some inputs, produces some outputs, and the other function takes in some inputs and produces some more outputs. So what do you do? You define a new function that calls the first one and then calls the second one. No problem. So this lets you write really, really slick, really, really short lowering code. It\u0026rsquo;s actually really nice. And of course, you don\u0026rsquo;t give up the ability to access the structured graph representation because all you need to do is run this function where you\u0026rsquo;ve overridden the behavior of the operations to basically mean, please write this out into an FX graph. So you have a way to reify the higher order abstract syntax into your graph. This is all done via this thing called virtualized, which basically takes all of the inductor core IR operations. These are things like add, sub, whatever that you have inside of loop bodies and allows you to change what exactly they do depending on the situation. So one common thing to do is I want this operation to write into an FX graph. But we also do other things by changing the abstract interpretation of these operations. For example, you can do read-write analysis to figure this out. Okay. So we\u0026rsquo;ve talked about what the actual inside of compute buffer and the point-wise and reduction nodes look like. And so essentially, you end up with this big pile of buffers and unfused computation. And there\u0026rsquo;s a few things going on in this situation. So one is that we have this notion of a buffer that has been realized versus a buffer that is just computation. So when you have a buffer that is realized, we are going to forbid fusing into it. We basically said, we guarantee you that this data is going to exist in physical form at this point in the IR. And this is important because if you are, for example, calling an extern kernel, which is expecting to see a tensor, or if you\u0026rsquo;re going to use this buffer a lot of times, you really don\u0026rsquo;t want to be recomputing its quantity over and over again. And so unfuse compute, which, you know, hasn\u0026rsquo;t been realized, is allowed to sort of go ahead and fuse or maybe even run multiple times if, you know, that\u0026rsquo;s profitable for the situation. And of course, the scheduler, which runs after we\u0026rsquo;ve lowered all of our A10 operations to Inductor IR, is that what\u0026rsquo;s actually responsible for deciding what order to run things? You know, how exactly should we fuse things together? What\u0026rsquo;s the most profitable fusion to do at any point in time? Okay. So I\u0026rsquo;ve talked a little bit about the high-level structure of an Inductor IR, how it models PyTorch faithfully, going from a tensor box to a storage box to a buffer, and then finally to the define-by-run IR that represents an operation in question. That\u0026rsquo;s most of the high-level information you need to know about how to work with Inductor IR. The most common things you have to do is you want to write a new lowering, or perhaps you want to write a new IR node. So let\u0026rsquo;s talk a little bit about some of the more practical nuts and bolts of working with Inductor IR. So one thing that I found pretty confusing about Inductor IR when I first read it is there are a lot of IR nodes. So I talked about the most basic ones, and Horace likes to tell me, well, you know, Inductor IR isn\u0026rsquo;t that complicated. There\u0026rsquo;s only, you know, point-wise introduction that really matter. But actually, if you look at IR, there\u0026rsquo;s lots and lots of other nodes doing all sorts of other things. There\u0026rsquo;s nodes for collectives. There\u0026rsquo;s a node for convolutions. And this is where my warning that, hey, you know, we don\u0026rsquo;t, it\u0026rsquo;s not entirely clean, right? Like, we probably don\u0026rsquo;t need this many IR nodes. The reason why people write IR nodes is because, let\u0026rsquo;s say you have an ATIN operation, and you need to generate some code for it, and none of the pre-existing IR nodes does exactly what it is you need for the code gen in this case. The easy thing to do is to just write a new IR node, you know, write the lowering to that IR node, and then write out all of the code gen that you need to do for that particular IR node. And so this actually is often the path of least resistance, so people added a bunch of IR nodes. But in actuality, it\u0026rsquo;s often the case that you can reuse some pre-existing IR node. For example, we have an IR node that represents calling into an external kernel. It\u0026rsquo;s called extern kernel, predictably speaking. And this IR node does a lot, right? Like, when you pass in inputs to an extern kernel, we have to do a lot of stuff to make sure they all exist as actual tensors, and there\u0026rsquo;s also a lot of logic for actually generating the code gen in the situation. So in a lot of cases, it would have been better if we had generalized extern kernel to work with more things and reused it for a bunch of IR nodes. But we haven\u0026rsquo;t. This is a good refactor if, you know, you\u0026rsquo;re interested in this sort of thing. When you\u0026rsquo;re working with an IR node, there are a bunch of things you can customize. And this is also one of the reasons people define an IR node. For example, the scheduler needs to decide what order to run. So you need to report what the read-write dependencies are. This is something that you can customize on an IR node level basis when you\u0026rsquo;re writing a new IR node. Similarly, we have a concept of side effects, right? If an IR node has a side effect, we\u0026rsquo;re not allowed to dead code eliminate it. So that\u0026rsquo;s also something you can change when you\u0026rsquo;re working with an IR node. One other thing that\u0026rsquo;s really useful when you\u0026rsquo;re working with IR nodes is we do keep track of origins for them. So the original ATEN graph has a bunch of ATEN FX nodes, and we keep track of which ATEN FX node produced a particular IR node. Now, this is not a single one-to-one mapping because when we fuse things together, lots of ATEN operations might go into the same IR node. Conversely, a single ATEN operation might get desugared into multiple IR nodes if it\u0026rsquo;s doing, say, a point-wise operation and then a reduction. But this is really useful, and it\u0026rsquo;s how we generate meaningful kernel names, for example, if you have that enabled in Trident. So, yeah, that\u0026rsquo;s a whirlwind tour to Inductor IR. As I said, it\u0026rsquo;s highly in flux, and I don\u0026rsquo;t claim to be the world expert on Inductor IR, but hopefully that gives you an idea for how to look around this pretty important Inductor IR data structure. Thanks for listening. See you next time.\nEP72 Unsigned-integers Unsigned-integers Hello everyone and welcome to the PyTorch Dev Podcast. Today I want to talk about unsigned integer support that we recently added to PyTorch. PyTorch has supported unsigned integers but only for 8-bit integers. So you can do UNT8 or also known as byte, but you don\u0026rsquo;t get any of the other unsigned integer types, UNT16, UNT32, UNT64. The reason for this is mostly historical. The torch, the TH library that PyTorch was originally built off of, didn\u0026rsquo;t have support for these dtypes. And so we never really added them. Most people could deal with having only the signed integer variants. That being said, it was kind of a pain not having them for several reasons. One is that sometimes you want the little bit of extra range that you get from an unsigned integer, say a 16-bit unsigned integer, that you\u0026rsquo;re losing half of the range with a signed integer if you\u0026rsquo;re only doing it for indexing. And also unsigned integers are great for doing bit manipulation because most of the bitwise operations are well-defined on them, as opposed to signed integers where if you overflow, that\u0026rsquo;s undefined behavior. Who knows what\u0026rsquo;s going to happen? So I finally got fed up with this and on my plane ride back from holidays, I decided to go ahead and implement it. So we now have unsigned integer support in PyTorch. It\u0026rsquo;s a bit restricted. So one of the problems, and probably the reason why TH didn\u0026rsquo;t have unsigned integer support to begin with, is you have to pay a cost whenever you add a new dtype to PyTorch, right? For every kernel that you want to support your particular dtype, you actually need to generate code for it. And so when you add a new dtype to PyTorch, that actually ends up being a lot of extra binary size for all the new kernels you have to add. And, you know, we are already a very, very large binary if you\u0026rsquo;ve ever had to download PyTorch. And, you know, adding some more binary size for some, you know, dtypes that people mostly don\u0026rsquo;t use in deep learning just didn\u0026rsquo;t seem like a good trade-off for us. It gets especially worse when you consider combinatorial explosion of operations. For example, let\u0026rsquo;s say that I want to do a operation between a signed integer and an unsigned integer. Well, if I want to avoid doing a conversion, I have to actually generate a fused kernel for this case to do all the operations together. And sometimes you can\u0026rsquo;t even do the operation conveniently without a fusion. Like if you want to do a comparison, if you want to do a quality test between a signed int64 and an unsigned int64, well, how are you even going to do that? You can\u0026rsquo;t do the conversion because the conversion will overflow. Well, if you\u0026rsquo;re okay with overflow semantics, then I suppose that\u0026rsquo;s fine. That\u0026rsquo;s another question that I\u0026rsquo;ll get you really shortly. So with unsigned integer support in PyTorch, I made a compromise. And the compromise was this, we\u0026rsquo;ll add a few new kernels, you know, a few few extra kernels isn\u0026rsquo;t going to break the camel\u0026rsquo;s back. The main problem is, you know, when you take the entirety of PyTorch\u0026rsquo;s operator space and multiply it by another, you know, three dtypes. So we\u0026rsquo;re going to take only the most important operations, you know, constructions, you know, construction, filling it with some constant, equality, but you know, not addition, not multiplication, not those types of things. And those are the only things we\u0026rsquo;re going to implement. And so essentially, it\u0026rsquo;s enough to like get you a little bit of interoperability with, you know, say numpy, which also supports unsigned integers, but not that much to do anything that useful. And then what we\u0026rsquo;re going to do is we\u0026rsquo;re going to do a twofold strategy. So one is that if you a user come to us, and you\u0026rsquo;re like, hey, I\u0026rsquo;ve got this use case, and I would really like support for unsigned integers, then you know, then you know, like, well, okay, fine. If you ask us for it, then you know, one more, it\u0026rsquo;s kind of like, we\u0026rsquo;ll add things if they actually are useful and used by someone. And we\u0026rsquo;re not going to have them if they\u0026rsquo;re just sort of randomly like, oh, you know, we have integer matrix multiply. So I guess we have to add, you know, 16 bit unsigned integer matrix multiply. No, I probably don\u0026rsquo;t actually want to spend, you know, binary size on that. So if you\u0026rsquo;ve got a good use case for it, then just send in the bug report. And chances are, it\u0026rsquo;s pretty easy, you just have to modify one of the macros that is going ahead and, you know, iterating through all the d types and stamping out versions of the code for each of them, they just go ahead and add the unsigned types to that. And you know, you\u0026rsquo;ll get a chrome for that. So I think I expect to like, accept a trickle of operations like this, slowly through the future. The other strategy we have, and this is not entirely implemented yet, some of it is implemented, but not all of it, is we\u0026rsquo;re going to use PyTorch 2 to implement all the operations. Because hey, you know, what is PyTorch 2? Well, PyTorch 2 lets us do code gen on the fly for integer types. So it doesn\u0026rsquo;t matter that you don\u0026rsquo;t have out of the box, you know, an equality test between n64 and you n64, you can just generate it on the fly. If you torch compile your operation in question. So this is sort of leaning into this idea that in general in PyTorch, you know, PyTorch 2 is this cool thing. It\u0026rsquo;s a compiler. Oh, normally we tell people to use it on their end models, but there\u0026rsquo;s also bits of, you know, the regular PyTorch library that we could implement with PyTorch 2. And a d type like, you know, the unsigned integers from 16 to 64 is a good example where, you know, if we don\u0026rsquo;t want to actually add all of the kernel support in eager mode, we can still, you know, get it cheaply by using torch.compile. Okay, so please send us any contributions you might like. You know, this is the sort of thing where I\u0026rsquo;ve gone ahead and put in the basic infrastructure. So basic testing things work, but you know, everything else doesn\u0026rsquo;t. So if you\u0026rsquo;re willing to roll up your sleeves and make some changes to PyTorch, at the same time you\u0026rsquo;re trying to apply unsigned integers for some sort of use case of yours, I think this is a great way to, you know, do a contribution. In fact, Thomas V. Mann messaged me on Slack and he was like, \u0026ldquo;Hey, you know, I\u0026rsquo;ve got a fix. Do you want to, you know, do you want me to set it in?\u0026rdquo; I was like, \u0026ldquo;Yes, please. Absolutely.\u0026rdquo; Okay, so I want to talk a little bit about a few things to know about the unsigned integer implementation because I thought it was going to be trivial, right? We already support integers, we support sign integers, and we support u and 8. So surely it\u0026rsquo;s just doing the same old thing. Well, not quite. So here are the main things that are problematic. So one, we need to decide what our semantics are regarding signed unsigned overflow situations. So for example, if I have negative one, and I compare this against, you know, hexadecimal 0xFFFFFF, well, you know, on a bit level, this is the same thing. But if you like ask Python, Python\u0026rsquo;s like, \u0026ldquo;Well, no, these are not the same number. One of them is negative, and one of them is a very large number.\u0026rdquo; So we need to decide whether or not we\u0026rsquo;re following C semantics or Python semantics. Actually, before this podcast, I should have checked what numpy semantics here were, but I didn\u0026rsquo;t. So we\u0026rsquo;ll need to check what numpy semantics are, we\u0026rsquo;ll need to check what the existing semantics for uintate and intate are, and then make a call about what exactly we want to do. In particular, I have, we actually have a class in C++ called C10 Scalar, which represents essentially any sort of scalar type that Python is able to represent. And I had a problem while I was implementing this. I was like, \u0026ldquo;Okay, well, I can store signed integers in this, and I can store unsigned integers in this, and I can also store booleans and floats and whatever.\u0026rdquo; And if someone asks me, \u0026ldquo;Hey, you know, what\u0026rsquo;s the equality between these two ints? What should I do?\u0026rdquo; And not, not an easy answer to this question. In the end, I believe I was like, \u0026ldquo;Okay, well, this semantically is representing a Python big int, which can be arbitrary precision. So no, these should not be the same thing.\u0026rdquo; But I\u0026rsquo;m not, I\u0026rsquo;m not convinced that the actual kernel should necessarily operate the same way. Of course, in torch compile, you know, it doesn\u0026rsquo;t really matter. You can get whatever semantics you want, we just need a way of actually spelling it out. And usually there is a way of spelling it out. Some other things. So we have a promotion problem regarding our compatibility with numpy. So let\u0026rsquo;s suppose that you have a uint8 tensor. So this existed in PyTorch before the new support we added, and you do a sum on it. What type does it promote into? Well, you know, the dumb answer is a uint8, which is not correct. It\u0026rsquo;s not what we do. And it\u0026rsquo;s also probably not what you want, because if you\u0026rsquo;re, you know, if you\u0026rsquo;re using these integer tensors, you usually want them to denote actual integers. So you probably don\u0026rsquo;t want them to overflow when you run out. But if you have a big pile of, you know, uint8s, you\u0026rsquo;re definitely going to overflow your 8-bit integer. So we actually promote this to int64. Now, why int64 and not uint64? Well, you know, remember, we didn\u0026rsquo;t have support for uint64. So, you know, producing a uint64 tensor is not possible. So we just gave it the next best type. And, you know, it\u0026rsquo;s not like you\u0026rsquo;re going to really miss that, you know, last half of the range, you know, 2 to the 63. However, this is not compatible with numpy\u0026rsquo;s behavior. When you do a numpy sum on a numpy uint8 nd array, it\u0026rsquo;ll give you a uint64. So we\u0026rsquo;re inconsistent. And if you do ever want to add the sum operations to the higher size ones, uint16, uint32, we have a choice to make, right? We can be consistent with how we currently do it with uint8 and produce an int64 tensor. Or we can be inconsistent, but match numpy semantics and have it be a uint64 tensor. This is especially poignant for the uint64 tensor, which we probably definitely want to be inconsistent with uint8. Because, you know, it would be extremely strange if you summed over a uint64 tensor and you got a int64 tensor. You just lost the entire\u0026hellip; You just\u0026hellip; There\u0026rsquo;s absolutely no reason to do it this way. But this is something we have to figure out. Another thing that\u0026rsquo;s a bit of a pain with PyTorch today is our handling for the very top range of uint64. We have lots of places in the PyTorch codebase where we\u0026rsquo;ve hard-coded int64. For example, when you do a rand int call, the rand int call takes an integer min and an integer max. And those are represented in C++ as int64. Well, you\u0026rsquo;re going to have a hard time actually representing a rand int call on a uint64 dtype that covers the entire range of uint64. Because you just can\u0026rsquo;t. You don\u0026rsquo;t have enough space in your int64. So we need to do something about that as well. I think probably the right call is to add a new overload to rand int that takes in a scaler. Because scalers\u0026hellip; Scalers are this union type. So I\u0026rsquo;ve got a tag and I can say, \u0026ldquo;Oh, this is big. Need to store it in a uint64 instead of an int64.\u0026rdquo; But it\u0026rsquo;s something that we\u0026rsquo;ll have to do. The random number generation doesn\u0026rsquo;t really work with uint64. Buyer beware. Probably your best bet is to generate two uint32s and then, you know, use some bit totaling to cat them together. Finally, one last thing I want to say is that we\u0026rsquo;ve added all of the uint, you know, large uint types. But we\u0026rsquo;re also considering adding some small sub-bite size unsigned integers. So that\u0026rsquo;s uint1 through uint7. So these are kind of strange because they\u0026rsquo;re not byte size. And in fact, we can\u0026rsquo;t implement them in C++ in the traditional way. But remember, we\u0026rsquo;ve got this awesome compiler that we can use to do things. So our plan on record with the sub-bite size unsigned integer types is that we are going to implement them via Python. So the idea is that, hey, you can\u0026rsquo;t actually directly do a uint1 operation typically, but you can reinterpret it as a uint8 tensor and then do whatever byte operations, bit-level operations you need to do to, you know, simulate the operation in question. And sometimes this is not very convenient to do, like, you know, if you want to do addition, the carries are probably kind of a pain. But, you know, you can do it, right? Like, especially because you probably don\u0026rsquo;t actually have int1 hardware. So you\u0026rsquo;re going to have to simulate it by doing a bunch of larger size operations anyway. On CUDA, probably the performance won\u0026rsquo;t even be that bad, assuming you are bandwidth-bound rather than compute-bound. And, you know, there are going to be a bunch of steps to getting all of this working. But we definitely don\u0026rsquo;t expect any good C++ eager mode support. So it\u0026rsquo;s going to all be via PyTorch 2. We do have some people working on this because, you know, sub-byte quantization is very popular. So the quantization team is working on this. That\u0026rsquo;s everything I wanted to say about unsigned integers. See you next time.\nEP73 Inductor\u0026mdash;Define-by-run-IR Inductor\u0026mdash;Define-by-run-IR Hello everyone and welcome to the PyTorch Dev Podcast. Today I want to talk a little more in detail about the defined by run portion of Inductor\u0026rsquo;s IR. What is the defined by run portion of Inductor\u0026rsquo;s IR? Well in our previous podcast episode about Inductor\u0026rsquo;s IR, we talked about the various IR nodes that are explicitly represented as an intermediate representation between A10 operations and when we actually do Triton or C++ code generation. Well the defined by run portion of the IR is the specific part of the IR which is responsible for representing the element-wise operations that you might be interested in doing when you\u0026rsquo;re representing some operation. So the canonical example of when we use defined by run IR in Inductor is for example when we\u0026rsquo;re representing a point-wise operation. So to represent a point-wise operation we get a regular point-wise IR node that represents the entirety of the point-wise operation but then there is an inner function which represents the actual compute that is going to happen inside the point-wise operation. So when you are thinking about where are the data structures for inductors IR, you\u0026rsquo;ll see all the top-level ones have classes that are subclasses of IR node, but then all the little actual compute, all of that stuff is going to be done via this defined by run IR. So how can I go about and read about what exactly this defined by run IR is? Well, if you were asking me a month ago, I\u0026rsquo;d say, well, you kind of have to figure it out by reading the code. Fortunately, I recently added a pull request to Inductor to basically document the entirety of the what I call ops handler inside Inductor. Because the way that the defined by run IR works is that we\u0026rsquo;re constructing functions that are calling other functions, in this case, operations in the ops namespace, which we have the ability to override the meaning of so that we can do different things depending on what we need to do. So just to break it down, in inside Inductor, there\u0026rsquo;s a module called virtualized. What this module does is it defines some dynamically scoped variables, which represent various things you might be interested in querying when you\u0026rsquo;re performing operations in Inductor. And the one we\u0026rsquo;re particularly interested in is a global variable, well, not really global, it\u0026rsquo;s thread local, variable called ops, which represents all of the potential operations you can do inside the defined by run Inductor IR. So ops has a method named add, it has a method named store, load, etc. When we are defining, for example, a pointwise operation, and we want to define the inner function for that pointwise operation, what the inner function is going to look like, it\u0026rsquo;s going to say, well, I\u0026rsquo;m a function. And once you pass me in some indexes, usually these functions are taken indexes as arguments saying, you know, hey, this is where you should actually get information from, what is typically going to happen is you\u0026rsquo;re going to go ahead and, you know, do a load to read out the information in question. And then actually, you know, do what, with the result call, you know, addition or multiplication or whatever, you know, actual operation that you want to do. And this all gets packaged up into the inner function, which gets associated with, for example, a pointwise operation. So when I do something like this, I have the ability to basically change the meaning of what calls to ops means, depending on what I need to do. So the, like, very most simple example of what you might want to do is you might want to turn one of these inner functions into a string representing, well, what is the actual computation that you want to do? When you print out a pointwise IR node, and you get out, you know, hey, the inner fun is this thing, we\u0026rsquo;re actually calling this function inner function string, which is doing this operation. So what exactly does it do? Well, it says, okay, let me go ahead and override the ops handler, the meaning of the ops object in virtualized. So that points to a kernel printing handler, which basically says, okay, well, you know, whenever you call me, what I\u0026rsquo;m going to do is I\u0026rsquo;m going to turn your call into a string representing whatever it is that you call me with, and then, you know, return those strings. And so when you\u0026rsquo;re done, you basically get a, you know, string representation of all the operations that happened in that case. And so everything that you want to do, code generation, semantic analysis, they all operate by overriding the meaning of ops in the virtualized namespace, and then going ahead and running the inner function directly. You can even verify this inner function into a plain FXIR. I mean, it\u0026rsquo;s in fact, in fact, very simple. What you do is you just say, okay, what I\u0026rsquo;m going to do is instead of passing in regular index variables, I\u0026rsquo;m just going to pass in FX proxies. And when I, you know, do, uh, calls on those FX proxies, I\u0026rsquo;m going to instead record, uh, you know, what calls actually happened into an FX graph. So, you know, very simple, uh, you know, very easy to write code very simply, um, writing things in this way is also very convenient in Python because Python supports a lot of metaprogramming. So if you\u0026rsquo;re running one of these operator handlers and you\u0026rsquo;re just like, well, you know, for most things, I have a very generic formula that works for any, because there, there are tons of these operations, right? Like every primitive math operation, actually the way to think about it is for every like torch operator, which we support point-wise compute on, you know, and that includes things like negate and sign. Each of these has a ops definition. Now, sometimes when we\u0026rsquo;re actually doing code gen, we can desugar these into more primitive operations and we often do, but just for ease of sort of wiring everything up, basically everything that is supported in the torch front end gets an ops operator inside of this define by run IR. So there\u0026rsquo;s a lot of these that you have to handle, and people often, you know, don\u0026rsquo;t need to handle them all individually. They can just write a generic get attribute that takes in some list of positional arguments, takes in some list of keyword arguments, and then does the operation on all of these things. So that\u0026rsquo;s pretty nice. So what exactly should I expect to see when I am looking at the supported operations in, you know, ops inside this define by run IR? So as I\u0026rsquo;ve mentioned, there\u0026rsquo;s all of the regular, you know, arithmetic computation that you might be interested in. Those are quite uniform, so I\u0026rsquo;m not going to talk about them too much. There\u0026rsquo;s also operations for reading from memory and storing from memory. So store and load. There\u0026rsquo;s also operations for interacting with randomness. Randomness is directly encoded in the define by run IR, you know, because they require special code generation, typically. And there\u0026rsquo;s also some kind of really unusual things that are also supported in this IR. So, for example, one of the very important things we need to do when we are generating code is we need to compute indexing expressions that say exactly where in memory we want to read from, right? And so the normal situation when you\u0026rsquo;re doing indexing is you get a bunch of SimPy expressions representing, you know, some sort of indexing compute. These are represented as SimPy expressions because we want to be able to simplify these expressions to, you know, basically get rid of because in general it\u0026rsquo;s, you know, going to be very complicated. You know, you need to multiply every index dimension by the stride and, you know, do all of that. But sometimes it can be simplified quite a bit and then, you know, maybe you only need a single index variable at the end. So you typically have these SimPy expressions floating around. But of course, sometimes, you know, we want to do operations which, for example, depend on, you know, a indirect. You want to do some indirect indexing where you have some computation that you did based on tensor data and then you want to actually do that to do an indexing expression. So there\u0026rsquo;s a indirect indexing function which essentially takes a regular, you know, regular value that you computed, you know, regular tensor compute value and then turns it into a SimPy expression so that you can use it in subsequent indexing operations. So this one\u0026rsquo;s very unusual and typically needs special handling because most of the operations inside ops return, I want to say just some tensor value. It\u0026rsquo;s actually not well defined what the ops handler returns because whenever you\u0026rsquo;re doing different analyses, we will override the return value to mean different things, right? If I\u0026rsquo;m formatting my inner function to be a string, these functions are going to take in strings representing the various, you know, inputs and then return a string saying, hey, this is what the output, you know, string format is going to be. And if I\u0026rsquo;m doing some sort of code generation, that\u0026rsquo;s typically what I\u0026rsquo;m typically passing around is not a string, but this thing called a CSE value, which is like a string, but also we\u0026rsquo;re doing some common sub expression elimination while we\u0026rsquo;re at it. But indirect indexing is different, it takes in one of these, you know, unspecified values and produces a SimPy expression. Now, unlike all of the like regular tensor compute point-wise operations, we don\u0026rsquo;t actually override the meaning of SimPy expressions. So SimPy expressions are always done via SimPy. They\u0026rsquo;re always represented explicitly as the SimPy, you know, abstract syntax tree. So, you know, you actually do need to provide a SimPy expression, even if it\u0026rsquo;s just like a bogus one when you\u0026rsquo;re implementing something like indirect indexing. Some of the other unusual operations we support. So, for example, the defined by run IR is also higher order in some cases. For example, the masked operator handles a situation where you are performing, you know, some sort of set of operations, like say some loads in stores, but they may not always be valid. For example, you are doing an indirect load, and sometimes the index is invalid. And in fact, what is happening is that you had some condition which said, hey, should I do the load or not? I can\u0026rsquo;t unconditionally do the load because if I unconditionally do the load, I\u0026rsquo;ll have an illegal memory access. So I need to mask out the load only on the, you know, parallel compute where the index is valid should I actually do the load. So the masked operator lets us do this by simply saying, okay, well, give me a mask saying whether or not the index is valid or not. And then give me some, you know, function like an inner function inside of my defined by run IR, which actually has the stores and loads that I want to have run in a masked fashion. And I actually, I checked the implementation while I was preparing this podcast, and all we do is we just override the meaning of store and load before we go ahead and execute the body of the masked load. So, you know, not only is defined by run IR, you know, like when you are at the top level and you\u0026rsquo;re trying to decide what to do, you override the meaning of operations. But also we can recursively override the meaning within these local scopes to make them do different things. So, you know, the last set of operations that you\u0026rsquo;ll get are some weird, you know, sort of collective style aggregation things. Like if you\u0026rsquo;re doing reductions or scans, we also have operations representing those in the IR because, well, you need a little more juice to actually represent that. We do have dedicated top level IR nodes representing reductions and things like that. And these special operations are typically not valid unless they\u0026rsquo;re run in a context like that. But, you know, they\u0026rsquo;re also something interesting to know about. Peter Bell is the expert on scan, having been the one who implemented it in the first place. So we have talked about the defined by run IR in more detail. We\u0026rsquo;ve talked about the operators inside it and the general way you work with this, namely by overriding virtualized. That\u0026rsquo;s everything I wanted to talk about today. Talk to you next time.\nEP74 PT2-extension-points PT2-extension-points Hello, everyone, and welcome to the PyTorch Developer Podcast. Today, I want to talk about extension points to PyTorch 2. A lot of the work we\u0026rsquo;re doing in PyTorch 2 involves adding new features, which sometimes have implications all over the stack. PyTorch 2\u0026rsquo;s stack has a lot of different layers, and so sometimes planning out a change like this can be quite daunting because it\u0026rsquo;s like, well, to add this feature, I need to understand Dynamo, and I need to understand AOT autograd, and I need to understand Inductor. Most people who work on PyTorch 2 full-time only really work on one layer of the stack at a time, so asking someone to know about all of these things just so that they get a new feature, that\u0026rsquo;s a bit of a lift. Fortunately, there are a number of pre-existing extension points in PyTorch 2 which you can use to implement functionality that otherwise doesn\u0026rsquo;t exist right now. And further than more than that, even we have some things that conceptually make sense but are just not implemented yet, but they could be implemented if someone wanted to go out and do them. So in today\u0026rsquo;s podcast, I want to walk us through some of the extension points in the PyTorch 2 stack and tell you about how these work and how come they\u0026rsquo;re consistent with the overall architecture of PyTorch 2. because one of the main themes about these extension points is the easy-to-implement extensions involve only a change to one part of our stack without changing any of the global invariants throughout our stack. And we have some limited cases where we have a way to customize the behavior of something all the way through, but, you know, that tends to be a lot more work because you have to tell every subsystem how to deal with something in that question. So to get started, let\u0026rsquo;s first quickly look at the topmost layer of the stack, namely Dynamo. Dynamo is all about understanding any given piece of Python code. what exactly is it doing, capturing it into a form that is an fx graph that is well-behaved enough that we can run AOT Autograd on it to trace out an actual set of functions in the end. So if we are thinking about what exactly, you know, we can do in the Dynamo frontend that isn\u0026rsquo;t too difficult to do, one of the most easy and easy-to-understand extensions is just adding support for other function calls. So, you know, what is Dynamo\u0026rsquo;s job in life? Dynamo\u0026rsquo;s job is to look at bytecode, figure out what it\u0026rsquo;s doing, and then put an appropriate function call into the graph so that AOT Autograd handles it. So you can change whether or not something is put into the graph simply by marking something as allowed in graph. Now, there are restrictions. When you mark something as allowed in graph, the function you place in the graph has to, quote-unquote, work with AOT Autograd because what you are saying is that this function is well-behaved enough so that AOT Autograd can trace through it. And so what that means is that, you know, it has to support a fake implementation where you can run it with fake tensors without actually having to have real data. It needs to not have side effects as long, or it can only have side effects in limited situations where it is only allowed to, you know, mutate tensors. And if it mutates a tensor, it needs to be able to tell AOT Autograd that it\u0026rsquo;s doing it in this way. Additionally, the function needs to only support, operate on basic types that are supported by FX. Normally, these are the set of types that are supported by TorchScript. So that\u0026rsquo;s tensor, list of tensors, int, you know, basic primitive types. If you\u0026rsquo;ve got a custom data type and you want that custom data type to be preserved inside of the FX graph that Dynamo is producing, that is much more of a lift. But just putting another function and asking it to be directly traced through, that\u0026rsquo;s something that you can do quite easily inside Dynamo itself. A step up from just putting in a, you know, function for regular tracing is the so-called higher order operators mechanism. We call them higher order operators because typically the reason they exist is because they are operators that take in not just regular arguments, but also arbitrary callables, which themselves tend to contain more graph operations. So typically, a higher order operation with one of these callables will call that callable maybe never or once or twice or whatever. So a canonical example of a higher order operator is the cond operator, which takes in two callables for the, you know, true side and the false side. And, you know, at runtime only executes one of them. These callable, these higher order operators can be pretty restrictive. They are typically not allowed to have side effects. They are typically, the bodies of these functions are typically not allowed to interact with the Python state in any non-trivial way. And when you implement a new higher order operator, you know, one of the things is that most of our basic infrastructure doesn\u0026rsquo;t work on them. So you have to say exactly, for example, how you want all of the AOT autograd passes to work on them. But this is also a sort of well-known extension point. And when people want to add, you know, new operations that are a bit more complicated, usually you use the higher order operator mechanism. So you can extend Dynamo by modifying what it is willing to output to give AOT autograd. You can also extend Dynamo by making changes to how Dynamo processes Python code that is operating over. For example, when you have some code in Dynamo that is calling some API, let\u0026rsquo;s say I have a NumPy call, I can have Dynamo transparently translate this API call into an equivalent Torch function call. And this is the mechanism by which we implemented our NumPy interoperability layer. So if you have some code that does some operations on NumPy end arrays, we actually support transparently compiling this into PyTorch operations. And you can often take a standard NumPy program and automatically get it running on CUDA without any modifications. This is a very, you know, local change because all that\u0026rsquo;s going on is Dynamo is producing a new set of Torch operations where previously it would have just graph-braked on the NumPy operations. So this change only requires you to know about how to deal with Dynamo. Similarly, if you have some sort of custom user library code or, you know, another C extension that you need to interoperate with, one of the things that, you know, you could do is you could add support for it in Dynamo simply by teaching Dynamo what the semantics of these operations are. We actually don\u0026rsquo;t have a public API extension mechanism for doing this right now because we just haven\u0026rsquo;t implemented it yet. But, you know, in principle, Dynamo is unable to handle anything that goes into C extensions or it often also can\u0026rsquo;t handle Python code that is too complicated that uses too many features. But you can always teach Dynamo internally to have a special case for this sort of situation and handle it in some direct way. We actually have had some discussions about what a good API for this might look like. One really promising idea is the concept of polyfills. A polyfill from JavaScript is when you have a implementation of some feature that normally is natively provided by your runtime in plain Python, in this case, JavaScript, in the case of the web. So a polyfill would make sense in Dynamo because if you\u0026rsquo;ve got some code which doesn\u0026rsquo;t work with Dynamo because it\u0026rsquo;s implemented in C, if you write an equivalent implementation of it in Python, then Dynamo can just transparently trace into the Python implementation and understand what your program is doing. So this is a really promising way for letting people who own C libraries and want to interoperate with Dynamo to let things work. And finally, one really interesting possibility that Michael So has been investigating is the possibility for allowing Dynamo to trace non-standard tensor types into the graph entirely. And so this is a good segue into the AOT Autograd segment of this podcast episode because to do this, Dynamo is actually the easy part, right? So to handle an arbitrary class, in our particular case, we wanted to reuse the mechanism from TorchScript called TorchBind, which lets you take arbitrary C++ classes and make them available in TorchScript programs. And all you need to do in Dynamo is just say, okay, well, if I see some operations on one of these TorchScript classes, one of these TorchBind classes, all I need to do is just go ahead and write these operations in the graph. So this is actually the easy part as far as Dynamo is concerned. You just need a way of, once again, dry running these operations without having real data. The real problem is once you have these operations in the graph, what exactly is AOT Autograd going to do with them? So what is exactly AOT Autograd going to do with things? So remember, AOT Autograd is the part of our stack which is responsible for taking the output Python graph that was produced by Dynamo and then actually using all of the semantics, all the layers of PyTorch, including Autograd, including functionalization, all of these things to trace out a low-level Aten representation, which is suitable for handing to the backend compiler. So this is the part that actually knows all the smarts about how all of the various subsystems in traditional Eager PyTorch work. And this is, for example, the place where when you add a new higher-order op, you now have to specify how this higher-order op should interact with each of the various things, like tracing or functionalization or fake tensors, because that\u0026rsquo;s what AOT Autograd is going to use. So if we talk about something like Torchbind, then, you know, if you do add the support for Torchbind, which this one\u0026rsquo;s not complete, you have these weird objects which aren\u0026rsquo;t actual tensor operations. And so if you wanted AOT Autograd to work with them, you\u0026rsquo;d also have to teach AOT Autograd how to either partition them away, which is a very valid thing to do, right? Like before you go from Dynamo to AOT Autograd, you could partition your graph up into multiple pieces and only feed in AOT Autograd the pieces that AOT Autograd actually understands. In fact, this is what we do for DDP Optimize. DDP Optimize is an option you can use when you are running PyTorch 2 with distributed data parallel. And what it does is it manually chunks up our graph so that you get pipelining with DDP where, you know, every chunk starts sending the gradients to the nodes before you finish running everything else. So you\u0026rsquo;re not waiting for all the communications at the very end. And that\u0026rsquo;s done by splitting up the graph before we pass it to AOT Autograd. So you can conceivably get rid of things AOT Autograd can understand by partitioning them into their own subgraphs before AOT Autograd handles them. You can also make AOT Autograd handle things directly. And with higher order ops, you can just specify how exactly, you know, the various layers should happen. Or, for example, Brian Hirsch recently added support for tensor subclasses. So, in fact, tensor subclasses are a really nice extension point in PyTorch 2. And the reason they\u0026rsquo;re so nice is because, you know, well, tensor subclasses act like normal tensors. So they typically don\u0026rsquo;t need that many changes on the Dynamo site. That\u0026rsquo;s not entirely true. For example, we use detensor with tensor subclasses. And that has some extra API on top. And sometimes Dynamo needs to be taught how to understand that API and transfer it into the graph. And once you get to AOT Autograd, the real question is basically how to go ahead and de-sugar this tensor subclass into a simplified program that doesn\u0026rsquo;t have any tensor operations in it. So tensor subclasses maybe require some Dynamo work, have some support for it in AOT Autograd, but it evaporates by the time you get to the backend compiler. So you don\u0026rsquo;t actually need to, you know, work on Inductor if you do something like this. And, of course, AOT Autograd has a bunch of other knobs which you can use. For example, we have decompositions, which are the entire way we, you know, break down operations into simpler forms for the compiler. And you can do pre-autograd decompositions. You can also do post-autograd decompositions. These are all valid things to do, and you can customize them. You can obviously implement custom operators, which are just, you know, just like regular operators that PyTorch has natively. But, you know, if you go ahead and use this API and implement what all the various operations on them should be, you can actually just preserve them all the way to Inductor. And Inductor will just call you when you actually want to run the operation. And finally, once you get to Inductor, there\u0026rsquo;s a few more things you can do. So, for example, at an Inductor level, you can introduce the concept of a new IR node, which lets you control how exactly code generation works when you go ahead and generate the Python code or the C++ code that\u0026rsquo;s going to represent the operation. This is, you know, usually you don\u0026rsquo;t need to because just being able to call some external function is usually good enough, and we have built-in support for that, but, you know, it\u0026rsquo;s something you can do and people have added a lot of IR nodes to Inductor for better or for worse. There\u0026rsquo;s also the ability to take a custom Trident code and send it all the way to Inductor. This is some work by Ogas. It\u0026rsquo;s pretty nice because it\u0026rsquo;s often people are writing these Trident kernels for, you know, the very most important pieces of their model, and it\u0026rsquo;s nice to have that interoperate with PyTurge 2. And Inductor also has some facilities for doing code generation. So, for example, let\u0026rsquo;s say that you are doing matrix multiplies. We have the ability to generate epilogues and fuse them in. And so this capacity basically says, hey, Inductor knows how to generate simple code for point-wise operations. So if you\u0026rsquo;ve got some complicated CUDA kernel and you want to, but you have a spot where you just want to paste in some arbitrary extra code that the user provided, that\u0026rsquo;s something you want to do. We also have some examples of people wanting to go ahead and add first-class concepts to the Inductor IR. For example, when we were working on nested tensor, this is something that, you know, you do need to generate different kernels that are pretty different from normal point-wise kernels when you want to do this generation. This is probably the hardest thing to do because, obviously, to get this concept all the way down to Inductor, you had to have, you know, made Dynamo and AOT Autograd play ball. So definitely a choice of last resort. So we\u0026rsquo;ve talked about a bunch of extension points which the PyTorch 2 stack provide. Some of them have public APIs for and you can use them directly. Some of them are just, you know, ideas that are, you know, architecturally consistent with how PyTorch 2 works, but just haven\u0026rsquo;t been implemented yet. So someone has to, you know, you know, roll their sleeves up and handle things. So that\u0026rsquo;s it for our whirlwind tour of all the things you can extend PyTorch 2 with. Hopefully in some later podcast episodes, we can dig into some of these things in more detail. Thanks for listening.\nEP75 Compiled-autograd Compiled-autograd Hello, everyone, and welcome to the PyTorch Dev Podcast. Today, I want to talk about Compiled Autograd, a feature implemented by Jason Ansell that allows us to compile the entirety of a backward pass in a PyTorch program in the same way PyTorch 2 normally is able to compile pieces of your forward. To understand why Compiled Autograd is its own thing and isn\u0026rsquo;t related to the regular strategy for handling forwards and backwards in PyTorch 2, we first have to briefly go over how does automatic differentiation work in PyTorch 2 normally? If you\u0026rsquo;ve listened to the AOT Autograd podcast, you may know that the way PyTorch 2 works is that after we capture the forward steps only of a graph, we pass this along to AOT Autograd, which is responsible for tracing out a joint forward-backwards graph that gets partitioned into a forward and a backwards that then get compiled separately and assembled together into a custom Autograd function, which is what\u0026rsquo;s actually responsible for hooking up to the Autograd engine. Now, it\u0026rsquo;s worth remembering that a custom Autograd function has a property, which is that when you run it, you run the forwards, and then later when you actually call backward, you are still running the normal Autograd engine that PyTorch has, and the normal Autograd engine is eventually going to call a big chunky compiled backward, once again, from your compiled Autograd function to actually run your thing instead. So we\u0026rsquo;re just using the good old-fashioned custom Autograd function functionality that Autograd already has, but in this case, we\u0026rsquo;re generating compiled regions for the forward and backward in this case. So this is how PyTorch 2 normally works. This is a really nice model for PyTorch 2 because it means we can transparently work with graph breaks. Remember, PyTorch 2 is all about if you can\u0026rsquo;t compile the entirety of your model, that\u0026rsquo;s fine. You can just compile the parts of it that Dynamo understands and fall back to eager mode. And so if you have a compiled Autograd function, this just smoothly works in this situation. But sometimes this is not what you want. For example, let\u0026rsquo;s suppose what if you wanted to fuse the forward and backwards of your program all in one go? So you wanted a single compiled region that handled everything in this case. Well, you don\u0026rsquo;t want to produce a custom Autograd function because a custom Autograd function requires you to always have that gap where after you\u0026rsquo;re done with the forwards, you have to go back to regular PyTorch and call the backward function to actually kick off the Autograd engine that\u0026rsquo;ll eventually call your backwards. So to actually do it all in one go, we need something else. We need something that actually can understand the backwards pass in the same level of fidelity as we are able to understand forward passes. So before we try to tackle this more complicated goal, compiled Autograd tackles a simpler version of this, which is can we just go ahead and compile the dot backward call that you have in your PyTorch program all in one go? And this is a little different than imagine that you have just a single compiled region and you immediately call backward after it. And the reason it\u0026rsquo;s different is because when you call backwards on a compiled Autograd function in the traditional PyTorch 2 sense, there are still things that we\u0026rsquo;re not able to fuse into the graph. For example, when we are done calculating the gradients of all of our parameters that were used by the forwards pass of our function, we still have to go ahead and put these into the grad fields of all of the parameters lying around. And if those grad fields actually already have tensors, we may actually have to do additions to sort of combine the gradients because this is something that PyTorch supports. You can run backward multiple times with the same parameters and we will just accumulate the gradients into them. So this addition necessarily must happen, or this gradient accumulation necessarily must happen outside of the graph because there may be outside of the compiled graph, because there may be other uses of the parameter that are not part of the compiled region, and we\u0026rsquo;re not supposed to actually go ahead and do the accumulation until we\u0026rsquo;ve gathered up everything and want to actually put it in. Because, you know, like if I\u0026rsquo;ve just finished a compiled Autograd function, I have a gradient for this parameter, but I don\u0026rsquo;t know if that\u0026rsquo;s the entire gradient for that function because if I\u0026rsquo;ve used this parameter somewhere else in a non-compiled region, that also cancels the usage that will also have a gradient. And really only the Autograd engine knows in this case. So how are we actually going to go ahead and do this? Well, another problem that is actually instructive to think about on how we\u0026rsquo;re going to do this is we also have another feature in PyTorch, which is backward hooks. Backward hooks show up in a number of situations. For example, if you write a custom Autograd function, you know, yourself. So instead of, you know, us using custom Autograd to, you know, feed in some compiled backwards, you know, you can write one of these yourself in case you\u0026rsquo;re writing some function that isn\u0026rsquo;t normally differentiable by Autograd and you want to manually specify what the backwards is. Well, these backward functions can have arbitrary Python code in them. And so while we support natively tracing custom Autograd functions that are not too complicated, if they\u0026rsquo;re really complicated, then we need something else to handle this case. Similarly, we support just directly specifying backward hooks on variables, which are just arbitrary functions, which we will call when the gradient for some function is computed. And when this occurs, we will just call into your function and you can do whatever you want. And things like DDP and FSTP often are implemented because they need to do special behavior in the backwards by implementing some sort of complicated backwards function that often is actually, you know, interacting with Python state, updating things, you know, triggering collectives, fairly complicated stuff. And most our standard playbook for handling backwards functions in PyTorch 2 doesn\u0026rsquo;t work because when we are doing the backwards handling, we\u0026rsquo;re out of Dynamo, we\u0026rsquo;re an AOT Autograd. AOT Autograd is all about doing a make effects trace of the graph and trace question. Make effects traces cannot in general deal with arbitrary Python, right? That\u0026rsquo;s what Dynamo\u0026rsquo;s job is for. So you have to only have traceable code. So if you have traceable hooks, traceable backwards, then you can, you know, sort of do everything in the traditional framework without dealing with trouble. But if you have like Python code that is like updating a global variable somewhere, you need Dynamo to do that. You cannot do that with make effects. But that gives us, you know, an idea for how we might want to go about handling this, right? If I want to somehow compile Autograd, then what I need to do is I need to somehow get my Autograd step, my backwards pass in a form so that I can just run Dino on it, right? Like if I could somehow replace the backward call with a big pile of Python functions called one after another, calling into various backward functions, then this would give me all the juice I need to, for example, handle some arbitrary Python code, or for example, to handle a fusion with AccumulateGrad because they\u0026rsquo;d all be there. They\u0026rsquo;d all be the stuff we need. But how are we going to do this? Well, let\u0026rsquo;s think about how Autograd works in PyTorch normally. In PyTorch eager mode to do Autograd, we construct an Autograd graph with every operation we do. And this Autograd graph represents the backwards computation that I need to do. So when I call backward, I traverse this graph that I\u0026rsquo;ve created while I was doing my forward operations, and that specifies the sequence of operations I need to do in order to run backwards. So in principle, this seems like something that I should be able to turn into some sort of Python code so that Dynamo can do it, right? I just need to take this graph, somehow do something to it so that I get some Python code. And then once I have that Python code, I can just go ahead and Dynamo it recursively in the normal way. And this is basically what Compile Autograd does. So if you are sort of nodding off at this point, and this is really the main idea, right? The main idea is take the Autograd graph, turn it into a Python code, and then Dynamo through it. And all of the things you expect work. In fact, graph breaks work, right? Because if you have some regular Python code, then if I have a graph break in the middle of it, I just say, okay, well, I need to just call into this thing that, you know, is doing something complicated. And then I can go on and keep compiling everything else. The main thing to know about this strategy is that if we take the entire Autograd graph and turn it into a Python program, which we are then going to trace, then this compiled Autograd region only works only if you have exactly the same Autograd graph that you had originally, right? Because if I, the next iteration around, have a different graph, perhaps depending on different values, then I will end up with a different Python, you know, unrolling of the Autograd graph. And obviously, you know, in regular PyTorch Eager, if I have two different functions, then I have to compile them separately. And so that would be the case here. So you have to make sure you actually do the same backwards every time. But in general, this is not a big problem for people who are working with relatively static computation patterns. Compile Autograd is a bad idea if you\u0026rsquo;re actually relying on Autograd to take care of sort of some sort of dynamic Autograd structure. So just something to know about if you\u0026rsquo;re trying to turn on Compile Autograd. Okay, let\u0026rsquo;s dig a little bit more into how exactly we do this confusion, because there are a conversion, because there are some things that are a little tricky about it, and are worth knowing if you actually need to dig in and work with the code in question. Intuitively, we have an Autograd graph. And so what we would like to do is we would like to traverse over the graph, you know, node by node, and go ahead and convert each of these Autograd function nodes into a corresponding Python code for the function node. So it turns out there are a few immediate problems you run into when you\u0026rsquo;re doing this. The first problem is that a Autograd function node, namely, you know, one of these things that says how exactly we\u0026rsquo;re going to call for backwards, does not correspond to a callable or really anything that I could put into the corresponding PyTorch graph. And one of the reasons for this is how we implemented Autograd in eager mode. We have this thing called derivatives.yaml, where for any given forward function, and these forward functions are regular old operators, and you can refer to them via functions in the Torch namespace, or really Torch.ops.810, if you want to be really technical about it. These derivatives.yaml derivatives are, you know, directly allowed to be specified inside with, as mathematical formulas, without having to write another operator for it. So no, you\u0026rsquo;re not going to find foo underscore backward for arbitrary functions. Sometimes we have a underscore backward function, because something is really complicated. But most of the mathematical formulas are just, you know, doing a few operations together. What this means is that a typical backwards formula is anonymous. There is no backward op that I can call when I want to actually put it into the graph. So what do we do to handle this case in compiled Autograd? Well, simple. If we can\u0026rsquo;t directly put the entire Autograd function node in the graph, let\u0026rsquo;s just go ahead and trace it. And tracing is okay, because backward functions implementations in the core library are typically very regular. They are traceable. In the same way, composites are typically traceable. So we go ahead and we take the Autograd function in question. We trace it using make effects into some actual sequence of A10 operations. And that\u0026rsquo;s what actually gets put into the on compiled Autograd graph. So one thing to know is that if you\u0026rsquo;re looking at the output of compiled Autograd, namely, what exactly is the Python program, the FX graph, a produce that I\u0026rsquo;m about to process with Dynamo? You may see a lot of A10 calls in it, but don\u0026rsquo;t be deceived. Just because there\u0026rsquo;s a bunch of A10 calls in it doesn\u0026rsquo;t mean that you\u0026rsquo;ve actually run AOT Autograd. You, in fact, haven\u0026rsquo;t. You are going to then remake effects it later once Dynamo has finished processing everything. Because remember, Dynamo is going to end up, you know, sometimes going into Python hooks. Those Python hooks are going to result in Torch function calls. And those do need to get decomposed. And the way you decompose them is by calling make effects, aka AOT Autograd. This is one important thing to know, right? So we\u0026rsquo;re doing this tracing step to actually get out a graph representation. Another thing that is important to know is that when we want to handle dynamic shapes, or we want to handle sort of variation in the backwards graph, we can\u0026rsquo;t actually trace out the compute exactly as is. In particular, let\u0026rsquo;s suppose that we have some variable stored inside of the Autograd function node, which was saved for backwards. And this is very common, because a lot of backward formulas need to reference the original forward arguments to actually express the mathematical derivative. Well, we don\u0026rsquo;t want to hard code that exact tensor the next time around, because the next time around, I will get another Autograd graph, it will have exactly the same structure, but all the same variables are going to be different, because hopefully, you know, your forward pass actually computed different values the next time you run backwards in this case. So to actually handle this, we need to actually make sure that our Autograd, our produced Python fx thing that we\u0026rsquo;re going to go ahead and dynamo later is parametric over the saved tensors, and in the case of dynamic shapes, also the saved integers. So we need some way of actually swapping out the Autograd function with a new one that\u0026rsquo;s generalized with, you know, our faked parameterized versions of all these things. Now, one obvious way to do this is to just go ahead and clone the Autograd function into a new one with, you know, all the parameters replaced with their parametric generic equivalents. But this turned out to be like annoying to do for various reasons, for example, because we don\u0026rsquo;t actually have a, you know, on polymorphic API for working with these data classes. And, you know, it\u0026rsquo;s C++, and everything has different fields, which is kind of a pain. So the way Jason decided to do it is instead we have a few functions for mutating the Autograd function node in question. So the basic model is that you can sort of save a bunch of new values into the record in question. So you overwrite them with your placeholders, you go ahead and do an operation, and then you restore it back to the original value. This is not very thread safe, but because the Autograd engine always takes out a lock when we\u0026rsquo;re running an Autograd engine, we don\u0026rsquo;t really allow multiple concurrent copies of the Autograd engine to run at the same time. You can actually observe this mutation. Okay, so that\u0026rsquo;s the, you know, how the sausage is made. And it\u0026rsquo;s important to emphasize how important compiled Autograd is, right? So it doesn\u0026rsquo;t seem like much. It\u0026rsquo;s just saying, hey, you can compile the entirety of the backward call in PyTorch 2. But we actually, we don\u0026rsquo;t turn this on by default. It\u0026rsquo;s a context manager that you have to explicitly opt into. And, you know, there\u0026rsquo;s still kind of bugs, because Autograd is complicated. And, you know, like getting exact parity for this conversion process is not an easy thing to do. But we are very committed to making compiled Autograd work, because it is an essential ingredient for any sort of non-trivial distributed compute that you might want to do on PyTorch 2. So our long-term plans for dealing with compiled DDP, as well as compiled FSDP, all rely on compiled Autograd so that we can actually handle their complicated backwards behavior that cannot just directly be traced with AOT Autograd. So if you\u0026rsquo;re looking for someone to go bug about what\u0026rsquo;s going on with compiled Autograd, Simon Phan has been working on improving coverage and enabling compiled Autograd on our benchmark suite by default. is a very nice update post that he made in earlier February that I\u0026rsquo;ve linked inside of my PyTorch 2 state of posts on DevDiscuss, which I recommend checking out. Another interesting work stream that\u0026rsquo;s going on at the same time is Jack Cao from Google has been working on that original goal I told you about, which is, can we go ahead and compile forward and backward all in one go? So the PyTorch XLA integration is very interested in this because it\u0026rsquo;s very expensive in XLA to do this, you know, swapping out of a graph back to Python and then back in again. And so it\u0026rsquo;s a lot more expensive than in, you know, regular PyTorch CUDA. And so what is he\u0026rsquo;s doing is he\u0026rsquo;s saying, okay, well, compile Autograd is this thing that lets you go ahead and take a single backwards call and turn it into a Python graph that you could then can go ahead and compile with Dynamo. Let me take that and embed it within a broader Torch compile call, which goes ahead and, you know, runs the forward and then go straight into running the backward without doing a graph break at all. This is very interesting, there are a lot of technical challenges going on here, but maybe we will talk about them some other time. That\u0026rsquo;s everything I wanted to say about compiled Autograd today. Talk to you next time.\nEP76 Tensor-subclasses-and-PT2 Tensor-subclasses-and-PT2 Hello, everyone, and welcome to the PyTorch Dev Podcast. Today, I want to talk about tensor subclass support in PyTorch 2. Before we talk about tensor subclass support in PyTorch 2 specifically, let\u0026rsquo;s do a brief refresher on what exactly tensor subclasses are. Tensor subclasses are a way to extend the functionality of the built-in tensor type in PyTorch entirely in Python without having to, for example, write a complicated C++ extension. There are a number of ways you can write tensor subclasses, but the one we are particularly interested in today is the Torch Dispatch mechanism, which allows you to write a magic method called Torch Dispatch that handles all calls to the low level of A10 operators that are processed on a tensor after all of PyTorch\u0026rsquo;s other subsystems have happened. So notably, Torch Dispatch happens under autograd. So this is a pretty nice piece of capability, and we\u0026rsquo;ve used it to build a lot of things. If you\u0026rsquo;re just thinking about PyTorch 2 itself, Torch Dispatch is actually the key mechanism which we use to implement proxy tensor tracing. But when we look more at user space, people innovating with different and interesting new tensor data types, we actually have a number of different examples using Torch Dispatch. For example, D-Tensor is a tensor that has a built-in knowledge about how its data is distributed over multiple nodes. That\u0026rsquo;s something implemented with Torch Dispatch. We\u0026rsquo;re prototyping Float 8 support in PyTorch using a tensor subclass because unlike other D-types, Float 8 is very strange. You need a scaling factor. There\u0026rsquo;s a lot of uncertainty about how exactly to put everything together. So we\u0026rsquo;re prototyping Float 8 tensor inside Python. There\u0026rsquo;s also nested jagged tensor, which is a non-uniform layout where you have a variable sequence dimension, which we\u0026rsquo;ve packed all together instead of padding them all out to get you a regular dense tensor. And there\u0026rsquo;s also a fun example in the subclass Zoo of re-implementing complex tensors, which we actually do have a traditional C++ implementation using a tensor subclass. It\u0026rsquo;s actually a little different, though, because the tensor subclass version of complex tensor keeps the real and the complex numbers in separate tensors instead of interleaving them together, which is what the C++ version does. The separate representation is actually better for a number of use cases, most notably matrix multiply, where there is no built-in complex matrix multiply inside your NVIDIA hardware. But if you actually do them separately, you can use the regular special matrix multiply instructions that are available. So subclasses, basically, they\u0026rsquo;re not really for typical end users, but if you\u0026rsquo;re a power user or you\u0026rsquo;re a developer on the PyTorch team, it\u0026rsquo;s a really nice way to go ahead and develop a new feature without having to muck around with C++ in Python. So how do subclasses work with PyTorch 2? Well, there are a number of things that we did to make this work, but let me first describe the overall implementation strategy. So the most important thing about how we\u0026rsquo;ve implemented subclasses in PyTorch 2 is fundamentally it is a desugaring, right? A tensor subclass is interposing at a very interesting point in the stack below Autograd, but before, you know, the actual tensor compute. But at the end of the day, a tensor subclass is just calling a bunch of other Torch operations. So intuitively, it makes sense that if I have a program that\u0026rsquo;s using tensor subclasses, in some sense, I could have manually written a big pile of Torch operations under the hood that does an equivalent thing, right? Instead of doing detensor, I could have manually written out, you know, the forwards and backwards of my program with manual collective calls put in. This would be very, very annoying and very, very, you know, like very tedious to do by hand. And so that\u0026rsquo;s the value out of doing it at the subclass. But if we have a compiler, we can just ask the compiler to do it for us, desugar it into these basic parts. And then hooray, you know, now we\u0026rsquo;ve got a simple representation of good old fashioned A10 operators that we can just pass to Inductor to compile without any special support. So in fact, tensor subclass support in PyTorch 2 was implemented primarily in AOT Autograd. And this is all thanks to the works of Brian Hirsch, who is our, you know, AOT Autograd maintainer right now. So let\u0026rsquo;s talk a little bit more about how exactly this works, right? So one of the things that we need to be able to do is if we have a tensor subclass input to our program, we need some way to flatten it into its constituent, you know, regular tensors, which, you know, then you can do your regular operations on. So when you write a tensor subclass that you want PyTorch 2 to support for, you need to write a torch flatten, sorry, a tensor flatten magic method, which says how to go about doing this flattening process. Similarly, we need a way to unflatten tensors, right? If we are done computing, and there are some subclass tensors that are flowing out of our graph, we need to be able to reassemble them into good old fashioned subclass tensors, because that is what the surrounding ear code is going to expect to see. And so this flattened, unflattened process typically produces some extra metadata, which is whatever extra stuff that a tensor subclass needs to sort of know what it\u0026rsquo;s doing. Like a D tensor is going to, you know, have metadata talking about, you know, if it\u0026rsquo;s sharded or replicated, whereas simple, you know, subclasses like complex tensors don\u0026rsquo;t need any extra metadata, right? They\u0026rsquo;re uniquely determined by their real and imaginary components. One other thing is because tensor subclasses are implemented in AOT Autograd, our plan is going to be, we\u0026rsquo;re going to trace through them, right? We\u0026rsquo;re going to trace through the subclass. The subclass internally is going to pull up a bunch of torch functions, and those torch functions eventually get traced using proxy tensors. So this means that your tensor subclass has to be traceable in some sense. It has to be okay to like just run it with, you know, fake tensors, look at what operations happen, and then have those be exactly the operations that show up in the very end. So, you know, no dynamic control flow or anything like that. This is different from torch function, which is the other way you could have implemented tensor subclasses, where torch function is a much more superficial thing that just lets you modify, you know, what happens right after you call a, you know, torch function on some sort of subclass. That can be handled directly in Dynamo. Torch to spec cannot be conveniently handled in Dynamo. So it is handled in AOT Autograd, you know, after we\u0026rsquo;ve done all of the Autograd-y stuff, which is also what AOT Autograd is doing. So that\u0026rsquo;s basically all you really need to know about, you know, tensor subclasses in PyTorch 2, right? Like it is a desugaring process, AOT Autograd does it. It is kind of a complicated implementation, and there are still lots of bugs that, you know, we\u0026rsquo;ve been working out, especially on the D tensor front. But hey, it works. And, you know, it is a really useful thing to be able to do for tensor subclasses, especially because torch dispatch tensor subclasses are not very fast. In fact, the overhead is pretty bad if you actually care about eager overhead. So like the glory of PyTorch 2 is you can write your tensor subclasses and then it\u0026rsquo;ll run actually really fast because you have the full weight of compilation behind you. All right. What are some other things that I want to tell you about? Well, one thing I want to say is there are a few extra details about tensor subclasses that, you know, are good to know that they are something that you have to deal with. One thing is that, you know, if you have a simple tensor subclass like complex tensors, they\u0026rsquo;re pretty easy to deal with because they don\u0026rsquo;t define any extra operations, right? What you can do with a complex tensor is just all the good old fashioned things that you can do with variable tensors. But if you have a detensor, there are some other operations that you might need to be able to do, right? Like, you know, moving things to replicated stuff like that. The preferred way to go about doing this sort of thing is to write custom operators that represent these things that you want to do. But sometimes you might need to take in some special data types. For example, detensor needs to take in a device mesh on some of its operations. So right now, we don\u0026rsquo;t really have a good way of adding extra support for like sort of primitive types that are allowed in your IR. So detensor has a bunch of hacks in Dynamo to get around this. But this is something that we do want to improve, especially because, and this was the, you know, subject of composability this week, especially because we do want to support, you know, exporting pre-dispatch IR. Pre-dispatch IR necessarily contains subclasses in it because, you know, pre-dispatch IR is before autograd. And if we haven\u0026rsquo;t processed autograd, we definitely can\u0026rsquo;t process subclasses, which are after autograd. So, you know, you need to make sure it\u0026rsquo;s still pretty normalized. You have regular operators for all the stuff you want to do on the subclass directly because in pre-dispatch export, you do need to export it. Another thing that\u0026rsquo;s kind of complicated with subclasses is views. Now, what do I mean by views? Well, let\u0026rsquo;s say that I have a dense tensor and then I want to construct a tensor subclass from that dense tensor. For example, I have a real and a matched tensor and I want to put them together in a complex tensor. Or let\u0026rsquo;s say that I have a tensor that represents packed data and then I want to wrap it into a nested jagged tensor. So ideally, this wrapping operation is a view, right? Like I share the storage between the original and what\u0026rsquo;s after. But now that actually, you know, makes things kind of difficult, right? Because now if we want to do autograd, we need to know how to see through these views because, you know, I do want gradients to flow between this transition. If I do mutation, I need to be able to functionalize in this case. And, you know, I need to be able to reconstruct these views when I am fake-ifying these tensors inside PyTorch 2. So Joel Slosher has been working on basically reconstructing views on subclasses, primarily motivated by the nested tensor case. But this infrastructure should be useful in a lot of other situations as well. One other thing that like has shown up in this case that\u0026rsquo;s kind of difficult to deal with is dynamic shapes. What\u0026rsquo;s difficult to deal about with dynamic shapes with tensor subclasses is with a regular tensor, we can just look at its size, and those are all the sizes that potentially can be dynamic. With a tensor subclass, there may be inner tensors which are dynamic. This actually shows up with nested tensors, for example. So for a nested jagged tensor, you\u0026rsquo;ve got some size at the top level, which sort of represents, you know, how many batch elements you have, you know, what the embedding size is, and it says, well, there\u0026rsquo;s some unspecified, you know, jagged dimension. And you can\u0026rsquo;t really express what the jagged dimension is with just an integer. But on the inside, inside the actual values tensor that contains the packed sequences, that does have a length, right? In fact, typically, that\u0026rsquo;s going to be some sort of dynamic size, which will vary. And so that dynamic size is not determined by the shape of the outside tensor. So, you know, this is the sort of thing that you\u0026rsquo;ve got to deal with. There\u0026rsquo;s also some funny interactions with views, because a view can have an integer in it, and you need to make a dynamic, oh, you know, lots of stuff going on. So that\u0026rsquo;s Joel\u0026rsquo;s neck of the woods. One final thing that is pretty interesting is tensor subclasses are more likely to run into one of our old sort of limitations in AOT Autograd. And this limitation is that when we do AOT Autograd, we are actually compiling the backward ahead of time before we even know what exactly the user is going to pass us for backwards. And this means we have to make assumptions, right? We don\u0026rsquo;t know what the tensor the user is going to give us in backwards is going to be. So we typically assume that it is contiguous. And if you give me a non-contiguous tensor, I just slap a contiguous call on it to make it the same. Well, with tensor subclasses, the situation can be even more complicated. Like, let\u0026rsquo;s say that I have a D tensor. And so, you know, when I produce an output D tensor, I have it at some replication. But, you know, when my D tensor comes back in gradients, in general, I don\u0026rsquo;t necessarily want the same replication pattern in forwards than backwards. So, you know, there can be a mismatch in this case. So Brian was working on this bug. And what he did was there are some more magic methods for basically testing if, you know, the metadata agrees and coercing to, you know, a standard metadata if they don\u0026rsquo;t. So, you know, that\u0026rsquo;s something you might need in some situations. A better solution that would solve this once and for all is if we actually lazily compile backwards, waiting until we actually know what exactly the input tensor is before actually committing to some particular thing. This would also improve performance with regular dense tensors because we would no longer have to call a contiguous call before you get some output. Okay. So that\u0026rsquo;s tensor subclasses in PyTorch 2. There are no docs about how to do this yet. So, like, you know, if you really want docs soon, you should go bug Brian about it. But, you know, I\u0026rsquo;m pretty excited about tensor subclasses because they are really driving a lot of the, like, really key new features that are going on in PyTorch these days. So we don\u0026rsquo;t write C++ subclasses these days. Most of the new development is going on in Python subclasses. Okay. That\u0026rsquo;s everything I wanted to talk about today. See you next time.\nEP77 AOTInductor AOTInductor Hello, everyone, and welcome to the PyTorchDev podcast. Today, I want to talk about AOT Inductor. I thought a bit about how to structure this podcast. Obviously, I can talk about what AOT Inductor is and how it\u0026rsquo;s implemented. But actually, I think it\u0026rsquo;s important to actually split this podcast into two parts. And the first is to sort of talk about what are the design goals of AOT Inductor? Because actually, there are a lot of things AOT Inductor, short for ahead of time could mean. And it gets a little confusing. And I often see this on GitHub issues and forum posts about AOT Inductor. People are like, oh, does it do X? And I\u0026rsquo;m like, well, no, it doesn\u0026rsquo;t really do that, because that\u0026rsquo;s not its design goal. But at the same time, and this is part two, is there is, you know, some common technical themes behind it. And so while AOT Inductor is specifically targeted at one particular use case, there are a lot of useful pieces that make up it and could be used in other contexts, where actually, you know, they do do the things that you might want them to do. So it\u0026rsquo;s important to, you know, understand what AOT Inductor is, as it is. So you know, what do you expect to get if you are looking to use it? And it\u0026rsquo;s also important to know, hey, there\u0026rsquo;s a bunch of stuff here, it doesn\u0026rsquo;t have to be used in this particular way, it can be used in other ways. And, you know, I feel a little bad these days, because I saw someone comment one day, I don\u0026rsquo;t know if on Twitter or Reddit being like, hey, you know, the PyTorch guys, they\u0026rsquo;re all working on internal stuff. And I was like, well, that\u0026rsquo;s kind of true. Like, you know, there\u0026rsquo;s a lot of stuff that we\u0026rsquo;re working on. And, you know, it can be plausibly used by people in the community. But, you know, a lot of it is being driven by well, you know, there\u0026rsquo;s this thing that we want to use. And so we\u0026rsquo;re working very hard on it. And so I apologize, because, you know, PyTorch would not be where it is without all our open source users. And, you know, I do feel very much embarrassed when you know, we\u0026rsquo;re not necessarily doing the things that you guys exactly want us to do. I think on the flip side, though, I do think like, fundamentally, we are working on a lot of core infrastructure that can be used in a lot of situations. And you know, one of the reasons why I do this podcast is to help, you know, the engagement of the open source community that wants to help us work on things in PyTorch. Because if you do decide, hey, you know, this thing makes sense. And you know, I say something in my podcast, and I\u0026rsquo;m like, yeah, you know, conceptually, this makes sense. We\u0026rsquo;re just sort of not working on it right now. That\u0026rsquo;s an opportunity. That\u0026rsquo;s a situation when you actually can actually come and, you know, contribute something to the project and, you know, get your use case going in a way because, you know, we are all about open source. We\u0026rsquo;re all about building things that can work in a lot of different situations. So let\u0026rsquo;s talk about AOT Inductor. So what are the design goals of AOT Inductor? So the main primary design goal of AOT Inductor is we want to produce some sort of export format for a PyTorch inference program that can be represented as a self-contained distributable executable, specifically a dynamic library, which you can load up in some other situation. And this dynamic library has no dependency on the PyTorch runtime. Now, does this sound like a kind of strange thing to want? Well, maybe. So, you know, like with the advent of frameworks like GGML, right, people very much like having a bunch of source code, which, you know, represents the model in question. And then you can go ahead and hack around it, embed it in whatever situation you want. So the reason for the binary distribution format is actually because we have this, we have this different production requirement, which is that we want to deploy models. And we wanted these models to be deployed in some format, which doesn\u0026rsquo;t require us to rebuild them. When the service, the service that\u0026rsquo;s actually, you know, deploying, serving the model changes over time. And so we need to be able to allow some sort of model runtime SKU where, you know, we can be upgrading the runtime and old models still work. And at the same time, we need a thing that, you know, is fast and we don\u0026rsquo;t have to actually have some sort of, you know, recompilation process just when you want to load the model. We want to be ready to go as is when you want to start, because, you know, a lot of these services, you know, when you start them up, we don\u0026rsquo;t want to wait a long time to warm up in this situation. So a self-contained dynamic library is perfect for this sort of situation, as long as you have some sort of stable ABI on it, right? Because what you do then is you say, okay, I have a minimal stable ABI that this this library depends on, I\u0026rsquo;m going to keep that stable across releases on my runtime system that is actually loading this. And now I can update my runtime. And these, these binaries, you know, keep only depend on the stable ABI, which keeps working. So I can keep using these binaries without having to regenerate them. And, you know, assuming that I have some sort of freshness requirement, I\u0026rsquo;ll probably eventually regenerate the model at some later time, but I\u0026rsquo;m not forced to regenerate the model whenever I want to upgrade my runtime. So and you know, you wouldn\u0026rsquo;t get this with a, you know, text format, because the binary format here already has all the Trident kernels compiled into, you know, CUDA code, sorry, not CUDA code, but you know, your actual, your actual bit code that can run on your GPUs. So you know, it\u0026rsquo;s very fast, you load up and it\u0026rsquo;s ready to go, as is. And also, when you have some sort of text format, like think TorchScript style, that\u0026rsquo;s a lot bigger backwards compatibility surface, because you have to worry about, you know, oh, no, like, what if I change the serialization format? What if I, you know, add remove operators, you know, you don\u0026rsquo;t really have to worry about that. If you have binary, you just have to worry about the ABI it works with. And that\u0026rsquo;s it. I did say operators, operators actually are kind of part of the BC service, but we\u0026rsquo;ll talk a little bit more about that in a moment. So that\u0026rsquo;s the like, motivating concern, right, is that we want to be able deploy these optimized models. And we have this skew problem between the runtime and the actual model itself. So if we talk about performance on AOT inductor, we do kind of care about performance, but sort of in the generalized sense, in the sense that, you know, Torch compile cares about performance, right? Most of the optimization juice, we\u0026rsquo;re getting an AOT inductor is just from the regular Trident, you know, code generation and optimization that we do, even if you\u0026rsquo;re running PyTorch 2 in eager mode. So CUDA is all about, you know, like when you do these CUDA models, it\u0026rsquo;s all about the portability and head of timeless, as opposed to, you know, having some extra optimizations on top. That being said, we also care about the CPU performance of these AOT inductor lobs. And while cogen does also matter in this case, we also care a bit about overhead reduction in this regime, because, you know, CPU models are way more likely to be compute bound. And you, you know, really don\u0026rsquo;t want to be spending a lot of time doing useless reference counting, and that sort of thing. CPU models tend to be very small in our regime. So, you know, the overhead really shows up in this case. So, you know, what, what, what other, you know, reasonable things might you want to do that AOT inductor doesn\u0026rsquo;t do? So one reasonable thing you might want to do is you might want to do training, right? Like training is also a situation where, you know, being able to compile something ahead of time, and then use it reliably automatically on all your nodes, without having to recompile every time, you know, that\u0026rsquo;s a really good use case. And AOT inductor is not there yet. In principle, it could be used for training. But it\u0026rsquo;s just, it\u0026rsquo;s a bit more difficult to get this set up, because there\u0026rsquo;s a lot more stuff you need to, you know, actually get training going, right? You need your data loading, you know, you need to actually have the loss function. You know, if you\u0026rsquo;re doing distributed training, you need some sort of, you know, actual distributed framework. So, you know, actually having an end to end, like training is actually a little, which you just run. And you know, it does the training ahead of time. This is like, eventually, we want to get here. But you know, AOT inductor is not there yet. So it\u0026rsquo;s definitely way more focused on, on the inductor use case. Similarly, AOT inductor is all about the export workflow, right? Like you only can do this on models that you can fully trace through 100% full graph, and then actually export it into some graph that gets used in situation. In principle, the binary products produced by AOT inductor could be integrated into eager mode. And that would give you a way of, you know, compiling something ahead of time, but then just going ahead and calling it from Python. So you just don\u0026rsquo;t have to like warm up PD two beforehand. And you know, like, yes, this is a reasonable thing to want to do. And hopefully, I think actually, this year, we do have some plans to actually spend some effort on making this better. But once again, this is not sort of the like original use case for AOT inductor. AOT inductor has this thing called, has this thing called C++ wrapper code gen in Trident, which sorry, in inductor, which basically says, hey, you know, when we generate inductor code, we generate a bunch of Trident kernels, but we also need a bunch of glue code that just goes ahead and calls the inductor, the Trident kernels, step by step. And normally, we generate Python code, because it\u0026rsquo;s very easy to hack. But you know, CPP wrapper code gen says, okay, we\u0026rsquo;re actually going to generate C++ code that, you know, calls into these one by one by one. So you know, this reduces some overhead. But importantly, for an inductor, this is needed, because we want to like put this all into an executable with no Python dependency. So this thing, this CPP wrapper code gen, you know, this could be very much useful in a integration with Python, but you know, like, you can actually ask for it in pytorch2 eager. And mostly what it does is it reduces your overhead if you\u0026rsquo;re not using CUDA graphs, and it makes your compile time take a lot longer, because you know, compiling C++ code is a lot, you know, slower than just interpreting Python bytecode. One final thing that AOT inductor is not is it\u0026rsquo;s not a TorchScript style interpreted front end, right? TorchScript was our first attempt at bringing compilers to pytorch. And the way to think about TorchScript is you have basically a Python file that is very regular, it has only limited control flow data structures, calls to pytorch operators, and then that\u0026rsquo;s your export product. And then you can load it into some other runtime, which needs basically a full pytorch implementation, so that it actually can go ahead and run these operators. So that\u0026rsquo;s not what AOT inductor is, it is very different from what AOT inductor is. And so, you know, those use cases, that\u0026rsquo;s not really what AOT inductor is here for. Okay, so we talked a little bit about the AOT inductor design goals. So what is actually going into the inside of AOT inductor to actually make it work? And so like the main thing, right, about AOT inductor is it\u0026rsquo;s all about this ABI boundary, right? We don\u0026rsquo;t want to depend on LibTorch directly, because LibTorch is this big and complicated C++ library that doesn\u0026rsquo;t really have any ABI compatibility guarantees. Instead, we want to shrink the service area for what we actually depend on to, you know, a small C ABI only set of operations that give basic functionality that we need that normally you would want to defer to the runtime. So examples of operations like this include allocating a tensor, or, you know, freeing a tensor when it\u0026rsquo;s no longer needed, that sort of thing. Another thing that\u0026rsquo;s very important that we need to support in the ABI is what I call fallback kernel delegation. So what I mean by this is, you know, for a lot of code that we generate in AOT inductor, in inductor, really, in Trident, involves just generating some Trident code, which, you know, actually ends up being some stuff that you can directly run on your GPU. And so this code can actually be distributed directly part of the part of the dynamic library we\u0026rsquo;re generating from AOT inductor, and you don\u0026rsquo;t need to do anything else. Like that\u0026rsquo;s it, it\u0026rsquo;s self contained. But of course, there are a lot of complicated operations like convolutions and matrix multiplies that we don\u0026rsquo;t actually have cogen capabilities for. Well, we do have some cogen capabilities for matrix multiply, but you know, sometimes we just use BLOS because that\u0026rsquo;s the best option. So in those situations, we don\u0026rsquo;t actually package these directly into the dynamic library itself, because that would be very wasteful and increase the dependency surface in some cases. Instead, we just have a ABI compatible call, which says, hey, runtime, please go ahead and run this operation for me. And the runtime does it and then returns control back to the dynamic library, which goes its merry way with the result in this question. And this applies also to the long tail of PyTorch operators that we don\u0026rsquo;t have direct cogen capabilities for, right? There are a lot of these, because you know, weird stuff like sorting, you know, that sort of thing. That\u0026rsquo;s not something you really can do in classic Trident. It\u0026rsquo;s a little difficult. And so our compiler doesn\u0026rsquo;t know how to actually generate these things. There\u0026rsquo;s actually a two tier separation. There are some operators that are so important, they have dedicated ABI for them, like a dedicated function that we call into for them. And then there\u0026rsquo;s like this big, very polymorphic function that\u0026rsquo;s just like everything else. And essentially, all you do is you say, okay, well, I want to call this function with these arguments, and the arguments are boxed up in some very regular format, so that you know, we can basically handle any arbitrary argument type. And this is very much similar to like a Tor script style thing, where, you know, you get all these boxed arguments, and then you just do a polymorphic dispatch to, you know, that particular operator using the same dispatch mechanism that Tor script does in this situation. Another thing that I want to mention about the implementation of AOT Inductor is that it has some implications for how you write code when you want to actually run them on AOT Inductor. So the most obvious implication is that your code does need to be exportable, right? You need a way to get your entire model into a single graph, because that\u0026rsquo;s the graph that we\u0026rsquo;re going to actually compile AOT Inductor into. That being said, you know, there\u0026rsquo;s a lot of attention these days on, you know, writing custom Trident kernels for, you know, you know, doing all sorts of fancy attention variants and that sort of thing. And we can actually deal with Trident kernels. So if you write a Trident kernel as part of your model, in a traditional export, this might be a little difficult to export, because, you know, what exactly is this Trident kernel? If you want to, you know, send it to a mobile device that is running, you know, some sort of Qualcomm, you know, hardware thingy, right? Like a Trident kernel is going to be useless for this case. But because AOT Inductor is specifically all about, you know, like producing an artifact that can run on CUDA, all we need to do is bundle up this Trident code with the rest of the Trident code that Inductor is generating. And, you know, this all can be saved directly into the model. And, you know, you don\u0026rsquo;t, you don\u0026rsquo;t have any runtime component. And so we have support for user defined Trident kernels without actually wrapping them in custom operator. And these can actually go straight into your AOT Inductor kernel. And, you know, similarly, if you have a custom op, that\u0026rsquo;s a little different, we can\u0026rsquo;t necessarily embed that directly, that\u0026rsquo;s just going to go the normal Tor script box style, you know, call back into the runtime to actually do things. So if you\u0026rsquo;re writing a new op, or you, you know, you can\u0026rsquo;t remove ops from your runtime, because you might have saved models that depend on them. Okay, so I talked a little bit about what\u0026rsquo;s inside AOT Inductor, right? So AOT Inductor is all about, you know, generating all just straight line C++ code against a fixed ABI that, you know, strings all the Trident kernels together. But that\u0026rsquo;s basically it. That\u0026rsquo;s the core of what you do. And like, you know, you can imagine using this technology in other situations. One of the things that AOT Inductor expressly does not solve is what I like to call the caching problem. The caching problem is essentially, hey, I\u0026rsquo;ve got some, you know, code in eager PyTorch 2. And I think I want to reuse it, I don\u0026rsquo;t want to like keep compiling every time, I want to compile it once and then reuse it subsequent times. And so I want to cache it on a separate run. The caching model for AOT Inductor is an export style caching model, which says you exported this thing. And there\u0026rsquo;s the thing you exported is the source of truth. That\u0026rsquo;s that\u0026rsquo;s that\u0026rsquo;s it, right? So if the thing you exported doesn\u0026rsquo;t do what you expect, then you know, well, what did you expect, right? You exported this thing. But when you\u0026rsquo;re in a, you know, sort of more fluid environment, like you\u0026rsquo;re running PyTorch 2 eager, it\u0026rsquo;s very tempting to be like, well, okay, I want to cache this thing. And I also want to like change my model code, or whatever. Or, you know, maybe I actually had a bunch of graphs, I maybe recompiled, you know, the same region multiple times under different parameters. And, you know, which thing do I want to use, and this is one of the like big problems we face when we were trying to figure out how to improve the warm start times of Torch Compile, right? Like warm start is very important, right? If you\u0026rsquo;re running a big training job, and you know, you have to restart from a checkpoint, because one of your nodes crashed, right? You really don\u0026rsquo;t want to be waiting 20 minutes for Torch Compile to recompile everything. And we just don\u0026rsquo;t, we don\u0026rsquo;t have a good solution for this. We have some patchwork solutions for when we know how to do the cache accurately, because, you know, we\u0026rsquo;re inside inductors, say, and we know all of the relevant input arguments that we get here. But at the very topmost level, at the dynamo level, it\u0026rsquo;s very hard to tell, does this code object actually apply the next time around, unless you force the user to make some assumptions. So A1T Inductor says nothing about this, right? It just makes the simple assumption, which is you exported this thing, this thing you exported is what you asked for, that\u0026rsquo;s what you\u0026rsquo;re going to get the next time you run it. And you know, we\u0026rsquo;re working on this. So I hope to do some more podcasts about this particular problem, because it is a big problem. And we are working like this is one of our top line things we\u0026rsquo;re working on coming this year. Okay, that\u0026rsquo;s everything I want to talk about in AOT Inductor today. See you all next time.\nEP78 Min-cut-partitioner Min-cut-partitioner Hello everyone and welcome to the PyTorchDev podcast. Today I want to talk about the mincut partitioner. Actually I\u0026rsquo;m not the best person to talk about the mincut partitioner. I should get Horace on the podcast at some point to talk about it but I do want to mention something very specific that Horace told me over the core offsite which I thought was really interesting and it\u0026rsquo;s that when we talk about the mincut partitioner naively you think of the mincut partitioner splitting the graph into a forward part and a backward graph but actually that\u0026rsquo;s inaccurate. What you\u0026rsquo;re actually doing is the forward graph always is the forward graph and the backward graph what you\u0026rsquo;re doing is you are carving out the backward graph out of the joint graph which means that potentially you actually can be putting some bits of the forward graph into the backward graph and this is the sense in which the mincut partitioner is also capable of doing rematerialization. All right so that\u0026rsquo;s what I want to talk about in this podcast but to get there I think I need to first explain what the mincut partitioner is why it exists and then I can say this again. So that\u0026rsquo;s that\u0026rsquo;s all I want to talk about today. So what exactly is the mincut partitioner? So the mincut partitioner is an essential component of AOT Autograd and what it is is essentially the way that we compute what the forward and the backward of a function should be before we wrap it up into a custom autograd function. So some backstory here. So remember that in PyTorch 2 we\u0026rsquo;re all about graph breaks. We\u0026rsquo;re all about being able to compile parts of your program while having other parts of your program run in conventional PyTorch eager mode. And one of the things we need to do when we do this is we need to be able to have the compiled pieces of your program interoperate with the rest of PyTorch\u0026rsquo;s regular eager autograd system. So what this means is that typically when I compile something I need to also manually specify and compile the backwards of it and wrap it all up in a custom autograd function because that\u0026rsquo;s the normal way that I introduce new differentiable primitives when I\u0026rsquo;m working in PyTorch eager mode. So AOT Autograd is the component that\u0026rsquo;s responsible for doing all of this true to its name. The AOT and AOT Autograd is all about you know doing autograd ahead of time. By the way I do have a podcast about AOT Autograd which is still pretty accurate so you might check that out if you want more details. But let\u0026rsquo;s think about how exactly we would go about actually doing this autograd ahead of time right. So what we\u0026rsquo;ll have is we have some forward graph which is precisely the region of code that we\u0026rsquo;re compiling and we want to differentiate it somehow. So how are we going to go about differentiating this? So we could imagine first you know taking out the forward graph and then somehow starting up another trace when we do backwards and then tracing out what the backwards is in the situation and that gives me the second backwards graph which is what I actually want to go ahead and put into my program. But we don\u0026rsquo;t do this in AOT Autograd. We do something a little different. What we do is we do is we trace something called the joint graph. So the joint graph is a single graph that has all of this all together in one go. It has the forward computation and then has the backward call which reads out the backward graph instead. And one very interesting thing about doing it this way is that the joint graph not only has the inputs that you have available when you are doing forwards, it also has all the tangents which are flowing in from the Autograd engine from the backwards. Because remember unlike when you\u0026rsquo;re differentiating an entire model where you\u0026rsquo;re returning a single scalar loss and so you can just assume that the gradient on that loss is one, we are going to be in general outputting a large number of tensors and we need Autograd engine to tell us you know which directions the gradients on those output tensors are which will tell us what gradients the inputs should be via our computation in this case. So we\u0026rsquo;ve got this weird graph right this joint graph which has both the forward and the backward inputs and this is not really that useful right because the way I normally want to run my PyTorch program is I want to go ahead and actually run the forward compute first and only later when I am when I am when I got my backward tangents available do I want to run the backwards. Of course I could wait until the very very end you know all the way to when the backwards is run to actually go ahead and run the forwards but this is actually you know not going to be very useful because I do need to actually have the forward output so I can run the rest of my forward computation and then if I go and run it all again you know in the backwards while I\u0026rsquo;m doubling the work in question. Actually this is not so strange right there\u0026rsquo;s something that there\u0026rsquo;s a technique that people do when they do this which is called activation checkpointing and this thing where I just decide hey I\u0026rsquo;m just going to you know recompute the entirety of my forwards pass when the backwards pass comes along is akin to just you know slapping an activation checkpoint on your entire model saying hey I don\u0026rsquo;t want to save anything for backwards I just want to recompute it all from scratch. If you slap one of these on the entirety of your model this doesn\u0026rsquo;t actually help with your peak memory usage because you know why did your peak memory usage go up? Well think of the typical memory usage of a deep learning model as looking like a mountain right which is that as you\u0026rsquo;re executing your forwards pass your memory usage is going up it\u0026rsquo;s going up because you are saving activations for the backwards pass then once you run the backwards pass we start the the memory mountain starts going down because as we do computations that used the saved activations from the forwards and these are going in reverse order because you know that\u0026rsquo;s the backwards is run in reverse order to the forwards I can release those saved activations as I go so now the memory usage goes down so the peak of the memory usage is at the very end of the forwards pass right as we\u0026rsquo;re about to do the backwards pass so if you do something like well I\u0026rsquo;m just not going to save any activations for backwards when I run my forwards sure the initial time you run the forwards is not going to have very much memory usage but then when the backward rolls around and you\u0026rsquo;re like okay well I need to recompute the forwards to get all of the things I needed for backwards well your memory usage is going to go up because what are you doing well you\u0026rsquo;re computing a big pile of saved activations from the wrong direction right you\u0026rsquo;re computing them forward from the front to the end whereas the backwards pass wants to use them to end to the front so you end up having that same mountain of memory usage again all over so it\u0026rsquo;s pretty pointless if you do this over your entire model of course it\u0026rsquo;s not pointless if you take a subset of your model and do it only there and that\u0026rsquo;s the idea behind activation checkpointing anyway so we have this joint graph right it\u0026rsquo;s got the forwards inputs it\u0026rsquo;s got the tangents inputs and it needs to produce the forward outputs and the final grad inputs associated for us and so the partitioning process basically says okay given this graph which you know if we just took it at face value we didn\u0026rsquo;t do anything to it would represent an activation checkpointing strategy which is probably the wrong thing to do if you\u0026rsquo;ve got the entirety of your graph in this case um how do I minimize the uh how how do I how do I strike a balance between uh v computation uh you know the amount of compute I do and the amount of memory that I need to save for things in backward uh you know and uh you know how do I want to do this and this is exactly the job of the partitioner right the partitioner is going to make a decision about what exactly we are going to save from backwards and it\u0026rsquo;s going to try to minimize the memory usage um you know that of the things we need when we do this subject to some other heuristics and that is going to basically uh reduce the amount of memory we actually need when we actually go ahead and compute uh when we uh compute our network overall because the less memory I\u0026rsquo;m saving for backwards the lower that mountain is when I\u0026rsquo;m climbing it okay so let\u0026rsquo;s go back to the thing that uh I wanted to talk about this podcast from the very beginning right so I used to think of the min cut partitioner as well I\u0026rsquo;ve got this joint graph I\u0026rsquo;m going to split it in two um the first half is the forward graph and the second half is the backward graph and you know that\u0026rsquo;s what I end up with but this is not accurate and the way you can realize that this is not accurate is that if you think about it there\u0026rsquo;s really you know nothing you want to do uh to the forward graph um in actuality okay sure there may be some you know a little bit of compute uh that depends only on forward inputs doesn\u0026rsquo;t depend on tangents um and that is not used by the forward compute and you can decide whether or not this compute should happen in the forwards or backwards this is not too difficult to figure out right you have to do this compute um at some point either way so you know you you\u0026rsquo;ll just put it either you know in the forward or backwards depending on you know what the maximum usage is but you don\u0026rsquo;t really um you don\u0026rsquo;t really get to make any more changes to the forwards graph right you can\u0026rsquo;t not compute things that are needed for the forward output because you\u0026rsquo;re obligated to produce all the forward outputs when you\u0026rsquo;re all done and you can\u0026rsquo;t compute anything that in the backwards depends on the tangent because you don\u0026rsquo;t have the tangent when you\u0026rsquo;re running forwards uh so well you know nothing you can do there but let\u0026rsquo;s talk about the backwards for a moment right so the backwards doesn\u0026rsquo;t have this constraint right the backwards as i said could in principle decide that it is going to do the entirety of the forward computation over again or you know more hopefully it doesn\u0026rsquo;t actually do that but it does some subset of it but essentially when i\u0026rsquo;m looking at the backwards i can actually decide to reincorporate pieces of the forward computation and that is okay there is nothing wrong with that so as i said i you know the forward graph doesn\u0026rsquo;t really have very much i can do to it but the backward graph i can use as much or as little of the forward pass as i want and when i put things from the forward pass into the backward pass and say hey go ahead and recompute this i am essentially reintroducing re-computation into my program and that can be useful for example when i\u0026rsquo;m doing activation checkpointing sometimes activation pointing checkpointing style things are free one particular case it is free is when you\u0026rsquo;re able to fuse all of the re-compute into some computation that you that you\u0026rsquo;re already going to do and backwards and the reason for this is typically we are memory bound so extra compute is free so as long as inductor is able to do the fusion then well you\u0026rsquo;re not going to pay anything right you did a little bit of compute but it doesn\u0026rsquo;t matter because you were paying uh the cost to go ahead and read memory in fact uh you know uh re-computation can actually make your program faster because if you\u0026rsquo;re reading less memory then you know you are reading less memory and if the computer is still free in that case then it doesn\u0026rsquo;t matter that you did more compute you reduce the memory and that was the thing you actually needed to reduce in this case okay so uh that\u0026rsquo;s um that\u0026rsquo;s a really interesting insight that uh i got from harris um as i said i should actually do a proper podcast with harris sometime about the mincut partitioner probably um we\u0026rsquo;ll call this podcast selective checkpointing activation checkpointing because that\u0026rsquo;s what harris has been working on and it\u0026rsquo;s some really interesting stuff and i\u0026rsquo;m really looking forward to uh you know being able to share it all with you um in the future one more thing that i want to mention so i talked a lot about how we have this constraint which is that i can\u0026rsquo;t do backwards compute ahead of time because i don\u0026rsquo;t know what the tangents are ahead of time right i only know what the tangents are when they actually get run in backwards actually we have a long-standing problem in aot autograd that stems from a very similar problem the problem is this when i trace in aot autograd i am tracing the forwards and the backwards ahead of time aka i am doing it before i actually know what my tangents in question are and so the thing is that while i do know some things about the tangents for example i know they have to have the same sizes as the output of my graph because that\u0026rsquo;s how automatic differentiation works but i don\u0026rsquo;t know for example whether or not the tangents are contiguous or not and in fact the way aot autograd works today is we just assume that the tangents are contiguous when we create up the fake tangents uh to go ahead and do our tracing with and sometimes this is not true sometimes i can get tangents which are not contiguous maybe they are transposed maybe they are channels last and now i have a graph that is slightly suboptimal for this case what we do when this happens is we just call dot contiguous on the tangents before feeding them into our compiled graph but this is a place where we\u0026rsquo;re leaving performance on the table and we actually have some internal models where we\u0026rsquo;ve noticed that this is actually a problem another case where this is a problem is when tensor subclasses are involved so let\u0026rsquo;s say that i have a program and it has a tensor subclass output and i\u0026rsquo;m i\u0026rsquo;m going to you know run backwards and i don\u0026rsquo;t know exactly what the backwards input is going to be is it going to be a tensor subclass is it going to be a normal tensor tensor subclasses can have metadata for example if i am running with d tensor aka distributed tensor distributed tensors know what sharding they have do i expect the gradients to be exactly sharded in the same way that the four outputs were in fact not necessarily because the communication patterns you need when computing backwards are actually quite different in fact you can also in the worst case scenario have a plain tensor output but actually the subclass output is going to be a subclass tensor so we\u0026rsquo;ve spent a long time arguing about what exactly we should do in this situation and we actually have a new plan once again thanks to harris\u0026rsquo;s insights about how the mincut partitioner works so the key insight is that when we run the forwards uh we can never ever depend on uh things from the tangents right but that also means that if i have a subclass or i have a different contiguity in the backwards it is only the parts of the graph which depend on those tangents that can actually change depending on this so when i\u0026rsquo;m making decisions about mincut partitioning um i actually uh you know have a lot of play because i never could have moved the things that could change um into the forwards pass um i only could have ever moved the things from the forwards pass which i know exactly whether or not they are going to be um what what they are because i know what all the forwards inputs are into the backwards pass so in this way what i need to do um is instead of going ahead and pre-committing to oh this is contiguous oh this is a particular uh subclass with particular metadata what i need to do is i can go ahead and do the trace i want the forwards graph to be fully elaborated and i want the backwards graph to be um you know essentially pre-dispatch i want to avoid making any commitments to contiguity or subclassness so that later when i actually get the tangents i can actually go ahead and retrace it and lower it to be actually what i see so this is our current plan on record um i think brian hirsch is planning to work on this and uh you know we\u0026rsquo;ve got a design doc that i\u0026rsquo;ve linked in the pytorch 2 weekly update which you can check out for more information okay that\u0026rsquo;s everything i wanted to talk about mincut partitioning today uh see you next time\nEP79 CUDA-graph-trees CUDA-graph-trees Hello everyone and welcome to the PyTorch Dev Podcast. Today I want to talk about CudaGraph Trees, our CudaGraph integration with PyTorch 2. Most of this was work done by Elias Ellison, so you know kudos to him for actually building all of this. So first off let\u0026rsquo;s remind ourselves what CudaGraph Trees are. I do have a podcast about it so if you want to know more details about CudaGraphs itself you can go there. But CudaGraphs is essentially a way to remove overhead from applications that are calling Cuda kernels by saying hey instead of running all of the possibly very expensive host code that glues a bunch of Cuda kernels together we just smash it all into a recording that just runs the Cuda kernels one after another exactly the same way that they were run before. So in PyTorch Eager we have a API for using CudaGraphs called make graph callables and it basically does exactly what you would expect. It will go ahead and CudaGraph record your function and you will get exactly what you asked for. And so maybe this is what you want, maybe it isn\u0026rsquo;t. It\u0026rsquo;s actually kind of hard to use CudaGraphs in a lot of situations, right? You have to make sure that there\u0026rsquo;s no CPU compute in your program, there\u0026rsquo;s nothing that varies from run to run, there\u0026rsquo;s no unsafe calls to unsafe operators, those will just cause CudaGraph recording to fail because CudaGraphs will say no, no, no, you can\u0026rsquo;t read out things from CPU. When you are passing in the inputs to CudaGraphs, they all actually have to be static addresses because those are being burned into your CudaGraph. So, you know, if you have an input, you have to make sure you copy that into a fixed buffer. All this needs to be handled by hand. So you can do it if you\u0026rsquo;re very motivated and people are often very motivated and will manually CudaGraph their code. But one of the things that we wanted to do with PyTorch 2 was to make it easier for people to get this overhead reduction without having to go through all this rigmarole. And of course, you know, PyTorch 2 actually does help a lot with overhead reduction intrinsically because we\u0026rsquo;re in the business of, you know, taking your models, you know, factoring out all the Python code. So we don\u0026rsquo;t actually have to run any of your Python code. We only have to run the residual bytecode afterwards that does exactly the Python state updates we need. And by fusing kernels together, we reduce fixed costs because, well, you know, the less kernels you\u0026rsquo;re running, the less overhead you have to do in this case. But it\u0026rsquo;s still the case that for a lot of really overhead bound models with very, very small compute and lots and lots of operations, it turns out CudaGraphs still gives you a pretty sizable efficiency improvement, even when you\u0026rsquo;re using the PyTorch 2 compiler. And this is something that we could reduce, we could reduce the overhead of PyTorch execution, even in PyTorch 2, like there are things that we could do. But CudaGraphs is just there\u0026rsquo;s nothing faster than zero, right? When you run with CudaGraphs, there is no host site overhead by construction, because you\u0026rsquo;re going straight to running the Cuda kernels one by one by one. So CudaGraphs is cool. And I want to turn back the clock back to the eve of the PyTorch 2 release. And we\u0026rsquo;re having a call. I actually I remember I was driving home from, you know, doing some maintenance on my Tesla. So I was on the highway and I was called phoned in to a conference call we were having, which was basically the question was, what are we going to do about CudaGraphs. And so the problem was CudaGraphs, we could tell from our benchmarks, made things a lot faster when we were running PyTorch 2. But they used too much memory. Why did CudaGraphs in PyTorch 2 use too much memory? So the problem was related to graph breaks. Specifically, let\u0026rsquo;s imagine that you\u0026rsquo;ve got your model and there\u0026rsquo;s some graph breaks. So, you know, you\u0026rsquo;ve got graph one, graph two, graph three. Obviously, we can\u0026rsquo;t CudaGraph the entire thing because we have no idea what\u0026rsquo;s going on between the two graph breaks. So instead, we CudaGraph each graph separately. And we CudaGraph each graph separately. Well, you know, how exactly does CudaGraphs work? So normally, the way CudaGraphs works is you end up with a isolated CudaGraph block, which contains enough memory to store all the inputs. Because remember, it\u0026rsquo;s it\u0026rsquo;s all static addresses, right? So the next time you call this CudaGraph, you have to give it tensors in exactly the same location they were last time. So to make sure you actually have those addresses available, you have to actually keep that memory around. So for every CudaGraph, you have a big Cuda memory allocation, which has enough space for all the inputs, modulo parameters, because parameters, you can just assume have some static location. And they, you know, don\u0026rsquo;t change. So everyone can reference those static addresses, it has all of your input space, and also enough space to do all the intermediate commute you might do because obviously, in the middle of your graph, you may do allocations. And those allocations also are going to have hard coded addresses, and you need to have them in your CudaGraph. So when you have three CudaGraphs, what you end up having is 3x the amount of memory you need, because each CudaGraph has its own pool of memory that\u0026rsquo;s disjoint from the other ones, being like, hey, this is the memory that I need to actually do my compute, because I\u0026rsquo;ve burned in all these static addresses. And so I need to reserve it for myself when I do it. And this is very memory expensive, because when I ordinarily run my program in eager mode, I don\u0026rsquo;t have this hoarding behavior, right? When I\u0026rsquo;m doing stuff with the Cuda caching allocator, I ask for some memory, I use it when I\u0026rsquo;m done, I return it to the Cuda caching allocator, and it\u0026rsquo;s allowed to send that memory off to someone else so that they can use it for something else. But these CudaGraphs can\u0026rsquo;t actually do that, they have to hold on to the memory, because the next time you call it, they need to make sure that memory is actually available for them to actually do things. Okay, so so CudaGraphs was, you know, using up too much memory, we were like, oh, my God, you know, what are we going to do about this? Like, how are we going to launch PD2 with a version of CudaGraphs that takes up this much memory? And, you know, we were thinking of ideas for how to do this. One of the ideas that we had was, hey, you know, when you, you know, do normal eager mode, we\u0026rsquo;re willing to reuse memory allocations between CudaGraphs. So there\u0026rsquo;s nothing stopping you from reusing the memory allocations between separate CudaGraphs, right? So I\u0026rsquo;ll remember all the CudaGraph is doing is saying, hey, you know, there\u0026rsquo;s a static address, and this the memory and the static address needs to be available when I use it. So if, for example, the CudaGraphs get called in exactly the same order, every single time, then what you can just do is say, okay, well, this memory is no longer being used. You know, I needed it for the first graph, but I\u0026rsquo;m no longer using it anymore at this point. Let me go ahead and use it for something else when I\u0026rsquo;m running my second CudaGraph. And so I don\u0026rsquo;t need to actually do the sum of the intermediates of all three graphs, I can do reuse. So my memory usage looks a lot more just like what the high watermark memory usage used to be. But there\u0026rsquo;s a problem with this. And the problem with this is when you have a graph break in PyTorch 2, you can\u0026rsquo;t actually guarantee that the same graph will be called next, right? Because maybe the reason you did a graph break was because the user had a, you know, dynamic conditional, which is going to shunt you between one graph or another graph. So if you do all this memory reuse, and then suddenly, some other graph gets called, well, oh, you know, maybe some memory that you were expecting to be available is no longer available, and you\u0026rsquo;re in trouble. But there is a, you know, maybe obvious next step to do in this case, right? Which is, what if when we, you know, diverge between the two Cuda graphs, we simply, you know, imagine that, well, there\u0026rsquo;s two paths we can take. So at the time I do memory allocation, and I\u0026rsquo;m done with the first graph, you know, the memory allocator is in some state. And then depending if I go to graph two, then I will, you know, do some things based on graph two. But if I go to graph graph three, instead, I\u0026rsquo;ll do some other things. And sort of imagine, like, you know, in one of those, like, time travel movies, where you make a decision, and depending on decision, you know, the future branches off into two possible different futures, we just want to do the same thing for Cuda graphs. And so this leads to this concept of Cuda graph trees. And this is what we actually implemented in part of two. And Cuda graph trees sort of completely solve the problem of memory reuse in Cuda graphs, because we simply say, well, it\u0026rsquo;s a choose your own adventure. The memory usage you\u0026rsquo;re going to end up using for the Cuda graph recording is the maximum of the memory usage for all the possible branches you would take. But because we are only allowed to evolve the Cuda graph in the sort of paths on the tree, every path on the tree is going to have a consistent allocation deallocation pattern. And so as long as I go down that same path, I can just simply reuse exactly the same memory addresses as before. And if I take a different path, well, that path is on its own execution. And I\u0026rsquo;m guaranteed not, you know, once I\u0026rsquo;ve made a choice, I can\u0026rsquo;t, you know, change my man and go down another path of the tree. So each of these paths are self contained. And then eventually, I get to the end of my training loop iteration, I go back to the beginning, and ostensibly, usually, you know, when you\u0026rsquo;re done with a, you know, single training step, you know, all your memories done, and so everything can be assumed to be cleared, and you can start going reusing things again. So this is a basic concept of Cuda graphs, right? So Cuda graph trees. So the main idea is we want to reuse memory across graphs. By reusing memories across graphs, we get rid of the big memory usage used by, you know, Cuda graphs, and sort of the tech you have to build actually do this is some sort of, you know, ability to checkpoint the state of the memory allocator. So that if you\u0026rsquo;re like, hey, you know, I\u0026rsquo;m running Cuda graphs, and I want to, you know, record if it goes this way. And I also want to record if it goes some other way, I need a way to reset the state of my allocator to what it was at that point in time, so that I can go ahead and then do a bunch of other allocations and deallocations based on what I see in the next case. Okay, so that\u0026rsquo;s the basic implementation idea behind Cuda graph trees. There are some operational implications to how, you know, we\u0026rsquo;ve implemented Cuda graph trees. So when I, one of the discussions that, you know, Les Kano opened up on GitHub is, hey, maybe we should turn on Cuda graph by default, this is mode reduce overhead when you\u0026rsquo;re running torch compile. And, you know, maybe this is a good idea. We\u0026rsquo;re a bit nervous about it. And so the reason we\u0026rsquo;re nervous about it is that although Cuda graph trees, you know, is pretty good at what it wants set out to be, which is it\u0026rsquo;s set out to be a way of using Cuda graphs, where we can basically let you say, okay, just try reduce overhead. And PyTorch 2 is going to take care of, you know, dealing with all the safety conditions you need, right? So we don\u0026rsquo;t have a problem. If you are, you know, doing CPU compute or unsafe operations, because, hey, we\u0026rsquo;re just, you know, we\u0026rsquo;re PyTorch 2. So we\u0026rsquo;re actually getting a graph, and then we can go look at it and say, are you doing any compute on CPU? Are you like calling non zero, and then we can just disable Cuda graph if those things have happened. And because we\u0026rsquo;re PyTorch 2, we also keep track of all the inputs. And so we know, oh, these inputs are parameters, so we can statically bake them in. These inputs are just regular eager inputs. So we\u0026rsquo;re going to allocate dedicated buffers for them in the Cuda graph memory pool, and copy them in. And we do all this for you, because we have a pretty deep understanding of what is going on in your code, because hey, you know, having graphs is great. And furthermore, because, you know, Cuda graph trees have this, you know, sort of choose your own adventure style, you know, property to them, we can even do this in the presence of graph breaks. So obviously, your code inside the graph breaks is going to run, you know, slow, but all the stuff inside PyTorch 2 is actually going to run fast. But this this safety, this abstraction is not complete. So one of the like big things that you have to be aware of, is that when eventually, when we\u0026rsquo;re doing Cuda graph trees, we want to sort of stop the tree, and go back to the beginning of the tree, right? If we always keep, you know, appending more kernels onto the tree. This is kind of pointless, because if you\u0026rsquo;re continuously recording new Cuda graphs, you\u0026rsquo;re never getting the benefit of replaying the Cuda graph, right? You only get the benefit of Cuda graphs, when you actually have a pre recorded Cuda graph, and you replay it again. So at some point, we had to be like, okay, we\u0026rsquo;re done recording, we\u0026rsquo;re going to go back to the root of the tree. And now we can follow a path. And hopefully that path has all Cuda graphs, we\u0026rsquo;ve already recorded, so we can go zip zap, very fast. So when we restart the tree, when we go back to the root of the tree, we now have the, you know, big constraint, which is that we actually need to have, you know, freed all the memory associated with the Cuda graph memory pool, because we\u0026rsquo;re going to go stomp over it again, in an unpredictable way, when we start using the memory again. And so I said, usually user code is written so that this isn\u0026rsquo;t a problem. But you can get it wrong, right? If you like hold on to a tensor that is an output of a Cuda graph tree, then well, that tensor, if it stays live, you know, is going to inhibit Cuda graph trees from actually, you know, being able to be used as a memory, because we don\u0026rsquo;t want to stomp over the data. And then you get a bunch of garbage in one of these tensors that\u0026rsquo;s hanging around. Another problem that, you know, is sort of non transparent with Cuda graph trees is what happens when you have mutations on input tensors. So remember that I said, when you do Cuda graphs on an input tensor that doesn\u0026rsquo;t have a static address, we go ahead and copy it into the Cuda graph. So once you\u0026rsquo;ve copied it into the Cuda graph, that\u0026rsquo;s a separate, you know, allocation for the input in in question. So if you remember, if you\u0026rsquo;re a programming question, as it goes ahead and tries to mutate this, it will mutate Cuda graphs internal representation of the memory in question, but it won\u0026rsquo;t mutate the actual, you know, original user input, which may have been allocated in eager mode. So we don\u0026rsquo;t do an unsafe thing. In this case, we actually just, you know, cancel Cuda graph trees in this situation. But, you know, if you\u0026rsquo;re just applying Cuda graph trees to some random code that you haven\u0026rsquo;t actually looked at, it\u0026rsquo;s possible that, you know, it doesn\u0026rsquo;t actually work because there are things that like look pretty benign. And, you know, we have like gotten past them with graph breaks, but then they just inhibit Cuda graphs from working. So you kind of like if you\u0026rsquo;re like, oh, I think my model actually should run with Cuda graphs, then you have to actually like look and see if Cuda graphs is actually running when you turn it on with PyTorch two, because the chances are, we actually may have turned it off for any number of reasons, some of which are, you know, like just fundamental framework limitations, but not limitations from you, the user, like it\u0026rsquo;s probably not too difficult to adjust your code to handle this case. Finally, um, Cuda graph trees are not free, right? They do change the cost model of your program. I already mentioned one of the things that changes, right? When you have a Cuda graph tree, and you have a lot of, you know, branches, ordinarily, you only use up the, um, you know, memory associated with the, uh, branch when you go down that branch, but a Cuda graph tree is going to have a standing allocation, which represents the maximum you memory usage of all the branches you could possibly take. So you better not be, you know, relying on the fact that, well, sometimes, uh, you know, my memory does go up, but you know, it doesn\u0026rsquo;t happen all the time. And therefore, you know, uh, there, there\u0026rsquo;s something okay in this case, right? Like you\u0026rsquo;re, you\u0026rsquo;re just always going to pay the worst case memory usage in this case. Also, your memory usage is going to be worse than it would have been in eager mode, because when you could graph things, those could graph allocations have to go in a separate memory pool than the eager memory pool. So, um, you know, the, oh, if you\u0026rsquo;re running everything in eager, the Cuda caching allocator may be able to like make better use of your memory by like serving things from a shared pool. But when you separate the pools, your memory usage generally gets worse because you know, like, uh, you\u0026rsquo;ve got two pools. So, you know, if something is free in one pool and you need an allocation in the other pool, that doesn\u0026rsquo;t work. You, you have to just go ahead and allocate in it. So there, there can be some memory inflation in this case. And, uh, finally, um, you know, CUDA graphs, um, in the worst case scenario could make your model run slower. And that\u0026rsquo;s because for all the inputs, which don\u0026rsquo;t have fixed memory addresses, we have to copy them into the CUDA graph region. So this is copy that you weren\u0026rsquo;t, you didn\u0026rsquo;t have to do in normal eager mode. Now in a situation where you don\u0026rsquo;t have lots of graph breaks, you have like one graph and your inputs are not too big. This is usually not a big deal because the savings from CUDA graphs, um, more than out swamp the fixed one-time cost, but if you have a lot of graph breaks, then this can potentially be a performance problem. So, um, maybe we can turn on CUDA graphs by default, but it\u0026rsquo;s, uh, you know, uh, we\u0026rsquo;re probably going to work on it some more because definitely CUDA graphs is one of those things where you need to like turn it on and then see if your model is doing what you expect to do or not. Um, there are some ideas that we have for improvements to CUDA graphs. Some of the things limitations that I talked about, such as mutations to input tensor in principle can be fixed with just more engineering. Another tool that we have in our toolbox is re-recording CUDA graphs. So we actually have already implemented this for dynamic shapes. The idea behind dynamic shapes and CUDA graphs is that, um, you know, when I have dynamic shapes, normally this doesn\u0026rsquo;t work with CUDA graphs because a CUDA graph has everything burned in, including the sizes of the tensors in questions. So, you know, you can\u0026rsquo;t have a single CUDA graph that works for multiple dynamic sizes, but what you can do in this case is for every dynamic size you see, you could re-record the CUDA graph, um, you know, using the same dynamic kernels that inductor had generated, but just with a different, you know, size in question. And this is profitable because it\u0026rsquo;s a lot cheaper to re-record a CUDA graph whose cost is on order of how fast it takes to run the model. Then it is to like actually do the entire PyTorch2 recompilation, uh, again, which, you know, is pretty expensive, um, in part because compile times are slow, but you know, it\u0026rsquo;s just going to be a lot more work. You\u0026rsquo;re actually generating kernels in that and stuff like that. So this is something that we can do, um, to like work around problems that CUDA graphs have. And, um, another case that, uh, on a mesh and lathe have been looking into is, Hey, um, you know, we probably also want to, uh, you know, we do re-recording of CUDA graphs. If we have a, um, CUDA graph that is referencing a lot of parameters, but actually we have a lot of different parameters. And so a common situation this occurs is let\u0026rsquo;s say you have a bunch of transformer blocks in your model and you\u0026rsquo;re only compiling the transformer block and you want to CUDA graph the transformer block. So it would be nice if you could have a single compile product that works for all of the transformer blocks in your program. But in this case, the parameters for these blocks are different. And, um, if you know, naively CUDA graph it, then you would have to do a copy in on the parameters, which is generally a terrible idea, unless it\u0026rsquo;s a diffusion model, because apparently, according to, uh, Dima, who I was talking to about this, uh, diffusion models don\u0026rsquo;t have as much of a problem with doing this copy in. So to deal with this problem, uh, what you can do is you just re-record the CUDA graph for each, uh, individual block. So the now compilation cost is you compile once, uh, with a generic, uh, with a version of your model, uh, that can work for an arbitrary parameter. And then for every particular transformer block, you re-record the CUDA graph with the new, uh, you know, static addresses for each parameter. And then, you know, once you\u0026rsquo;ve, you know, done, however, you know, dozens of transformer blocks, then they can all be reused. And this doesn\u0026rsquo;t cause memory usage because you\u0026rsquo;re going to reuse the same memory for each of these recordings. Okay. So I hope that told you a little bit about what to expect with CUDA graph trees. This is what happens when you do mode, reduce overhead in torch compile. That\u0026rsquo;s everything I wanted to say today. Talk to you next time.\nEP80 Inductor\u0026mdash;Post-grad-FX-passes Inductor\u0026mdash;Post-grad-FX-passes Hello, everyone, and welcome to the PyTorch Dev Podcast. Today, I want to talk about the post-grad FX passes in Inductor. What do I mean by post-grad FX passes? Well, let\u0026rsquo;s think about the entirety of the PyTorch 2 compilation stack, right? So we\u0026rsquo;ve got our input graph that comes from Dynamo. That graph has a bunch of torch operations in it. It\u0026rsquo;s not normalized at all. We feed it into AOT Autograd, and a bunch of stuff happens. But what AOT Autograd is responsible for doing is functionalizing, desugaring, doing all sorts of transformations so that we eventually end up with a pair of two graphs, one for forward, one for backward, which are fully normalized. They are A10. They are functional, except maybe for some mutations at the end, and those get sent to Inductor. And so the Inductor is going to go ahead and lower these into Inductor-level IR. But before it does that, it takes a whack at this graph, this normalized A10 graph, with a few FX passes that operate at sort of more of a graph-level type optimization attitude. And these are called the post-grad FX passes. True to its name, there\u0026rsquo;s also a pre-grad FX pass. There\u0026rsquo;s also a joint graph FX pass. But we\u0026rsquo;re not going to talk about those. We\u0026rsquo;re just talking about post-grad FX passes. These passes are, in some sense, the easiest ones to write, because we give you the most invariants in the IR, because you can assume that we\u0026rsquo;ve already gone ahead and done all of the decompositions. We can assume that the graph is functional. A graph being functional is really helpful because it means a lot of transformations, like moving nodes around, replacing aliases with non-aliases, duplicating uses of inputs. All of this is safe because you don\u0026rsquo;t have to worry about maintaining some sort of aliasing analysis to figure out, oh, am I allowed to move this, you know, read to some other spot? What if someone is mutating it? So you don\u0026rsquo;t have to worry about any of that stuff. Except that we do, for efficiency reasons, permit the IR to do some mutations on inputs. And the reason we allow the IR to do mutations on inputs is because it can be important to make sure that we promptly do the mutations, you know, while we\u0026rsquo;re executing the graph, instead of waiting until the very end, um, producing a pure, you know, output tensor and then doing the mutation, which would increase the overall memory usage that you might need in the situation. So the IR environment, um, and, uh, you know, Brian has a nice document. He wrote, uh, recently this week, uh, what mutations does AOT and Autograd allow Inductor to see? Um, and so the general category of what things we are allowed to do, what things we\u0026rsquo;re allowed to put in Inductor are essentially only input mutations. And these mutations can only happen at the end of the graph. And they look like a bunch of copy underscores into inputs. So we never have mutations on intermediates. We never have, uh, mutations on, uh, in randomly in the middle of the graph, they\u0026rsquo;re all at the end. And so you can sort of look for them when you\u0026rsquo;re writing a pass and make sure you sort of step around them when you\u0026rsquo;re doing things on them. And so, uh, there\u0026rsquo;s also one other thing, which is that, um, so with some of the recent work going on with, uh, uh, dynamo capturing per parameter FSTP, we also are, uh, also allowing a storage mutation, um, namely, uh, you know, doing some sort of, uh, resize to zero on storages inside of the graph. This is mostly so that we can promptly cause a storage to get deallocated because we know that the memory is definitely not going to be used, but we need to, there\u0026rsquo;s a bunch of references to it, live references to it from backwards, uh, say things saved from backwards. And those will just, we will refill the empty storages later when we\u0026rsquo;re coming back in backwards. But for now we want to deallocate it. Okay. That\u0026rsquo;s a very special, special case, but, um, post-grad ethics passes largely a functional IR. And, um, you know, actually inductor itself is not functional. Like inductor, when doing code generation has, uh, the ability to do mutation. It knows about control dependencies. It knows, uh, you know, when, uh, it is basically very mutation aware, but when you\u0026rsquo;re in the nice, happy FX graph universe, um, this is prior to us sort of going into the mutation world. So you get to do, you know, sort of nice, easy optimizations. And then once we\u0026rsquo;re done with the post-grad FX passes, that\u0026rsquo;s the point in which we re-inject all the mutations, go to a lower level IR, and now you, you got a reason about mutation because, you know, at some point in like, even, even in like a functional compiler, like a functional compiler, which isn\u0026rsquo;t like Haskell, at some point you\u0026rsquo;re going to actually start mutating things because that\u0026rsquo;s, what\u0026rsquo;s going to need to happen, uh, you know, at the hardware level. So, uh, you know, the whole point of the IR is, you know, once, when you\u0026rsquo;re higher level on the FX nodes, that\u0026rsquo;s when you can do sort of very, you know, loose reasoning, you can, you know, move things around. And then as things get successfully more and more refined, it\u0026rsquo;s harder to do these sort of higher level abstract optimizations because, you know, you have more constraints, but at the same time, it\u0026rsquo;s now possible to express optimizations that weren\u0026rsquo;t accessible before because we\u0026rsquo;re, we\u0026rsquo;re moving to a lower level things that, you know, previously you couldn\u0026rsquo;t really talk about now become expressible. Okay. So post-grad FX passes. So what do I want to talk about in this, uh, podcast? So one thing is I wanted to tell you that they exist because, um, it\u0026rsquo;s easy to, you know, forget that you can go ahead and do an FX pass. Most of the optimization work in inductor is not really an FX passes. Like the way to think about the post-grad FX passes are there sort of very domain specific optimizations for particular situations and the bulk of the smarts, you know, sort of the bulk of, you know, when you\u0026rsquo;re doing just average optimization in PyTorch 2 is happening during the lowering and the scheduling. Like that\u0026rsquo;s actually the bread and butter. This is very different from like a traditional optimizing compiler where like, you know, your graph optimizations really are the, you know, name of the game. Like you, you, you\u0026rsquo;re running very simple optimizations over and over again, you know, until you quiesce and like by doing lots of small things over and over again, you eventually end up with something that, you know, is very optimized. So that\u0026rsquo;s not really what is going on in the PyTorch 2 compiler. The graph passes are mostly like, oh, there\u0026rsquo;s a special thing and it\u0026rsquo;s very semantic. It requires some high level understanding and that\u0026rsquo;s what we\u0026rsquo;re going to do. So, okay. So I want to tell you that post-grad FX passes do exist, even though it\u0026rsquo;s not the bread and butter, because it is a really useful thing to be able to do in some situations. And the other thing I want to talk about today is what exactly are some of the post-grad FX passes. So to prepare for this podcast, I just popped open the post-grad Python file in inductor and just read through all the passes to get an idea for what\u0026rsquo;s going on. So let\u0026rsquo;s see what\u0026rsquo;s in here. So one of the first things we do is dead code elimination. Actually, this one\u0026rsquo;s not on by default. It\u0026rsquo;s controlled by a config flag, DCE in inductor config, and it\u0026rsquo;s off by default because apparently there\u0026rsquo;s some problem with inference mode, mutations, I don\u0026rsquo;t know. We do dead code elimination at a lot of points in the stack. AOT Autograd does some dead code elimination after it\u0026rsquo;s done functionalization. And the inductor scheduler also does dead code elimination. So if something\u0026rsquo;s dead, something will get rid of it, especially the inductor scheduler. That DCE pass has given me a lot of trouble in the past. So it\u0026rsquo;s not that important for the post-grad FX pass to do the DCE. Okay, what\u0026rsquo;s the next thing we do? The next thing we do is a pass called reorder for locality. So what\u0026rsquo;s the idea behind this pass? The idea behind this pass is let\u0026rsquo;s say I\u0026rsquo;ve got a node and it depends on some arguments. So let\u0026rsquo;s just look at these arguments. So like, okay, argument one. Okay, what\u0026rsquo;s going on with argument one? If argument one is solely used by, you know, my node, the node that is using it and any nodes that are after my node, then I know, well, you know, there\u0026rsquo;s no point in, you know, trying to compute this early, right? Because this node, this node is the first node, which actually makes use of this node. And so I can actually just sync this node as late as possible. I want to be lazy. I want to, you know, only do the nodes producer right before I\u0026rsquo;m actually going to use it. And that\u0026rsquo;s the sense in which this is a reorder for locality thing, right? I want to, you know, do the thing right before I use it because that makes it easier for me, for example, to do fusion. This is not always a profitable optimization, because if you have an operation where, you know, it depends on some big amount of data, and it\u0026rsquo;s like, say, a reduction, so then you have a small amount of data coming out at the other end, then you might not want to push this as late as possible, because you\u0026rsquo;d be keeping the input live until all the way in the end, whereas previously, you may have been able to do the reduction. And then now you only have to hold on to a scaler in that situation. So we actually only turn this on at inference. But, you know, it is a useful thing to do, you know, especially if you don\u0026rsquo;t really think the user made good choices about when exactly to go ahead and run things. The next pass that goes on is actually a custom hook. So we offer in the inductor config a way for you to plug in your own custom pre and post. Sorry, we give you a hook into post grad FX, and we actually give you two hooks, a pre hook, which runs, you know, before all of the pattern matching we do in post grad FX, and a post hook that runs after all of them has run. So you can just pass in a callable to the inductor config, and, you know, do whatever, you know, optimization you want to do, you know, go, go while, just, you know, remember the iron variance, right, we do allow mutation, but it\u0026rsquo;s always at the end. After the custom hook, we have the bulk of the passes that are going on here, these are the pattern matching passes. So as I said, you know, most of the graph passes we are doing are all about just, you know, looking for particular patterns of operations in the graph, and going ahead and substituting them with some other type of thing. And so what are some of the patterns that are going on? So I\u0026rsquo;m not going to actually talk about all of them. But I am going to talk about the ones that are specifically named in the source code, and, you know, some representative examples. So one of the ones that we\u0026rsquo;ve got that actually is its own thing, because it\u0026rsquo;s fairly complicated, is a pass called group batch fusion passes. So there are a few ways to explain what this is. But the probably the simplest is, let\u0026rsquo;s say you\u0026rsquo;re doing a matrix multiply, right? So if you\u0026rsquo;re doing just one matrix multiply, you just go ahead and call mm, and you\u0026rsquo;re done. But what if you\u0026rsquo;re doing, say, five matrix multiplies, and they\u0026rsquo;re just, you know, they\u0026rsquo;re doing all the same size input, they\u0026rsquo;re doing different inputs, different weights, different weights, different weights, but their sizes are all the same, right? So I could call the matrix multiply kernel five times. But what I could do instead is I could stack all of the inputs and the weights together into a single batched tensor, and then use a batched matrix multiply instead. And this can be, you know, a lot better for efficiency, because you can get, you know, much better occupancy now that you see all of the work as opposed to only some of it. So group batch fusion does a bunch of optimizations along these lines. We also have a library called fbgem, which is used pervasively internally that has all sorts of, you know, fusion stuff. And another thing it has is like a gmm op, which actually lets you fuse together matrix multiplies that don\u0026rsquo;t necessarily have the same shape. I don\u0026rsquo;t really know how this works. But, you know, this is something you can do. The batching matrix multiplies together is a very, very common optimization. Actually, if you, you know, ever have looked at the transformer architecture, you know, when you do the QKV matrix multiplies, they are typically batched together. And yes, you know, hypothetically, you could not batch them, but, you know, they do want to batch together. Although, you know, you want to use STPA anyway. So, you know, that\u0026rsquo;s what you should use in that case. But we do lots of matrix multiplies and, you know, fusing them together can be a useful thing to do. Another one of the passes we have is called remove no ops. So this one\u0026rsquo;s pretty easy. It says, hey, if you are doing an operator, but actually the operator does nothing, get rid of it. So, for example, if I have an int64 tensor and then I say convert it into an int64 tensor, remove no ops says, okay, I can get rid of it. Now, you might be wondering, well, hang on. You know, that\u0026rsquo;s really trivial. Why didn\u0026rsquo;t we just get rid of it in decompositions? And the reason we didn\u0026rsquo;t get rid of it in decompositions is actually because it\u0026rsquo;s not, it\u0026rsquo;s technically not a no op. And the sense it\u0026rsquo;s not a no op is that let\u0026rsquo;s say that you did one of these, you know, like conversions on an int64 tensor and it gave you a int64 tensor out. That output tensor doesn\u0026rsquo;t alias with the input. It\u0026rsquo;s actually a fresh tensor. We guarantee that it\u0026rsquo;s that case. We do have operators like dot2 and dot contiguous, which are, which have the possibility of giving you back, you know, the exact original tensor. But these actually are all our composites. And as composites, they actually have to decompose before we get to, before we get to inductor into an actual op that, an actual op that does the work that unconditionally does the conversion or not. So in some circumstances, you know, you might have ended up, ended up directly calling the underlying, we always do this. And so now we need to get rid of it. And remember, the fx graph in postgrad is purely functional. It doesn\u0026rsquo;t have mutation. And so we can just change the aliasing relationships of things willy nilly without worrying about, you know, if that is going to change the observable side effects of mutations in the graph. So that\u0026rsquo;s pretty cool. Remove no op ops. That\u0026rsquo;s another pass. We also have a big pile of just graph patterns, which just say, hey, if you see this particular pattern of nodes in the graph, replace it with, you know, this other pattern. And this is, I mean, some of these are pretty good. A lot of these are kind of like benchmark hacking. It\u0026rsquo;s like, oh, you know, we were looking at some model. Why is it slow? Oh, because there\u0026rsquo;s this, you know, pattern of code and we can\u0026rsquo;t really generate good code for it. So let\u0026rsquo;s just rewrite it in something that\u0026rsquo;s a little better. One, one that I noticed while I was preparing for this podcast is a like cum sum optimization, which is like, hey, if you\u0026rsquo;re allocating a tensor of constants and then you\u0026rsquo;re doing a cumulative sum on them, we don\u0026rsquo;t need to. We don\u0026rsquo;t need to actually allocate the tensor and do a cumulative sum on them. Like I can just constant fold that into a tensor in a range style tensor where I just, you know, go ahead and directly increment things, you know, stuff like that. Like, yeah, in some sense, you know, maybe people should have written their models a little more carefully and then we didn\u0026rsquo;t have to write these patterns. But, you know, we put a bunch of patterns in here. The other really big set of pattern matching passes we have are the split cat patterns. So what is split cat? So split cat is a situation that occurs a lot in the recommendation models we care a lot about inside meta internally, where I\u0026rsquo;ve got a packed tensor that contains, you know, a lot of, you know, typically it\u0026rsquo;s not a, it\u0026rsquo;s not a dense tensor. It\u0026rsquo;s actually some sort of ragged tensor with a ragged dimension where I have a bunch of sparse features and they\u0026rsquo;ve all been concatenated together. And so one of the common things that I need to do is I need to do some processing, maybe only on a few features, not all of them. And the most convenient way to write this when you\u0026rsquo;re writing normal PyTorch code is to go ahead and split the tensor into a bunch of itty-bitty tensors, do the operations on the, you know, few tensors that you actually care about, and then cat them all together back into the fused thing. So, you know, this can, as you can imagine, is quite inefficient. And so there\u0026rsquo;s a bunch of things that, you know, we can do to, like, make this more optimized. And so this is the, like, the split cat category of optimizations. So that\u0026rsquo;s all the pattern matching optimizations I want to talk about. There\u0026rsquo;s still a few more optimizations that don\u0026rsquo;t fall into the category of pattern matching. There\u0026rsquo;s an optimization for fusing DDP communication. So what\u0026rsquo;s the idea behind that? Well, the idea is that, you know, when you are doing distributed data parallel, one of the things you\u0026rsquo;re doing is, you know, as you are doing your backwards, you want to do an all reduce on gradients to, like, get them all together into all of the nodes. And so if you have multiple gradients, then you don\u0026rsquo;t actually need to do them as separate all reduces. You can fuse them together into a single all reduce and do it all at once. So concatenate and then all reduce. We typically write this pattern. When you\u0026rsquo;re writing eager mode PyTorch, we typically, you know, manually make sure we do the fusion ourselves because there\u0026rsquo;s no compiler involved. But you can write your, you know, distributed code a lot more simply if you\u0026rsquo;re just like, well, you know, just do the all reduce whenever it\u0026rsquo;s necessary. And in compiler, we trust to go ahead and fuse the communications together. So this is the path that goes ahead and does that. And we have some other ones like moving constructors to CUDA. So the idea behind this is if you allocate a tensor, you know, a fresh new tensor, like a constant or whatever, and then you go ahead and move it to CUDA, that\u0026rsquo;s pointless. Just go ahead and allocate it on CUDA directly in this case. So that\u0026rsquo;s a bunch of passes. You know, that\u0026rsquo;s actually all of them. Like, you know, every single pass. Like if you look at this file as of April 2024. That being said, there are a few special passes, which are especially interesting. And they happen at the very, very end. And they have to happen at the very end because they do the thing that I was talking about, which is reintroduce mutation back into the graph. So all the other passes I talked about don\u0026rsquo;t, you know, do mutation. They may need to be a little careful around the copy underscore nodes at the end of the graph. But in general, they are, you know, ridden with the idea that you\u0026rsquo;re dealing with a purely functional graph. And so the special passes, there are two of them. So one of them is to re-enplace in place of lobs. So if I\u0026rsquo;ve got a tensor and I compute the add of it, like I compute add plus two on it and I have a new tensor, and now my old tensor is never used again, I can just turn that into an add underscore. Similarly, we have a pass which decomposes so-called auto-functionalized operators. What is an auto-functionalized operator? So let\u0026rsquo;s say that you have a custom operator and your custom operator only has a mutable version. So all it does is it takes in some tensors, mutates them, and then bam, you\u0026rsquo;re done. So when we want to represent this in one of these purely functional graphs, we need a functional representation of this operator. We don\u0026rsquo;t force users to go ahead and, you know, write the functional op representation because it\u0026rsquo;s very boilerplate-y. It\u0026rsquo;s basically, okay, we\u0026rsquo;ll allocate the outputs as necessary and then pass the outputs into the mutating one so that, you know, they get mutated and then return them. So we don\u0026rsquo;t force you to write that boilerplate. We have a auto-functionalized high-order op that takes one of these mutating ops and turns it into a functional op. But of course, you know, this functional op is a lie, right? All it\u0026rsquo;s really doing is, you know, allocating the outputs and then passing them in into the mutating kernel in the end. So, you know, if you know what the outputs are, then there\u0026rsquo;s no need to actually go ahead and, you know, do this functional op. You just want to turn it back into the original mutating op so there\u0026rsquo;s none of this overhead in this case. So when you are doing these special passes, now we\u0026rsquo;re actually reintroducing mutation to arbitrary points in the graph. And so we do really need to know about aliasing information. For example, I can\u0026rsquo;t re-emplace an implacable op if there is another alias to it, you know, somewhere else. Because I, you know, the semantics are that the alias is the original thing because that\u0026rsquo;s what my graph had. My graph didn\u0026rsquo;t have any mutations in it. And so to make sure we actually have this information, we actually, you know, need to look at the fake tensors that are stored as metadata on the FX nodes. So inside the FX graph that we\u0026rsquo;re processing, every node is annotated with a fake tensor, not only saying what the shape and the D type and, you know, regular metadata of the data at that node is, But also recording accurate aliasing information by way of the storages associated with the fake tensors. Fake tensors have storages and you can actually say, hey, are two fake tensors, do they have the same storage or not? And that tells you if two nodes aliased or not. This is like super, super simple alias analysis. It bypasses a lot of the complexity that you would have to do in a normal compiler because we say you get one alias pattern. It is exactly this alias pattern. It cannot change. We don\u0026rsquo;t have, you know, anything like, oh, you\u0026rsquo;ve got two inputs. Do they alias? Do they not alias? Who knows? We always specialize on the aliasing pattern of the inputs. So, you know, we need accurate fake tensor information to actually do the reemplacing because we need to know whether the storages are right. And in the old days, we used to just rerun fake tensor propagation on the FX graph before doing these passes. But these days we have this thing called the fake tensor updater. And all it does is it says, well, you know, most of the optimizations that are happening in the graph are local. So, you know, you can go ahead and do your optimization and, you know, be a little sloppy about, you know, actually remembering to put in updated fake tensor metadata. And then we\u0026rsquo;ll just go ahead and, you know, compute. Recompute only the fake tensors for the areas of the graph that changed until, you know, we\u0026rsquo;ve reached a fixed point in the graph. And, you know, that\u0026rsquo;s pretty nice and saved us a bunch of compile time. Okay, so that\u0026rsquo;s our whirlwind tour of post-grad FX passes. One last thing I\u0026rsquo;ll note, most of the passes are kind of poorly documented. Sorry, that\u0026rsquo;s what happens when you write a compiler really, really fast. The commit messages tend to be pretty good, though. So if you\u0026rsquo;re, like, looking at some code and you\u0026rsquo;re like, oh, what does this do? There\u0026rsquo;s no comment. Go look at the history, see who added it. There might be a pretty good explanation of what the heck\u0026rsquo;s going on. Okay, that\u0026rsquo;s everything I wanted to talk about today. Talk to you next time.\nEP81 Higher-order-operators Higher-order-operators Hello everyone and welcome to the PyTorch Dev Podcast. Today I want to talk about higher order operators in PyTorch as well as in PyTorch 2. Now the name higher order operators is a slight misnomer because technically you can use them to build any sort of operator, not just higher order ones, but their primary use case is all about control flow, so it sort of makes sense. What do I mean by higher order operator? Well in functional programming we typically refer to higher order as some sort of function which itself takes a function as an argument. And as you will see many higher order operators in PyTorch are in exactly this way. They are operators that take other operators or even graphs of computations as arguments to do various operations that you might want to do. So why do higher order operators exist? Well there are a few reasons they exist. So one is that in PyTorch 2 our general mode of operation is we only support straight line code. So you know you can write Python code, it can have loops, it can have conditionals, all sorts of random stuff. But by the time it gets to our compiler, we have flattened away and inlined all the control flow so that we have a single straight line graph, one op, then the next op, then the next op. And this is really great for the compiler optimizations we want to do because straight line code is very easy to optimize. You know, you don\u0026rsquo;t have to do any sort of control flow analysis or anything like that. But it\u0026rsquo;s kind of annoying for users sometimes because sometimes, you know, you really, you really, really need that control flow op. You know, really, really, really. And maybe for some reason, you don\u0026rsquo;t want to just have that control flow be implemented in host Python. Because remember, PyTorch 2 is integrated with regular PyTorch in Python. So you can, you know, seamlessly transition in and out of compiled regions, and regular, you know, Python control flow. So maybe you have some reason you want to have the control flow inside of your model itself. For example, if you\u0026rsquo;re exporting a model, well, you know, you\u0026rsquo;re not going to bring Python along for the ride in that case. So you need some sort of way to represent this sort of thing. And higher order operators are a way of defining control flow operators, so that you can go ahead and actually write an operation, which will be recognized by all the pieces of the stack. And then you can actually use it. And it then it turned out that we could also use higher order operators to do a bunch of other things. And I\u0026rsquo;ll talk about each of the potential use cases. So what exactly is special about a higher order operator? So there\u0026rsquo;s a few important things. So one is that when you traditionally think of what we call an operator in the PyTorch 2 compile stack, that is to say some sort of thing in torch.ops. Normally, it has a restriction on what arguments it\u0026rsquo;s allowed to take. Specifically, this restriction is exactly the torch script schema restriction. So back in the day, when we had torch script, our first compiler for PyTorch, there was some restriction on what kinds of arguments you could take. So you could take tensors, you could take ints, you could take list of ints, but you couldn\u0026rsquo;t take a dictionary or an arbitrary Python object or a callable. So one of the reasons why this restriction existed is because operators need to interface into C++ code. And obviously, we can\u0026rsquo;t have, you know, random Python objects leaking into C++ that just doesn\u0026rsquo;t work all that well. So higher order operators are sort of an extension to the PyTorch operator mechanism. So they behave the same way as regular operators, you could implement a regular operator as a higher order operator, there\u0026rsquo;s nothing forcing you, for example, to take a callable as an argument. But, you know, it\u0026rsquo;s, it\u0026rsquo;s going to be done entirely in Python. And because it\u0026rsquo;s done entirely in Python, you can pass higher operators, operators, any arguments you want. So there are benefits and downsides, right? The benefit is you have a lot more expressivity, you can now pass in, you know, an FX graph, you can pass in an operator, all sorts of random stuff that you couldn\u0026rsquo;t do. And the downside is that, well, basically, with higher operators, you have to do everything by scratch. And, you know, let me let me emphasize what I mean by everything. So normally, when you define an operator, like a normal operator in PyTorch, you get a bunch of things for free, you get serialization, you get autograd, especially if you do it in core PyTorch, if you do it as a custom operator, you know, it\u0026rsquo;s a little more work, you have to write an autograd function, but still all this sort of stuff makes sense is available for you. We even have things like functionalization, which lets us, you know, convert a mutating operator and a non-muting operator. So all this stuff you get for free, when you write normal operators, we sort of ask you for the minimum possible implementation needed. And that\u0026rsquo;s essentially all the user stuff, you know, all the stuff that we can\u0026rsquo;t actually, you know, figure out automatically for you, you need to give us. But if you write a higher order op, right, you\u0026rsquo;re saying, hey, you know, I\u0026rsquo;m going to do some special stuff with some special arguments. And my behavior is very, very custom. And so actually, I have to go and implement every single one of these transformations by scratch. So I have to say how to do autograd, I have to say how to do fake tensor propagation, I have to say how to do functionalization, I have to say how to actually run the thing, well, you\u0026rsquo;d expect to have to do that. All of these things, everything that the PyTorch dispatcher, you know, normally would handle when you\u0026rsquo;re doing an operator, you have to implement when you\u0026rsquo;re doing a higher order operator. And this kind of makes sense, because, you know, many of the sort of generic implementations we have in the dispatcher involves saying, hey, there\u0026rsquo;s a fixed universe of types. And I know how to handle every single type when I want to do something. For example, vmap. Vmap is our, you know, way of vectorizing operations. Vmap says, well, all I need to do to vmapify an operator is look at all of the tensor arguments. And then, you know, go ahead and extract out the batch dimensions from them. So I can\u0026rsquo;t do that. If you\u0026rsquo;re giving me, you know, some random Python objects, because I don\u0026rsquo;t know if there are tensor arguments lurking inside them. So that\u0026rsquo;s why you have to re-implement everything. And so really, actually, the implementation of higher order operators is very simple. Once you actually have done all the hard work of defining what you\u0026rsquo;re supposed to do on every step of the dispatcher, then all the higher order operator calling mechanism does is re-simulate the same, you know, sequence of dispatcher calls you would have gotten on a regular PyTorch operator just doing it entirely in Python. In fact, the main implementation mechanism for higher order operators, the Python dispatcher, was actually built for a different reason, namely that we wanted a little more customizability on the dispatcher from Python. The dispatcher is all in C++ for eager mode performance. And then, you know, it turned out, hey, we can actually use this to implement higher order operators. If you\u0026rsquo;re interested in more about the dispatcher, I have a really nice blog post on my blog about the dispatcher. And I think there\u0026rsquo;s also a podcast about it. So plenty of material on this. Okay, so that\u0026rsquo;s the sort of high level on higher order operators, you know, they let you put arbitrary arguments inside of your operations, including graphs and other operators, that\u0026rsquo;s why they\u0026rsquo;re higher order, and they\u0026rsquo;re pretty difficult to implement. So, you know, what exactly in the PyTorch2 stack, you know, involves higher order operators? So let\u0026rsquo;s just go through them, one by one. So for the first class is the class of control flow operators. And this is sort of the original raison d\u0026rsquo;etre for higher order ops. So you\u0026rsquo;ve got things like cond, map, while loop, they do what you expect, right? Cond lets you do a runtime conditional on data that you don\u0026rsquo;t actually know ahead of time. And you\u0026rsquo;ll branch to one side of the left, super useful when doing export, and also not too, you know, difficult to deal with from a static analysis perspective. We have the while loop, which lets you, you know, loop over and over again, doing an operation until some condition is true. This one, we have some restrictions, you know, they\u0026rsquo;re not unrestricted while loops, the, you know, the return type of the while loop has to be exactly the same every single time. You know, there\u0026rsquo;s restrictions on loop carrier dependencies. Actually, a lot of these are just sort of like very closely modeled off of their equivalents in the TensorFlow slash JAX world, you know, where, you know, they\u0026rsquo;ve been ahead of the game on us, because, you know, we didn\u0026rsquo;t really need used to need this in eager mode, you could just write a regular while loop in Python. So that\u0026rsquo;s a bunch of host side control flow, it sort of works the way you would expect. We also have device side control flow. So for example, when you want to do a cumulative sum, you know, that that\u0026rsquo;s just a, you know, built in operator in pytorch, torch.cumesum. But let\u0026rsquo;s say you want to do a custom reduction, how would you go ahead and implement that? Well, you might want to specify some sort of reducer function, maybe subject to some constraints like it having to be associative, because we\u0026rsquo;re doing some sort of like tree reduction type scheme. And so after you specify this reduction, which, you know, is probably a bunch of operators, you know, as I said, a bunch of operators, you want to wrap it up into a thing that actually turns it into a, you know, reduction operation. So cumulative sum is an example of a, you know, device side, you know, operation, we also have things that don\u0026rsquo;t really resemble control flow, but still require that, you know, give me some sort of function, and I will do something with it, inside the context of this function. So for example, something that we\u0026rsquo;re going to be releasing soon is templated attention kernels. These are pretty cool. This work is from Horace and from Driss. What it essentially does is, you know, there\u0026rsquo;s a standard attention kernel that we have written in Trident that, you know, you can just use directly if you call regular attention. But there are various things that you might want to customize, like the scoring and other things. And these things are embedded directly inside of the attention kernel. So you can\u0026rsquo;t just, you know, go ahead and tweak some arguments to the call of the attention. And what you actually probably want to do is pass in some custom functionality, you know, some operation on scalers that specifies what you don\u0026rsquo;t want to do. So similar to the cumulative sum case, you know, what we do is we define a higher order op, you can pass in some, you know, callable, which specifies a bunch of scalar operations you want to do on the inside. And then we\u0026rsquo;ll, you know, bundle this up with our template, attention template. And then you get a custom attention kernel that does all this stuff for you. And yes, in principle, you could have, you know, copy pasted out the Trident code, and then, you know, made the modifications you need. And in some sense, it\u0026rsquo;s not that hard, like there\u0026rsquo;s not that many lines of code you need to change. But you know, you kind of need to know Trident to do this, it\u0026rsquo;s kind of not so easy. So having something like template attention makes it a lot easier to just go ahead and do lots of variations on things you might want to do. We also even use this higher orderness, this ability to take in graphs, for things that aren\u0026rsquo;t even really control flow or like kernels at all. So for example, activation checkpointing is done as a compiler pass inside PyTorch 2. It\u0026rsquo;s not done by tracing out the eager implementation of activation checkpointing for technical reasons. And so to do this, we need to say some way of what the region we want to checkpoint is. And that\u0026rsquo;s a higher order operator, right? It\u0026rsquo;s a higher order operator that contains a graph that is just, you know, the stuff you want to checkpoint. And this doesn\u0026rsquo;t really have any runtime meaning. It\u0026rsquo;s just used by the compiler to control what we actually do. Okay, so that\u0026rsquo;s all of the like sort of control flowy, higher order operators, but there\u0026rsquo;s more. So I mentioned that higher order operators lets you pass arbitrary crap as arguments to the Python function. And so this is actually useful in a bunch of situations. So for example, Ose recently added the ability to write custom user Triton kernels and have them directly embed into the PyTorch 2 component. And so what exactly does this look like from a compiler\u0026rsquo;s perspective? Well, you write some Triton code, and then we need to somehow represent it inside of, you know, our compiled graph, which normally is just a bunch of Aten ops. And what these like Triton call kernels need to do is they need to hold on to a reference to the actual Triton code that you wrote ahead of time. Hmm, doesn\u0026rsquo;t sound like an int or a float. It\u0026rsquo;s not really a callable of operators. It\u0026rsquo;s just some random, you know, Python object. That\u0026rsquo;s the Triton representation. And so, you know, if you have a higher order operator, you can just go ahead and put it in that operator. And then we can preserve it all the way to Inductor. And then Inductor can do smart things with it because Inductor works at the Triton level and it can incorporate other Triton code into its code. Another example of sort of non-standard type arguments is our Torchbind integration. So Torchbind is an old, it\u0026rsquo;s from the TorchScript days. It\u0026rsquo;s a way of binding arbitrary user-defined classes so that you can call them inside of the compiler. A useful thing to want to be able to do and also useful in PyTorch2 context. And so, you know, when you make references to, you make references to these, you know, sort of user-defined objects, well, once again, these are strange arguments. They don\u0026rsquo;t, they\u0026rsquo;re normally not handleable by the dispatcher. So that\u0026rsquo;s also done as a higher order op. The final class of higher order ops, I would say, are what I call operator variants. That is to say, in principle, the higher order op is not necessary. We could have just manually written out a bunch of extra operators representing the thing you want to do, but you actually have to write a lot of custom operators. So instead of having to write a lot of custom operators, the higher order op lets you take an operator and turn it into some different variation, which, you know, wants to be treated as a single operator, but, you know, has some different semantics. So let\u0026rsquo;s give some examples. So one of the higher operator ops that does this is out D type. What does out D type do? It\u0026rsquo;s very simple. It says, hey, do this operation, and the output D type should be this, rather than the normal D type in this case. And so one of the, like, sort of primary reasons this is necessary is when you\u0026rsquo;re doing low precision matrix multiply, you may want to control what the output precision is, and in particular, have it be higher precision than the inputs, because that lets you do accumulations and higher precision. Maybe that\u0026rsquo;s just what you want in that case. So we could have just added another matrix multiply operator that has a out D type, you know, argument on it. But instead of doing that, and we actually argued about this a lot, because this is this is one of the serious proposals on the table, we instead introduced a higher operator that controls out D type. And like, basically, now you can use this on any operator. And actually, most operators don\u0026rsquo;t do anything special. But you know, if you need it for other things, you can just use it in that case. Another example is auto functionalize. So I mentioned that we have this thing called functionalization, which takes your graph with possibly mutating operations, and turns it all into non mutating operations. And so the problem with functionalization is that you need a pure version of the operator. So for everything built in in regular PyTorch, we have both a mutating version, like add underscore as well as the pure version, add without the underscore. But if you\u0026rsquo;re writing a custom operator, and you know, you just wanted to like, you know, mutate some of the arguments and you know, return some other arguments, you probably don\u0026rsquo;t want to go through all the boilerplate of writing the functionalized version. So auto functionalize just goes ahead and does it automatically for you, right? It just looks at the schema of your operator and is like, okay, these arguments are mutated. So let me go ahead and pre allocate, you know, buffers for them, so that they can get mutated into and then return them. And now you have a functional version of the operator in question, saving you from having to actually manually write these things out. And you know, auto functionalization kicks in, you know, early in the compiler stack, we do a bunch of optimization passes on the purely functional IR. And then what we actually do is we de-functionalize them, we replace them with the original operators. And that, you know, means you don\u0026rsquo;t actually pay any runtime cost for this. And finally, we have, and this is very new from Yidi and Richard, is effects support. And basically, what effects support is, is we there are some operations we want to support in PyTorch IR that have side effects, like printing, you know, things that are like for logging. And so in particular, we don\u0026rsquo;t want to reorder them when we\u0026rsquo;re doing operations. So there\u0026rsquo;s a bunch of ways you can prevent reordering, but our choice with the FX, the higher level FX IR, the functional IR, is that order doesn\u0026rsquo;t matter. Instead, we\u0026rsquo;re just going to manually insert fake data dependencies between, sorry, fake control dependencies, model as data dependencies between nodes. And if they\u0026rsquo;re just regular data dependencies, it\u0026rsquo;s just fake data dependencies, then the normal, you know, sort of respecting of data dependencies will make sure we don\u0026rsquo;t reorder things as well. So once again, when you have an operator, and you want it to like, actually, you know, have some more strict ordering requirement, well, now you need some new version of the operator that takes in this, you know, control dependency node, this token, as we call it, and then produces a new token that you can thread on to the next thing. So once again, a pain to actually write these operators all by scratch. So there\u0026rsquo;s a higher order operator that does this wrapping and then also adds the token to the input and output. So that\u0026rsquo;s a whirlwind tour of all the higher order operators in PyTorch. There\u0026rsquo;s a bunch of them. They are sort of important because, you know, if you\u0026rsquo;re an export backend, you kind of need to know how to deal with higher order operators. They\u0026rsquo;re very, very custom. So we try not to add too many of them. But you know, they\u0026rsquo;re a bit too useful not to use. So we are using them relatively frequently. That\u0026rsquo;s everything I wanted to talk about today. Talk to you next time.\nEP82 TORCH_TRACE-and-tlparse TORCH_TRACE-and-tlparse Hello, everyone, and welcome to the PyTorch Dev Podcast. Today, I want to talk about Torch Trace and TL Parse, our structured logging framework for PyTorch 2. You may have already heard of Torch Logs, which is a very nice developer-oriented feature that gives you debug logging for the PyTorch 2 stack. Traditionally, PyTorch didn\u0026rsquo;t have very much logging, but we found it very useful when working on a compiler because compilers are complicated, and so we have a lot of logs and you can use them to get useful information. However, what we noticed was that for bigger jobs, big and complicated models, the amount of data that we got from Torch Logs was actually too much. It was very difficult for people to find the information they needed. For example, if you\u0026rsquo;re running Dynamo in debug mode, you get a line of debug logging for every single bytecode you process. So as you can imagine, that\u0026rsquo;s pages and pages of bytecodes for large models, and suddenly you just can\u0026rsquo;t figure out, you know, did this graph break, whatever. There was another problem, which is that when we were running models on clusters, sometimes there would be bugs, and people would say, hey, you know, I ran my PyTorch 2 model, and it crashed or it had some problem, you know, please take a look. And we\u0026rsquo;d want to take a look, but one of the things we\u0026rsquo;d want to look at in the situation is some of the generated code that Inductor had or some of the intermediate code, and we had no way of getting at it because even though we have things like Torch compiled debug, which dumps all of the intermediate products to disk, if you\u0026rsquo;re running on some cluster, you know, it can often be inconvenient to actually get those things, because by the time the user has come to you with their problem, all of the machines that the job is running on have already been released, and their file system is scrubbed, and you no longer have access to any of the logs. So TL parse was born out of this problem. Originally, my idea was, hey, you know, what if we had a log parser for PyTorch 2 logs, and, you know, we\u0026rsquo;ll just go ahead and, you know, take all of the plaintext logs we\u0026rsquo;re generating and then parse them into something useful. But it turned out that we have lots and lots of logs. And also, it\u0026rsquo;s not so easy to tell apart our logs from other logs. And there were a bunch of other logs, and we ended up having gigabytes of log files, which didn\u0026rsquo;t even have the information we wanted. So TL parse and Torch trace are now two pieces. So part one is it is a structured logging mechanism. So unlike the plaintext logs, the logs that Torch trace emits are much more structured. They are emitted as JSON. And there\u0026rsquo;s only a few of them, basically things that we thought would actually be useful for TL parse. So when you run PyTorch 2 program with the environment variable torch underscore trace, you\u0026rsquo;ll get these trace files, which are structured logs. And then you can feed them to part two of TL parse, the log parser, which is written in Rust and just lets you, you know, go ahead and take those JSON and output a nice formatted HTML document for this sort of thing. Actually, there\u0026rsquo;s, you know, this is a very modular system. You can use the structured logs for anything else you want. So for example, I did a Easter hack called TorchDBG. It\u0026rsquo;s just a little, you know, time travel debugger that lets you get a trace of a model and then look at things in a React UI that lets you, you know, forward step, backward step. And I actually use the structured logs mechanism. So I didn\u0026rsquo;t use the regular torch trace logs. I, you know, added my own custom structure tracing log from a dispatch mode. And I didn\u0026rsquo;t use the log parser, TL parse. I actually made a custom React UI that read in the structured log format. And, you know, just, you know, structured logs are useful, right? They can be used for a lot of things. And in this case, I just use this container format for this other use case. But we\u0026rsquo;re going to talk about torch trace and TL parse because that\u0026rsquo;s actually useful. And TorchDBG is just a fun side project that I don\u0026rsquo;t really know what to do with. Okay, so let\u0026rsquo;s talk a little bit about what kind of structured logs, you know, torch trace emits. So there are a few things that we emit. So one is that every time we do a compilation, we emit a compilation metrics, which basically says what happened? You know, how many operators do we compile? Did the compilation succeed, fail? Did we restart? You know, basic metrics like this. We actually also send these to our internal structured logging system at meta so that, you know, we can go ahead and query them. But it\u0026rsquo;s pretty handy to just have these useful, available for a single log, for a single run. So you can look at it in all place. The other thing in the log is all of the compilation artifacts. So, you know, when Dynamo is done executing and it\u0026rsquo;s generated in FX graph, we dump that to the torch trace log. Then for each of the intermediate FX passes, AOT autograd, inductor passes, we dump their FX passes. We also dump the final inductor generated code, you know, the Python code that\u0026rsquo;s got trident code in it. And, you know, bundling all this together, you know, you\u0026rsquo;re mostly interested in, you know, any particular compilation. So we have this thing called compilation IDs. And these let you identify distinct compilations that happen in the system. They\u0026rsquo;re numerically ordered. And so they usually come in the form of X slash Y, where X is the particular frame we\u0026rsquo;re compiling. So, you know, if you\u0026rsquo;re compiling function F, maybe that will get the number zero. Then when you compile G, you\u0026rsquo;ll get the number one and so forth and so forth until you have all the compiled frames. And then the second number, Y, tells us the number of recompiles we\u0026rsquo;ve done on this frame. because sometimes we\u0026rsquo;ll compile a frame multiple times because of guard failures. So then you\u0026rsquo;ll get 00, 01, 02, 03, and so forth. Actually, there\u0026rsquo;s a third number. It\u0026rsquo;s appended at the end, underscore blah. And that happens when Torch Dynamo restarts. So you also get to see restarts of analysis in Torch Dynamo. So that\u0026rsquo;s it. That\u0026rsquo;s basically all the structured logs we actually emit. So these get put into a log file in JSON, and then they get sent to TL parse, which actually does some sort of visualization. So what exactly does TL parse do? Well, it\u0026rsquo;s also pretty simple, okay? So one of the things it does is it is an HTML file, and HTML files mean we can, you know, do things like hyperlinks. So instead of, you know, bladding all of the intermediate products into one giant log file, which is what you would normally get if you were doing regular plain text logging, I can just create separate files for each of the compilation artifacts, and then you can click links to get to them. It\u0026rsquo;s not, it doesn\u0026rsquo;t sound like much, but it\u0026rsquo;s a huge, huge difference for readability. Another thing that we do, and this one\u0026rsquo;s pretty neat, and I like it a lot, is we build a stack try of all of the compilations that we did. So remember, a try is a data structure where, you know, if you have a bunch of, you know, strings, strings in the computer science sense, where, you know, they have shared prefixes, and then at some point they diverge, a try lets you, you know, sort of put them all into a tree-like data structure where shared prefixes share, you know, a path, and then once the strings diverge, then they go down different paths in the tree. Well, a try doesn\u0026rsquo;t have to operate only on strings, you can have a generalized try. So our stack try operates on frames instead of characters. So for every shared frame in any given stack trace we have, they get the same, you know, node representation inside of the stack try, and then when things diverge, we, you know, actually branch the tree. So this gives you a really nice bird\u0026rsquo;s eye view of all the compilations that happen. I\u0026rsquo;ve definitely debugged problems involving, you know, for example, Dynamo trying to compile too many things. I\u0026rsquo;m just looking through the stack try and being like, well, do I expect these stack traces and looking for something that looks out of place? Like, oh, somehow we\u0026rsquo;re compiling something that was triggered from an import statement inside of some random code. And then, you know, I know, oh, that\u0026rsquo;s the problem. Normally, if I just have a big pile of stack traces, one for every compilation, that might be pretty hard to find the needle in the haystack. But the stack try compresses away all the redundant information so you only see things. There are lots of other possibilities for the visualization. And one of the reasons why we have this two-step architecture where you generate structured logs in JSON and then you have a separate log parser is so that we can iterate on log parser separately. So, you know, it\u0026rsquo;s kind of difficult sometimes to update the trace generation code because that\u0026rsquo;s often associated with some deployed, you know, version of PyTorch, some packaged binary that you can\u0026rsquo;t easily update. But once you have one of these traces and assuming it has all the information you need, you can just download it to your local machine and then keep iterating on TL parse until you have some sort of format that, you know, looks like what you want. Some quick brief things about a log format design. So as I said, it\u0026rsquo;s a structured log where the structure is just JSON for human readability. Really, the main idea behind this log format is it\u0026rsquo;s designed to interoperate with your logging system. because if you\u0026rsquo;re, you know, doing any sort of, you know, infrastructure work with PyTorch, you\u0026rsquo;re actually running jobs, you probably have some way of actually capturing logs already and putting them somewhere. So Torch Trace is designed to piggyback on that. Of course, these log files are, you know, put in separate files away from the rest of your regular logging. But assuming you have some way of sending things to your logging store, you just need to point those files at your logging store and you can store them. So in particular, for example, we don\u0026rsquo;t actually, you know, allow for arbitrary sized, sorry, we try not to generate lines that are arbitrarily long because often, you know, regular logging systems can\u0026rsquo;t handle that. So that\u0026rsquo;s like one of the things. Some other things are that, you know, we do intern strings. So we generate a string table to reduce size. This is mostly useful for stack traces which are very, very repetitive. The traces are still pretty repetitive though. So, you know, I do recommend gzipping them if you can. And finally, how exactly did we design the JSON format? So it was mostly co-designed with Rust-30 JSON which, you know, basically is able to conveniently deserialize JSON objects into Rust structs as long as the structs have some particular way. And the most important thing to know is we do protobuf style unions. So whenever we have a message where there are multiple possibilities, we just have fields that optionally contain all of them, one per possibility. This means that it\u0026rsquo;s possible to have multiple fields set even though this is technically illegal because it\u0026rsquo;s supposed to be an enum. But it\u0026rsquo;s really good for backwards compatibility because you can always add new possibilities by just adding new fields. How exactly did we design TL parse? So one thing that I did was I didn\u0026rsquo;t implement it in Rust. I\u0026rsquo;m actually a little unsure about whether or not, you know, Rust is the right program for this. So originally when I wasn\u0026rsquo;t, when I didn\u0026rsquo;t have a structured trace mechanism in PyTorch itself, I was planning to parse regular logs. And parsing regular logs would have been a problem because we tended to get gigabytes and gigabytes of them. So it would have been prohibitively slow to actually implement them in Python. So Rust is really good at these like command line, you know, text processing applications. The one thing that\u0026rsquo;s a little awkward about using Rust for this program is there\u0026rsquo;s often a bit of iteration needed in, you know, what exactly I want for the design. And, you know, sometimes Rust, you know, Rust wants you to like do a bunch of refactoring to get all your lifetimes right. So that\u0026rsquo;s kind of irritating sometimes, but it\u0026rsquo;s not, it\u0026rsquo;s not a huge deal. And I\u0026rsquo;ve definitely been able to add stuff fairly quickly when I needed to. The other thing is that, you know, James Wu, who has also been helping with the development of TL parse, we have a little bit of structure for artifact parsing. So instead of just having a giant command if statement of doom, we have a trait for artifact parsers. So basically every time you add a new artifact, you just write a new parser struct and define a trait for it that says how to do the parsing. And this is a little bit of structure. You basically can cargo cult it if you want to write a new trait. That being said, I\u0026rsquo;ve noticed that, so the original way I designed the JSON format was I like basically did a separate struct for every single message type that I wanted to do. But I think that\u0026rsquo;s probably a mistake. It\u0026rsquo;s probably better to have a single generic like artifact text format. And what that means is that I can just easily add new structure traces that generate more artifacts without also having to update TL parse at the same time, which is what I currently have to do because everything gets their own special snowflake enum. If you work at Meta, we actually have an internal version of TL parse, so you don\u0026rsquo;t have to download and install the regular one from pip. And it has some niceties like it knows how to talk to our internal job systems and download the logs directly from there so you don\u0026rsquo;t have to download it yourself. Very, very convenient. You just paste in the URL and it does everything for you. Okay, so what\u0026rsquo;s next for TL parse? So I think the main thing I\u0026rsquo;ve noticed is that as I use it to debug problems in production, there\u0026rsquo;s a lot of small bugs that just sort of become obvious when you\u0026rsquo;re dogfooding. For example, one recent site outage I was helping debug was we were trying to figure out what was wrong with stack. and there\u0026rsquo;s something wrong with stack try, which is that we\u0026rsquo;re actually, the stack try is only supposed to show user frames, but in some situations it also shows a dynamic compile frames. And this was very confusing. I thought, oh, are we compiling these things? But actually the answer was no, we\u0026rsquo;re not actually compiling those things. You know, they\u0026rsquo;re just showing up for some reason. So, you know, that\u0026rsquo;s something to figure out. And there\u0026rsquo;s some cases where we\u0026rsquo;re just missing stack traces where it\u0026rsquo;d be helpful to have a full stack trace. So, there\u0026rsquo;s always improvements to do for the TL parse UI. And if you like Rust and, you know, you like PyTorch, you know, this might be a fun little thing to work on. For example, right now we do know whether or not compilations succeeded or failed because you can see them, you know, by looking at the compilation metrics page. But in the stack try, it doesn\u0026rsquo;t tell you. It just gives you a bunch of links to the various things. So, you have to click on the particular compilation ID you want and then click on compilation metrics to find out if it actually compiled or not. So, you know, just inlining that information in the stack tree, that seems like a useful thing to do. These are very easy to work on because once you have a trace and it\u0026rsquo;s not too hard to generate a synthetic trace, you can just, you know, make some changes, you know, run the program and then see what it looks like. It\u0026rsquo;s very pleasant. We also, you know, can add more structured traces to PyTorch. This is also something that, you know, I often, you know, think, oh, you know, it would be nice if I had this information. One thing to be careful about is we don\u0026rsquo;t want to add too many things because then the log files get very large and because it takes longer to parse them and it\u0026rsquo;s more load on the storage system. But there is a bunch of stuff that currently is only available in text logs and I think would be pretty useful to have available. One thing in particular that I, you know, care a lot about is symbolic shapes logging. So I haven\u0026rsquo;t figured out exactly how I want to put it into TL parse, but there\u0026rsquo;s definitely something here that I want to put in. And finally, one thing for like us internal users is we have a lot of models that we torch compile that are actually dynamically generated. So what happened is we used a torch dot fx to sort of generate a IR and then we generated Python code for it and then we\u0026rsquo;re dynamoing into that Python code. So this Python code doesn\u0026rsquo;t exist anywhere in the source, the source file system, you know, they\u0026rsquo;re just completely generated on the fly. And so if you have errors in this source, you\u0026rsquo;re just like, what the heck is in this source code? I have no idea. There is no access to it. So it would be really nice to actually dump that to TL parse. So so TL parse could show it for you when you have the stack traces. There\u0026rsquo;s some fiddly bits in implementing this, so I\u0026rsquo;m not exactly sure. So there you have it TL parse. So if you work at meta and you\u0026rsquo;re listening to this podcast, TL parse is really, really useful. So if you haven\u0026rsquo;t tried it already, the next time you have some sort of problem, even if you\u0026rsquo;re like debugging unit tests, you know, just say torch trace, blah, and you can get out a trace and TL parse it and look at it. I promise you it actually is really, really useful. And if you don\u0026rsquo;t work at meta, you know, I think TL parse could still be useful. You know, you some of the integrations don\u0026rsquo;t exist yet, but you know, as I said, you can just run it, take a look at things, you know, you might be surprised by what you could find out about your model. That\u0026rsquo;s everything I wanted to talk about today. Talk to you next time.\nEP83 Compiler-collectives Compiler-collectives Hello everyone and welcome to the PyTorch Dev Podcast. Today I want to talk about compiler collectives, a new feature in PyTorch 2 compilation which allows the compiler to communicate to other instances of the compiler on other ranks in distributed training in order to communicate information that may be useful to other nodes in the training. To explain why compiler collectives are useful, I first need to recollect a particular problem that we encountered in our production deployment of PyTorch 2 inside meta. The problem looks something like this. Occasionally we would have jobs that were running with PyTorch 2 enabled and they would nickel timeout. Now nickel timeouts occur whenever you have a nickel collective and some of the collectives just have to wait too long for a result and there\u0026rsquo;s a timeout because one of the reasons, common reasons why you you know wait too long is because there\u0026rsquo;s a deadlock or it\u0026rsquo;s never actually going to finish. So we have a timeout to make sure we actually kill the nodes and make sure we release resources in this situation. So in this particular case we were nickel timeouting and the first thing you do when you have a nickel timeout is you go and look and you see what the heck all the jobs were doing at the time they crashed and we noticed that in this particular case some of the jobs were doing compilation. Now why were some of the jobs compiling code in Torch Compile while other ranks were just you know waiting in the network collective so using TL parse a log parser that we have for PyTorch 2 which can tell you what was going on on all the nodes see a previous podcast for this for more information we noticed that the ranks that were compiling were actually doing an extra recompilation that the other ranks were not so some of the ranks had just gone ahead and run the code and got in all the way to the collective and this poor unlucky rank was actually recompiling and further inspection of the trace revealed that the reason why this rank had decided that it needed to recompile was that there was some particular input to one of the graphs that it had compiled and that input had changed and the graph that the node had previously compiled was static it had thought that the size of the node size of the input at that location was static and when you compare this to the other nodes those other nodes had already compiled a dynamic node for them so actually what had happened here is a consequence of something that we call automatic dynamic so automatic dynamic in PyTorch 2 says hey we don\u0026rsquo;t know whether or not your inputs are static or dynamic unless you explicitly tell us so if you don\u0026rsquo;t tell us we will assume that all of your inputs are static and then depending on what we see at runtime if you pass us a tensor with size five and then you pass us a tensor of size seven on the second run we will realize oh actually it looks like you want this to be dynamic and we will recompile recompile your graph so that it is dynamic in this case so the problem is that most of the other nodes had gone in particular inputs that had varied between the first run and the second run say going from five to seven and they had all recompiled with that input being dynamic but the unlucky node the node that was actually recompiling at the time of the nickel timeout had actually unluckily gone in an input of exactly the same size both instances and so it had happily assumed well you know let\u0026rsquo;s just keep it being static and only got caught out not being prepared to deal with it at the end of the next run when they suddenly had to recompile so when we first ran in this problem i thought oh my god automatic dynamic was a mistake except that i it\u0026rsquo;s not really a mistake because um if we didn\u0026rsquo;t have automatic dynamic then you know this model would not have compiled at all uh you know so it\u0026rsquo;s like you know it\u0026rsquo;s a useful mistake but in some sense it\u0026rsquo;s architecturally and it\u0026rsquo;s a bit questionable because the whole point of spmd distributed training is you want all the nodes to be doing the same thing and so one of the things you want is you want all the nodes to be compiling at the same time it\u0026rsquo;s really bad if one of the nodes is recompiling even if we adjusted the nickel timeout so that we didn\u0026rsquo;t time out because if you wait long enough then the recompiling node will eventually get to the end and you will be able to make progress it\u0026rsquo;s still not optimal for you know this divergence to happen because all of the other ranks are waiting for this one straggler rank to finish compilation now there was a another ongoing problem with our production deployment where things that were supposed to compile in 30 minutes were actually taking two hours to compile so that really exacerbated the problem a lot in that particular case but still you know when we noticed this problem while it was kind of a interesting issue and it wasn\u0026rsquo;t entirely clear what we should do about it right should we go ahead and you know force a bigger timeout in this situation should we do something else and so the one solution that we settled on uh which was a balance of sort of being easy to implement and not requiring too many extra constraints from the user is this thing called compiler collectives so compiler collectors are an abstract idea the abstract idea is hey when i am doing compilation on my uh piter shoot process let me actually assume and this is a new assumption that everyone in the group in inside you know my training job is compiling at the same time now i can assume everyone is compiling at the same time then what i can do is during compilation i can do a collective to all the other nodes to basically tell them hey what\u0026rsquo;s going on so this is an abstract idea you can use this for all sorts of things but what we\u0026rsquo;re going to use it for and to solve this particular recompilation problem is this we are going to say hey have all the ranks talk to each other whenever you see an input a tensor input and when you see the tensor input i want you to tell all the other ranks what you saw the size of that tensor input and so in this particular uh in this particular case what happened was the input that was dynamic actually is variable across all of the ranks because it\u0026rsquo;s it\u0026rsquo;s some sort of like data dependent size uh this is like a recommendation model so there\u0026rsquo;s like a sparse feature going on and you know not all the ranks are getting the same sizes so when you have this situation where it\u0026rsquo;s unbalanced across all the ranks then if all the ranks talk to each other to try to figure out what\u0026rsquo;s going on they\u0026rsquo;ll say hey actually everyone has a different size for this so maybe even though this is the first time i\u0026rsquo;ve run and i don\u0026rsquo;t necessarily know what the size of the rank should be oh let me just go ahead and make it dynamic and more importantly because all the nodes are talking to each other we can ensure that they consistently decide whether or not a particular input should be dynamic or static so in this way we either never recompile or if every rank happens to be unlucky and sees the exact same size input iteration one and iteration two everyone recompiles at the next stage so hey like you know that\u0026rsquo;s not great but at least you\u0026rsquo;re not going to nickel time out because everyone is still doing the same thing now i slightly lied in this explanation i i suggested that you know we do a communication every time we see a tensor input but you don\u0026rsquo;t really want to do that right because communications are expensive you want to batch them together typically so what we actually do is we run the dynamo tracing process to the very end of the region we want to compile collecting up all the inputs we\u0026rsquo;ve seen along the way and then at that point in time we go ahead and do the collective have everyone talk be like hey you know here are the sizes of all of the inputs i\u0026rsquo;ve seen and then because um dynamic tracing is something you sort of can\u0026rsquo;t do retroactively you you need to like have made the decision to make something dynamic at the very beginning we just tell everyone to go ahead and restart your dynamo analysis and this time uh you know make decisions about whether or not inputs are dynamic or not based on this compiler collective we actually already have this restart capability we use this restart capability to deal with graph breaks because when a graph break happens um if we\u0026rsquo;re in some the middle of some inline call stack we actually don\u0026rsquo;t have the ability to graph break inside a nested user frame so we pop we need to pop all the way back to when the first inline function call happened but that involves in in full generality rewinding back arbitrary changes to the mutable state so instead of you know having to figure out how to reverse all that we just say okay whatever we\u0026rsquo;re going to start over again but this time we\u0026rsquo;re just going to stop immediately when we get to the function call so same idea we\u0026rsquo;re going to restart and then use our new knowledge to you know make different decisions when we\u0026rsquo;re compiling so compiler collectors are pretty cool um we actually uh you know i actually successfully ran them on the production model that you know sort of sent us down this goose chase in the first place and there\u0026rsquo;s there\u0026rsquo;s a really interesting consequence to it so not only does it solve the recompile problem which you know actually it happens pretty rarely like i don\u0026rsquo;t even know that i\u0026rsquo;ve actually necessarily solved it this is something that i\u0026rsquo;d have to actually you know run the real model about i have a synthetic test case that like it shows that it works but you know i don\u0026rsquo;t know definitively that it works for the real model but the thing is that because the compile uh the compiler is talking to each other even on the first iteration i actually can skip the stutter step that happens typically when you have automatic dynamic the stutter step being the very first time i compile it with static shapes and then the second time i compile it with dynamic shapes i don\u0026rsquo;t need to do that anymore because i figure out immediately that the shapes are all dynamic and this actually drops down compile time for this model from 95 minutes to 63 minutes so that\u0026rsquo;s pretty cool um the problem with this uh approach is that it\u0026rsquo;s not universally applicable right i said that we\u0026rsquo;re going to assume that every rank compiles at the same time but it\u0026rsquo;s really easy for me to have a valid spmd program with torch compile that doesn\u0026rsquo;t have this property like just say that i have you know one rank doing one thing another rank doing another thing and it just so happens that the first rank has one graph to compile but the second rank has two ranks to compile like there\u0026rsquo;s a kind of strange architecture and you know uh you\u0026rsquo;re definitely doing something unusual if this sort of thing happens but you know it\u0026rsquo;s possible and in this situation you can\u0026rsquo;t turn on compiler collectors because you\u0026rsquo;re just going to deadlock when um one of the compiled regions is trying to talk to the other ones fortunately the deadlock is pretty obvious because you know if you have any sort of uh you know when your job deadlocks and you go look at the stacks i i really hope you do have the ability to look at all the stacks when you\u0026rsquo;re thinking deadlocks like you know basic basic capability that you should have when doing distributed training um you just look at the stacks you\u0026rsquo;ll see someone was blocked in a compiler collective and you\u0026rsquo;ll be like okay yeah i guess that\u0026rsquo;s what happened but it it does you know give me a little trouble figuring out how i\u0026rsquo;m going to roll this out because right now in um nightly\u0026rsquo;s it\u0026rsquo;s it\u0026rsquo;s a configuration option it\u0026rsquo;s not on by default i actually want this to be on for most of the jobs we\u0026rsquo;re running but and it\u0026rsquo;s going to be a little bit of work to figure out how to roll it out okay so that\u0026rsquo;s basically it behind compiler collectors the the original pr is actually very simple um and i had to fix some bugs because you know there are some funny interactions but uh it basically um worked pretty well but i i do kind of wonder you know if this is the right approach and there are a bunch of other approaches that we thought about um which you know i just want to talk about briefly here because they they are kind of interesting alternate approaches so one of the other ideas that i had was hey you know why don\u0026rsquo;t you just mark dynamic the input in question and so i don\u0026rsquo;t have to like go through all of this rigmarole of you know doing compiler collectives to talk to each other to figure it out and sometimes i think this is exactly what you should do but in this particular model it\u0026rsquo;s actually um it\u0026rsquo;s not a single graph that\u0026rsquo;s getting compiled it\u0026rsquo;s actually um 10 sub graphs um five of which have non-trivial graph content and the particular graph that is getting recompiled is like embedded in the middle of like this you know opaque model that i don\u0026rsquo;t really know what it is i actually due to some vagaries in our environment i can\u0026rsquo;t even edit it directly it\u0026rsquo;s like produced as a side effect of some other compilation process yeah yeah this is kind of a crazy thing to do but like that\u0026rsquo;s just the place we are um for this particular model so it\u0026rsquo;s not obvious where to put the mark dynamic because where to put it depends on you know where the graph breaks that dynamo decided to put were and in general that\u0026rsquo;s like not well defined another idea that i\u0026rsquo;ve had in the past about automatic dynamic is this thing where we have to run it once and then we run it again to figure things out it\u0026rsquo;s always been sort of a stick up my back like uh like can\u0026rsquo;t we just like record this somehow and then the next time around just do the right thing like seems pretty reasonable right and uh you know if you imagine some sort of like profile guided optimization setup right the way profile guided optimizers in uh traditional compilers work is you just you know you run your program you get a profile you put it up somewhere and then compiler uses it and optimizes your code and if the profile changes if the code changes right then the profile might be a little bit stale out of date and your compilation might not be as good but you know as long as you refresh the profile then things will be good again so yeah it\u0026rsquo;s kind of operationally complicated to run and um maybe we still want to do this but uh i got talked out of it so i don\u0026rsquo;t know not really what i\u0026rsquo;m going to do another idea is it\u0026rsquo;s like hey you know you know what\u0026rsquo;s the what\u0026rsquo;s the point of like forcing every um compiler to compile at the same time right don\u0026rsquo;t you really want just one compiler to compile everything and then you know send it everywhere like if you\u0026rsquo;re doing real if you\u0026rsquo;re really doing spmd then it\u0026rsquo;s really going to be the same thing everywhere and it\u0026rsquo;s true like you know that would be pretty nice um there are some problems so one of the problems is that uh you don\u0026rsquo;t you know you don\u0026rsquo;t have an obvious like artifact um without running dynamo because uh when you have a bunch of graphics once again like dynamo is calling the shots about where the graph breaks are so actually you know you can imagine some sort of record replay so that where you record you know a dynamo execution and then you replay that on subsequent runs and that\u0026rsquo;s exactly what you want um that would certainly work but but it\u0026rsquo;s like kind of operation like you actually still have to run the entire training script to actually get the recording which is your you know quote unquote compile product there\u0026rsquo;s there\u0026rsquo;s no like offline compilation process actually you know one of the big problems with trying to make pi church 2 more ahead of time is that it\u0026rsquo;s really it really leans into the eager mode so like a lot of the time we we are solving a lot of problems by just being able to assume that you know we\u0026rsquo;re actually running the model with real data like this solves a lot of problems so if you like take that away if you\u0026rsquo;re like trying to do a full expert workflow things get a lot harder in a lot of aspects but you know this is like compiling only one place it is kind of a good idea and you know we\u0026rsquo;re kind of talking about this for you know some of the easier regimes so for example once you get to inductor in some sense the inductor compilation is much more well behaved than dynamo because what\u0026rsquo;s inductor well it takes in an fx graph and a big pile of config options and then produces you know a bunch of you know trident kernels that you want to actually uh run so you know this is this is actually a good old-fashioned style compiler input output and so you could very much imagine you know like just go ahead and say do the compilation of this graph somewhere else right somewhere some other service where you know if all the ranks ask that service for the same compiler result you can notice that they\u0026rsquo;re the same you know batch them into a single request compile it and return the result to everyone we all this remote execution for inductor we actually talked about this at the most recent composability sync so you know go check that out if you\u0026rsquo;re more interested so yeah maybe maybe we\u0026rsquo;ll have that in the future um we\u0026rsquo;re still kind of fighting fires with uh our existing um cache deployment and caching is like kind of a uh easier harder it shares a lot of similarities with remote execution like you have to solve a lot of the same problems with that so we\u0026rsquo;re still like working on getting caching under control so that\u0026rsquo;s kind of where we are right now all right so that\u0026rsquo;s everything i want to talk about with compiler collectives talk to you next time\nRef Running Whisper on an M1 Max to transcribe audio data — Dag-Inge Aas\njerinphilip/pytorch-dev-podcasts-transcribe: Scripts to transcribe https://pytorch-dev-podcast.simplecast.com/episodes\n","permalink":"https://blog.niuhemoon.win/posts/tech/whisper-transcript-pytorch-dev-podcast/","summary":"","title":"whisper模型转录Pytorch播客内容"},{"content":"在机器学习和深度学习的工作流程中，模型的可视化是理解和调试模型的重要环节。Netron 是一个强大的工具，可以帮助我们可视化各种深度学习模型，包括 ONNX 格式的模型。本文将介绍如何使用 Netron 可视化一个 ResNet-50 模型，并展示其结构。\n环境准备 首先，我们需要安装必要的库，包括 netron 和 onnx。可以使用以下命令进行安装：\npip install netron onnx onnxruntime 导出模型为 ONNX 格式 我们将使用 Hugging Face 的 transformers 库加载一个预训练的 ResNet-50 模型，并将其导出为 ONNX 格式。以下是导出模型的代码：\nfrom transformers import ResNetModel import torch # 加载预训练的 ResNet-50 模型 model = ResNetModel.from_pretrained(\u0026#34;microsoft/resnet-50\u0026#34;) model.eval() # 创建一个虚拟输入以进行导出 dummy_input = torch.randn(1, 3, 224, 224) # 导出模型到 ONNX 格式 onnx_file_path = \u0026#39;resnet50_model.onnx\u0026#39; torch.onnx.export(model, dummy_input, onnx_file_path, export_params=True, opset_version=11, do_constant_folding=True, input_names=[\u0026#39;input\u0026#39;], output_names=[\u0026#39;output\u0026#39;], dynamic_axes={\u0026#39;input\u0026#39;: {0: \u0026#39;batch_size\u0026#39;}, \u0026#39;output\u0026#39;: {0: \u0026#39;batch_size\u0026#39;}}) print(\u0026#34;ONNX模型已成功导出！\u0026#34;) 验证模型的有效性 导出模型后，我们需要验证模型是否有效。可以使用 onnx 库进行检查：\nimport onnx try: onnx.checker.check_model(onnx_file_path) print(\u0026#34;The model is valid!\u0026#34;) except onnx.checker.ValidationError as e: print(\u0026#34;The model is invalid: %s\u0026#34; % e) 使用Netron可视化模型 接下来，我们将使用 Netron 可视化导出的 ONNX 模型。可以通过以下代码启动 Netron：\nimport netron netron.start(onnx_file_path) 在浏览器中打开 Netron 后，您将看到模型的结构图，可以直观地了解各个层之间的连接关系。\n处理动态形状 如果需要处理动态形状，可以使用 onnx-tool 进行形状推断和分析：\npython -m onnx_tool -i resnet50_model.onnx -o resnet50_model_shape.onnx --dynamic_shapes input:fp32:1x3x224x224 然后，您可以使用 Netron 可视化处理后的模型：\nnetron.start(\u0026#39;resnet50_model_shape.onnx\u0026#39;) 可视化数据集中的图像 在展示模型之前，我们还可以加载一些图像数据集进行可视化。以下是加载并展示 CIFAR-10 数据集的示例代码：\nfrom datasets import load_dataset import matplotlib.pyplot as plt # 加载 CIFAR-10 数据集 dataset = load_dataset(\u0026#34;cifar10\u0026#34;) # 选择前4张图像 images = dataset[\u0026#34;train\u0026#34;][\u0026#34;img\u0026#34;][:4] fig, axs = plt.subplots(2, 2, figsize=(10, 10)) for i, image in enumerate(images): axs[i].imshow(image) axs[i].axis(\u0026#39;off\u0026#39;) axs[i].set_title(f\u0026#39;Image {i+1}\u0026#39;) plt.tight_layout() plt.show() 总结 Netron 是一个非常实用的工具，可以帮助我们可视化和理解深度学习模型的结构。通过本文的介绍，您可以轻松地将模型导出为 ONNX 格式并使用 Netron 进行可视化。希望这篇博客对您理解深度学习模型有所帮助！\n参考文献 9.1 使用ONNX进行部署并推理 — 深入浅出PyTorch\nlutzroeder/netron: Visualizer for neural network, deep learning and machine learning models\nonnx/models: A collection of pre-trained, state-of-the-art models in the ONNX format\n","permalink":"https://blog.niuhemoon.win/posts/tech/netron-visualize-model/","summary":"","title":"使用Netron可视化ONNX模型"},{"content":"在这篇技术博客中，我们将使用 PyTorch 框架构建一个卷积神经网络（CNN），以识别 MNIST 数据集中的手写数字。我们将重点介绍如何在 GPU 上运行模型，以提高训练和推理的速度。\n环境准备与库导入 确保您已经安装了 PyTorch 和 torchvision。如果您使用 Google Colab，可以直接在代码单元中运行以下命令：\npip install torch torchvision 接下来，导入必要的库：\nimport torch import torchvision import matplotlib.pyplot as plt import torch.nn as nn import torch.nn.functional as F import torch.optim as optim 设备设置与超参数 检查是否有可用的 GPU，并相应地设置设备：\ndevice = torch.device(\u0026#39;cpu\u0026#39;) if torch.cuda.is_available(): device = torch.device(\u0026#39;cuda\u0026#39;) print(\u0026#34;Using CUDA!\u0026#34;) 设置超参数，包括训练的轮数、批量大小、学习率等：\nn_epochs = 3 batch_size_train = 64 batch_size_test = 1000 learning_rate = 0.01 momentum = 0.5 log_interval = 10 random_seed = 1 torch.manual_seed(random_seed) 数据加载与可视化 使用 torchvision 加载 MNIST 数据集，并进行归一化处理：\ntrain_loader = torch.utils.data.DataLoader( torchvision.datasets.MNIST(root=\u0026#39;./MNIST\u0026#39;, train=True, download=True, transform=torchvision.transforms.Compose([ torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.1307,), (0.3081,)) ])), batch_size=batch_size_train, shuffle=True) test_loader = torch.utils.data.DataLoader( torchvision.datasets.MNIST(root=\u0026#39;./MNIST\u0026#39;, train=False, download=True, transform=torchvision.transforms.Compose([ torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.1307,), (0.3081,)) ])), batch_size=batch_size_test, shuffle=True) 可视化一些测试数据，以便更好地理解数据集：\nexamples = enumerate(test_loader) batch_idx, (example_data, example_targets) = next(examples) fig = plt.figure() for i in range(6): plt.subplot(2, 3, i + 1) plt.tight_layout() plt.imshow(example_data[i][0], cmap=\u0026#39;gray\u0026#39;, interpolation=\u0026#39;none\u0026#39;) plt.title(\u0026#34;Ground Truth: {}\u0026#34;.format(example_targets[i])) plt.xticks([]) plt.yticks([]) plt.show() 模型定义与训练 定义一个简单的卷积神经网络：\nclass Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(1, 10, kernel_size=5) self.conv2 = nn.Conv2d(10, 20, kernel_size=5) self.conv2_drop = nn.Dropout2d() self.fc1 = nn.Linear(320, 50) self.fc2 = nn.Linear(50, 10) def forward(self, x): x = F.relu(F.max_pool2d(self.conv1(x), 2)) x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2)) x = x.view(-1, 320) x = F.relu(self.fc1(x)) x = F.dropout(x, training=self.training) x = self.fc2(x) return F.log_softmax(x) 设置优化器和损失函数，并定义训练和测试函数：\nnetwork = Net().to(device) optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum) train_losses = [] train_counter = [] test_losses = [] test_counter = [i * len(train_loader.dataset) for i in range(n_epochs + 1)] def train(epoch): network.train() for batch_idx, (data, target) in enumerate(train_loader): data, target = data.to(device), target.to(device) optimizer.zero_grad() output = network(data) loss = F.nll_loss(output, target) loss.backward() optimizer.step() if batch_idx % log_interval == 0: print(\u0026#39;Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\u0026#39;.format( epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item())) train_losses.append(loss.item()) train_counter.append((batch_idx * 64) + ((epoch - 1) * len(train_loader.dataset))) def test(): network.eval() test_loss = 0 correct = 0 with torch.no_grad(): for data, target in test_loader: data, target = data.to(device), target.to(device) output = network(data) test_loss += F.nll_loss(output, target, reduction=\u0026#39;sum\u0026#39;).item() pred = output.data.max(1, keepdim=True)[1] correct += pred.eq(target.data.view_as(pred)).sum() test_loss /= len(test_loader.dataset) test_losses.append(test_loss) print(\u0026#39;\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\u0026#39;.format( test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset))) 进行模型训练和测试：\nfor epoch in range(1, n_epochs + 1): train(epoch) test() 可视化训练过程与模型预测 可视化训练和测试损失：\nfig = plt.figure() plt.plot(train_counter, train_losses, color=\u0026#39;blue\u0026#39;) plt.scatter(test_counter, test_losses, color=\u0026#39;red\u0026#39;) plt.legend([\u0026#39;Train Loss\u0026#39;, \u0026#39;Test Loss\u0026#39;], loc=\u0026#39;upper right\u0026#39;) plt.xlabel(\u0026#39;number of training examples seen\u0026#39;) plt.ylabel(\u0026#39;negative log likelihood loss\u0026#39;) plt.show() 使用训练好的模型进行预测：\nwith torch.no_grad(): example_data_device = example_data.to(device) output = network(example_data_device) output = output.cpu() fig = plt.figure() for i in range(6): plt.subplot(2, 3, i + 1) plt.tight_layout() plt.imshow(example_data[i][0], cmap=\u0026#39;gray\u0026#39;, interpolation=\u0026#39;none\u0026#39;) plt.title(\u0026#34;Prediction: {}\u0026#34;.format(output.data.max(1, keepdim=True)[1][i].item())) plt.xticks([]) plt.yticks([]) plt.show() 继续训练模型 如果需要继续训练模型，可以加载之前保存的模型和优化器状态：\ncontinued_network = Net().to(device) continued_optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum) network_state_dict = torch.load(\u0026#39;./model.pth\u0026#39;) continued_network.load_state_dict(network_state_dict) optimizer_state_dict = torch.load(\u0026#39;./optimizer.pth\u0026#39;) continued_optimizer.load_state_dict(optimizer_state_dict) for i in range(4, 6): test_counter.append(i * len(train_loader.dataset)) train(i) test() 总结 在本文中，我们介绍了如何使用 PyTorch 构建一个简单的卷积神经网络，以识别 MNIST 数据集中的手写数字，并在 GPU 上运行以提高性能。希望这篇博客能帮助您更好地理解深度学习的基本概念和实践。\n如需进一步学习，请参考 PyTorch 官方文档。\n","permalink":"https://blog.niuhemoon.win/posts/tech/pytorch-mnist-tutorial/","summary":"","title":"使用 PyTorch 进行 MNIST 手写数字识别"},{"content":"简介 相比p2p即点对点通信，集合通信的参与方可以大于2个，同时在通信中引入同步点，所有代码在达到同步点后才能继续执行后续的代码。 通常包含如下通信类型：\nBroadcast Scatter Gather AllGather Reduce AllReduce 将用mpi4py(mpi的一个python包装库)代码加深对集合通信的理解，c语言的mpi代码可以参考mpitutorial/tutorials/run.py at gh-pages · mpitutorial/mpitutorial，也将提供一些torch进行集合通信的示例。\n因此，需要首先安装依赖包\napt-get update \u0026amp;\u0026amp; apt-get install mpich mpirun --version pip install mpi4py pip install torch 执行环境 常见的分布式程序需要一个launcher，例如mpi、torchrun、ray等，这里会使用两种\nmpiexec torchrun 主要使用mpiexec\n示例代码如下：\n# torch_mpi_send.py import os import argparse import torch import torch.distributed as dist LOCAL_RANK = 0 WORLD_SIZE = 0 WORLD_RANK = 0 def init_global_vars(frontend): global LOCAL_RANK global WORLD_SIZE global WORLD_RANK env_vars = os.environ if frontend == \u0026#34;mpi\u0026#34;: # Environment variables set by mpiexec LOCAL_RANK = int(os.environ[\u0026#39;OMPI_COMM_WORLD_LOCAL_RANK\u0026#39;]) WORLD_SIZE = int(os.environ[\u0026#39;OMPI_COMM_WORLD_SIZE\u0026#39;]) WORLD_RANK = int(os.environ[\u0026#39;OMPI_COMM_WORLD_RANK\u0026#39;]) else: # Environment variables set by torch.distributed.launch LOCAL_RANK = int(os.environ[\u0026#39;LOCAL_RANK\u0026#39;]) WORLD_SIZE = int(os.environ[\u0026#39;WORLD_SIZE\u0026#39;]) WORLD_RANK = int(os.environ[\u0026#39;RANK\u0026#39;]) def run(backend): tensor = torch.zeros(10,10) # Need to put tensor on a GPU device for nccl backend if backend == \u0026#39;nccl\u0026#39;: # device = torch.device(\u0026#34;cuda:{}\u0026#34;.format(LOCAL_RANK)) device = torch.device(\u0026#34;cuda:{}\u0026#34;.format(0)) tensor = tensor.to(device) if WORLD_RANK == 0: for rank_recv in range(1, WORLD_SIZE): tensor = torch.ones(10,10) dist.send(tensor=tensor, dst=rank_recv) print(\u0026#39;worker_{} sent data to Rank {}\\n\u0026#39;.format(0, rank_recv)) else: dist.recv(tensor=tensor, src=0) print(f\u0026#39;worker_{WORLD_RANK} has received data {tensor.cpu()} from rank 0\\n\u0026#39;) def init_processes(frontend, backend): init_global_vars(frontend=frontend) print(f\u0026#34;local rank {LOCAL_RANK} world size {WORLD_SIZE} world rank {WORLD_RANK}\u0026#34;) dist.init_process_group(backend, rank=WORLD_RANK, world_size=WORLD_SIZE) run(backend) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser() parser.add_argument(\u0026#34;--local-rank\u0026#34;, type=int, help=\u0026#34;Local rank. Necessary for using the torch.distributed.launch utility.\u0026#34;) parser.add_argument(\u0026#34;--backend\u0026#34;, type=str, default=\u0026#34;nccl\u0026#34;, choices=[\u0026#39;nccl\u0026#39;, \u0026#39;gloo\u0026#39;]) parser.add_argument(\u0026#34;--frontend\u0026#34;, type=str, default=\u0026#34;mpi\u0026#34;, choices=[\u0026#39;mpi\u0026#39;, \u0026#39;torch\u0026#39;]) args = parser.parse_args() init_processes(frontend=args.frontend, backend=args.backend) mpi 用mpi启动一个pytorch的分布式训练程序，特点是只需要在master节点执行即可，跨节点时候需要配置节点之间ssh互信免密访问。\n如果--backend nccl，需要环境上至少有两块GPU卡\nmpirun --allow-run-as-root -n 2 --use-hwthread-cpus \\ -H localhost:2 \\ -x MASTER_ADDR=localhost \\ -x MASTER_PORT=1234 \\ -x PATH \\ -bind-to none -map-by slot \\ -mca pml ob1 -mca btl ^openib \\ python torch_mpi_send.py --backend gloo --frontend=mpi 输出如下\nlocal rank 1 world size 2 world rank 1 [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:1234 (errno: 99 - Cannot assign requested address). local rank 0 world size 2 world rank 0 worker_1 has received data tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]) from rank 0 worker_0 sent data to Rank 1 torchrun torchrun跨节点时需要在多节点上执行命令\ntorchrun \\ --nproc_per_node=2 --nnodes=1 --node_rank=0 \\ --master_addr=localhost --master_port=1234 \\ torch_mpi_send.py \\ --backend=gloo --frontend torch 输出如下\nlocal rank 0 world size 2 world rank 0 local rank 1 world size 2 world rank 1 worker_0 sent data to Rank 1 worker_1 has received data tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]) from rank 0 Broadcast 广播将一个进程中的数据发送到所有其他进程。通常用于将一个进程的消息或数据复制到所有参与者。\nmpi4py实现 # broadcast.py from mpi4py import MPI comm = MPI.COMM_WORLD rank = comm.Get_rank() if rank == 0: data = {\u0026#39;key1\u0026#39; : [7, 2.72, 2+3j], \u0026#39;key2\u0026#39; : ( \u0026#39;abc\u0026#39;, \u0026#39;xyz\u0026#39;)} else: data = None data = comm.bcast(data, root=0) print(f\u0026#34;rank {rank} data is {data}\u0026#34;) mpiexec --allow-run-as-root -n 4 --use-hwthread-cpus --oversubscribe python broadcast.py # rank 0 data is {\u0026#39;key1\u0026#39;: [7, 2.72, (2+3j)], \u0026#39;key2\u0026#39;: (\u0026#39;abc\u0026#39;, \u0026#39;xyz\u0026#39;)} # rank 1 data is {\u0026#39;key1\u0026#39;: [7, 2.72, (2+3j)], \u0026#39;key2\u0026#39;: (\u0026#39;abc\u0026#39;, \u0026#39;xyz\u0026#39;)} # rank 2 data is {\u0026#39;key1\u0026#39;: [7, 2.72, (2+3j)], \u0026#39;key2\u0026#39;: (\u0026#39;abc\u0026#39;, \u0026#39;xyz\u0026#39;)} # rank 3 data is {\u0026#39;key1\u0026#39;: [7, 2.72, (2+3j)], \u0026#39;key2\u0026#39;: (\u0026#39;abc\u0026#39;, \u0026#39;xyz\u0026#39;)} pytorch实现 #!/usr/bin/env python import os import torch import torch.distributed as dist LOCAL_RANK = int(os.environ[\u0026#39;LOCAL_RANK\u0026#39;]) WORLD_SIZE = int(os.environ[\u0026#39;WORLD_SIZE\u0026#39;]) WORLD_RANK = int(os.environ[\u0026#39;RANK\u0026#39;]) def run_broadcast(rank, size): # create a group with all processors group = dist.new_group(list(range(size))) if rank == 0: tensor = torch.tensor([rank], dtype=torch.float32) else: tensor = torch.empty(1) # sending all tensors to the others dist.broadcast(tensor, src=0, group=group) # all ranks will have tensor([0.]) from rank 0 print(f\u0026#34;[{rank}] data = {tensor}\u0026#34;) def init_process(backend=\u0026#39;gloo\u0026#39;): dist.init_process_group(backend, init_method=\u0026#34;file:///tmp/sharedfile\u0026#34;, rank=WORLD_RANK, world_size=WORLD_SIZE) run_broadcast(WORLD_RANK, WORLD_SIZE) init_process() !torchrun \\ --nproc_per_node=4 --nnodes=1 --node_rank=0 \\ torch_broadcast.py 所有rank都收到了同样的数据\n[3] data = tensor([0.]) [1] data = tensor([0.]) [2] data = tensor([0.]) [0] data = tensor([0.]) Scatter Scatter将一个进程中的数据分发到多个进程中。源进程将数据分成多个部分，并将每部分发送到不同的目标进程。\nmpi4py实现 # scatter.py from mpi4py import MPI comm = MPI.COMM_WORLD size = comm.Get_size() rank = comm.Get_rank() if rank == 0: data = [list(range(i+2)) for i in range(size)] else: data = None data = comm.scatter(data, root=0) print(f\u0026#34;rank {rank} data is {data}\u0026#34;) rank 0 上数据是[[0, 1], [0, 1 ,2], [0, 1, 2, 3], [0, 1, 2, 3, 4]]，列表共两个元素，分发到两个rank上，rank0获得第一个元素，rank1获得第二个元素, 以此类推。 --use-hwthread-cpus --oversubscribe是因为执行环境只有2核的cpu，如果有更多的cpu，可以去掉这两个参数。\nmpiexec --allow-run-as-root -n 2 --use-hwthread-cpus --oversubscribe python scatter.py rank 2 data is [0, 1, 2, 3] rank 1 data is [0, 1, 2] rank 0 data is [0, 1] rank 3 data is [0, 1, 2, 3, 4] mpi4py还支持numpy对象，具体可以参考使用文档Tutorial — MPI for Python 4.0.0 documentation\npytorch实现 #!/usr/bin/env python import os import torch import torch.distributed as dist LOCAL_RANK = int(os.environ[\u0026#39;LOCAL_RANK\u0026#39;]) WORLD_SIZE = int(os.environ[\u0026#39;WORLD_SIZE\u0026#39;]) WORLD_RANK = int(os.environ[\u0026#39;RANK\u0026#39;]) def run_scatter(rank, size): # create a group with all processors group = dist.new_group(list(range(size))) # sending all tensors from rank 0 to the others tensor = torch.empty(1) if rank == 0: tensor_list = [torch.tensor([i + 1], dtype=torch.float32) for i in range(size)] # tensor_list = [tensor(1), tensor(2), tensor(3), tensor(4)] dist.scatter(tensor, scatter_list=tensor_list, src=0, group=group) else: dist.scatter(tensor, scatter_list=[], src=0, group=group) # each rank will have a tensor with their rank number dist.barrier() print(f\u0026#39;\\nRank {rank} received data {tensor}\u0026#39;) def init_process(backend=\u0026#39;gloo\u0026#39;): dist.init_process_group(backend, init_method=\u0026#34;file:///tmp/sharedfile\u0026#34;, rank=WORLD_RANK, world_size=WORLD_SIZE) run_scatter(WORLD_RANK, WORLD_SIZE) init_process() torchrun \\ --nproc_per_node=4 --nnodes=1 --node_rank=0 \\ torch_scatter.py Rank 2 received data tensor([3.]) Rank 0 received data tensor([1.]) Rank 1 received data tensor([2.]) Rank 3 received data tensor([4.]) Gather Gather操作是将多个进程中的数据汇聚到一个进程中。每个参与进程将其数据发送到指定的根进程，根进程将所有数据整合在一起。\nmpi4py实现 # gather.py from mpi4py import MPI comm = MPI.COMM_WORLD size = comm.Get_size() rank = comm.Get_rank() if rank == 0: data = [list(range(i+2)) for i in range(size)] else: data = None data = comm.scatter(data, root=0) print(f\u0026#34;rank {rank} data is {data}\u0026#34;) pytorch实现 #!/usr/bin/env python import os import torch import torch.distributed as dist LOCAL_RANK = int(os.environ[\u0026#39;LOCAL_RANK\u0026#39;]) WORLD_SIZE = int(os.environ[\u0026#39;WORLD_SIZE\u0026#39;]) WORLD_RANK = int(os.environ[\u0026#39;RANK\u0026#39;]) def run_gather(rank, size): # create a group with all processors group = dist.new_group(list(range(size))) tensor = torch.tensor([rank], dtype=torch.float32) # sending all tensors from rank 0 to the others if rank == 0: # create an empty list we will use to hold the gathered values tensor_list = [torch.empty(1) for i in range(size)] dist.gather(tensor, gather_list=tensor_list, dst=0, group=group) else: dist.gather(tensor, gather_list=[], dst=0, group=group) # only rank 0 will have the tensors from the other processed # [tensor([0.]), tensor([1.]), tensor([2.]), tensor([3.])] if rank == 0: print(f\u0026#34;[{rank}] data = {tensor_list}\u0026#34;) def init_process(backend=\u0026#39;gloo\u0026#39;): dist.init_process_group(backend, init_method=\u0026#34;file:///tmp/sharedfile\u0026#34;, rank=WORLD_RANK, world_size=WORLD_SIZE) run_gather(WORLD_RANK, WORLD_SIZE) init_process() 输出为[0] data = [tensor([0.]), tensor([1.]), tensor([2.]), tensor([3.])]\nAll-Gather All Gather 操作是将所有进程中的数据汇聚到每个进程中。每个进程不仅接收来自根进程的数据，还接收来自其他所有进程的数据。\nmpi4py实现 # allgather.py from mpi4py import MPI import numpy as np comm = MPI.COMM_WORLD rank = comm.Get_rank() size = comm.Get_size() # 每个进程生成一个数据 data = np.array(rank + 1) # 每个进程的数据为其 rank + 1 gathered_data = np.zeros(size, dtype=int) # 执行 AllGather 操作 comm.Allgather(data, gathered_data) print(f\u0026#34;Rank {rank} gathered data: {gathered_data}\u0026#34;) mpiexec --allow-run-as-root -n 4 --use-hwthread-cpus --oversubscribe python mpi_allgather.py Rank 1 gathered data: [1 2 3 4] Rank 3 gathered data: [1 2 3 4] Rank 0 gathered data: [1 2 3 4] Rank 2 gathered data: [1 2 3 4] pytorch实现 #!/usr/bin/env python import os import torch import torch.distributed as dist LOCAL_RANK = int(os.environ[\u0026#39;LOCAL_RANK\u0026#39;]) WORLD_SIZE = int(os.environ[\u0026#39;WORLD_SIZE\u0026#39;]) WORLD_RANK = int(os.environ[\u0026#39;RANK\u0026#39;]) def run_all_gather(rank, size): # create a group with all processors group = dist.new_group(list(range(size))) tensor = torch.tensor([rank], dtype=torch.float32) # create an empty list we will use to hold the gathered values tensor_list = [torch.empty(1) for i in range(size)] # sending all tensors to the others dist.all_gather(tensor_list, tensor, group=group) # all ranks will have [tensor([0.]), tensor([1.]), tensor([2.]), tensor([3.])] print(f\u0026#34;[{rank}] data = {tensor_list}\u0026#34;) def init_process(backend=\u0026#39;gloo\u0026#39;): dist.init_process_group(backend, init_method=\u0026#34;file:///tmp/sharedfile\u0026#34;, rank=WORLD_RANK, world_size=WORLD_SIZE) run_all_gather(WORLD_RANK, WORLD_SIZE) init_process() Reduce Reduce操作将多个进程中的数据通过某种运算（如求和、取最大值等）整合成一个结果，并将该结果发送到一个指定的根进程\nmpi4py实现 # reduce.py from mpi4py import MPI import numpy as np comm = MPI.COMM_WORLD rank = comm.Get_rank() # 每个进程生成一个数据 data = np.array(rank + 1) # 每个进程的数据为其 rank + 1 # 只有根进程接收结果 if rank == 0: result = np.zeros(1, dtype=int) else: result = None # 执行 Reduce 操作 comm.Reduce(data, result, op=MPI.SUM, root=0) if rank == 0: print(f\u0026#34;Result after Reduce: {result[0]}\u0026#34;) mpiexec --allow-run-as-root -n 4 --use-hwthread-cpus --oversubscribe python mpi_reduce.py # Result after Reduce: 10 pytorch实现 #!/usr/bin/env python import os import torch import torch.distributed as dist LOCAL_RANK = int(os.environ[\u0026#39;LOCAL_RANK\u0026#39;]) WORLD_SIZE = int(os.environ[\u0026#39;WORLD_SIZE\u0026#39;]) WORLD_RANK = int(os.environ[\u0026#39;RANK\u0026#39;]) def run_reduce(rank, size): # create a group with all processors group = dist.new_group(list(range(size))) tensor = torch.ones(1) # sending all tensors to rank 0 and sum them dist.reduce(tensor, dst=0, op=dist.ReduceOp.SUM, group=group) # can be dist.ReduceOp.PRODUCT, dist.ReduceOp.MAX, dist.ReduceOp.MIN # only rank 0 will have four print(f\u0026#34;[{rank}] data = {tensor[0]}\u0026#34;) def init_process(backend=\u0026#39;gloo\u0026#39;): dist.init_process_group(backend, init_method=\u0026#34;file:///tmp/sharedfile\u0026#34;, rank=WORLD_RANK, world_size=WORLD_SIZE) run_reduce(WORLD_RANK, WORLD_SIZE) init_process() All-Reduce All Reduce操作是将所有进程中的数据进行归约运算，并将结果发送到所有进程。每个进程都能获得归约后的结果。\nmpi4py实现 # allreduce.py from mpi4py import MPI import numpy as np comm = MPI.COMM_WORLD rank = comm.Get_rank() # 每个进程生成一个数据 data = np.array(rank + 1) # 每个进程的数据为其 rank + 1 # 执行 AllReduce 操作 result = np.zeros(1, dtype=int) comm.Allreduce(data, result, op=MPI.SUM) print(f\u0026#34;Rank {rank} has result after AllReduce: {result[0]}\u0026#34;) mpiexec --allow-run-as-root -n 4 --use-hwthread-cpus --oversubscribe python all_reduce.py Rank 1 has result after AllReduce: 10 Rank 3 has result after AllReduce: 10 Rank 0 has result after AllReduce: 10 Rank 2 has result after AllReduce: 10 Pytorch实现 #!/usr/bin/env python # torch_all_reduce.py import os import torch import torch.distributed as dist # Environment variables set by torch.distributed.launch or torchrun LOCAL_RANK = int(os.environ[\u0026#39;LOCAL_RANK\u0026#39;]) WORLD_SIZE = int(os.environ[\u0026#39;WORLD_SIZE\u0026#39;]) WORLD_RANK = int(os.environ[\u0026#39;RANK\u0026#39;]) \u0026#34;\u0026#34;\u0026#34; All-Reduce example.\u0026#34;\u0026#34;\u0026#34; def run(rank, size): \u0026#34;\u0026#34;\u0026#34; Simple collective communication. \u0026#34;\u0026#34;\u0026#34; group = dist.new_group([0, 1, 2, 3]) tensor = torch.ones(2,3) dist.all_reduce(tensor, op=dist.ReduceOp.SUM, group=group) dist.barrier(group) print(f\u0026#39;\\nRank {rank} has data {tensor}\u0026#39;) def init_process(backend=\u0026#39;gloo\u0026#39;): \u0026#34;\u0026#34;\u0026#34; Initialize the distributed environment. \u0026#34;\u0026#34;\u0026#34; dist.init_process_group(backend, init_method=\u0026#34;file:///tmp/sharedfile\u0026#34;, rank=WORLD_RANK, world_size=WORLD_SIZE) run(WORLD_RANK, WORLD_SIZE) init_process() torchrun \\ --nproc_per_node=4 --nnodes=1 --node_rank=0 \\ torch_all_reduce.py Rank 0 has data tensor([[4., 4., 4.], [4., 4., 4.]]) Rank 1 has data tensor([[4., 4., 4.], [4., 4., 4.]]) Rank 3 has data tensor([[4., 4., 4.], [4., 4., 4.]]) Rank 2 has data tensor([[4., 4., 4.], [4., 4., 4.]]) 参考 Writing Distributed Applications with PyTorch — PyTorch Tutorials 2.4.0+cu121 documentation Tutorial — MPI for Python 4.0.0 documentation\nCollective Communication in Distributed Systems with PyTorch\nmpitutorial/mpitutorial: MPI programming lessons in C and executable code examples\nMPI 广播以及集体(collective)通信 · MPI Tutorial PyTorch分布式训练详解教程 scatter, gather \u0026amp; isend, irecv \u0026amp; all_reduce \u0026amp; DDP - 天靖居士 - 博客园\n","permalink":"https://blog.niuhemoon.win/posts/tech/collective-communication-introduction/","summary":"","title":"集合通信入门"},{"content":"介绍 MPI(Message Passing Interface)是上世纪90年代定义的一个接口标准，这个标准接口使得程序员写的并发程序可以在所有主流的并发框架中运行。\nMPI 在消息传递模型设计上的一些经典概念，第一个是通讯器（communicator）通讯器定义了一组能够互相发消息的进程。在这组进程中，每个进程会被分配一个序号，称作秩（rank），进程间显性地通过指定秩来进行通信。\n通信的基础建立在不同进程间发送和接收操作。一个进程可以通过指定另一个进程的秩以及一个独一无二的消息标签（tag）来发送消息给另一个进程。接受者可以发送一个接收特定标签标记的消息的请求（或者也可以完全不管标签，接收任何消息），然后依次处理接收到的数据。类似这样的涉及一个发送者以及一个接受者的通信被称作点对点（point-to-point）通信。\n当然在很多情况下，某个进程可能需要跟所有其他进程通信。比如主进程想发一个广播给所有的从进程。在这种情况下，手动去写一个个进程点对点的信息传递就显得很笨拙。而且事实上这样会导致网络利用率低下。MPI 有专门的接口来帮我们处理这类所有进程间的集合（collective）通信。\n把点对点通信和集体性通信这两个机制合在一起已经可以创造十分复杂的并发程序了。\n安装 mpi有多种不同的实现，这里使用mpich的实现\nLinux从镜像源安装(推荐) # Ubuntu apt-get update \u0026amp;\u0026amp; apt-get install mpich # Centos yum install mpich mpich-devel # 安装完成后 mpiexec --version 下载源码Downloads | MPICH编译安装 ./configure --disable-fortran --prefix=/usr/local/mpich make; sudo make install 单机 安装完成后测试一下单机使用，编辑如下hello_world.c文件\n#include \u0026lt;mpi.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; int main(int argc, char** argv) { // Initialize the MPI environment MPI_Init(NULL, NULL); // Get the number of processes int world_size; MPI_Comm_size(MPI_COMM_WORLD, \u0026amp;world_size); // Get the rank of the process int world_rank; MPI_Comm_rank(MPI_COMM_WORLD, \u0026amp;world_rank); // Get the name of the processor char processor_name[MPI_MAX_PROCESSOR_NAME]; int name_len; MPI_Get_processor_name(processor_name, \u0026amp;name_len); // Print off a hello world message printf(\u0026#34;Hello world from processor %s, rank %d out of %d processors\\n\u0026#34;, processor_name, world_rank, world_size); // Finalize the MPI environment. MPI_Finalize(); } 使用mpicc编译，并使用mpirun执行\nmpicc hello_world.c -o hello_world # 使用mpiexec或mpirun启动执行 mpiexec -n 4 ./hello_world # 输出如下 Hello world from processor VM38776, rank 1 out of 4 processors Hello world from processor VM38776, rank 0 out of 4 processors Hello world from processor VM38776, rank 3 out of 4 processors Hello world from processor VM38776, rank 2 out of 4 processors 多机 多机执行，首先需要确保多个机器上安装同版本或者版本接近的mpi工具，并且机器之间可以互相无密码ssh访问。\n配置免密访问 免密需要节点间建立互信，免密操作略。\n下面操作可选，即配置/etc/hosts和~/.ssh/config来便于地址解析和ssh连接，ssh参考配置如下\nHost mpihost1 HostName 91.200.139.182 User root Port 10022 StrictHostKeyChecking no Host mpihost2 HostName 91.200.139.183 User root Port 10022 StrictHostKeyChecking no 编辑mpi host配置 编辑如下host_file文件，并指定每个节点的进程数\nmpihost1:3 mpihost2:1 含义是在mpihost1上启动3个进程，在mpihost2上启动1个进程\n接着在两个节点分别编译hello_world.c文件，并将可执行文件放在/tmp/hello_world\n# 启动4个进程 mpiexec -n 4 -f host_file /tmp/hello_world # 输出如下 Hello world from processor mpihost1, rank 0 out of 4 processors Hello world from processor mpihost1, rank 1 out of 4 processors Hello world from processor mpihost1, rank 2 out of 4 processors Hello world from processor mpihost2, rank 3 out of 4 processors 总结多机mpi运行要满足条件：\nmpi安装版本和路径一致 节点间可以免密ssh通信 mpiexec运行的可执行文件在不同节点的路径相同 参考 mpitutorial/mpitutorial: MPI programming lessons in C and executable code examples\nMPI 教程介绍 · MPI Tutorial\nMPI Hello World · MPI Tutorial\n","permalink":"https://blog.niuhemoon.win/posts/tech/mpi-tutorial-install/","summary":"","title":"MPI入门-安装和基本使用"},{"content":"收敛比 超售在生活中很常见，航空公司假设并不是所有购票的乘客都会按时到达，因此会多售出一些机票。在云计算领域，也会存在一定程度的超售，一台128核心的物理服务器，可能会运行16台虚拟机，每台虚拟机都有16个cpu内核，那么实际售出了16*16=256个核心，超售比是2:1。这么做是假设用户虚拟机的CPU使用率不会同时高于50%，如果同时高于50%，此时请求的资源大于所能提供的资源，用户就可能会感受到机器卡顿。 网络收敛比是一个比较奇怪的称呼，对应到英文应该是over-subscription ratio这个含义，指的是网络设备南北向带宽超售的比例。为了能够描述不同的收敛程度，我们通常用一个系统所有南向（下行）接口的总带宽比上这个系统所有北向（上行）接口总带宽的数值来表示，我们也将这个数值称为这个系统的收敛比，即over-subscription ratio。\n举个例子，假设你有10台服务器，每台服务器通过10GE的接口连接到一个接入交换机，那我们一共就有100G（10×10G=100G）的南向带宽。假设这台交换机还有2个40GE的接口可以用于接入到更高一层的汇聚交换机，那我们一共就有80G（2×40G=80G）的北向带宽。此时，我们得到的收敛比则是1.25：1（100G÷80G=1.25）\n收敛比计算公式 收敛比表示网络中下行链路总带宽与上行链路总带宽之间的关系。通过计算下行链路的总带宽与上行链路的总带宽的比值，可以评估网络的超售程度和潜在的瓶颈。较低的收敛比通常意味着更高的网络性能和更低的延迟，而较高的收敛比则可能导致网络拥堵和性能下降。\n收敛比的计算公式为：\n$(P_n \\times P_s):(U_n \\times U_s)$\n其中：\n$P_n$是连接的Leaf端口的数量，表示叶交换机上可用的下行链路数量 $P_s$ 是这些端口的速度，表示每个下行链路的带宽 $U_n$ 是上行链路的数量，表示连接Leaf交换机的上行链路数量。 $U_s$ 是上行链路的速度，表示每个上行链路的带宽。 例如下图，一个具有 48x10g 端口的 Leaf 交换机，用于连接服务器，提供 480Gb/s 的端口容量。如果此 Leaf 以 40Gb/s 的速率连接到 4 个Spine交换机，则其总上行链路容量将为 160Gb/s。此时收敛比为3:1\n在设计数据中心网络收敛比时，我们需要根据网络业务和流量模型。综合考虑东西、南北流量的大小、比例，来制定合适的收敛比和选择相应的设备。通常收敛比应该低于3:1，理想情况是1:1，可以认为是无阻塞网络（在部分场合也称为无收敛网络），对于任意的通信模式，总有路径让他们的通信带宽达到网卡带宽。\n传统三层架构网络 如下的是一个收敛比较高的三层组网。这种三层设计收敛比高，存在上行链路瓶颈，并增加了东西向流量（数据中心设备之间的流量）的延迟。因此，现代数据中心通常使用后年的叶脊Leaf-Spine架构，叶脊架构的延迟处于可预测的水平，并且数据包跳数最小。\nLeaf-Spine叶脊架构网络 也称为分布式核心网络，优势如下\n扁平化：扁平化设计缩短服务器之间的通信路径，从而降低延迟，可以显著提高应用程序和服务性能。 易扩展：如果 Spine 交换机的带宽不足，我们只需要增加 Spine 节点数，也可以提供路径上的负载均衡；如果接入连接不足，则只需增加 Leaf 节点数。 低收敛比：容易实现 1:X 甚至是无阻塞的 1:1 的收敛比，而且通过增加 Spine 和 Leaf 设备间的链路带宽也可以降低链路收敛比。 简化管理：叶脊结构可以在无环路环境中使用全网格中的每个链路并进行负载平衡，这种等价多路径设计在使用 SDN 等集中式网络管理平台时处于最佳状态。SDN 允许在发生堵塞或链路故障时简化流量的配置，管理和重新分配路由，使得智能负载均衡的全网状拓扑成为一个相对简单的配置和管理方式。 边缘流量处理：随着物联网（IoT）等业务的兴起，接入层压力剧增，可能有数千个传感器和设备在网络边缘连接并产生大量流量。Leaf 可以在接入层处理连接，Spine 保证节点内的任意两个端口之间提供延迟非常低的无阻塞性能，从而实现从接入到云平台的敏捷服务。 多云管理：数据中心或云之间通过 Leaf Spine 架构仍可以实现高性能、高容错等优势，而多云管理策略也逐渐成为企业的必选项。 上图的组网，数据中心中拥有 960 台 10G 带宽的服务器，收敛比(over-subscription ratio)为 2.4:1。Leaf交换机位于机架顶部，支持 48 x 10GB 服务器端口和 8 x 100G 上行链路端口。Spine交换机支持 64 x 100G 端口。为了覆盖所有 960 台服务器，我们需要 20 台Leaf交换机（960 台服务器/ 48 个端口）和两台Spine交换机。每个Leaf交换机都通过两个 100G 上行链路连接到Spine。收敛比为 2.4:1 时，服务器的最大数量为 960 台(48 x 10Gbps 下行链路到服务器/2 x 100Gbps 上行链路到Spine = 2.4)。\n如果再添加两台Spine交换机，收敛比例将为 1.2:1 (480G / 400G)。这接近1:1，因此不存在网络瓶颈。此时，Leaf交换机转发流量时不会丢失数据包。但1:1收敛比可能会导致非高峰时段容量过剩，成本较高。\n总结 网络收敛比是设计高效数据中心网络的关键因素。通过合理的收敛比设计，能够有效平衡资源利用率与网络性能，避免瓶颈和延迟。现代的Leaf-Spine架构提供了更低的收敛比和更高的可扩展性，使得数据中心能够灵活应对不断增长的业务需求。在选择网络架构时，需综合考虑业务流量、成本与性能，以实现最佳的网络设计。\n参考 Oversubscription in Networking | Noction\nConverged Networking: OVER-Subscription Ratio | by Mike Lyons | Medium\n第四章 收敛比 - 华为数据中心网络设计指南 - 华为\nOverlay介绍 - 华为数据中心网络设计指南 - 华为\n数据中心网络架构的问题与演进 — CLOS 网络与 Fat-Tree、Spine-Leaf 架构 - 云物互联 - 博客园\n","permalink":"https://blog.niuhemoon.win/posts/tech/network-oversubscription-ratio/","summary":"","title":"数据中心组网收敛比概念和计算"},{"content":"FTP测试服务 可以快速通过容器搭建一个FTP服务，可以将远程的FTP内容，同步到本地，并且用web服务队外提供，支持权限控制。\ndelfer/alpine-ftp-server - Docker Image | Docker Hub USERS格式：用户名｜密码｜目录｜user_id\nADDRESS: 需要可以解析到容器宿主机，推荐容器宿主机的ip地址\ndocker run -d -p 21:21 -v /home/nh/ftp:/home/one/ftp -p 21000-21010:21000-21010 -e USERS=\u0026#34;one|1234|/home/one/ftp|1000\u0026#34; -e ADDRESS=192.168.5.42 --name ftp delfer/alpine-ftp-server 此时建立了一个FTP server\nhost: 192.168.5.42 port: 21 username: one password: 1234 可以用FTP client工具，例如FileZilla - The free FTP solution进行测试\nFTP镜像工具 用FTP抓取工具crazy-max/ftpgrab: Grab your files periodically from a remote FTP or SFTP server easily，可以完整的备份一个FTP的远程目录到指定的目录.\n# ./ftpgrab.yml db: path: ftpgrab.db server: ftp: host: 192.168.5.42 port: 21 username: one password: 1234 sources: - /home/one/ftp timeout: 5s download: output: ./download retry: 3 hideSkipped: false createBaseDir: false ftpgrab --config ./ftpgrab.yml 经过如上命令，即可将远程（192.168.5.42）的ftp文件目录下载到本地目录，后续远端有更新，命令可以重复执行。\n# 每分钟执行一次 ftpgrab --config conf.yaml --schedule=\u0026#34;* * * * *\u0026#34; # 每12小时执行一次 ./ftpgrab --config conf.yaml --schedule=\u0026#34;0 */12 * * *\u0026#34; # 每天执行一次更新 ./ftpgrab --config conf.yaml --schedule=\u0026#34;0 0 * * *\u0026#34; HTTP文件服务 FTP服务已经不被主流浏览器支持，需要用客户端软件访问，带来一定的不便，可以将FTP的内容转换成HTTP服务，对外提供\n使用filebrowser/filebrowser: 📂 Web File Browser来部署一个文件浏览服务\n基本使用 # 首先在/home/nh/browser/目录创建一个空的filebrowser.db文件 touch filebrowser.db # 启动容器 docker run -d -v /home/nh/ftp:/srv -v /home/nh/browser/filebrowser.db:/database.db -u $(id -u):$(id -g) -p 8080:80 --name filebrowser filebrowser/filebrowser 接着可以在浏览器访问http://host:8080\n初始用户名/密码admin/admin，登录后修改密码，并且添加其它只读用户即可。\n开启https 首先自己签发证书\n# 签署cert和key openssl genpkey -algorithm RSA -out mykey.pem -pkeyopt rsa_keygen_bits:2048 openssl req -new -x509 -key mykey.pem -out mycert.pem -days 365 编辑如下setting.json\n{ \u0026#34;address\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;port\u0026#34;: 80, \u0026#34;locale\u0026#34;: \u0026#34;zh-cn\u0026#34;, \u0026#34;baseURL\u0026#34;: \u0026#34;/\u0026#34;, \u0026#34;log\u0026#34;: \u0026#34;stdout\u0026#34;, \u0026#34;database\u0026#34;: \u0026#34;/filebrowser.db\u0026#34;, \u0026#34;root\u0026#34;: \u0026#34;/srv\u0026#34;, \u0026#34;cert\u0026#34;: \u0026#34;/mycert.pem\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;/mykey.pem\u0026#34; } 开启容器服务\ndocker run -d \\ -v /home/nh/ftp:/srv \\ -v /home/nh/browser/mycert.pem:/mycert.pem \\ -v /home/nh/browser/mykey.pem:/mykey.pem \\ -v /home/nh/browser/filebrowser.db:/filebrowser.db \\ -v /home/nh/browser/setting.json:/.filebrowser.json \\ -p 8080:80 \\ --name filebrowser \\ --restart=always \\ filebrowser/filebrowser 即可通过https访问服务\n参考 搭建ftp服务器 # | busyrat blog\n","permalink":"https://blog.niuhemoon.win/posts/tech/ftp-http-file-share/","summary":"","title":"局域网搭建文件分享服务"},{"content":"pip basic # 列出当前包列表 pip list # 查看包详情 pip show requests # 指定源安装 pip install -i https://test.pypi.org/simple/ rptree # 也可以在~/.pip/pip.conf中定义源头 [global] index-url = https://test.pypi.org/simple/ # 可以从github安装 pip install git+https://github.com/realpython/rptree # 编辑模式安装，适用于调试自己的包 # 会在site-packages目录建立链接， # 链接到当前项目到路径 python -m pip install -e . # 生成requirements.txt pip freeze \u0026gt; requirements.txt # 安装依赖环境 pip install -r requirements.txt # 更新依赖环境 pip install -U -r requirements.txt # 卸载package pip uninstall urllib3 -y # 卸载整个依赖 pip uninstall -r requirements.txt -y requirements # requirements.txt certifi==x.y.z charset-normalizer==x.y.z idna==x.y.z requests\u0026gt;=x.y.z, \u0026lt;3.0 urllib3==x.y.z 隔离生产和开发环境\n# requirements_dev.txt -r requirements.txt pytest\u0026gt;=x.y.z python包调试并编译发布 主要分成如下步骤\n定义pyproject.toml 定义setup.cfg pyproject.toml示例如下 [build-system] requires = [\u0026#39;setuptools\u0026gt;=42\u0026#39;] build-backend = \u0026#39;setuptools.build_meta\u0026#39; setup.cfg示例如下 [metadata] name = carlton_template version = 0.1.0 author = Carlton author_email = carlton2tang@gmail.com description = A simple example to understand the basics of developing your first Python package. long_description = file: README.md long_description_content_type = text/markdown url = https://github.com/2niuhe/python_template_carlton project_urls = Bug Tracker = https://github.com/2niuhe/python_template_carlton/issues repository = https://github.com/2niuhe/python_template_carlton classifiers = Programming Language :: Python :: 3 License :: OSI Approved :: MIT License Operating System :: OS Independent [options] zip_safe = False include_package_data = True packages = find: package_dir = = src python_requires = \u0026gt;=3.6 [options.entry_points] console_scripts = test-hello = carlton_template.mod:hello [options.packages.find] where = src 使用pip安装本地包调试 # 编辑模式安装包 pip install -e . # 调试正常后生成tar包 pip install -U build python -m build # 在dist目录下生成了tar.gz包和whl包 # tar.gz是源码包 # whl是编译的包 # 试着本地安装whl包或者源码包，都可以正常安装使用 pip install python_template-1.0.0-py3-none-any.whl 上传到testpypi 先到testpypi网站注册并申请api token\npip install --upgrade twine python -m twine upload --repository testpypi dist/* 从testpypi安装 pip install --index-url https://test.pypi.org/simple/ --no-deps \u0026lt;package_name\u0026gt; 测试无问题后可以同样的方式发布到正式的pypi仓库中\n参考 Using Python\u0026rsquo;s pip to Manage Your Projects\u0026rsquo; Dependencies – Real Python\nHow to Create and Upload Your First Python Package to PyPI\nHow to Publish an Open-Source Python Package to PyPI – Real Python\nCommand Line Scripts — Python Packaging Tutorial\nsetuptools详解 - 形同陌路love - 博客园\n3. Writing the Setup Configuration File — Python 3.11.8 documentation\nentry_points：程序的入口 | 鲁老师\n2niuhe/python_template_carlton: a python project template\n","permalink":"https://blog.niuhemoon.win/posts/tech/create-pip-package/","summary":"","title":"Python创建自己的package"},{"content":"Dockerfile FROM alpine:latest # 安装OpenSSH服务 RUN apk add --no-cache openssh-server openssh-client sudo docker-cli util-linux bash RUN ln -s /bin/bash /usr/bin/bash RUN mkdir /var/run/sshd # 生成主机密钥 RUN ssh-keygen -A # 添加一个名为\u0026#39;sshuser\u0026#39;的用户，设置密码为\u0026#39;sshpassword\u0026#39;，并将其添加到\u0026#39;sshuser\u0026#39;组 RUN adduser -D sshuser \u0026amp;\u0026amp; \\ echo \u0026#39;sshuser:sshpassword\u0026#39; | chpasswd \u0026amp;\u0026amp; \\ addgroup sshuser wheel RUN echo \u0026#39;%wheel ALL=(ALL) NOPASSWD:ALL\u0026#39; \u0026gt;\u0026gt; /etc/sudoers EXPOSE 22 # 拷贝entrypoint脚本到容器中 COPY entrypoint.sh /entrypoint.sh RUN chmod +x /entrypoint.sh # 指定entrypoint脚本作为容器启动时的执行命令 ENTRYPOINT [\u0026#34;/entrypoint.sh\u0026#34;] 使用的entrypoint.sh如下\n#!/bin/sh if [ -n \u0026#34;$USERNAME\u0026#34; ]; then adduser -D \u0026#34;$USERNAME\u0026#34; if [ -n \u0026#34;$PASSWORD\u0026#34; ]; then echo \u0026#34;$USERNAME:$PASSWORD\u0026#34; | chpasswd else echo \u0026#34;$USERNAME:sshpassword\u0026#34; | chpasswd fi addgroup \u0026#34;$USERNAME\u0026#34; wheel sed -i \u0026#34;s#^$USERNAME:\\([^:]*\\):#$USERNAME:/bin/bash:#\u0026#34; /etc/passwd fi /usr/sbin/sshd \u0026amp; tail -f /dev/null Usage 默认的用户名是sshuser 密码是sshpassword 启动容器后可以从外部使用ssh命令登录容器\n# 创建镜像 docker build -t fake_linux_node:latest . docker tag fake_linux_node:latest 2niuhe/fake_linux_node # run container docker run -d -p 9000:22 2niuhe/fake_linux_node docker run -d -p 9001:22 -e USERNAME=test -e PASSWORD=test123 2niuhe/fake_linux_node # ssh login ssh sshuser@127.0.0.1 -p 9000 ssh test@127.0.0.1 -p 9001 接着可以上传到dockerhub中，已经上传了一个arm版本的，可以如下pull使用\ndocker pull 2niuhe/fake_linux_node 其它用法可以参考Docker Hub\nRef Publish your image | Docker Docs Docker Hub\n","permalink":"https://blog.niuhemoon.win/posts/tech/linux-fake-node/","summary":"","title":"制作Docker镜像模拟服务器节点"},{"content":"Linear Regression 这里举一个回归模型的例子，展示几种模型可视化的方法，分别是\nprint torchinfo.summary torchsummary torchviz torchview netron工具 首先创建一个简单的回归模型\nimport torch # 定义带有两个全连接层和ReLU激活函数的线性回归模型 class LinearRegression(torch.nn.Module): def __init__(self, input_dim, hidden_dim, output_dim): super(LinearRegression, self).__init__() self.fc1 = torch.nn.Linear(input_dim, hidden_dim) self.relu = torch.nn.ReLU() self.fc2 = torch.nn.Linear(hidden_dim, output_dim) def forward(self, x): out = self.fc1(x) out = self.relu(out) out = self.fc2(out) return out # 输入输出维度和隐藏层维度 input_dim = 1 hidden_dim = 64 output_dim = 1 batch_size = 32 # 创建模型 model = LinearRegression(input_dim, hidden_dim, output_dim) # 打印模型 print(model) # LinearRegression( # (fc1): Linear(in_features=1, out_features=64, bias=True) # (relu): ReLU() # (fc2): Linear(in_features=64, out_features=1, bias=True) # ) Model Summary 打印模型的简要信息，包括可训练参数等\nimport torchinfo # 打印模型summary torchinfo.summary( model, input_size=(batch_size, input_dim), col_names=[\u0026#34;input_size\u0026#34;, \u0026#34;output_size\u0026#34;, \u0026#34;num_params\u0026#34;, \u0026#34;trainable\u0026#34;] ) ============================================================================================================================================ Layer (type:depth-idx) Input Shape Output Shape Param # Trainable ============================================================================================================================================ LinearRegression [32, 1] [32, 1] -- True ├─Linear: 1-1 [32, 1] [32, 64] 128 True ├─ReLU: 1-2 [32, 64] [32, 64] -- -- ├─Linear: 1-3 [32, 64] [32, 1] 65 True ============================================================================================================================================ Total params: 193 Trainable params: 193 Non-trainable params: 0 Total mult-adds (Units.MEGABYTES): 0.01 ============================================================================================================================================ Input size (MB): 0.00 Forward/backward pass size (MB): 0.02 Params size (MB): 0.00 Estimated Total Size (MB): 0.02 ============================================================================================================================================ 还可以打印简单的信息\nimport torchsummary torchsummary.summary(model, input_size=(batch_size, input_dim)) ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Linear-1 [-1, 32, 64] 128 ReLU-2 [-1, 32, 64] 0 Linear-3 [-1, 32, 1] 65 ================================================================ Total params: 193 Trainable params: 193 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.00 Forward/backward pass size (MB): 0.03 Params size (MB): 0.00 Estimated Total Size (MB): 0.03 ---------------------------------------------------------------- Torchviz 可以导出graphviz图，需要计算机安装graphviz 执行dot -V 验证graphviz成功安装\nimport torchviz # 定义一个示例输入 example_input = torch.randn(batch_size, input_dim) model = LinearRegression(input_dim, hidden_dim, output_dim) # 使用torchviz绘制计算图 output = model(example_input) dot = torchviz.make_dot(output, params=dict(model.named_parameters()), show_attrs=False, show_saved=True) dot.render(\u0026#34;linear_regression_torchviz\u0026#34;, format=\u0026#34;png\u0026#34;, cleanup=True, view=False) # 如果不是在Jupyter中，注释下面两行 from IPython.display import Image, display display(Image(\u0026#39;./linear_regression.png\u0026#39;)) ◎ Torchviz图示 Torchview 可以绘制规范的线框图，便于展示模型的层次\nfrom torchview import draw_graph model = LinearRegression(input_dim, hidden_dim, output_dim) # device=\u0026#39;meta\u0026#39; -\u0026gt; no memory is consumed for visualization model_graph = draw_graph( model, input_size=(batch_size, input_dim), expand_nested=True, save_graph=True, filename=\u0026#34;linear_regression_torchview\u0026#34;, device=\u0026#39;meta\u0026#39;) model_graph.visual_graph.render(\u0026#34;linear_regression_torchview\u0026#34;, format=\u0026#34;png\u0026#34;) model_graph.visual_graph netron工具 可以看出详细的计算图，但是需要将模型导出成ONNX格式\n# 导出onnx并用netron展示 model = LinearRegression(input_dim, hidden_dim, output_dim) example_input = torch.randn(batch_size, input_dim) torch.onnx.export(model, example_input, \u0026#34;linear_regression.onnx\u0026#34;, verbose=True) Transfomer 如下是Pytorch的Transformer图示\n# Transformer模型可视化 import torch from torch.nn import Transformer from torch.nn import TransformerDecoder from torch.nn import TransformerDecoderLayer transformer_model = Transformer(num_encoder_layers=2, num_decoder_layers=2) src = torch.rand(10,32,512) tgt = torch.rand(20,32,512) print(transformer_model) Model Summary import torchinfo torchinfo.summary( transformer_model, input_size=((10,32,512), (20,32,512)), col_names=[\u0026#34;input_size\u0026#34;, \u0026#34;output_size\u0026#34;, \u0026#34;num_params\u0026#34;, \u0026#34;trainable\u0026#34;] ) ================================================================================================================================================= Layer (type:depth-idx) Input Shape Output Shape Param # Trainable ================================================================================================================================================= Transformer [10, 32, 512] [20, 32, 512] -- True ├─TransformerEncoder: 1-1 [10, 32, 512] [10, 32, 512] -- True │ └─ModuleList: 2-1 -- -- -- True │ │ └─TransformerEncoderLayer: 3-1 [10, 32, 512] [10, 32, 512] 3,152,384 True │ │ └─TransformerEncoderLayer: 3-2 [10, 32, 512] [10, 32, 512] 3,152,384 True │ └─LayerNorm: 2-2 [10, 32, 512] [10, 32, 512] 1,024 True ├─TransformerDecoder: 1-2 [20, 32, 512] [20, 32, 512] -- True │ └─ModuleList: 2-3 -- -- -- True │ │ └─TransformerDecoderLayer: 3-3 [20, 32, 512] [20, 32, 512] 4,204,032 True │ │ └─TransformerDecoderLayer: 3-4 [20, 32, 512] [20, 32, 512] 4,204,032 True │ └─LayerNorm: 2-4 [20, 32, 512] [20, 32, 512] 1,024 True ================================================================================================================================================= Total params: 14,714,880 Trainable params: 14,714,880 Non-trainable params: 0 Total mult-adds (Units.MEGABYTES): 126.18 ================================================================================================================================================= Input size (MB): 1.97 Forward/backward pass size (MB): 64.23 Params size (MB): 33.64 Estimated Total Size (MB): 99.84 ================================================================================================================================================= Torchview from torchview import draw_graph transformer_model = Transformer(num_encoder_layers=1, num_decoder_layers=1) model_graph = draw_graph( transformer_model, input_size=((10,32,512), (20,32,512)), expand_nested=True, save_graph=True, filename=\u0026#34;transformer_torchview\u0026#34;, device=\u0026#39;meta\u0026#39;) model_graph.visual_graph.render(\u0026#34;transformer_torchview\u0026#34;, format=\u0026#34;png\u0026#34;) model_graph.visual_graph 单纯decoder图示\nimport torch from torch.nn import Transformer from torch.nn import TransformerDecoder from torch.nn import TransformerDecoderLayer decoder_layer = TransformerDecoderLayer(d_model=512, nhead=8) decoder_model = TransformerDecoder(decoder_layer=decoder_layer, num_layers=2) model_graph = draw_graph( decoder_model, input_size=((10,32,512), (20,32,512)), expand_nested=True, save_graph=True, filename=\u0026#34;transformer_torchview\u0026#34;, device=\u0026#39;meta\u0026#39;) model_graph.visual_graph.render(\u0026#34;transformer_torchview\u0026#34;, format=\u0026#34;png\u0026#34;) model_graph.visual_graph Ref pytorch模型网络可视化画图工具合集(文后附上完整代码) | by MLTalks | Medium\nNetron\n","permalink":"https://blog.niuhemoon.win/posts/tech/pytorch-model-view/","summary":"","title":"Pytorch模型可视化"},{"content":"记录一下CKA考试的备考过程\nCKA考试备考笔记\n记录备考CKA的过程\n学习K8S的资源主要有三个：\nCertified Kubernetes Administrator (CKA) Practice Exam Tests | Udemy kubectl Cheat Sheet | Kubernetes Udemy Labs - Certified Kubernetes Administrator with Practice Tests - KodeKloud 其中kodekloud提供了在线的实验环境，无需自己搭建k8s集群测试环境 最终考了94分，高分飘过\n考试技巧 考试环境是类似远程桌面，是一个远程的ubuntu桌面，只能在远程桌面内操作，不能使用你自己电脑上的浏览器，但是可以使用远程桌面里的firefox浏览器\n提前30分钟就可以进入考场，出示证件并检查环境\n考试命令行各种快捷键和命令补全都已经配置好了，不用操心\n如果要求是写shell命令行到文件中，不能用k缩写，必须是完整的kubelet命令\n考试时可以复制粘贴k8s官网的内容\n一定要看看网上历年真题，并实际联系，题目几乎一样（很关键）\n最好看英文试题，不会因为翻译产生误解\n最好参加下Killer Shell - Exam Simulators的模拟考试，模拟难度很大，分析学习完会有明显提高\n必学命令\nvim grep命令 yaml语法 netstat命令 ip命令 systemctl命令 journalctl命令 ipcalc命令(可选) apt-cache madison apt-get json-path 笔记 核心概念 核心概念 一切皆是资源\n通过如下命令可以列出api支持的资源\nkubectl api-resources Short name Full name csr certificatesigningrequests cs componentstatuses cm configmaps ds daemonsets deploy deployments ep endpoints ev events hpa horizontalpodautoscalers ing ingresses limits limitranges ns namespaces no nodes pvc persistentvolumeclaims pv persistentvolumes po pods pdb poddisruptionbudgets psp podsecuritypolicies rs replicasets rc replicationcontrollers quota resourcequotas sa serviceaccounts svc services 通用命令 # 列出api接口支持的资源(也包含了资源的apiVersiono和kind信息) kubectl api-resources # 解释具体的资源 kubectl explain \u0026lt;resource-name\u0026gt; kubectl explain replicaset kubectl explain rs | head -n3 # 列出当前namespace所有资源 kubectl get all Pod # 查看所有pods kubectl get pods kubectl get pods -o wide # 创建一个nginx容器 kubectl run nginx --image=nginx # 查看pod的详情 kubectl describe pod \u0026lt;podname\u0026gt; | less # 删除pod kubectl delete pod \u0026lt;podname\u0026gt; # 查找特定lable的pods k get pods --selector env=dev,bu=finance 利用声明式接口创建pod\ndry run机制生成yaml kubectl create -f 从yaml文件生成pod controlplane ~ ➜ kubectl run redis --image=redis123 --dry-run=client -o yaml \u0026gt; redis-definition.yaml controlplane ~ ➜ cat redis-definition.yaml apiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: redis name: redis spec: containers: - image: redis123 name: redis resources: {} dnsPolicy: ClusterFirst restartPolicy: Always status: {} controlplane ~ ➜ kubectl create -f redis-definition.yaml pod/redis created 刚才创建的pod中image名字redis123无法pull镜像，现在进行修复\n方式1:\nkubectl edit pod redis将spec段中的redis123修改为redis，保存即可生效\n方式2:\n直接编辑redis-definition.yaml文件，修改image名后\nkubectl apply -f redis-definition.yaml 方式3:\n先获取pod的yaml\nkubectl get pod redis -o yaml \u0026gt; redis-definition.yaml\n编辑修改后再进行替换\nkubectl replace -f redis-definition.yaml --force\n一些例子\n# 创建带lable的pod kubectl run redis --image=redis:alpine --labels tier=db # 为pod创建一个service k expose pod redis --port 6379 --name redis-service # 创建pod并制定端口 k run custom-nginx --image=nginx --port=8080 # 创建pod同时创建同名的service k run httpd --image httpd:alpine --port=80 --expose 查看/创建static pod\nstatic pod由kubelet直接管理，只能从api接口查看，无法从api管理\n# 查看哪些是static pod，以-controlplane结尾的是的 k get pods --all-namespaces | grep -controlplane # 查找static pod的yaml配置路径 # step1: 查看kubelet的配置文件（启动时通过命令行传递） ps -aux | grep /usr/bin/kubelet # kubelet的配置路径为/var/lib/kubelet/config.yaml grep -i staticpod /var/lib/kubelet/config.yaml # 创建static pod # 首先创建定义pod的yaml文件，然后放到/etc/kubernetes/manifests路径下 kubectl run --restart=Never --image=busybox static-busybox --dry-run=client -o yaml --command -- sleep 1000 \u0026gt; /etc/kubernetes/manifests/static-busybox.yaml Replicaset # 列出replicasets资源 kubectl get replicasets kubectl get rs # 查看replicasets更多信息 kubectl get rs -o wide kubectl describe replicaset \u0026lt;replicaset-name\u0026gt; # 从yaml文件创建replicasets # 命令式：资源已存在会报错 kubectl create -f replicaset-definition.yaml # 声明式：资源已存在不会报错，更适用于版本更新 kubectl apply -f replicaset-definition.yaml # 删除replicasets kubectl delete rs \u0026lt;replicaset-name\u0026gt; # 修复错误的replicasets # step1: 修复配置 kubectl edit rs \u0026lt;replicaset-name\u0026gt; # step2: 删除之前未成功的pods k get po | grep \u0026lt;replicaset-name\u0026gt; | awk \u0026#39;{print $1}\u0026#39; | xargs -n 1 k delete po # scale扩容或缩容 k scale rs \u0026lt;replicaset-name\u0026gt; --replicas=5 # 或者直接k edit编辑replicas k edit rs \u0026lt;replicaset-name\u0026gt; 一些例子\nk create deployment webapp --image kodekloud/webapp-color --replicas DaemonSet # 获取所有namespace中的daemonsets k get daemonsets --all-namespaces # 查看daemonset详情 kubectl describe ds \u0026lt;daemonset-name\u0026gt; --namespace=\u0026lt;namespace-name\u0026gt; # 创建一个daemonset的yaml # step1: 先通过dry run方式创建一个deployment的yaml # step2: 编辑yaml，移除replicas, strategy and status 段 # step3: 将kind从Deployment 改为 DaemonSet 创建daemonset的例子\nk create deployment elasticsearch -n kube-system --image registry.k8s.io/fluentd-elasticsearch:1.20 --dry-run=client -o yaml \u0026gt; daemonset.yaml apiVersion: apps/v1 kind: DaemonSet metadata: creationTimestamp: null labels: app: elasticsearch name: elasticsearch namespace: kube-system spec: selector: matchLabels: app: elasticsearch template: metadata: creationTimestamp: null labels: app: elasticsearch spec: containers: - image: registry.k8s.io/fluentd-elasticsearch:1.20 name: fluentd-elasticsearch resources: {} k apply -f daemonset.yaml Deployment # 列出deployments资源 kubectl get deployments kubectl get deploy # 创建deployment # 方式1:直接命令创建 kubectl create deployment --image httpd:2.4-alpine --replicas 3 httpd-frontent # 方式2:先dry-run生成yaml，然后修改后再apply kubectl create deployment --image httpd:2.4-alpine --replicas 3 --dry-run=client httpd-frontent -o yaml \u0026gt; httpd-deployment.yaml kubectl apply -f httpd-deployment.yaml # 删除deployment k delete deploy \u0026lt;deployment-name\u0026gt; 一些例子\n# 指定namespace创建deployment kubectl create deployment redis-deploy -n dev-ns --image redis --replicas 2 Namespace # 列出所有namespace k get namespaces k get ns # 获取环境中namespace个数 k get ns --no-headers | wc -l # 列出特定namespace中的pods k get pods -n \u0026lt;namespace-name\u0026gt; # 获取所有namespace中的pods k get pods --all-namespaces # 在特定namespace中创建pod k run redis --image=redis -n finance k run redis --image=redis --dry-run=client -n finance -o yaml # 列出特定namespace中的service k get service -n \u0026lt;namespace-name\u0026gt; # 创建一个新的namespace k create namespace \u0026lt;namespace-name\u0026gt; 备注：\nservice的DNS解析策略 同namespace，可以直接用service的名字 不同namesapce，需要加上namespace的后缀，例如 db-service.dev db-service.dev.svc.cluster.local Service # 列出所有services k get services k get svc # 查看特定service详情 k describe svc \u0026lt;service-name\u0026gt; # 创建service，使用类似下民的yaml创建 k apply -f service-definition.yaml --- apiVersion: v1 kind: Service metadata: name: webapp-service namespace: default spec: ports: - nodePort: 30080 port: 8080 targetPort: 8080 selector: name: simple-webapp type: NodePort Node # 获取集群中所有node的信息 kubectl get nodes # 查看节点详情 k describe node \u0026lt;node-name\u0026gt; 调度部分 通过指定nodeName指定节点部署\n--- apiVersion: v1 kind: Pod metadata: name: nginx spec: nodeName: node01 containers: - image: nginx name: nginx Taint和Tolerance\n# 为节点node01添加taint，效果是NoSchedule k taint node node01 spary=mortein:NoSchedule # 为节点node01移除key是spary的taint k taint node node01 spary- 创建可以容忍特定taint的pod\n--- apiVersion: v1 kind: Pod metadata: name: bee spec: containers: - image: nginx name: bee tolerations: - key: spray value: mortein effect: NoSchedule operator: Equal Label\n# 为节点添加lable color=blue kubectl label node node01 color=blue 使用label实现一个deployment中的pod调度亲和\n--- apiVersion: apps/v1 kind: Deployment metadata: name: blue spec: replicas: 3 selector: matchLabels: run: nginx template: metadata: labels: run: nginx spec: containers: - image: nginx imagePullPolicy: Always name: nginx affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: color operator: In values: - blue 遇到pod定义的资源不足以运行\nkubectl get pod elephant -o yaml \u0026gt; ele.yaml # 编辑资源request后 kubectl replace -f ele.yaml --force pod通过schedulerName指定调度器\n--- apiVersion: v1 kind: Pod metadata: name: nginx spec: schedulerName: my-scheduler containers: - image: nginx name: nginx 度量和日志Metric \u0026amp; Log Metrics # Metrics度量 git clone https://github.com/kodekloudhub/kubernetes-metrics-server.git cd kubernetes-metrics-server/ kubectl create -f . # wait a minute kubectl top node # cpu占用最高的节点 kubectl top node --sort-by=\u0026#39;cpu\u0026#39; --no-headers | head -1 # 内存占用最高的节点 kubectl top node --sort-by=\u0026#39;memory\u0026#39; --no-headers | head -1 # 内存占用最多的pod kubectl top pod --sort-by=\u0026#39;memory\u0026#39; --no-headers | head -1 # 内存占用最少的pod kubectl top pod --sort-by=\u0026#39;memory\u0026#39; --no-headers | tail -1 Log # 查看pod日志 kubectl logs \u0026lt;pod-name\u0026gt; # 指定pod中容器查看日志（一个pod可以有多个container） kubectl logs \u0026lt;pod-name\u0026gt; -c \u0026lt;container-name\u0026gt; # 登入pod中指定的容器 kubectl exec -it \u0026lt;pod-name\u0026gt; -c \u0026lt;container-name\u0026gt; -- /bin/bash 应用生命周期管理 更新deployment版本 # 查看deployment的策略 k describe deploy \u0026lt;deployment-name\u0026gt; | grep StrategyType # RollingUpdate意味着一次只更新部分pod # Recreate意味着同时终止pod并重新创建 k describe deploy \u0026lt;deployment-name\u0026gt; | grep RollingUpdateStrategy # 25% max unavailable, 25% max surge意味着最多可以同时终止25%的pod # 更新deployment的镜像 k edit deploy \u0026lt;deployment-name\u0026gt; 命令行 指定容器命令行\napiVersion: v1 kind: Pod metadata: name: ubuntu-sleeper-2 spec: containers: - name: ubuntu image: ubuntu command: - \u0026#34;sleep\u0026#34; - \u0026#34;5000\u0026#34; apiVersion: v1 kind: Pod metadata: name: webapp-green labels: name: webapp-green spec: containers: - name: simple-webapp image: kodekloud/webapp-color command: [\u0026#34;python\u0026#34;, \u0026#34;app.py\u0026#34;] args: [\u0026#34;--color\u0026#34;, \u0026#34;pink\u0026#34;] Secret # 列出secrets k get secrets # 查看secret详情 k describe secrets \u0026lt;secret-name\u0026gt; # 创建secret，名为db-secret kubectl create secret generic db-secret --from-literal=DB_Host=sql01 --from-literal=DB_User=root pod使用secret\n--- apiVersion: v1 kind: Pod metadata: labels: name: webapp-pod name: webapp-pod namespace: default spec: containers: - image: kodekloud/simple-webapp-mysql imagePullPolicy: Always name: webapp envFrom: - secretRef: name: db-secret Multi Container apiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: yellow name: yellow spec: containers: - image: busybox name: lemon resources: {} command: [\u0026#34;sleep\u0026#34;, \u0026#34;1000\u0026#34;] - image: redis name: gold resources: {} dnsPolicy: ClusterFirst restartPolicy: Always status: {} 多容器共享文件系统\n--- apiVersion: v1 kind: Pod metadata: name: app namespace: elastic-stack labels: name: app spec: containers: - name: app image: kodekloud/event-simulator volumeMounts: - mountPath: /log name: log-volume - name: sidecar image: kodekloud/filebeat-configured volumeMounts: - mountPath: /var/log/event-simulator/ name: log-volume volumes: - name: log-volume hostPath: # directory location on host path: /var/log/webapp # this field is optional type: DirectoryOrCreate Init Container --- apiVersion: v1 kind: Pod metadata: name: red namespace: default spec: containers: - command: - sh - -c - echo The app is running! \u0026amp;\u0026amp; sleep 3600 image: busybox:1.28 name: red-container initContainers: - image: busybox name: red-initcontainer command: - \u0026#34;sleep\u0026#34; - \u0026#34;20\u0026#34; Cluster 维护 升级OS # 升级node01，首先清空node01上的pod # 效果 pod evicted, node drained # drain会默认将node cordon，即不可调度 kubectl drain node01 --ignore-daemonsets # node01维护完毕后，将node01设置为可调度 kubectl uncordon node01 # NOTE: drain只能处理replicaset，单独的pod需要强制执行 k drain node01 --ignore-daemonsets --force # 对不属于replicaset的pod，将会永久丢失 # 将node01设置不可调度 kubectl cordon node01 升级K8S # 查看控制节点版本 k version --short # 查看node版本，当前1.26 k get nodes # 查看远端可升级版本 kubeadm upgrade plan # 有两个node分别是controlplane和node01 # 先清空controlplane的pod kubectl drain controlplane --ignore-daemonsets # 开始升级controlplane apt update # 首先升级kubeadm apt-get install kubeadm=1.27.0-00 kubeadm upgrade apply v1.27.0 # 升级命令行 apt-get install kubelet=1.27.0-00 # 重启服务 systemctl daemon-reload systemctl restart kubelet # controlplane升级完毕，标记为可调度 kubectl uncordon controlplane # 需要解除controlplane的taint k drain node01 # 接着ssh登录到node01上，升级node01 apt-get update apt-get install kubeadm=1.27.0-00 kubeadm upgrade node apt-get install kubelet=1.27.0-00 systemctl daemon-reload systemctl restart kubelet # 最后让node01可调度 kubectl uncordon node01 备份恢复 # 查看etcd版本（查看日志或者镜像） kubectl -n kube-system logs etcd-controlplane | grep -i \u0026#39;etcd-version\u0026#39; kubectl -n kube-system describe pod etcd-controlplane | grep Image: # 查看etcd的endpoint kubectl -n kube-system describe pod etcd-controlplane | grep \u0026#39;\\--listen-client-urls\u0026#39; # 查看etcd的cert文件 kubectl -n kube-system describe pod etcd-controlplane | grep \u0026#39;\\--cert-file\u0026#39; # 查看etcd的ca cert文件 kubectl -n kube-system describe pod etcd-controlplane | grep \u0026#39;\\--trusted-ca-file\u0026#39; # 在维护或者对环境做操作前先备份etcd ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 \\ --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ --cert=/etc/kubernetes/pki/etcd/server.crt \\ --key=/etc/kubernetes/pki/etcd/server.key \\ snapshot save /opt/snapshot-pre-boot.db # 在维护窗口期间丢失了一些pod或deployment，需要恢复 # 首先将etcd快照恢复到一个新的目录 ETCDCTL_API=3 etcdctl --data-dir /var/lib/etcd-from-backup \\ snapshot restore /opt/snapshot-pre-boot.db # 修改etcd这个staticpod的yaml，指向恢复的data-dir vim /etc/kubernetes/manifests/etcd.yaml # 修改成如下，将etcd-data这个volume指向hostPath: /var/lib/etcd-from-backup # volumes: # - hostPath: # path: /var/lib/etcd-from-backup # type: DirectoryOrCreate # name: etcd-data # 更新yaml后，etcd会自动重建，若etcd一直PENDING，手动删除触发重启 kubectl delete pod -n kube-system etcd-controlplane 多cluster\n# 查看cluster信息 kubectl config view # 或者 kubectl config get-clusters # 查看当前的cluster kubectl config view | grep current-context # 切换cluster kubectl config use-context cluster2 # Type1: Stacked ETCD Topology # 如下命令可以在cluster中查找到etcd的pod kubectl get pods -n kube-system | grep etcd kubectl -n kube-system describe pod etcd-cluster1-controlplane # Type2: External ETCD # cluster内没有etcd的pod，kube-apiserver直接指向一个外部的etcd endpoint ps -aux | grep etcd # 查看ETCD集群的members ETCDCTL_API=3 etcdctl \\ --endpoints=https://127.0.0.1:2379 \\ --cacert=/etc/etcd/pki/ca.pem \\ --cert=/etc/etcd/pki/etcd.pem \\ --key=/etc/etcd/pki/etcd-key.pem \\ member list # 备份Stacked ETCD kubectl config use-context cluster1 kubectl describe pods -n kube-system etcd-cluster1-controlplane | grep advertise-client-urls # --advertise-client-urls=https://10.1.218.16:2379 kubectl describe pods -n kube-system etcd-cluster1-controlplane | grep pki # --cert-file=/etc/kubernetes/pki/etcd/server.crt # --key-file=/etc/kubernetes/pki/etcd/server.key # --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt # --peer-key-file=/etc/kubernetes/pki/etcd/peer.key # --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt # --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt # /etc/kubernetes/pki/etcd from etcd-certs (rw) # Path: /etc/kubernetes/pki/etcd # 登录到etcd pod所在的节点 ETCDCTL_API=3 etcdctl --endpoints=https://10.1.220.8:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key snapshot save /opt/cluster1.db # 恢复External ETCD ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/etcd/pki/ca.pem --cert=/etc/etcd/pki/etcd.pem --key=/etc/etcd/pki/etcd-key.pem snapshot restore /root/cluster2.db --data-dir /var/lib/etcd-data-new systemctl status etcd.service vim /etc/systemd/system/etcd.service systemctl daemon-reload # service的User是etcd，修改文件权限 chown -R etcd:etcd /var/lib/etcd-data-new systemctl restart etcd # 推荐重启控制平面的pod(kube-scheduler, kube-controller-manager, kubelet) 安全Security TLS和证书介绍 Certificate: Data: Version: 3 (0x2) Serial Number: 3478545236123834268 (0x304646764e49d79c) Signature Algorithm: sha256WithRSAEncryption Issuer: CN = kubernetes Validity Not Before: Dec 1 12:24:30 2023 GMT Not After : Nov 30 12:24:30 2024 GMT Subject: CN = kube-apiserver Subject Public Key Info: Public Key Algorithm: rsaEncryption RSA Public-Key: (2048 bit) Modulus: 00:ce:fc:11:8c:a0:2c:83:2d:20:b2:47:83:dc:38: ec:3f:7f:b5:9a:09:c8:a5:7a:16:7a:c7:2d:1d:62: ae:a6:02:7e:d0:be:6a:c6:fd:71:d3:1a:a8:fd:9b: 4d:11:45:f1:21:aa:20:a1:9d:4e:d7:be:f1:22:25: a9:82:52:87:f8:e5:ce:d5:30:e6:1f:99:a5:13:56: e1:38:e3:68:f5:54:de:67:e1:d1:7e:7a:30:12:6c: 48:fd:d9:89:95:07:2a:51:8e:d8:fa:0c:02:79:54: c4:8f:16:42:b1:f4:a9:0e:ac:83:20:f7:d4:eb:c6: 8f:e2:74:2a:03:c7:2a:b6:d9:c4:ea:28:3c:b8:14: 3f:dd:f0:d9:d9:b2:1f:6c:89:93:0b:37:cd:1b:57: 1c:8e:53:fe:d1:40:f5:80:ee:2d:8d:c6:ce:c2:39: 03:d6:c7:aa:61:cb:b5:8d:5c:d6:73:99:ef:c8:6b: 87:ac:0e:3b:59:bb:ec:e2:c5:04:54:4b:ad:d5:da: 48:16:f5:15:0c:bb:29:fe:13:c7:ed:29:dc:bc:01: b5:ac:dd:84:c9:01:e1:fc:40:1e:8f:c5:4a:82:c5: 69:3a:4a:54:b3:22:c6:4b:61:78:54:59:e9:21:ef: a9:5d:cf:a1:b4:c8:f2:18:4e:6a:03:d3:44:3c:be: 9e:d9 Exponent: 65537 (0x10001) X509v3 extensions: X509v3 Key Usage: critical Digital Signature, Key Encipherment X509v3 Extended Key Usage: TLS Web Server Authentication X509v3 Basic Constraints: critical CA:FALSE X509v3 Authority Key Identifier: keyid:27:89:3A:1C:08:4F:74:4E:60:20:1A:44:E0:47:AC:D6:F9:05:93:E5 X509v3 Subject Alternative Name: DNS:controlplane, DNS:kubernetes, DNS:kubernetes.default, DNS:kubernetes.default.svc, DNS:kubernetes.default.svc.cluster.local, IP Address:10.96.0.1, IP Address:192.21.43.6 Signature Algorithm: sha256WithRSAEncryption 8e:75:b2:8e:47:5c:8f:a1:6c:c8:49:da:ef:e0:09:09:6d:cf: dd💿35:f0:e2:df:b2🇩🇪b0:f0:8a:0a:4b:4b:32:7e:46:45: 6b:0b:52:7b:8d:ad:17:67:59:fb:7d:68:86:2b:d1:91:7d:99: c8:ff:d2:17:46:a0:92:ae:3c:55:9a:e4:f5:ee:59:48:a5:2a: 93:4d:8d:02:ba:02:73:f6:07:36:2a:5a:99:4a:33:52:ce:36: ea:44:29:19:cb:d1:6f:4a:db:1f:d9:47:7e:8c:e7:2b:6a:7f: 11:43:57:f1:f6:7a:19:c1:b6:ff:81:37:71:3a:f6:14:d5:63: ab:9d:31:f7:bc:4c:0a:19:fb:36:d7:84:f2:1c:fd:c5:fc:8d: 83:b0:8f:ec:1b:9c:ae:57:4f:f1:96:f5:45:f5:c5:4e:8f:a0: ac:04:97:fa:87:2c:0d:5c:83:a9:d8:94:2f:d5:64:8d:ec:13: c6:b4:93:d2:29:f9:87:23:a0:90:7b:68:8c:d6:5c:fd:a3:97: 96:68:a7:e0:2a:22:8b:a9:d4:01:fc:f5:06:39:7f:63:6b:33: 8e:3f:6b:23:18:9c:c8:7c:0f:0c:c1:4c:73:3f:a3:c2:d7:ea: cb:2a:a8:5d:86:a1:0d:4a:5e:12:51:ba:6c:1e:1c:20:75:d6: 5f:62:5f:d3 证书查看 # 查看kube-api的cert文件 cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep tls-cert-file # 查看certfile详情 openssl x509 -in ./apiserver.crt -text -noout # kube-api故障，排查问题 # 列出所有的容器 crictl ps -a crictl ps -a | grep kube-api crictl logs \u0026lt;container-id\u0026gt; CertificateSigningRequest # 有签名文件akshay.csr，先base64编码 cat akshay.csr | base64 -w 0 # 按照如下yaml，执行k apply创建csr # 查看csr，刚创建的状态是Pending k get certificatesigningrequests k get csr # 授权，授权后状态是Approved,Issued k certificate approve \u0026lt;csr-name\u0026gt; # 拒绝 csr kubectl certificate deny \u0026lt;csr-name\u0026gt; # 删除csr k delete csr \u0026lt;csr-name\u0026gt; # 查看csr详情 k get csr \u0026lt;csr-name\u0026gt; -o yaml --- apiVersion: certificates.k8s.io/v1 kind: CertificateSigningRequest metadata: name: akshay spec: groups: - system:authenticated request: \u0026lt;Paste the base64 encoded value of the CSR file\u0026gt; signerName: kubernetes.io/kube-apiserver-client usages: - client auth Kube Config Client-certificate（客户端证书）和client-key（客户端密钥）是在SSL/TLS通信中用于客户端身份验证的两个重要组件。\n客户端证书是一种数字证书，用于验证客户端的身份。它包含了客户端的公钥以及与之相关联的身份信息，通常由证书颁发机构（CA）签发。客户端证书可以被服务器用来验证客户端的身份，确保通信双方的合法性和安全性。\n客户端密钥是客户端证书中包含的私钥部分。私钥是一种加密密钥，用于对数据进行签名和解密。客户端使用私钥对数据进行签名，服务器使用客户端证书中的公钥对签名进行验证，从而确认客户端的身份。\n在SSL/TLS通信中，客户端证书和客户端密钥通常一起使用，以确保通信的安全性和完整性。客户端证书用于验证客户端的身份，客户端密钥用于生成数字签名以进行身份验证和数据加密。\n这三个文件名后缀分别代表以下含义：\n.crt：代表证书文件，包含了公钥和证书相关信息。 .csr：代表证书签名请求文件，包含了公钥和一些个人信息，用于向证书颁发机构申请签名。 .key：代表密钥文件，包含了与证书相关的私钥信息。 这三个文件之间的关联性在于：\n证书签名请求文件（.csr）是用来向证书颁发机构申请签名的，其中包含了公钥和一些个人信息。 证书文件（.crt）是由证书颁发机构签名后的证书，包含了公钥和证书相关信息。 密钥文件（.key）包含了与证书相关的私钥信息，用于与证书进行配对，以进行加密和解密操作。 这三个文件通常一起使用，以建立安全的通信连接和进行加密操作。\n# config默认文件路径 ls ~/.kube/config # 查看所有cluster k config view # 指定config文件 kubectl config view --kubeconfig \u0026lt;config-file-path\u0026gt; # 查看config中的current-context kubectl config current-context --kubeconfig \u0026lt;config-file-path\u0026gt; # 切换context kubectl config --kubeconfig=\u0026lt;config-file-path\u0026gt; use-context \u0026lt;new-context-name\u0026gt; # 将自定义的config文件替换~/.kube/config文件，即可设置为默认的config RBAC # 查看kube-api的authorization-mode kubectl describe pod kube-apiserver-controlplane -n kube-system | grep mode # 列出roles k get roles # 列出所有namespace的roles k get roles.rbac.authorization.k8s.io --all-namespaces --no-headers # 查看roles详情 k describe roles \u0026lt;role-name\u0026gt; # 列出rolebinding k get rolebindings --all-namespaces # 查看rolebinding详情 k describe rolebindings \u0026lt;rolebinding-name\u0026gt; -n kube-system # 查看config k config view # 指定config中的用户执行kubectl命令 k get pods --as \u0026lt;user-name\u0026gt; # 上述命令执行报错，用户无权限，需要创建role和rolebinding # 创建一个role，名为developer k create role developer --namespace=default --verb=list,create,delete --resource=pod --dry-run=client -o yaml # 创建rolebinding，将deveploer绑定到用户dev-user上 kubectl create rolebinding dev-user-binding --namespace=default --role=developer --user=dev-user # 编辑role的权限，编辑后无需重新binding k edit role \u0026lt;role-name\u0026gt; Cluster Role # 列出clusterrole k get clusterrole # 列出clusterrolebinding k get clusterrolebindings # 查看环境中cluster数量 k get clusterrole --no-headers | wc -l # 查看clusterrolebinding数量 k get clusterrolebindings.rbac.authorization.k8s.io --no-headers | wc -l # 查看所有与namspace无关的resource k api-resources --namespaced=false # 查看clusterrole详情 k describe clusterrole \u0026lt;clusterrole-name\u0026gt; # 查看clusterrolebinding详情 k describe clusterrolebindings.rbac.authorization.k8s.io \u0026lt;name\u0026gt; # 创建一个管理node的clusterrole k create clusterrole nodeadmin --resource=nodes --verb=get,watch,list,create,delete --dry-run=client -o yaml # 创建clusterrolebinding k create clusterrolebinding michella-binding --clusterrole nodeadmin --user=michelle --dry-run=client -o yaml # 测试是否有执行权限 kubectl auth can-i list nodes --as michelle # 需要给michelle用户增加权限，新建存储相关的clusterrole和binding k create clusterrole storage-admin --resource=persistentvolumes,storageclasses --verb=get,watch,list,create,delete --dry-run=client -o yaml k create clusterrolebinding michelle-storage-admin --clusterrole=storage-admin --user=michelle --dry-run=client -o yaml Service Account # 获取所有service account k get serviceaccounts k get sa # 查看service account详情 k describe sa \u0026lt;sa-name\u0026gt; # 查看pod的serviceaccount k get po web-dashboard-97c9c59f6-vmw8g -o yaml | grep -i account # 创建一个serviceaccount k create serviceaccount dashboard-sa # 用如下yaml为serviceaccount创建role和rolebinding赋予权限 # 为dashboard-sa这个service account创建token kubectl create token dashboard-sa # 编辑deployment的serviceaccount（默认是default） k edit deployments.apps web-dashboard # 或者 k set serviceaccount deploy/web-dashboard dashboard-sa --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: namespace: default name: pod-reader rules: - apiGroups: - \u0026#39;\u0026#39; resources: - pods verbs: - get - watch - list --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: read-pods namespace: default subjects: - kind: ServiceAccount name: dashboard-sa # Name is case sensitive namespace: default roleRef: kind: Role #this must be Role or ClusterRole name: pod-reader # this must match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io Image Secret # 应用使用私有仓库的镜像 k create secret --help # 创建docker-registry类型的密钥 k create secret docker-registry private-reg-cred --docker-username=dock_user --docker-password=dock_passwd --docker-server=myprivateregistry.com:5000 --docker-email=dock_user@myprivateregistry.com apiVersion: apps/v1 kind: Deployment metadata: annotations: deployment.kubernetes.io/revision: \u0026#34;3\u0026#34; creationTimestamp: \u0026#34;2023-12-02T13:18:12Z\u0026#34; generation: 3 labels: app: web name: web namespace: default resourceVersion: \u0026#34;2598\u0026#34; uid: b375109f-b184-4cb1-9a80-425f1a940e3d spec: progressDeadlineSeconds: 600 replicas: 2 revisionHistoryLimit: 10 selector: matchLabels: app: web strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: app: web spec: containers: - image: myprivateregistry.com:5000/nginx:alpine imagePullPolicy: IfNotPresent name: nginx resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst imagePullSecrets: - name: private-reg-cred restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 Security Context # 执行whoami查看容器内部的执行用户 kubectl exec \u0026lt;pod-name\u0026gt; -- whoami # 强制删除pod kubectl delete pod \u0026lt;pod-name\u0026gt; --force # 更新security context必须先删除再新建pod，可以用k replace命令 # 编辑如下yaml，容器内用user ID 1010执行sleep命令 --- apiVersion: v1 kind: Pod metadata: name: ubuntu-sleeper namespace: default spec: securityContext: runAsUser: 1010 containers: - command: - sleep - \u0026#34;4800\u0026#34; image: ubuntu name: ubuntu-sleeper securityContext: capabilities: add: [\u0026#34;SYS_TIME\u0026#34;] 不同level的context\napiVersion: v1 kind: Pod metadata: name: multi-pod spec: securityContext: runAsUser: 1001 containers: - image: ubuntu name: web command: [\u0026#34;sleep\u0026#34;, \u0026#34;5000\u0026#34;] securityContext: runAsUser: 1002 - image: ubuntu name: sidecar command: [\u0026#34;sleep\u0026#34;, \u0026#34;5000\u0026#34;] 编辑如下yaml，为容器增加SYS_TIME的能力\n--- apiVersion: v1 kind: Pod metadata: name: ubuntu-sleeper namespace: default spec: containers: - command: - sleep - \u0026#34;4800\u0026#34; image: ubuntu name: ubuntu-sleeper securityContext: capabilities: add: [\u0026#34;SYS_TIME\u0026#34;] Network Policy # 列出networkpolicy k get networkpolicies.networking.k8s.io k get netpol # 查看netpol详情 k describe netpol \u0026lt;netpol-name\u0026gt; # 使用netpol中对应的selector查询应用的pod k get pods --selector name=payroll # 或者 k get po --show-labels # 创建network policy使得Internal只能访问mysql和payroll # 注意同时放开了53端口，因为需要通过53端口访问DNS # 使用如下yaml实现 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: internal-policy namespace: default spec: podSelector: matchLabels: name: internal policyTypes: - Egress - Ingress ingress: - {} egress: - to: - podSelector: matchLabels: name: mysql ports: - protocol: TCP port: 3306 - to: - podSelector: matchLabels: name: payroll ports: - protocol: TCP port: 8080 - ports: - port: 53 protocol: UDP - port: 53 protocol: TCP 存储Storage Persistent Volume Claim # 查看pod内的日志 kubectl exec \u0026lt;pod-name\u0026gt; -- cat /log/app.log # 为pod添加持久化的日志卷 kubectl get po webapp -o yaml \u0026gt; webapp.yaml # 配置hostpath类型的volume apiVersion: v1 kind: Pod metadata: name: webapp spec: containers: - name: event-simulator image: kodekloud/event-simulator env: - name: LOG_HANDLERS value: file volumeMounts: - mountPath: /log name: log-volume volumes: - name: log-volume hostPath: # directory location on host path: /var/log/webapp # this field is optional type: Directory 用如下yaml创建Persistent Volume\napiVersion: v1 kind: PersistentVolume metadata: name: pv-log spec: persistentVolumeReclaimPolicy: Retain accessModes: - ReadWriteMany capacity: storage: 100Mi hostPath: path: /pv/log 创建Persistent Volume Claim\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: claim-log-1 spec: accessModes: - ReadWriteOnce resources: requests: storage: 50Mi # 列出persistent volume k get persistentvolume k get pv # 列出persistent volume claim k get pvc k get persistentvolumeclaims # 列出多种资源 k get pv,pvc # pod中使用pvc apiVersion: v1 kind: Pod metadata: name: webapp spec: containers: - name: event-simulator image: kodekloud/event-simulator env: - name: LOG_HANDLERS value: file volumeMounts: - mountPath: /log name: log-volume volumes: - name: log-volume persistentVolumeClaim: claimName: claim-log-1 被pod使用中的PVC无法直接删除，需要等到pod删除后才能删掉PVC\nStorage Class # 列出storage class k get storageclasses.storage.k8s.io k get sc # 查看sc详情 k describe sc \u0026lt;sc-name\u0026gt; # 创建storage class，使用如下yaml --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: delayed-volume-sc provisioner: kubernetes.io/no-provisioner volumeBindingMode: WaitForFirstConsumer 注意WaitForFirstConsume模式的storage，pvc绑定pv时，不会立刻申请存储，而是处于PENDING状态，等到使用pvc的pod被调度时，才会实际上申请存储\n如下为一个使用pvc的yaml\n--- apiVersion: v1 kind: Pod metadata: name: nginx labels: name: nginx spec: containers: - name: nginx image: nginx:alpine volumeMounts: - name: local-persistent-storage mountPath: /var/www/html volumes: - name: local-persistent-storage persistentVolumeClaim: claimName: local-pvc 网络Networking 网络基本命令 # 查看node的internal ip地址 k get nodes -o wide # 查看ip地址对应的网口 ip a | grep \u0026lt;ip\u0026gt; # 查看网口的mac地址/状态 ip link show \u0026lt;interface-name\u0026gt; # 查看Containerd创建的网口(Container Network Interface) ip link | grep cni # 查看路由表 ip route list # 查看默认路由 # 默认路由：默认路由是指当目标网络不在路由表中时，数据包将被发送到的默认网关 ip route show default # 查看kube服务监听的port netstat -ntpl | grep kube # 查看etcd的tcp监听服务 netstat -ntpl | grep etcd # 查看etcd的所有链接（包含client） netstat -anp | grep etcd CNI # 查看kubelet的container runtime endpoint ps aux | grep kubelet | grep runtime # 查看cni的插件 ls /opt/cni/bin/ # 查看当前cluster配置的cni插件 ls /etc/cni/net.d/ 部署weave net # 部署weave-net k apply -f weave-daemonset-k8s.yaml # serviceaccount/weave-net created # clusterrole.rbac.authorization.k8s.io/weave-net created # clusterrolebinding.rbac.authorization.k8s.io/weave-net created # role.rbac.authorization.k8s.io/weave-net created # rolebinding.rbac.authorization.k8s.io/weave-net created # daemonset.apps/weave-net created apiVersion: v1 kind: List items: - apiVersion: v1 kind: ServiceAccount metadata: name: weave-net labels: name: weave-net namespace: kube-system - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: weave-net labels: name: weave-net rules: - apiGroups: - \u0026#39;\u0026#39; resources: - pods - namespaces - nodes verbs: - get - list - watch - apiGroups: - extensions resources: - networkpolicies verbs: - get - list - watch - apiGroups: - \u0026#39;networking.k8s.io\u0026#39; resources: - networkpolicies verbs: - get - list - watch - apiGroups: - \u0026#39;\u0026#39; resources: - nodes/status verbs: - patch - update - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: weave-net labels: name: weave-net roleRef: kind: ClusterRole name: weave-net apiGroup: rbac.authorization.k8s.io subjects: - kind: ServiceAccount name: weave-net namespace: kube-system - apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: weave-net namespace: kube-system labels: name: weave-net rules: - apiGroups: - \u0026#39;\u0026#39; resources: - configmaps resourceNames: - weave-net verbs: - get - update - apiGroups: - \u0026#39;\u0026#39; resources: - configmaps verbs: - create - apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: weave-net namespace: kube-system labels: name: weave-net roleRef: kind: Role name: weave-net apiGroup: rbac.authorization.k8s.io subjects: - kind: ServiceAccount name: weave-net namespace: kube-system - apiVersion: apps/v1 kind: DaemonSet metadata: name: weave-net labels: name: weave-net namespace: kube-system spec: # Wait 5 seconds to let pod connect before rolling next pod selector: matchLabels: name: weave-net minReadySeconds: 5 template: metadata: labels: name: weave-net spec: initContainers: - name: weave-init image: \u0026#39;weaveworks/weave-kube:2.8.1\u0026#39; command: - /home/weave/init.sh env: securityContext: privileged: true volumeMounts: - name: cni-bin mountPath: /host/opt - name: cni-bin2 mountPath: /host/home - name: cni-conf mountPath: /host/etc - name: lib-modules mountPath: /lib/modules - name: xtables-lock mountPath: /run/xtables.lock readOnly: false containers: - name: weave command: - /home/weave/launch.sh env: - name: IPALLOC_RANGE value: 10.32.1.0/24 - name: INIT_CONTAINER value: \u0026#34;true\u0026#34; - name: HOSTNAME valueFrom: fieldRef: apiVersion: v1 fieldPath: spec.nodeName image: \u0026#39;weaveworks/weave-kube:2.8.1\u0026#39; readinessProbe: httpGet: host: 127.0.0.1 path: /status port: 6784 resources: requests: cpu: 50m securityContext: privileged: true volumeMounts: - name: weavedb mountPath: /weavedb - name: dbus mountPath: /host/var/lib/dbus readOnly: true - mountPath: /host/etc/machine-id name: cni-machine-id readOnly: true - name: xtables-lock mountPath: /run/xtables.lock readOnly: false - name: weave-npc env: - name: HOSTNAME valueFrom: fieldRef: apiVersion: v1 fieldPath: spec.nodeName image: \u0026#39;weaveworks/weave-npc:2.8.1\u0026#39; #npc-args resources: requests: cpu: 50m securityContext: privileged: true volumeMounts: - name: xtables-lock mountPath: /run/xtables.lock readOnly: false hostNetwork: true dnsPolicy: ClusterFirstWithHostNet hostPID: false restartPolicy: Always securityContext: seLinuxOptions: {} serviceAccountName: weave-net tolerations: - effect: NoSchedule operator: Exists - effect: NoExecute operator: Exists volumes: - name: weavedb hostPath: path: /var/lib/weave - name: cni-bin hostPath: path: /opt - name: cni-bin2 hostPath: path: /home - name: cni-conf hostPath: path: /etc - name: cni-machine-id hostPath: path: /etc/machine-id - name: dbus hostPath: path: /var/lib/dbus - name: lib-modules hostPath: path: /lib/modules - name: xtables-lock hostPath: path: /run/xtables.lock type: FileOrCreate priorityClassName: system-node-critical updateStrategy: type: RollingUpdate # 查看weave的agent/peer k get pods --all-namespaces | grep weave # weave pod在每个节点上存在一个（包括master和work node） # 查看weave创建的网口 ip link | grep weave # 查看weave网口的ip地址段 ip addr show weave ip route list # 查看work node上weave net的默认网关，首先登录到work node ip route | gre weave Service Networking # 查看整个cluster的网络范围 ip addr | grep eth0 # 215: eth0@if216: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1450 qdisc noqueue state UP group default # inet 192.4.210.9/24 brd 192.4.210.255 scope global eth0 ipcalc -b 192.4.210.9 # 计算的cluster的地址范围192.4.210.0/24 # 查看cluster中pod的网络地址范围 # 若pod使用weave网络，使用如下查询方式 k logs -n kube-system weave-net-8v8hr | grep ipalloc-range # pod地址范围是10.244.0.0/16 # 查看service的网络地址范围 cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep cluster-ip-range # --service-cluster-ip-range=10.96.0.0/12 # 查看kube-proxy的类型 k logs -n kube-system \u0026lt;kube-proxy-pod-name\u0026gt; CoreDNS # 查看cluster的DNS解决方案 k get pods -n kube-system | grep dns # 查看dns service k get service --all-namespaces | grep dns # 查看coredns的配置文件 k -n kube-system describe deployments.apps coredns | grep -A2 Args # 查看Corefile的注入方式 k get po -n kube-system coredns-5d78c9869d-5sxxz -o yaml k describe cm -n kube-system coredns # service的DNS规则如下：service.namespace.svc.cluster.local # web-service.payroll.svc.cluster.local # 查看DNS结果 kubectl exec -it hr -- nslookup mysql.payroll \u0026gt; /root/CKA/nslookup.out Ingress # 查看ingress kubectl get all -A | grep -i ingress k get ingress -A # 查看ingress详情 kubectl describe ingress \u0026lt;ingress-name\u0026gt; # 编辑ingress修改rules kubectl edit ingress # 创建ingress，注意如下命令缺少annotation，创建的yaml添加annotation才能正常工作 # pathType: Exact k create ingress -n critical-space test-ingress --rule=/pay=pay-service:8282 --dry-run=client -o yaml # pathType: Prefix k create ingress -n critical-space test-ingress --rule=/pay*=pay-service:8282 --dry-run=client -o yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: nginx.ingress.kubernetes.io/rewrite-target: / nginx.ingress.kubernetes.io/ssl-redirect: \u0026#34;false\u0026#34; name: ingress-wear-watch namespace: app-space spec: rules: - http: paths: - backend: service: name: wear-service port: number: 8080 path: /wear pathType: Prefix - backend: service: name: video-service port: number: 8080 path: /stream pathType: Prefix 如下为cluster创建ingress controller\n# 创建ingress独立的namespace(便于隔离) k create namespace ingress-nginx # 创建configmap kubectl create configmap ingress-nginx-controller --namespace ingress-nginx # 创建service account k create sa ingress-nginx -n ingress-nginx k create sa ingress-nginx-admission -n ingress-nginx # 创建role，rolebinding，clusterrole，clusterrolebinding # 略 # ingress controller 的yaml如下 apiVersion: apps/v1 kind: Deployment metadata: labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.1.2 helm.sh/chart: ingress-nginx-4.0.18 name: ingress-nginx-controller namespace: ingress-nginx spec: replicas: 1 minReadySeconds: 0 revisionHistoryLimit: 10 selector: matchLabels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx template: metadata: labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx spec: containers: - args: - /nginx-ingress-controller - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller - --election-id=ingress-controller-leader - --watch-ingress-without-class=true - --default-backend-service=app-space/default-http-backend - --controller-class=k8s.io/ingress-nginx - --ingress-class=nginx - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller - --validating-webhook=:8443 - --validating-webhook-certificate=/usr/local/certificates/cert - --validating-webhook-key=/usr/local/certificates/key env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: LD_PRELOAD value: /usr/local/lib/libmimalloc.so image: registry.k8s.io/ingress-nginx/controller:v1.1.2@sha256:28b11ce69e57843de44e3db6413e98d09de0f6688e33d4bd384002a44f78405c imagePullPolicy: IfNotPresent lifecycle: preStop: exec: command: - /wait-shutdown livenessProbe: failureThreshold: 5 httpGet: path: /healthz port: 10254 scheme: HTTP initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 name: controller ports: - name: http containerPort: 80 protocol: TCP - containerPort: 443 name: https protocol: TCP - containerPort: 8443 name: webhook protocol: TCP readinessProbe: failureThreshold: 3 httpGet: path: /healthz port: 10254 scheme: HTTP initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 resources: requests: cpu: 100m memory: 90Mi securityContext: allowPrivilegeEscalation: true capabilities: add: - NET_BIND_SERVICE drop: - ALL runAsUser: 101 volumeMounts: - mountPath: /usr/local/certificates/ name: webhook-cert readOnly: true dnsPolicy: ClusterFirst nodeSelector: kubernetes.io/os: linux serviceAccountName: ingress-nginx terminationGracePeriodSeconds: 300 volumes: - name: webhook-cert secret: secretName: ingress-nginx-admission --- apiVersion: v1 kind: Service metadata: creationTimestamp: null labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.1.2 helm.sh/chart: ingress-nginx-4.0.18 name: ingress-nginx-controller namespace: ingress-nginx spec: ports: - port: 80 protocol: TCP targetPort: 80 nodePort: 30080 selector: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx type: NodePort # 接着创建ingress，ingress资源是在namespace范围下的，所以ingress要和service同一个namespace k create ingress -n app-space --rule /wear*=wear-service:8080 --rule /watch*=video-service:8080 --annotation nginx.ingress.kubernetes.io/rewrite-target=/ --annotation nginx.ingress.kubernetes.io/ssl-redirect=false test-ingress --dry-run=client -o yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: nginx.ingress.kubernetes.io/rewrite-target: / nginx.ingress.kubernetes.io/ssl-redirect: \u0026#34;false\u0026#34; creationTimestamp: null name: test-ingress namespace: app-space spec: rules: - http: paths: - backend: service: name: wear-service port: number: 8080 path: /wear pathType: Prefix - backend: service: name: video-service port: number: 8080 path: /watch pathType: Prefix status: loadBalancer: {} 部署K8S # 在每个节点上执行如下安装操作 # step1: 修改网络策略 cat \u0026lt;\u0026lt;EOF | sudo tee /etc/modules-load.d/k8s.conf br_netfilter EOF cat \u0026lt;\u0026lt;EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sudo sysctl --system # step2: 安装包 sudo apt-get update sudo apt-get install -y apt-transport-https ca-certificates curl sudo mkdir -m 755 /etc/apt/keyrings curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.27/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg echo \u0026#39;deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.27/deb/ /\u0026#39; | sudo tee /etc/apt/sources.list.d/kubernetes.list sudo apt-get update # To see the new version labels sudo apt-cache madison kubeadm sudo apt-get install -y kubelet=1.27.0-2.1 kubeadm=1.27.0-2.1 kubectl=1.27.0-2.1 sudo apt-mark hold kubelet kubeadm kubectl Installing kubeadm | Kubernetes\n# step3: 查看kubelet版本 kubelet --version # step4: 使用kubeadm启动kubernetes集群 # step4-1: 启动controlplane # 查看eth0网口ip地址 ifconfig eth0 # 假设上一步eth0地址是192.17.132.9 kubeadm init --apiserver-cert-extra-sans=controlplane --apiserver-advertise-address 192.17.132.9 --pod-network-cidr=10.244.0.0/16 # 或者执行如下命令 IP_ADDR=$(ip addr show eth0 | grep -oP \u0026#39;(?\u0026lt;=inet\\s)\\d+(\\.\\d+){3}\u0026#39;) kubeadm init --apiserver-cert-extra-sans=controlplane --apiserver-advertise-address $IP_ADDR --pod-network-cidr=10.244.0.0/16 # 执行完毕后，配置kubeconfig mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \u0026#34;kubectl apply -f [podnetwork].yaml\u0026#34; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 192.17.132.9:6443 --token b2134k.u3alyaj4iwkjhm6w \\ --discovery-token-ca-cert-hash sha256:23471ecdb6d52790d13880d333fa0a667cc249297ed26faa7b75171b84157c46 # Step 4-2: 将node01加入到cluster # kubeadm生成join token kubeadm token generate # 或 kubeadm token create --print-join-command # 生成如下命令，ssh登录到node01上执行 kubeadm join 192.17.132.9:6443 --token 2uh5xs.qoh26441ba0l4du4 --discovery-token-ca-cert-hash sha256:23471ecdb6d52790d13880d333fa0a667cc249297ed26faa7b75171b84157c46 # Step5: 安装Flannel网络 # 下载kube-flannel.yml curl -LO https://raw.githubusercontent.com/flannel-io/flannel/v0.20.2/Documentation/kube-flannel.yml # 编辑kube-flannel.yml # 定位到 args: - --ip-masq - --kube-subnet-mgr # 添加参数 - --iface=eth0 kubectl apply -f kube-flannel.yml # 网络创建成功后，node状态变成Ready controlplane ~ ➜ kubectl get nodes NAME STATUS ROLES AGE VERSION controlplane Ready control-plane 15m v1.27.0 node01 Ready \u0026lt;none\u0026gt; 15m v1.27.0 Trouble Shooting Application Failure：\ndns解析的service名称不对 service中selector没有选中工作的pod service的targetPort配置错误 db的用户名密码错误 service暴露的node端口错误 Control Plane Failure：\nkube-scheduler故障，修改/etc/kubernetes/manifests/下的yaml scale无响应，查看kube-controller故障 Work Node Failure:\n计算节点containerd服务或者kubelet服务停掉了，用systemctl命令手动启动 /var/lib/kubelet/config.yaml配置文件有问题 /etc/kubernetes/kubelet.conf中kube-api的端口配置 Network Failure:\n缺少网络插件Creating a cluster with kubeadm | Kubernetes\ncurl -L https://github.com/weaveworks/weave/releases/download/latest_release/weave-daemonset-k8s-1.11.yaml | kubectl apply -f -\n查看所有kube-proxy是否运行正常，查看关联的configMap是否正确，修改daemonset\nLighting Lab # json path定义输出 kubectl get deployments.apps -n admin2406 -o custom-columns=DEPLOYMENT:.metadata.name,CONTAINER_IMAGE:.spec.template.spec.containers[*].image,READY_REPLICAS:.status.readyReplicas,NAMESPACE:.metadata.namespace --sort-by=.metadata.name 参考 Kubectl Command Cheat Sheet - Learn Essential Kubernetes Commands\nKubectl Cheatsheet | Free Cheatsheet\nLF 认证考试攻略｜认证考试流程全介绍\u0026ndash;购买、注册及预约考试篇（建议收藏）-Linux Foundation开源软件学园\nCKA (Certified Kubernetes Administrator)\nKiller Shell - Exam Simulators\nLinux Foundation Certification Exam: Candidate Handbook (using PSI BRIDGE Proctoring platform) - T\u0026amp;C DOCS (Candidate Facing Resources)\n考试入口Certified Kubernetes Administrator China Exam (CKA-CN) - Exam | The Linux Foundation\nCKA考试心得分享 - 木二 - 博客园\nCKA考试经验总结 - 简书\nIntroduction to YAML - KodeKloud\n互联网最全cka真题解析-2022.9.9 - 知乎\n2023年CKA考试真题及注意事项 - jiayou111 - 博客园\nKubernetes CKA真题解析-20200402真题_check to see how many nodes are ready (not includi-CSDN博客\n","permalink":"https://blog.niuhemoon.win/posts/tech/cka-study-record/","summary":"\u003cp\u003e记录一下CKA考试的备考过程\u003c/p\u003e","title":"CKA备考笔记"},{"content":" 参加了一个LLM的应用比赛，赛题目标是编码优化prompt提高text2sql推理的正确率 给定的资源如下：\n所有数据库schema 一个100条记录的训练集（其实是基于spider数据集做了修改） 三个LLM的接口（Baichuan13B/LLama2/Code-LLama） text2sql正确率提高到了60%左右，在此记录一些心得\nprompt的组成包四个元素：\nInstruction（指令，必须） Context（上下文信息，可选） Input Data（输入，需要处理的数据，可选） Output Indicator（输出指引，规定输出的类型或格式，可选） 一个面向复杂任务的prompt的一般都包含Instruction，Context，Input Data，Output Indicator。 所以面向大语言模型的开发应用过程就是如下公式：\nLMM(Instruction + Context + Input Data + Output Indicator) = Output\n题目要求根据自然语言的Question，给出对应数据库的SQL Query语句\n做Prompt优化的核心步骤有：\n利用LLM的翻译能力将输入的中文Question转换成英文 解析数据库schema，格式化后加入prompt，其中不同table间的外键很关键，column的字段类型可有可无 few shot，给出数据库关联的三个案例（和问题数据库关联的案例有明显提升效果） chain of thought，指导LLM如何思考，注意构造sql语句的哪些步骤 诱导慢思考（Let\u0026rsquo;s think step by step） 多LLM交叉验证和单LLM self consistency，即多问几次，投票选择出现次数多的sql语句 后处理校验sql语句的正确性 当然更好的办法是对LLM进行预训练，Prompt优化或者LLM agent的提升效果有限\n","permalink":"https://blog.niuhemoon.win/posts/tech/text2sql-prompt-engineering/","summary":"参加了一个LLM的应用比赛，赛题目标是编码优化prompt提高text2sql推理的正确率 给定的资源如下： 所有数据库schema 一个100条记录的训练集（其实是基于spider数据集做了修改） 三个LLM的接口（Baichuan13B/LLama2/Code-LLama） text2","title":"text2sql Prompt 调优笔记"},{"content":"介绍 Xpath全称XML Path Language，功能上使用类似路径的语法来识别和导航XML文档中的节点，同时支持HTML语言。XPath是W3C的推荐标准\nNodes Xpath中有七种节点：\nelement attribute text namespace processing-instruction comment root nodes 句法 选中Nodes 表达式 描述 nodename 选中所有同名节点 / 从root node开始选择 // 从当前节点开始选择，不管层级关系 . 选择当前节点 .. 选择当前节点的父节点 @ 选择属性 举例如下\n表达式 效果 bookstore Selects all nodes with the name \u0026ldquo;bookstore\u0026rdquo; /bookstore Selects the root element bookstore bookstore/book Selects all book elements that are children of bookstore //book Selects all book elements no matter where they are in the document bookstore//book Selects all book elements that are descendant of the bookstore element, no matter where they are under the bookstore element //@lang Selects all attributes that are named lang Predicates 谓词 Path Expression Result /bookstore/book[1] Selects the first book element that is the child of the bookstore element. /bookstore/book[last()] Selects the last book element that is the child of the bookstore element /bookstore/book[last()-1] Selects the last but one book element that is the child of the bookstore element /bookstore/book[position()\u0026lt;3] Selects the first two book elements that are children of the bookstore element //title[@lang] Selects all the title elements that have an attribute named lang //title[@lang=\u0026lsquo;en\u0026rsquo;] Selects all the title elements that have a \u0026ldquo;lang\u0026rdquo; attribute with a value of \u0026ldquo;en\u0026rdquo; /bookstore/book[price\u0026gt;35.00] Selects all the book elements of the bookstore element that have a price element with a value greater than 35.00 /bookstore/book[price\u0026gt;35.00]/title Selects all the title elements of the book elements of the bookstore element that have a price element with a value greater than 35.00 选择未知节点 Wildcard Description * Matches any element node @* Matches any attribute node node() Matches any node of any kind Path Expression Result /bookstore/* Selects all the child element nodes of the bookstore element //* Selects all elements in the document //title[@*] Selects all title elements which have at least one attribute of any kind 选择多条路径 通过使用|或操作符，可以选择多条路径\nPath Expression Result //book/title | //book/price Selects all the title AND price elements of all book elements //title | //price Selects all the title AND price elements in the document /bookstore/book/title | //price Selects all the title elements of the book element of the bookstore element AND all the price elements in the document Axes Axes(轴)用来表示和当前节点之间的关系，用于定位树上当前节点的关联节点\nAxisName Result ancestor Selects all ancestors (parent, grandparent, etc.) of the current node ancestor-or-self Selects all ancestors (parent, grandparent, etc.) of the current node and the current node itself attribute Selects all attributes of the current node child Selects all children of the current node descendant Selects all descendants (children, grandchildren, etc.) of the current node descendant-or-self Selects all descendants (children, grandchildren, etc.) of the current node and the current node itself following Selects everything in the document after the closing tag of the current node following-sibling Selects all siblings after the current node namespace Selects all namespace nodes of the current node parent Selects the parent of the current node preceding Selects all nodes that appear before the current node in the document, except ancestors, attribute nodes and namespace nodes preceding-sibling Selects all siblings before the current node self Selects the current node 路径定位表达式 绝对路径以/开头 定位的语法如下：axisname::nodetest[predicate] Axisname轴名称 Nodetest节点选择 Predicate 谓词细化选择 Example Result child::book Selects all book nodes that are children of the current node attribute::lang Selects the lang attribute of the current node child::* Selects all element children of the current node attribute::* Selects all attributes of the current node child::text() Selects all text node children of the current node child::node() Selects all children of the current node descendant::book Selects all book descendants of the current node ancestor::book Selects all book ancestors of the current node ancestor-or-self::book Selects all book ancestors of the current node - and the current as well if it is a book node child::*/child::price Selects all price grandchildren of the current node 操作符 Xpath操作符可以返回如下类型的值：\nnode-set string number boolean Operator Description Example | Computes two node-sets //book | //cd + Addition 6 + 4 - Subtraction 6 - 4 * Multiplication 6 * 4 div Division 8 div 4 = Equal price=9.80 != Not equal price!=9.80 \u0026lt; Less than price\u0026lt;9.80 \u0026lt;= Less than or equal to price\u0026lt;=9.80 \u0026gt; Greater than price\u0026gt;9.80 \u0026gt;= Greater than or equal to price\u0026gt;=9.80 or or price=9.80 or price=9.70 and and price\u0026gt;9.00 and price\u0026lt;9.90 mod Modulus (division remainder) 5 mod 2 Functions函数 函数 作用 name() 获取node的名称//[starts-with(name(), \u0026lsquo;h\u0026rsquo;)] text() 获取node的文本//button[text()=\u0026ldquo;Submit\u0026rdquo;] lang(str) 获取字符串的语言 count() node计数//table[count(tr)=1] position() 计算位置//ol/li[position()=2] number() boolean() not() 取反button[not(starts-with(text(),\u0026ldquo;Submit\u0026rdquo;))] contains() 字符串包含font[contains(@class,\u0026ldquo;head\u0026rdquo;)] starts-with() font[starts-with(@class,\u0026ldquo;head\u0026rdquo;)] ends-with() font[ends-with(@class,\u0026ldquo;head\u0026rdquo;)] concat(x, y) substring(str, start, len) 浏览器使用示例 chrome浏览器有两种使用方法：\nDevtool-Elements, CTRL+F Devtool-Console, $x(xpath_syntax) 案例 解释 /bookstore/book/title 选择所有书的title /bookstore/book[1]/title 选择第一本书的title /bookstore/book/price[text()] 选择所有书的价格 /bookstore/book[price\u0026gt;35]/price 选择价格大于35的price节点 /bookstore/book[price\u0026gt;35]/title 选择价格大于35的title节点 $x(\u0026#39;//bookstore/book/title\u0026#39;) (4) [title, title, title, title] $x(\u0026#39;//bookstore/book[1]/title\u0026#39;) [title] $x(\u0026#39;//bookstore/book[position() \u0026gt; 2]/title\u0026#39;) (2) [title, title] $x(\u0026#39;//bookstore/book[price \u0026gt; 30]/title\u0026#39;) (2) [title, title] $x(\u0026#39;.//bookstore/book[1]/price\u0026#39;)[0].textContent \u0026#39;30.00\u0026#39; $x(\u0026#39;.//bookstore/book[1]/price[text()]\u0026#39;)[0].textContent \u0026#39;30.00\u0026#39; $x(\u0026#39;.//bookstore/book[1]/price/text()\u0026#39;)[0].textContent \u0026#39;30.00\u0026#39; $x(\u0026#39;.//bookstore/book[@id=\u0026#34;testid\u0026#34;]/title\u0026#39;)[0].textContent \u0026#39;Everyday Italian\u0026#39; $x(\u0026#39;.//bookstore/book[@class=\u0026#34;testclass\u0026#34;]/title\u0026#39;)[0].textContent \u0026#39;Harry Potter\u0026#39; $x(\u0026#39;.//bookstore/book[contains(@class, \u0026#34;test\u0026#34;)]/title\u0026#39;)[0].textContent \u0026#39;Harry Potter\u0026#39; Xpath测试html打开开发者工具可以试验 \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e Everyday Italian Giada De Laurentiis 2005 30.00 Harry Potter J K. Rowling 2005 29.99 XQuery Kick Start James McGovern Per Bothner Kurt Cagle James Linn Vaidyanathan Nagarajan 2003 49.99 Learning XML Erik T. Ray 2003 39.95 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;bookstore\u0026gt; \u0026lt;book id=\u0026#34;testid\u0026#34; category=\u0026#34;cooking\u0026#34;\u0026gt; \u0026lt;title lang=\u0026#34;en\u0026#34;\u0026gt;Everyday Italian\u0026lt;/title\u0026gt; \u0026lt;author\u0026gt;Giada De Laurentiis\u0026lt;/author\u0026gt; \u0026lt;year\u0026gt;2005\u0026lt;/year\u0026gt; \u0026lt;price\u0026gt;30.00\u0026lt;/price\u0026gt; \u0026lt;/book\u0026gt; \u0026lt;book class=\u0026#34;testclass\u0026#34; category=\u0026#34;children\u0026#34;\u0026gt; \u0026lt;title lang=\u0026#34;en\u0026#34; href=\u0026#34;test.pdf\u0026#34;\u0026gt;Harry Potter\u0026lt;/title\u0026gt; \u0026lt;author\u0026gt;J K. Rowling\u0026lt;/author\u0026gt; \u0026lt;year\u0026gt;2005\u0026lt;/year\u0026gt; \u0026lt;price\u0026gt;29.99\u0026lt;/price\u0026gt; \u0026lt;/book\u0026gt; \u0026lt;book category=\u0026#34;web\u0026#34;\u0026gt; \u0026lt;title lang=\u0026#34;en\u0026#34; href=\u0026#34;/test\u0026#34;\u0026gt;XQuery Kick Start\u0026lt;/title\u0026gt; \u0026lt;author\u0026gt;James McGovern\u0026lt;/author\u0026gt; \u0026lt;author\u0026gt;Per Bothner\u0026lt;/author\u0026gt; \u0026lt;author\u0026gt;Kurt Cagle\u0026lt;/author\u0026gt; \u0026lt;author\u0026gt;James Linn\u0026lt;/author\u0026gt; \u0026lt;author\u0026gt;Vaidyanathan Nagarajan\u0026lt;/author\u0026gt; \u0026lt;year\u0026gt;2003\u0026lt;/year\u0026gt; \u0026lt;price\u0026gt;49.99\u0026lt;/price\u0026gt; \u0026lt;/book\u0026gt; \u0026lt;book category=\u0026#34;web\u0026#34;\u0026gt; \u0026lt;title lang=\u0026#34;en\u0026#34; href=\u0026#34;https://www.google.com\u0026#34;\u0026gt;Learning XML\u0026lt;/title\u0026gt; \u0026lt;author\u0026gt;Erik T. Ray\u0026lt;/author\u0026gt; \u0026lt;year\u0026gt;2003\u0026lt;/year\u0026gt; \u0026lt;price\u0026gt;39.95\u0026lt;/price\u0026gt; \u0026lt;/book\u0026gt; \u0026lt;/bookstore\u0026gt; 参考 Xpath cheatsheet\nXPath Tutorial\n","permalink":"https://blog.niuhemoon.win/posts/tech/xpath-cheetsheet/","summary":"介绍 Xpath全称XML Path Language，功能上使用类似路径的语法来识别和导航XML文档中的节点，同时支持HTML语言。XPath是W3C的推荐标准 Nodes Xpath中有七种节点： element attribute text namespace processing-instruction comment root nodes 句法 选中Nodes 表达式 描述 nodename 选中所有同名节点 / 从root node开始选择 // 从当前节点开","title":"xpath基本使用"},{"content":" 2023 年 7 月:\n怨仇星域三部曲 2023 年 6 月:\n回忆爱玛侬 2023 年 5 月:\n山月记 2023 年 4 月:\n第三次浪潮 Where Wizards Stay Up Late A Philosophy of Software Design 2023 年 2 月：\n我每天只工作三小时 置身事内 程序员修炼之道 2023 年 1 月：\n二手时间 认知天性：让学习轻而易举的心理学规律 数字思维 ","permalink":"https://blog.niuhemoon.win/posts/read/%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%952023/","summary":"2023 年 7 月: 怨仇星域三部曲 2023 年 6 月: 回忆爱玛侬 2023 年 5 月: 山月记 2023 年 4 月: 第三次浪潮 Where Wizards Stay Up Late A Philosophy of Software Design 2023 年 2 月： 我每天只工作三小时 置身事内 程序员修炼之道 2023 年 1 月： 二手时间 认知天性：让学习轻而易举的心理学规律 数字思维","title":"阅读记录2023"},{"content":"在现代Web应用开发中，HTTPS已经成为标准配置。本文将详细介绍如何使用Node.js实现HTTPS服务器，包括证书生成、单向认证和双向认证的完整实现。\n前言 HTTPS（HTTP Secure）是HTTP协议的安全版本，它通过TLS/SSL加密来保护数据传输。在实现HTTPS服务器时，我们需要了解两种认证模式：单向认证和双向认证。\n准备工作 首先，我们需要安装必要的依赖：\nnpm install express https 一、单向认证实现 单向认证是最常见的HTTPS实现方式，只有服务器需要提供证书，客户端验证服务器的身份，但服务器不验证客户端的身份。这是我们日常访问大多数网站时使用的模式。\n1. 生成证书 首先，我们需要生成自签名证书：\n# 生成私钥 openssl genpkey -algorithm RSA -out private_key.pem # 生成证书签名请求(CSR) openssl req -new -key private_key.pem -out certificate.csr # 使用私钥对CSR进行签名，生成自签名证书 openssl x509 -req -in certificate.csr -signkey private_key.pem -out certificate.pem -days 365 2. 服务器实现 以下是单向认证的HTTPS服务器实现：\nconst express = require(\u0026#39;express\u0026#39;); const https = require(\u0026#39;https\u0026#39;); const fs = require(\u0026#39;fs\u0026#39;); const app = express(); // 中间件，用于验证HTTPS const forceHttps = (req, res, next) =\u0026gt; { if (req.secure) { next(); } else { res.status(401).send(\u0026#39;Unauthorized - HTTPS only.\u0026#39;); } }; // 应用中间件，对所有请求进行HTTPS验证 app.use(forceHttps); // 在这里添加其他路由和处理程序 app.get(\u0026#39;/\u0026#39;, (req, res) =\u0026gt; { res.send(\u0026#39;Hello, this is a secure HTTPS server!\u0026#39;); }); // HTTPS配置 const options = { key: fs.readFileSync(\u0026#39;./private_key.pem\u0026#39;), cert: fs.readFileSync(\u0026#39;./certificate.pem\u0026#39;) }; // 启动HTTPS服务器 const server = https.createServer(options, app); const port = 3443; // 使用非特权端口 server.listen(port, () =\u0026gt; { console.log(`Server running on port ${port}`); }); 3. 客户端实现 以下是单向认证的客户端实现：\nconst https = require(\u0026#39;https\u0026#39;); const options = { hostname: \u0026#39;127.0.0.1\u0026#39;, port: 3443, // 使用与服务器相同的非特权端口 path: \u0026#39;/\u0026#39;, // 请求的路径 method: \u0026#39;GET\u0026#39;, // 请求方法，可以是GET、POST等 rejectUnauthorized: false, }; // 发送HTTPS请求 const req = https.request(options, (res) =\u0026gt; { console.log(\u0026#39;statusCode:\u0026#39;, res.statusCode); // 接收响应数据 res.on(\u0026#39;data\u0026#39;, (data) =\u0026gt; { process.stdout.write(data); }); }); // 错误处理 req.on(\u0026#39;error\u0026#39;, (error) =\u0026gt; { console.error(\u0026#39;Error:\u0026#39;, error); }); // 结束请求 req.end(); 在单向认证中，注意客户端代码中的rejectUnauthorized: false选项。这允许客户端接受自签名证书，在生产环境中应该设置为true并使用受信任的证书。\n二、双向认证实现 双向认证（也称为相互认证）要求服务器和客户端都提供证书，双方互相验证对方的身份。这种模式通常用于需要高安全性的场景，如银行API、企业内部系统等。\n1. 证书生成 双向认证需要一个更完整的PKI（公钥基础设施）。以下是生成所需证书的脚本：\n#!/bin/bash # 创建证书目录 mkdir -p certs # 生成CA私钥和自签名证书 openssl genrsa -out certs/ca.key 2048 openssl req -new -x509 -key certs/ca.key -out certs/ca.crt -subj \u0026#34;/CN=My CA\u0026#34; -days 365 # 生成服务器私钥和CSR openssl genrsa -out certs/server.key 2048 openssl req -new -key certs/server.key -out certs/server.csr -subj \u0026#34;/CN=localhost\u0026#34; # 使用CA签署服务器证书 openssl x509 -req -in certs/server.csr -CA certs/ca.crt -CAkey certs/ca.key -CAcreateserial -out certs/server.crt -days 365 # 生成客户端私钥和CSR openssl genrsa -out certs/client.key 2048 openssl req -new -key certs/client.key -out certs/client.csr -subj \u0026#34;/CN=client\u0026#34; # 使用CA签署客户端证书 openssl x509 -req -in certs/client.csr -CA certs/ca.crt -CAkey certs/ca.key -CAcreateserial -out certs/client.crt -days 365 echo \u0026#34;所有证书已生成完成\u0026#34; 2. 服务器实现 以下是双向认证的HTTPS服务器实现：\nconst https = require(\u0026#39;https\u0026#39;); const fs = require(\u0026#39;fs\u0026#39;); const express = require(\u0026#39;express\u0026#39;); const app = express(); // 创建简单的路由 app.get(\u0026#39;/\u0026#39;, (req, res) =\u0026gt; { // 获取客户端证书信息 const clientCert = req.socket.getPeerCertificate(); if (req.client.authorized) { res.send(`安全连接已建立! 客户端证书CN: ${clientCert.subject.CN}`); } else { res.status(401).send(\u0026#39;未授权: 客户端证书无效\u0026#39;); } }); // HTTPS服务器选项 - 启用双向认证 const options = { key: fs.readFileSync(\u0026#39;./certs/server.key\u0026#39;), cert: fs.readFileSync(\u0026#39;./certs/server.crt\u0026#39;), ca: fs.readFileSync(\u0026#39;./certs/ca.crt\u0026#39;), // 用于验证客户端证书的CA requestCert: true, // 请求客户端证书 rejectUnauthorized: true // 拒绝未授权的客户端 }; // 创建HTTPS服务器 const server = https.createServer(options, app); const port = 4443; server.listen(port, () =\u0026gt; { console.log(`双向认证HTTPS服务器运行在端口 ${port}`); }); 3. 客户端实现 以下是双向认证的客户端实现：\nconst https = require(\u0026#39;https\u0026#39;); const fs = require(\u0026#39;fs\u0026#39;); // HTTPS请求选项 - 包含客户端证书 const options = { hostname: \u0026#39;localhost\u0026#39;, port: 4443, path: \u0026#39;/\u0026#39;, method: \u0026#39;GET\u0026#39;, // 客户端证书设置 key: fs.readFileSync(\u0026#39;./certs/client.key\u0026#39;), cert: fs.readFileSync(\u0026#39;./certs/client.crt\u0026#39;), ca: fs.readFileSync(\u0026#39;./certs/ca.crt\u0026#39;), // 用于验证服务器证书的CA rejectUnauthorized: true // 验证服务器证书 }; // 发送HTTPS请求 const req = https.request(options, (res) =\u0026gt; { console.log(\u0026#39;状态码:\u0026#39;, res.statusCode); // 接收响应数据 res.on(\u0026#39;data\u0026#39;, (data) =\u0026gt; { console.log(\u0026#39;响应:\u0026#39;, data.toString()); }); }); // 错误处理 req.on(\u0026#39;error\u0026#39;, (error) =\u0026gt; { console.error(\u0026#39;请求错误:\u0026#39;, error); }); // 结束请求 req.end(); 单向认证与双向认证的区别 特性 单向认证 双向认证 服务器证书 需要 需要 客户端证书 不需要 需要 服务器验证客户端 否 是 客户端验证服务器 是 是 安全级别 中等 高 适用场景 公共网站 企业内部系统、金融API 使用curl测试HTTPS服务器 以下是一些使用curl测试HTTPS服务器的命令：\n# 测试单向认证服务器（忽略证书验证） curl https://127.0.0.1:3443 --insecure # 测试单向认证服务器（使用CA证书验证） curl --cacert certificate.pem https://localhost:3443 # 测试双向认证服务器（提供客户端证书） curl --cacert certs/ca.crt --cert certs/client.crt --key certs/client.key https://localhost:4443 总结 本文详细介绍了如何使用Node.js实现HTTPS服务器的单向认证和双向认证。单向认证适用于大多数公共网站场景，而双向认证则提供了更高级别的安全性，适用于需要严格访问控制的场景。\n在实际应用中，建议在生产环境使用受信任的CA签发的证书，而不是自签名证书。此外，妥善保管私钥和证书文件，定期更新证书，以确保系统安全。\n","permalink":"https://blog.niuhemoon.win/posts/tech/nodejs-https-server-authentication/","summary":"\u003cp\u003e在现代Web应用开发中，HTTPS已经成为标准配置。本文将详细介绍如何使用Node.js实现HTTPS服务器，包括证书生成、单向认证和双向认证的完整实现。\u003c/p\u003e","title":"Node.js实现HTTPS服务器：单向认证与双向认证详解"},{"content":"《奈飞文化手册》 范珂译\n◆ 推荐序一 文化，奈飞成功的原动力\n奈飞的企业文化主要是由“自由”与“责任”这两个关键词构成的。我之前认为：“现代企业文化的核心就是要靠制度管人，而不是靠人管人。”但奈飞在这个基础上又进化了一步，它提出，现代企业文化的核心还是人管人，但这个管人的人不是企业的管理者，而是员工自己。“自由”与“责任”的核心就是要将权力还给员工，让他们能够在自由的环境中充分施展自己的能力，履行自己的责任。\n◆ 前言 自由与责任，奈飞文化的核心\n当我介绍我们在奈飞开发的管理方法时，我会对所有关于管理的基本假设发起挑战，管理并不是要在员工忠诚度、人员稳定和职业发展方面做工作，同时实施相关项目来提升员工的敬业度和幸福感。这里面没有一项是管理应该做的工作。我的一个极端主张是，管理者的本职工作是建立伟大的团队，按时完成那些让人觉得不可思议的工作。只有这一项工作是管理应该做的。\n◆ 成年人最渴望的奖励，就是成功\n今天的管理理念认为，如果想让员工提高生产力，就必须先用奖金来激励他们，然后让他们知道自己的行为是受到监控的，以便让他们保持做事的责任心。因此，很多公司都有部门目标、团队目标和个人目标，还有一套正式的年度绩效评估流程，用以测量绩效。这种结构的逻辑性非常强，也非常合理。但是，仅有这些是远远不够的。对员工说：“如果你做X，你就会得到奖励Y”，背后的假设是“系统是静态的”。但是，今天没有哪项业务是静态的。更为根本的是，尽管有奖励很不错，但是，没有哪种奖励能比为迎接挑战而做出重大贡献更具激励效果。\n你能够为员工做的最好的事情，就是只招聘那些高绩效的员工来和他们一起工作。这可远比桌上足球、免费寿司、一大笔签约奖金或者股票期权更有吸引力。\n◆ 培养基层员工的高层视角\n如何做到让每位员工都理解公司业务1．建立新员工大学，保持沟通的强节奏。2．双向沟通，为员工提供向所有管理者提问的机会。3．让每一位员工了解，他为客户带来的体验是如何直接影响公司利润的。4．如果只选择一门课程面向公司全员开授，请选择公司业务运作和客户服务的基本知识。5．最好的福利，是让员工有机会去更好地了解业务和客户。\n◆ 人前人后言行一致\n人力资源部门副总裁会把我叫到他的办公室，大声质问：“你是不是开工程师的玩笑了？”我说：“是的，不过，有没有搞错！他们抱怨浴缸里的水不够热，毛巾不够软，还有游泳池里的水太凉了。”副总裁就会责备我：“工程师是我们最重要的资源，你必须给予他们特殊对待！”我并不认同这一点。就像前面讲过的，我实在是厌倦了把工程师当作上帝一样对待。到了哈斯廷斯那里，一切都不一样了。在面试我时，他的第一个问题是：“你的人力资源理念是什么？”记住，我可是在太阳计算机系统和柏兰德两家公司工作过的，于是我用人力资源术语流利地回答道：“我认为每个人都应该有野心，坦诚，能够通过被赋能为企业做出贡献。”哈斯廷斯看着我说：“你说的是什么？你知道你刚才说的那番话毫无意义吗？那些词串起来，甚至都不能组成一句有逻辑的话！”我镇定自若地回答道：“嘿，你根本不了解我！”哈斯廷斯针锋相对：“如果我们像这样谈话，你觉得我能怎么了解你？告诉我，你会做什么来帮助公司成长？”\n给予反馈最重要的是要针对行为，而不是笼统地给一个人定性，比如“你不够专心。”反馈的内容必须是可操作的，反馈对象必须理解他们的行为需要做出哪些特定的改变。像“你做得不错，但是还不够”，这样的评论实际上是毫无意义的。你可以这么说：“我能看到你工作非常努力，我很欣赏这一点。但是，我也注意到，你在某些事情上花了太多时间，而这些时间本可以花在更为重要的事情上。”这样，你接下来就可以和对方更好地确定事情的优先顺序了。\n信任是建立在坦诚沟通的基础之上的，我发现当员工听到半真半假的话时就会开始冷嘲热讽。冷嘲热讽就像是癌细胞，它容易扩散、转移成牢骚和不满，并导致阿谀奉承和背后中伤等不良风气。\n传统观点认为，如果允许人们匿名，他们就会表现得更坦诚。但根据我的经验，情况并非如此。坦诚的人会坦诚地对待任何事情。而且，如果你不知道是谁给你的反馈，你怎么把他的评语和他的工作背景、他的上级以及他的性格特点结合起来呢？匿名反馈最大的问题就在于它传递出这样一个信息：人们只有在对方不知道自己是谁的时候才是最坦诚的。\n◆ 坚持你的观点，用事实为它辩护\n我们在奈飞提出了一个要求，即人们必须通过探求事实来完善自己的观点，并且以开放的心态去倾听那些他们并不认同，但以事实为依据的辩论。这个要求执行得很顺利，因为我们早期的员工都是数学家和工程师，科学方法就是他们的生命，他们依靠这些方法来发现事实，然后调整自己对问题的理解以及处理问题的方式。随着公司的壮大，我们有意培养员工对事实驱动和科学方法的痴迷，不只是在工程部，而是在全公司。你的公司也可以广泛推行这套行为准则，即便不是工程师的公司也可以。\n我们根据直觉采取了不少行动，在为团队寻找人才时，我会留意那些会分析数据，而且凭直觉就知道该如何忽略数据的人。”泰德还警告说，数据可以被当作一个挡箭牌，抵挡本该用主观判断来做出决定的责任。基于真实数据来做决定会令人们感到更自在，部分原因是：如果决定是错误的话，就可以把责任推到数据上面。\n◆ 不要让招聘成为一场数字游戏\n当时的美国国防部长唐纳德·拉姆斯菲尔德（Donald Rumsfeld）有一句名言：“你要带着你现有的军队，而不是你想有的或者你以后希望有的军队参加战争。”当我和管理者们讨论如何建立优秀团队时，我告诉他们要按照完全相反的方式来行事。你必须现在就招聘你未来希望拥有的团队成员。\n晋升员工并指导他们扮演新角色，对团队领导者来说是一件相当有满足感的事情，对团队绩效来说也是一件好事。但是，晋升和培养员工对于团队绩效来说，通常并不是最理想的方式。管理者不应该期望自己成为员工的职业规划者。在今天快速发展的商业环境中，试图扮演这种角色是很危险的。\n我相信，对于今天职场人士的最佳建议就是：保持灵活，不断学习新技能，不断考虑新机会，经常接受新挑战，这样可以保持工作的新鲜感和延展性。奈飞鼓励员工为自己的成长负责，利用好公司提供的大量机会，向那些优秀的同事和管理者学习，无论这样做是意味着在公司内部获得晋升还是在公司外部获得一个好机会。\n有时候，在某个时间段对一个组织来说是非常合适，也喜欢为这个组织工作的那些人，最好是跳槽去那些有类似挑战和环境的新组织。我告诉这位工程师：“没关系，你不必非得成为其中的一分子，也许你在一家50人的公司里会更快乐，也许那里才是你能找到最大乐趣的地方。”\n◆ 人才保留不是团队建设的目标\n正如奈飞最好的用人经理之一的约翰·辛库提曾经跟我说的：“知道什么时候让员工离开与引进一个拥有你所需技能的顶尖人才，是相辅相成的。他们就像一枚硬币的两面。如果你不是很擅长招聘优秀人才，那么在让优秀员工离开时，你的心里就不会好受。二者缺一，你都无法做好另外一项，也永远无法建立一支高绩效的团队。”\n最有竞争力的公司能够保持灵活，不断创新和增长，很大原因在于它们总在积极地引进新的人才。最好的员工则总是在寻找有挑战的新机会，尽管他们大多极度忠诚，但是他们中的很多人最终还是会到别处寻找机会。你永远也不会知道他们何时决定离开，通常你也没有办法阻止他们。\n员工在工作中的幸福不应该是美味沙拉、睡袋或桌球。工作中真实和持久的幸福源于和你认识的优秀人才一起深入地解决一个问题，源于客户喜爱你付出辛勤努力所创造的产品或服务。\n但谷歌在招聘方面做得更好，因为它的目标更大，它要把世界上所有的信息组织起来。还有比这个更大的目标吗？所以，谷歌尽可能多地招募那些聪明人，把他们放到一个拥有所有资源的环境中，让他们产出大量创意，然后把那些最顶级的创意利用起来。谷歌的领导者希望利用不同的方式推动公司的发展，因此人才数量对他们来说非常重要\n我有一个铁律，如果有招聘人员看见一个陌生人坐在那里等候面试，他们必须过去和他说：“嗨，我叫XX，您是哪一位？您是要面试吗？您在等谁？让我看看您今天的面试安排，我来帮您安排一下。”我知道招聘团队成员清楚地听到了这个规则，因为如果我在面试某个应聘者时迟到的话，我会说：“对不起，我希望有人已经跟您聊过了。”他们会说：“有6个人跟我聊过了。”面试的重要性高于用人经理事先预订的任何会议，这也是高管会议的与会者可能缺席或提前离开会议的唯一理由。候选人在评估你，就像你在评估他们一样，但人们很容易忘记这一点。\n◆ 薪酬与绩效评估流程无关，只与绩效有关\n根据我的经验，如果你有意招聘你能发现的最佳人选、给他们支付最高的薪水，你会发现，他们为业务增长带来的价值总是会大大超过他们的薪水。\n◆ 每10场比赛就做一次评估\n有很多次，在完成演讲之后，人们会走上前来向我征求职业发展意见。我告诉他们：你需要成为一个终身学习者。你需要不断获取新技能并积累新经验，但不是非得在同一家公司取得它们。事实上，有时候公司聘用你做某件事，你做完就完了。如果我雇用别人来整修我的车库，当他们干完活儿之后，我并不需要他们来整修我的后院。\n1．如果员工的表现不够好，及时告诉他们要么纠正过来，要么去一家新公司。2．不要把与工作不再匹配的员工归结为失败者。3．不要给员工无法实现的承诺，这只会让他们感觉自己被背叛了。4．积极地帮助离职员工找到新的好机会。\n","permalink":"https://blog.niuhemoon.win/posts/read/%E5%A5%88%E9%A3%9E%E6%96%87%E5%8C%96%E6%89%8B%E5%86%8C/","summary":"\u003cp\u003e\u003cstrong\u003e《奈飞文化手册》\u003c/strong\u003e\n范珂译\u003c/p\u003e","title":"奈飞文化手册"},{"content":"《世界观：现代人必须要懂的科学哲学和科学史（原书第2版）》 理查德·德威特（Richard DeWitt）\n◆ 第1章 世界观\n一个观点如果即使本身发生变化也不对其所在的观点体系产生实质性改变，那它就是一个典型的外围观点。\n总之，我们只能为我们所秉持的极小一部分观点拿出直接证据。对我们的大多数观点（也许是几乎所有观点）来说，我们之所以秉持这些观点，主要在于它们可以跟一个很大的、其中各个观点相互联结的观点集合拼合在一起。换句话说，我们之所以秉持这样的观点主要是因为它们可以跟我们的世界观拼合在一起。\n◆ 第2章 真理\n所以，总结一下，根据真理符合论，决定一个观点为真的因素是这个观点与独立、客观的现实相符合；决定一个观点为假的因素是这个观点没能与那样的现实相符合。\n在这里列出个人主义融贯论和以科学为基础的融贯论主要是为了说明在融贯论这个理论类别中还有许多不同的小类别。由于不同种类的融贯论的主要区别在于考虑了哪些人的观点，同时，存在很多不同的方法来解释具体考虑了哪些人的观点，因此我们必须明白，可能存在大量差异巨大的融贯论。\n从本质上讲，知觉表征论的核心是：感官为我们提供了外部世界各种物体的表征（对视觉来说，这些表征大致类似图画）。同样地，这是一个几乎所有人都认为理所当然的观点。不过，这个观点同时也有些有趣的推论，而这些推论却直接影响了真理符合论。这些推论中最重要的一个是，这个观点意味着我们每个人从某种意义上来说与这个世界都是隔绝的。更具体地说，我们没有办法确定自身感官所提供的表征是否准确。\n如果自身感官为我们提供了外部世界的表征，那么接下来一个合理的问题就是这些表征是否准确。正如我们刚刚讨论过的，要评估感官提供的表征是否准确，我们需要把这些表征和表征所代表的事物进行对比。然而，让我们再看一看图2-1中萨拉的意识图解。假设萨拉想评估她关于苹果的视觉表征是否正确，要达到这个目的，她需要把苹果的视觉表征与真正的苹果进行对比。但是，萨拉没有办法这么做。萨拉不能把苹果的视觉表征与真正的苹果进行对比的原因是她无法从自己的意识中走出来。从萨拉的角度来看，她所能运用的都在她的意识里。\n这个情形就像是为了评估恶魔塔照片的准确性而把照片和恶魔塔的地形图或者恶魔塔周围道路的地图进行对比。在这种情况下，对比是在两个表征之间进行的，而评估表征准确性所需要的对比，也就是表征与这个表征所代表的事物之间的对比，并没有进行。这个推论说明，我们根本没有办法评估感官给我们提供的表征是否准确，或者换句话说，我们没有办法确定现实到底是什么样子的\n总之，尽管我们都认为自己的体验来自于“正常”的现实，但我们并不能确定这些体验不是来自于某种《全面回忆》情境植入我们大脑中的现实。简言之，我们无法确定现实真正的样子。\n总结一下，个人主义融贯论似乎会陷入一种让人无法接受的相对主义。另一方面，团体融贯论似乎避免了相对主义的问题，但是同时又带来了几个新的、不容忽视的问题。所以，不管是真理融贯论还是真理符合论，对关于真理的核心问题，都无法提供让人完全满意的答案。\n甚至“我有一个身体”的观点也经不起测试，因为可能邪恶骗子正在往我没有实体的大脑中植入身体的图像。那有没有观点经得起这个测试呢？也就是说，是否存在可以让我们感到完全确定的观点？笛卡尔认为他找到了至少一个这样的观点，就是他的名言“Cogito，ergosum”，也就是“我思，故我在”。笛卡尔表示，这是一个可以让他感到完全确定的观点。\n◆ 第3章 经验事实和哲学性/概念性事实\n我猜测你之所以这样认为，是源于你看待这个世界的方式。我们大部分人无法想象物体在我们观察不到的时候就不再存在了。我们对自己所生活的这个世界有一个判断，那就是“组成这个世界的大部分物体是稳定的，即使在没有被观察到的时候，仍然保持存在”。对此，我们深信不疑，而这正是我们认为抽屉里有一支铅笔的根源。\n把事实与观点区分开来就意味着两者之间存在相当清晰的区别，也就意味着事实是事实，而观点仅仅是观点。然而，两者之间实际上没有这样一个明确的区别，至少在一个人的生命过程中或者一个人自身的世界观中不会有这样的区别（在这里，可以再考虑一下“书桌上的铅笔和抽屉里的铅笔”的例子）。从一个人自己的世界观来看，那些他感到深信不疑而又有强有力证据支撑的观点似乎就是事实。\n在我们所处的时代，事实只是对我们来说看起来像事实，它们看起来都差不多。只有经过仔细思考，有时在思考过程中还要克服极大的困难，然后我们才会发现自己所秉持的某些观点更偏向于以经验为基础，而另一些观点则更偏向于以哲学性/概念性观点为基础。\n◆ 第4章 证实与不证实证据和推理\n证实推理是一种归纳推理，而不证实推理则是一种演绎推理。证实推理的归纳推理性质和不证实推理的演绎推理性质都具有一些重要影响。要理解这些影响，我们首先需要明确归纳推理和演绎推理之间的不同之处。\n这就是归纳推理的特点：在一个好的归纳推理过程中，即使所有前提条件都是真的，所得出的结论也有可能是错的。相比之下，在一个好的演绎推理论证过程中，真的前提条件就保证了真的结论。也就是说，在一个好的演绎推理论证过程中，如果所有前提条件都是真的，那么其所得出的结论就一定是真的。\n证实推理模式和不证实推理模式是科学领域内外两个常见的推理模式。一方面，证实推理模式由于是一种归纳推理模式，因而无法在证明一个理论正确的同时保证这一正确性不受质疑。因此，对于一个科学理论来说，不管有多少可以证明其正确性的证据，这个理论是错误的这种可能性始终存在。除此之外，在实际的例子里，归纳得出的证据和归纳推理通常非常复杂且相互交织。证实推理模式及证据往往远没有它们乍看起来那么直接明确。另一方面，不证实推理模式是一种演绎推理。然而实际上，由不证实推理模式得出的证据往往同样很复杂。具体来说，通常不证实推理模式涉及大量辅助假设。因此，通过不证实推理模式得出的证据只能表明要么是所使用的理论不正确，要么就是一个或几个辅助假设不正确（经常出现的是后者）。因此，不证实推理模式及证据同样也远没有它们乍看起来那么直接明确。\n◆ 第5章 奎因-迪昂论点和对科学方法的意义\n笛卡尔这个方法的基本问题是，它不足以成为一个基础。简言之，在寻找关于这个世界的必定为真的起始点时，笛卡尔的问题与亚里士多德的问题是一样的，也就是，似乎不存在得到一致认可的、必定为真的起始点。尽管，一个人至少可以在“我存在”（至少作为一个思考主题存在）的主张上找到某些确定性，对这一观点可能有更多共识，但是这个观点同样太单薄了，无法成为进行知识构建的基础。\n◆ 第6章 哲学插曲：归纳的问题和困惑\n总之，每一天他们所处的世界都与前一天有一些不同。由于自己所处的世界始终在变化，这两人不知道每天会遇到什么。对他们来说，未来不会像过去一样。因此，他们无法对未来做出那种我们都认为是理所当然的归纳推理。（大概他们所能做出的唯一一种关于未来的归纳推理，就是未来不会继续像过去一样，而这当然并不是一个特别有帮助的推理结论。）所以，要理解休谟的归纳问题，应认识到的第一个关键点是：前面提到的那句话，也就是未来将继续像过去一样，是每一个关于未来的推理所必需的隐含前提，尽管通常都不为人察觉。\n总结一下，休谟的观点是每一个归纳推理都依赖于“未来将继续像过去一样”的隐含前提。但是，用来解释支撑这个隐含前提的主要（似乎也是唯一的）方法是循环的，因此，看起来这个关键的隐含前提无法得到足够支撑。所以，关于未来的推理依赖于一个无法得到支撑的假设，这些推理从逻辑上也就无法得到支撑。\n休谟的问题是，我们是否可以从逻辑上为我们关于未来的推理提供依据，而他的答案是：我们不可以。\n◆ 第8章 工具主义和现实主义\n对工具主义者来说，一个适当的理论可以给出预言和解释，至于这个理论是否反映或模拟现实世界，并不是一个重要的考量。而对现实主义者来说，事情恰恰相反，一个合理的理论必须不仅可以给出预言和解释，而且要反映现实事物的真实情况。\n真理符合论的支持者认为真理是符合现实的观点，而真理融贯论的支持者则认为当一个观点可以与一个整体的观点体系相融合，或者说是拼合在一起时，那么这个观点就是真理。因此，接下来的疑问就会是真理的符合论和融贯论是否与工具主义和现实主义紧密相连。\n◆ 第9章 亚里士多德世界观中的宇宙结构\n总的来说，目的论解释是从目标、目的或功能角度提出的解释，而机械论解释则是不使用目标、目的和功能的解释。\n◆ 第14章 哥白尼体系\n他认为有很多各种各样客观存在，但又没有实体的永恒“形式”。这些形式是知识的客观存在，也就是说，相对于仅仅得到一个信念或观点，当我们得到了知识时，我们的知识就是关于一个或多个这样客观存在，但又没有实体的永恒形式的。举个例子，当我们知道了毕达哥拉斯定理，或数学中的其他真理，我们所得到的知识并不是关于地球上某种物体的（比如，画在纸上的三角形），而是关于一个客观存在，但又没有实体的永恒形式的。\n◆ 第20章 新科学和牛顿世界观概述\n回到向地面下落的钢笔的例子，请注意钢笔和地球之间似乎不存在联系，并没有橡皮筋把地球和钢笔绑在一起，也没有细绳，什么都没有。然而，尽管如此，钢笔在被松开以后仍然向地球移动。从这个角度来看，重力听起来并不像是科学，而像是魔法。\n牛顿世界观也随时间推移而经历了发展，不过一个机械论的、像机器一样的宇宙一直保留了下来，并且成为这一世界观的核心观点。\n◆ 第21章 哲学插曲：什么是科学定律\n反映无例外的规律性似乎是科学定律的一个关键特点。\n◆ 第三部分 科学及世界观的新近发展\n与17世纪发生的情况一样，我们看到某些我们一直认为显而易见的经验事实，在运用了新近科学发展的视角后，都被证明是错误的哲学性/概念性事实。\n◆ 第23章 狭义相对论\n光速恒定原则是狭义相对论赖以为基础的基本原则之一\n◆ 第24章 广义相对论\n火星围绕太阳沿椭圆轨道运转并不是火星与太阳之间相互的吸引力或者说万有引力的结果。相反，与其他运动的物体一样，火星沿直线运动。然而，在一个弯曲的空间中，“直线”其实是测地线。正如我们在前面看到的，根据广义相对论，像太阳这样的物体会导致时空曲率。根据广义相对论方程式，这个曲率之大，会使火星运动所沿的测地线变成围绕太阳的一个椭圆形。换句话说，在广义相对论中，像火星和太阳这样的物体之间不存在吸引“力”。事实上，火星只是沿直线运动，但是由于时空曲率，这条直线变成了围绕太阳的一个椭圆形。\n◆ 第28章 演化的哲学与概念影响\n生命及其蕴含之力能，最初注入到寥寥几个或单个类型之中；当这一行星按照固定的引力法则循环运行之时，无数最美丽与最奇异的类型，即是从如此简单的开端演化而来、并依然在演化之中；生命如是之观，何等壮丽恢弘。（Darwin，1964\n◆ 第29章 世界观：总结思考\n我认为，相对论真正更重要的影响是，它深刻表明了在一些看起来显而易见的命题上，我们犯了多么严重的错误。或者换句话说，它表明了哲学性/概念性事实伪装成显而易见的经验事实是多么容易。举个例子，我认识的所有人在了解相对论之前都认为空间和时间对任何人来说都是相同的，而且把这当作一个显而易见的经验事实。\n再来梳理一下：在亚里士多德世界观中，宇宙被看作像一个生物有机体，各部分分别发挥其作用，从而共同实现天然的目标和目的；在牛顿世界观中，宇宙被看作像一台机器，各个部分通过推拉与其他部分发生相互作用，与机器里的零部件彼此发生相互作用的方式一样。这类隐喻既很有魅力，又很有用——这一点很容易理解，因为它们提供了一种方便而又简单的方式来总结对宇宙的整体观点。不过，新近的这些发现都有一个有趣的特点，那就是它们所主张的宇宙与我们经历过的任何事物都不一样。\n","permalink":"https://blog.niuhemoon.win/posts/read/%E4%B8%96%E7%95%8C%E8%A7%82/","summary":"\u003cp\u003e《世界观：现代人必须要懂的科学哲学和科学史（原书第2版）》\n理查德·德威特（Richard DeWitt）\u003c/p\u003e","title":"世界观"},{"content":" Tip\n文中操作基于Manjaro系统\n涉及软件:\ncalibre管理电子书 mega网盘 同步备份图书库 微信读书 手机便捷阅读 (web端同步，支持导出批注) okular 阅读pdf (功能强大，支持标注) zlib 下载电子书 (目前要用tor浏览器才能访问) 之前读了很多技术和非技术的书籍，非技术的书基本都是epub,mobi格式，技术类的基本是pdf格式。书籍零散，趁着元旦用calibre 整理一下。\n安装配置calibre # 安装calibre yay -Sy calibre 从豆瓣下载元数据和封面插件\nReleases · fugary/calibre-douban\ncalibre默认有两个用户:\n本地电子阅读器用户 (即直接双击calibre书籍条目阅读) 匿名的内容服务器用户 (calibre启动内容服务器，在网页上阅读) 默认这两个用户的批注是不互通的，需要合并批注的话，双击一本epub书，在首选项-杂项-与内容服务器用户同步书签/突出显示里进行配置\n安装并配置calibre-web calibre-web目前阅读功能较弱，胜在UI比较好看，主要用来从网页管理图书\nmkdir calibre-web \u0026amp;\u0026amp; cd calibre-web python3 -m venv ./venv source venv/bin/activate # install calibreweb pip install calibreweb # start calibreweb cps # open browser http://localhost:8083 calibre-web添加豆瓣源数据api\ncalibre-web-douban-api\n安装mega桌面同步calibre目录 mega网盘免费用户有50GB的空间，足够同步图书库做一个备份。国内可能网速不稳定需要挂梯子。 从mega官网下载源码包，并使用pacman安装。不推荐从yay源安装megasync，因为目前yay源的包有bug，无法正常使用。\n桌面应用程序- MEGA\n","permalink":"https://blog.niuhemoon.win/posts/read/calibre-usage/","summary":"使用calibre管理电子书","title":"calibre-usage"},{"content":"Tmux基本概念 ◎ Tmux基本概念 session 会话：可以理解成是一个特定的终端组合，通常将同一任务下的工作放到一个会话中。 window 窗口：一个会话可以包含多个窗口，一个窗口就相当于普通终端的一个标签，通常在不同的窗口中完成不同的工作。 pane 窗格：一个窗口可以被分割成多个小的窗格。 Tmux配置文件(可选) 全局配置 /etc/tmux.conf 用户配置 ~/.tmux.conf 开箱即用的配置文件:\n🇫🇷 Oh my tmux! ❤️\n笔者配置文件内容如下:\n# 开启鼠标 set -g mouse on Tmux基本操作 Tip\n本文中prefix是tmux的前缀快捷键，默认是Ctrl+b\n会话管理 # prefix + ? 查看帮助信息 # 新建会话tmux tmux new -s \u0026lt;session-name\u0026gt; tmux new-session -s \u0026lt;session-name\u0026gt; # 离开会话 # prefix + d tmux detach # 查看所有tmux会话 tmux list-session tmux ls # 接入会话 tmux attach -t \u0026lt;session-name\u0026gt; tmux a -t \u0026lt;session-name\u0026gt; # 杀死会话 tmux kill-session -t \u0026lt;session-name\u0026gt; # 切换会话 tmux switch -t \u0026lt;session-name\u0026gt; # 重命名会话 tmux rename-session -t \u0026lt;old-name\u0026gt; \u0026lt;new-name\u0026gt; # 重命名当前会话 tmux rename-session \u0026lt;new-name\u0026gt; 窗格管理 # 划分上下窗格prefix + \u0026#34; tmux split-window # 划分左右窗格prefix + % tmux split-window -h # 移动光标prefix + ←↑→↓ tmux select-pane -U # 光标切到上方窗格 tmux select-pane -D tmux select-pane -L tmux select-pane -R # 交换窗格位置 # 窗格上移 tumx swap-pane -U # 窗格下移 tumx swap-pane -D # 窗格转变为窗口 prefix + ! # 窗格全屏显示prefix + z, 再使用一次恢复原大小 # 调整窗格大小prefix + Ctrl + ←↑→↓ 窗口管理 # 新建窗口 tmux new-window tmux new-window -n \u0026lt;window-name\u0026gt; # 查看窗口列表 tmux list-window # 切换窗口 tmux select-window -t \u0026lt;window-name\u0026gt; # 重命名当前窗口 tmux rename-window \u0026lt;new-name\u0026gt; # 重命名指定窗口 tmux rename-window -t \u0026lt;old-name\u0026gt; \u0026lt;new-name\u0026gt; # 切换上一个窗口 prefix + p # 切换下一个窗口 prefix + n # 切换指定窗口 prefix + \u0026lt;number窗口编号\u0026gt; 其它快捷键 命令 说明 ? 列出所有快捷键；按 q 返回 d 脱离当前会话,可暂时返回 Shell 界面 s 选择并切换会话；在同时开启了多个会话时使用 [ 复制模式，光标移动到复制内容位置，空格键开始，方向键选择复制，回车确认，q/Esc 退出 ] 进入粘贴模式，粘贴之前复制的内容，按 q/Esc 退出 t 显示当前的时间 c 创建新窗口 \u0026amp; 关闭当前窗口 [0-9] 数字键切换到指定窗口 p 切换至上一窗口 n 切换至下一窗口 l 前后窗口间互相切换 w 通过窗口列表切换窗口 , 重命名当前窗口，便于识别 . 修改当前窗口编号，相当于重新排序 f 在所有窗口中查找关键词，便于窗口多了切换 \u0026quot; 将当前面板上下分屏 % 将当前面板左右分屏 x 关闭当前分屏 ! 将当前面板置于新窗口,即新建一个窗口,其中仅包含当前面板 q 显示面板编号 o 选择当前窗口中下一个面板 { 向前置换当前面板 } 向后置换当前面板 z 最大化当前所在面板 方向键 移动光标选择对应面板 page up 向上滚动屏幕，q 退出 page down 向下滚动屏幕，q 退出 alt+o 逆时针旋转当前窗口的面板 ctrl+o 顺时针旋转当前窗口的面板 ctrl+方向键 以 1 个单元格为单位移动边缘以调整当前面板大小 alt+方向键 以 5 个单元格为单位移动边缘以调整当前面板大小 参考 Tmux 简介与使用\nTmux 使用教程 - 阮一峰的网络日志\n","permalink":"https://blog.niuhemoon.win/posts/tech/tmux-usage/","summary":"tmux是将会话和窗口解绑的工具","title":"tmux-usage"},{"content":"记录hugo 一些常用的shortcode\nRaw 如下是一个使用svg绘制的钟表\nAsciinema Plantuml Grpahvizo DOT Language (GraphViz) Example Chart Vega Plotly Blockquote 花开花谢 白天黑夜\n一切自然 又不尽然\n春夏秋冬 经过才懂\n世间冷暖 无非自然\n李健 《懂得》 Imagecap 带标题的图片 ◎ Tmux基本概念 不带标题的图片 折叠内容 测试折叠内容 测试折叠内容\nprint(\u0026#39;hello\u0026#39;) RawHTML \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e 《从前慢》 木心 记得早先少年时 大家诚诚恳恳 说一句 是一句 \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e Everyday Italian Giada De Laurentiis 2005 30.00 Harry Potter J K. Rowling 2005 29.99 XQuery Kick Start James McGovern Per Bothner Kurt Cagle James Linn Vaidyanathan Nagarajan 2003 49.99 Learning XML Erik T. Ray 2003 39.95 Music 基于meeting js\nspotify iframe\nNotice Warning\n告警\nInfo\n引言/通知\nNote\n记录\nTip\n提示\nBilibili \u003c!DOCTYPE HTML\u003e Gist Codepen See the Pen Draft Countdown by littlebookboy (@littlebookboy) on CodePen. Youtube Audio A custom comment Video Your browser does not support HTML5 video. Here is \"\u003ea link to download the video. A custom caption Twitter Tweets by elonmusk Ref Theme Documentation - Extended Shortcodes - LoveIt\n","permalink":"https://blog.niuhemoon.win/posts/tech/hugo-shortcode-example/","summary":"\u003cp\u003e记录hugo 一些常用的shortcode\u003c/p\u003e","title":"hugo shortcode example"},{"content":"在manjaro上搭建minikube测试环境\n安装 minikube 安装依赖 sudo pacman -Sy libvirt qemu ebtables dnsmasq sudo usermod -a -G libvirt $(whoami) newgrp libvirt sudo systemctl start libvirtd.service sudo systemctl enable libvirtd.service sudo systemctl start virtlogd.service sudo systemctl enable virtlogd.service 装docker和kvm2 驱动 sudo pacman -Sy docker-machine yaourt -Sy docker-machine-driver-kvm2 装minikube yay -Sy minikube kubectl-bin minikube version whereis kubectl kubectl -h 装环境 注意关掉 vpn\nminikube start --vm-driver kvm2 --registry-mirror=https://dockerhub.azk8s.cn --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers minikube status kubectl cluster-info kubectl get nodes 参考 https://www.howtoforge.com/learning-kubernetes-locally-via-minikube-on-linux-manjaro-archlinux/\n","permalink":"https://blog.niuhemoon.win/posts/tech/install-minikube-on-manjaro/","summary":"\u003cp\u003e在manjaro上搭建minikube测试环境\u003c/p\u003e","title":"Manjaro 搭建 minikube 环境"},{"content":"安装 Tinygo 在 Ubuntu 上安装开发环境，其他环境见参考\n# x86_64 wget https://github.com/tinygo-org/tinygo/releases/download/v0.17.0/tinygo_0.17.0_amd64.deb sudo dpkg -i tinygo_0.17.0_amd64.deb # arm wget https://github.com/tinygo-org/tinygo/releases/download/v0.17.0/tinygo_0.17.0_arm.deb sudo dpkg -i tinygo_0.17.0_arm.deb 配置环境变量\nexport PATH=$PATH:/usr/local/tinygo/bin 验证安装成功\ntinygo version 安装 esp32 环境 # install dep sudo apt-get install git wget make libncurses-dev flex bison gperf pip3 install pyserial # install esp32 toolchain wget https://dl.espressif.com/dl/xtensa-esp32-elf-linux64-1.22.0-80-g6c4433a-5.2.0.tar.gz -P ~/ mkdir -p ~/esp cd ~/esp tar -xzf ~/xtensa-esp32-elf-linux64-1.22.0-80-g6c4433a-5.2.0.tar.gz # add env path export PATH=\u0026#34;$PATH:$HOME/esp/xtensa-esp32-elf/bin\u0026#34; # install esptool pip3 install esptool 烧录 esp32 程序 可以用 usb 连接 esp32，也可以 ttl 转 usb 线，我用的是 ttl-usb，将连接 esp 对应的 RX 和 TX 串口，自己使用串口线烧录时需要同时按住两个板载按键。\n烧录完成后，按 en 使能键，程序开始运行。\n# chmod /dev/ttyUSB0 sudo chmod 666 /dev/ttyUSB0 # flash example program tinygo flash -target=esp32-mini32 -port=/dev/ttyUSB0 examples/blinky1 example/blinky1 代码在/usr/local/lib/tinygo/src 目录下\n也可以编写自己的程序进行烧录，将下面程序保存在/tmp/blink/blink.go\npackage main // This is the most minimal blinky example and should run almost everywhere. import ( \u0026#34;machine\u0026#34; \u0026#34;time\u0026#34; ) func main() { led := machine.LED led.Configure(machine.PinConfig{Mode: machine.PinOutput}) for { led.Low() time.Sleep(time.Millisecond * 300) led.High() time.Sleep(time.Millisecond * 2000) } } 编译并烧录\ntinygo flash -target=esp32-mini32 -port=/dev/ttyUSB0 /tmp/blink/blink.go Tinygo 对 esp32 支持情况\n当前 tinygo 对 esp32 的支持还很不完善，比较鸡肋，不支持 goroutine，不支持网络和蓝牙等。相比 arduino 和 micropython，实用性不强。\nInterface Hardware Supported TinyGo Support GPIO YES YES UART YES YES SPI YES YES I2C YES Not Yet ADC YES Not Yet PWM YES Not Yet WiFi YES Not Yet Bluetooth YES Not Yet 参考 https://tinygo.org/getting-started/linux/\nhttps://docs.espressif.com/projects/esp-idf/en/release-v3.0/get-started/linux-setup.html#standard-setup-of-toolchain-for-linux\n","permalink":"https://blog.niuhemoon.win/posts/tech/tinygo-esp32/","summary":"\u003ch3 id=\"安装-tinygo\"\u003e安装 Tinygo\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003e在 Ubuntu 上安装开发环境，其他环境见参考\u003c/p\u003e\n\u003c/blockquote\u003e","title":"tinygo点亮esp32"},{"content":" 使用select语句和带缓冲区的channel来控制函数并发执行次数\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) // cocurrency number of the do func const cnum = 3 func main() { var jobs = make(chan bool, cnum) for { for i := 0; i \u0026lt; 10; i++ { go frqLimit(do, jobs, i) } time.Sleep(4e9) } } func frqLimit(f func(), jobChan chan bool, i int) { select { case jobChan \u0026lt;- true: f() \u0026lt;-jobChan default: fmt.Println(\u0026#34;job channel is full. pass \u0026#34;, i) } fmt.Println(\u0026#34;jobs\u0026#34;, i, \u0026#34;exit\u0026#34;) } func do() { time.Sleep(3e9) fmt.Println(\u0026#34;job done\u0026#34;) } ","permalink":"https://blog.niuhemoon.win/posts/tech/golang-frequency-limit/","summary":"使用select语句和带缓冲区的channel来控制函数并发执行次数 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) // cocurrency number of the do func const cnum = 3 func main() { var jobs = make(chan bool, cnum) for { for i := 0; i \u0026lt; 10; i++ { go frqLimit(do, jobs, i) } time.Sleep(4e9) } } func frqLimit(f func(), jobChan chan bool, i int) { select { case jobChan \u0026lt;- true: f() \u0026lt;-jobChan default: fmt.Println(\u0026#34;job channel is full. pass \u0026#34;, i) } fmt.Println(\u0026#34;jobs\u0026#34;, i, \u0026#34;exit\u0026#34;) } func do() { time.Sleep(3e9) fmt.Println(\u0026#34;job done\u0026#34;) }","title":"golang限制函数同时调用次数"},{"content":"Makefile基础 语法\n# 目标 ：依赖 # 根据依赖生成目标的命令 targets : prerequisites command 变量\nMakefile 允许使用等号自定义变量。\ntxt = Hello World test: @echo $(txt) 上面代码中，变量 txt 等于 Hello World。调用时，变量需要放在 $( ) 之中。\n调用Shell变量，需要在美元符号前，再加一个美元符号，这是因为Make命令会对美元符号转义。\ntest: @echo $$HOME 有时，变量的值可能指向另一个变量。\nv1 = $(v2) 上面代码中，变量 v1 的值是另一个变量 v2。这时会产生一个问题，v1 的值到底在定义时扩展（静态扩展），还是在运行时扩展（动态扩展）？如果 v2 的值是动态的，这两种扩展方式的结果可能会差异很大。\n为了解决类似问题，Makefile一共提供了四个赋值运算符 （=、:=、？=、+=），它们的区别请看StackOverflow。\nVARIABLE = value # 在执行时扩展，允许递归扩展。 VARIABLE := value # 在定义时扩展。 VARIABLE ?= value # 只有在该变量为空时才设置值。 VARIABLE += value # 将值追加到变量的尾端。 特殊变量\n$@ ： target文件名 $\u0026lt; ： 第一个dependencies文件 $? ： 所有比target文件更新的dependencies文件 $^ ： 所有的dependencies文件，不管文件修改时间如何。 跨平台\n让makefile支持跨平台，在不同平台作出不同的反应\n# Detect system OS. ifeq ($(OS),Windows_NT) detected_OS := Windows else detected_OS := $(shell sh -c \u0026#39;uname -s 2\u0026gt;/dev/null || echo not\u0026#39;) endif Go语言Makefile模板 -include .env PROJECTNAME=$(shell basename \u0026#34;$(PWD)\u0026#34;) # Binary package name for release BINARY=go_trans VERSION=0.0.3 # Go related variables. GOBASE=$(shell pwd) GOPATH :=$(shell echo ${GOPATH}) # Go binary store place. GOBIN=$(GOBASE)/bin GORELEASE=$(GOBASE)/release GOFILES=$(wildcard *.go) GOBUILD_RACE=go build -race -o GOBUILD=go build -o # Redirect error output to a file, so we can show it in development mode. STDERR=/tmp/.$(PROJECTNAME)-stderr.txt # PID file will keep the process id of the server PID=/tmp/.$(PROJECTNAME).pid # Make is verbose in Linux. Make it silent. MAKEFLAGS += --silent .PHONY: help all: help help: Makefile @echo @echo \u0026#34; Choose a command run in \u0026#34;$(PROJECTNAME)\u0026#34;:\u0026#34; @echo @sed -n \u0026#39;s/^##//p\u0026#39; $\u0026lt; | column -t -s \u0026#39;:\u0026#39; | sed -e \u0026#39;s/^/ /\u0026#39; @echo go-compile: clean-bin go-get go-build go-build: @echo \u0026#34; \u0026gt; Building binary...\u0026#34; @GOOS=linux GOARCH=amd64 $(GOBUILD) $(GOBIN)/$(PROJECTNAME) @echo \u0026#34; \u0026gt; Building done.\u0026#34; go-race-check: go-get @echo \u0026#34; \u0026gt; Race check start...\u0026#34; @GOOS=linux GOARCH=amd64 $(GOBUILD_RACE) $(GOBIN)/$(PROJECTNAME)_tmp @echo \u0026#34; \u0026gt; Race check done.\u0026#34; @-rm $(GOBIN)/$(PROJECTNAME)_tmp go-generate: @echo \u0026#34; \u0026gt; Generating dependency files...\u0026#34; @GOPATH=$(GOPATH) GOBIN=$(GOBIN) go generate $(generate) go-get: @echo \u0026#34; \u0026gt; Checking if there is any missing dependencies...\u0026#34; #@GOPATH=$(GOPATH) GOBIN=$(GOBIN) go get $(get) go-install: @GOPATH=$(GOPATH) GOBIN=$(GOBIN) go install $(GOFILES) clean-bin: @echo \u0026#34; \u0026gt; Cleaning build cache...\u0026#34; @-rm $(GOBIN)/* 2\u0026gt; /dev/null clean-release: @echo \u0026#34; \u0026gt; Cleaning release file...\u0026#34; @-rm $(GORELEASE)/* 2\u0026gt; /dev/null ## test: Run all test. test: echo \u0026#34;Test Not Implement.\u0026#34; ## install: Install missing dependencies. Runs `go get` internally. e.g; make install get=github.com/foo/bar install: go-get stop: stop-server start-server: stop-server @echo \u0026#34; \u0026gt; $(PROJECTNAME) is available at $(ADDR)\u0026#34; @-$(GOBIN)/$(PROJECTNAME) 2\u0026gt;\u0026amp;1 \u0026amp; echo $$! \u0026gt; $(PID) @cat $(PID) | sed \u0026#34;/^/s/^/ \\\u0026gt; PID: /\u0026#34; stop-server: @-touch $(PID) @-kill `cat $(PID)` 2\u0026gt; /dev/null || true @-rm $(PID) restart-server: stop-server start-server ## compile: Compile the x86_64 Linux binary without race check. compile: @-touch $(STDERR) @-rm $(STDERR) @-$(MAKE) go-compile 2\u0026gt; $(STDERR) @cat $(STDERR) | sed -e \u0026#39;1s/.*/\\nError:\\n/\u0026#39; | sed \u0026#39;s/make\\[.*/ /\u0026#39; | sed \u0026#34;/^/s/^/ /\u0026#34; 1\u0026gt;\u0026amp;2 ## clean: Clean build files and release file. clean: clean-bin clean-release ## race: Race check. race: @-touch $(STDERR) @-rm $(STDERR) @-$(MAKE) go-race-check 2\u0026gt; $(STDERR) @cat $(STDERR) | sed -e \u0026#39;1s/.*/\\nError:\\n/\u0026#39; | sed \u0026#39;s/make\\[.*/ /\u0026#39; | sed \u0026#34;/^/s/^/ /\u0026#34; 1\u0026gt;\u0026amp;2 ## release: Release arm64 and x86_64 linux binary package. release: clean-release @echo \u0026#34; \u0026gt; Creating release file...\u0026#34; # Build for arm linux @CGO_ENABLED=0 GOOS=linux GOARCH=arm64 $(GOBUILD) $(GORELEASE)/$(BINARY)-arm64-liunx-$(VERSION) # Build for x86_64 linux @CGO_ENABLED=0 GOOS=linux GOARCH=amd64 $(GOBUILD) $(GORELEASE)/$(BINARY)-x86-64-linux-$(VERSION) 参考 跟我一起写Makefile\nhttps://github.com/crossoverJie/btb/blob/master/Makefile Makefiles for Go Developers\ngo-makefile-example\nMake 命令教程\n一个为go准备的优秀makefile\nGNU make\nmakefile跨平台\n","permalink":"https://blog.niuhemoon.win/posts/tech/golang-makefile/","summary":"Makefile基础 语法 # 目标 ：依赖 # 根据依赖生成目标的命令 targets : prerequisites command 变量 Makefile 允许使用等号自定义变量。 txt = Hello World test: @echo $(txt) 上面代码中，变量 txt 等于 Hello World。调用时，变量需要放在 $( ) 之中。 调用Shell变量，需要在美元符号前，再加一个美元符号，这是因为Make命令会对美元符号转义。 test: @echo $$HOME 有","title":"Go语言和Makefile"},{"content":"简介 Typescript 可以在代码编写写做类型检查，可以编写更健壮的代码。\n安装 npm config set registry https://registry.npm.taobao.org sudo npm install -g typescript # 安装REPL sudo npm install -g tsun 基本概念 联合类型\n表示取值是多种类型中的一种，当 TypeScript 不确定一个联合类型的变量到底是哪个类型的时候，我们只能访问此联合类型的所有类型里共有的属性或方法\nlet myFavoriteNumber: string | number; myFavoriteNumber = \u0026#34;seven\u0026#34;; myFavoriteNumber = 7; 接口\nTypeScript 中的接口是一个非常灵活的概念，除了可用于对类的一部分行为进行抽象以外，也常用于对「对象的形状（Shape）」进行描述。 接口是一个类型，不是一个真正的值，它在编译结果中会被删除\n//？可选属性 interface Person { name: string; age?: number; } let tom: Person = { name: \u0026#34;Tom\u0026#34;, }; let tom: Person1 = { name: \u0026#34;Tom\u0026#34;, age: 25, }; //接口可以继承 interface ApiError extends Error { code: number; } 接口可以定义任意类型，但是当同时存在可选类型和任意类型，可选类型需要是任意类型的子集\ninterface Person { name: string; age?: number; //任意类型为联合类型 [propName: string]: string | number; } let tom: Person = { name: \u0026#34;Tom\u0026#34;, age: 25, gender: \u0026#34;male\u0026#34;, }; 接口属性只读，意味着，只有在创建对象时可被赋值，其后无法修改，而且只读属性必须在对象初始化时进行赋值。\ninterface Person { readonly id: number; name: string; age?: number; [propName: string]: any; } let tom: Person = { id: 89757, name: \u0026#34;Tom\u0026#34;, gender: \u0026#34;male\u0026#34;, }; 数组\n习惯性的将数组中的元素类型保持相同\nlet fibonacci: number[] = [1, 1, 2, 3, 5]; //泛型 let fibonacci: Array\u0026lt;number\u0026gt; = [1, 1, 2, 3, 5]; //接口表示数组 //用接口表示数组通常用来标识类型 interface NumberArray { //索引是数字，类型是数字 [index: number]: number; } let fibonacci: NumberArray = [1, 1, 2, 3, 5]; interface IArguments { [index: number]: any; length: number; callee: Function; } 函数\n在 JavaScript 中，有两种常见的定义函数的方式——函数声明（Function Declaration）和函数表达式（Function Expression）; 函数声明和函数表达式的词法环境和执行上下文是不一样的，函数声明会做类型提升。\n// 函数声明（Function Declaration） function sum(x, y) { return x + y; } // 函数表达式（Function Expression） let mySum = function (x, y) { return x + y; }; 可以手动给函数表达式添加类型，也可以使用类型推断 在 TypeScript 的类型定义中，=\u0026gt; 用来表示函数的定义，左边是输入类型，需要用括号括起来，右边是输出类型。 在 ES6 中，=\u0026gt; 叫做箭头函数，应用十分广泛\nlet mySum: (x: number, y: number) =\u0026gt; number = function ( x: number, y: number ): number { return x + y; }; 可选参数用？标识，必须接在必需参数的后面\nfunction buildName(firstName: string, lastName?: string) { if (lastName) { return firstName + \u0026#34; \u0026#34; + lastName; } else { return firstName; } } let tomcat = buildName(\u0026#34;Tom\u0026#34;, \u0026#34;Cat\u0026#34;); let tom = buildName(\u0026#34;Tom\u0026#34;); ES6 中允许给参数添加默认值，Typescript 将添加默认值的参数识别为可选参数，并且添加默认值后，就不受「可选参数必须接在必需参数后面」的限制了 默认值参数在必需参数前的话，需要传一个 undefined 进去占位，因此推荐将默认值参数放在后面\nfunction buildName(firstName: string = \u0026#34;Tom\u0026#34;, lastName: string) { return firstName + \u0026#34; \u0026#34; + lastName; } let tomcat = buildName(\u0026#34;Tom\u0026#34;, \u0026#34;Cat\u0026#34;); //如果默认值参数在必需参数前边，必须传入undefined console.log(buildName(undefined, \u0026#34;cat\u0026#34;)); function buildName1(firstName: string, lastName: string = \u0026#34;Man\u0026#34;) { return firstName + \u0026#34; \u0026#34; + lastName; } console.log(buildName(\u0026#34;good\u0026#34;)); 剩余 rest 参数（不定长参数） rest 参数只能是最后一个参数\n//items是一个数组 function push(array, ...items) { items.forEach(function (item) { array.push(item); }); } let a: any[] = []; push(a, 1, 2, 3); 函数重载允许一个函数接受不同数量或类型的参数时，作出不同的处理。 Typescript 会从最前面的函数定义开始匹配\n//前声明（定义）后实现，将精确的声明写在前面 function reverse(x: number): number; function reverse(x: string): string; function reverse(x: number | string): number | string { if (typeof x === \u0026#34;number\u0026#34;) { return Number(x.toString().split(\u0026#34;\u0026#34;).reverse().join(\u0026#34;\u0026#34;)); } else if (typeof x === \u0026#34;string\u0026#34;) { return x.split(\u0026#34;\u0026#34;).reverse().join(\u0026#34;\u0026#34;); } } 类型断言\n值 as 类型 类型断言只会影响 TypeScript 编译时的类型，类型断言语句在编译结果中会被删除 它不会真的影响到变量的类型。 应用场景:\n将一个联合类型断言为其中一个类型，欺骗 tsc 编译器，可能导致运行时出错 将一个父类断言为更加具体的子类 将任何一个类型断言为 any 将 any 断言为一个具体的类型 限制： typescript 是结构类型系统，不关心定义时的类型关系，只比较最终结构有什么关系\n联合类型可以被断言为其中一个类型 父类可以被断言为子类 任何类型都可以被断言为 any any 可以被断言为任何类型 要使得 A 能够被断言为 B，只需要 A 兼容 B 或 B 兼容 A 即可 //类型比较 //下面两种Cat的定义是等价的 interface Animal { name: string; } interface Cat { name: string; run(): void; } interface Cat extends Animal { run(): void; } //联合类型断言 interface Cat { name: string; run(): void; } interface Fish { name: string; swim(): void; } function isFish(animal: Cat | Fish) { if (typeof (animal as Fish).swim === \u0026#34;function\u0026#34;) { return true; } return false; } //子类断言 interface ApiError extends Error { code: number; } interface HttpError extends Error { statusCode: number; } function isApiError(error: Error) { if (typeof (error as ApiError).code === \u0026#34;number\u0026#34;) { return true; } return false; } //确保代码无误后，绕过类型检查 //在类型的严格性和开发的便利性之间掌握平衡 (window as any).foo = 1; //明确类型，后续有了代码补全，提高可维护性 function getCacheData(key: string): any { return (window as any).cache[key]; } interface Cat { name: string; run(): void; } const tom = getCacheData(\u0026#34;tom\u0026#34;) as Cat; tom.run(); 类型声明比类型断言约束更严格，如 animal 断言为 Cat，只需要满足 Animal 兼容 Cat 或 Cat 兼容 Animal 即可 animal 赋值给 tom，需要满足 Cat 兼容 Animal 才行 可以用泛型替代类型断言\nfunction getCacheData\u0026lt;T\u0026gt;(key: string): T { return (window as any).cache[key]; } interface Cat { name: string; run(): void; } const tom = getCacheData\u0026lt;Cat\u0026gt;(\u0026#34;tom\u0026#34;); tom.run(); 声明文件\n常用的声明语法\ndeclare var 声明全局变量 declare const 声明全局常量 declare function 声明全局方法 declare class 声明全局类 declare enum 声明全局枚举类型 declare namespace 声明（含有子属性的）全局对象 interface 和 type 声明全局类型 export 导出变量 export namespace 导出（含有子属性的）对象 export default ES6 默认导出 export = commonjs 导出模块 export as namespace UMD 库声明全局变量 declare global 扩展全局变量 declare module 扩展模块 /// \u0026lt;reference /\u0026gt; 三斜线指令 类型别名\n类型 c 语言 typedef，在 typescript 中用 type 创建类型别名\ntype Name = string; type NameResolver = () =\u0026gt; string; type NameOrResolver = Name | NameResolver; function getName(n: NameOrResolver): Name { if (typeof n === \u0026#34;string\u0026#34;) { return n; } else { return n(); } } 字面量类型\n约束取值只能是某几个值中的一个\ntype EventNames = \u0026#34;click\u0026#34; | \u0026#34;scroll\u0026#34; | \u0026#34;mousemove\u0026#34;; function handleEvent(ele: Element, event: EventNames) { // do something } handleEvent(document.getElementById(\u0026#34;hello\u0026#34;), \u0026#34;scroll\u0026#34;); // 没问题 handleEvent(document.getElementById(\u0026#34;world\u0026#34;), \u0026#34;dblclick\u0026#34;); // 报错，event 不能为 \u0026#39;dblclick\u0026#39; 元组\n数组合并了相同类型的对象，而元组（Tuple）合并了不同类型的对象; 可以对元组中的单个元素赋值； 当直接对元组类型的变量进行初始化或者赋值的时候，需要提供所有元组类型中指定的项; 当添加越界的元素时，它的类型会被限制为元组中每个类型的联合类型\nlet tom: [string, number] = [\u0026#34;Tom\u0026#34;, 25]; let tom: [string, number]; tom[0] = \u0026#34;Tom\u0026#34;; tom[1] = 25; tom[0].slice(1); tom[1].toFixed(2); tom = [\u0026#34;Tom\u0026#34;, 25]; tom.push(\u0026#34;male\u0026#34;); 枚举\n枚举（Enum）类型用于取值被限定在一定范围内的场景\nenum Days { Sun, Mon, Tue, Wed, Thu, Fri, Sat, } console.log(Days[\u0026#34;Sun\u0026#34;] === 0); // true console.log(Days[\u0026#34;Mon\u0026#34;] === 1); // true console.log(Days[\u0026#34;Tue\u0026#34;] === 2); // true console.log(Days[\u0026#34;Sat\u0026#34;] === 6); // true console.log(Days[0] === \u0026#34;Sun\u0026#34;); // true console.log(Days[1] === \u0026#34;Mon\u0026#34;); // true console.log(Days[2] === \u0026#34;Tue\u0026#34;); // true console.log(Days[6] === \u0026#34;Sat\u0026#34;); // true 类\n传统方法中，JavaScript 通过构造函数实现类的概念，通过原型链实现继承。而在 ES6 中，我们终于迎来了 class 使用 class 定义类，使用 constructor 定义构造函数。 通过 new 生成新实例的时候，会自动调用构造函数。 使用 extends 关键字实现继承，子类中使用 super 关键字来调用父类的构造函数和方法。 使用 getter 和 setter 可以改变属性的赋值和读取行为： 使用 static 修饰符修饰的方法称为静态方法，它们不需要实例化，而是直接通过类来调用，不可以通过实例来调用：\nES6 中实例的属性只能通过构造函数中的 this.xxx 来定义，ES7 提案中可以直接在类里面定义 ES7 提案中，可以使用 static 定义一个静态属性，静态属性属于类； 当构造函数修饰为 private 时，该类不允许被继承或者实例化; 当构造函数修饰为 protected 时，该类只允许被继承，不允许被实例化； 类属性/方法的访问限定符如下:\npublic 修饰的属性或方法是公有的，可以在任何地方被访问到，默认所有的属性和方法都是 public 的 private 修饰的属性或方法是私有的，不能在声明它的类的外部访问 protected 修饰的属性或方法是受保护的，它和 private 类似，区别是它在子类中也是允许被访问的 class Animal { private _name: string; age = 23; static num = 42; constructor(name) { this._name = name; } get name() { return \u0026#34;get \u0026#34; + this._name; } //name是public的，但是_name是私有的 //不能在set name中再对name赋值，会造成死循环 set name(value) { if (value === \u0026#34;Dog\u0026#34;) { console.log(\u0026#34;Animal cannot be dog\u0026#34;); return; } this._name = value; console.log(\u0026#34;setter: \u0026#34; + value); } sayHi() { console.log(`My name is ${this._name}`); } static isAnimal(a) { return a instanceof Animal; } } class Cat extends Animal { constructor(name) { super(name); // 调用父类的 constructor(name) console.log(this.name); } //函数重写 sayHi() { return \u0026#34;Meow, \u0026#34; + super.sayHi(); // 调用父类的 sayHi() } } let a = new Animal(\u0026#34;Jack\u0026#34;); Animal.isAnimal(a); // true let c = new Cat(\u0026#34;Tom\u0026#34;); // Tom console.log(c.sayHi()); // Meow, My name is Tom 参数属性 修饰符和 readonly 还可以使用在构造函数参数中，等同于类中定义该属性同时给该属性赋值 只读属性关键字，只允许出现在属性声明或索引签名或构造函数中 注意如果 readonly 和其他访问修饰符同时存在的话，需要写在其后面。 abstract 用于定义抽象类和其中的抽象方法。 抽象类不允许被实例化，抽象类中的抽象方法必须被子类实现\nabstract class Animal { //public readonly name; public constructor(public readonly name: string) { this.name = name; } //abstract method public abstract eat(); } class Cat extends Animal { public eat() { console.log(`${this.name} is eating.`); } } let a = new Cat(\u0026#34;Tom\u0026#34;); console.log(a.name); // Tom 类和接口\n实现（implements）是面向对象中的一个重要概念。一般来讲，一个类只能继承自另一个类，有时候不同类之间可以有一些共有的特性，这时候就可以把特 性提取成接口（interfaces），用 implements 关键字来实现。这个特性大大提高了面向对象的灵活性。\n接口中所有属性和方法都要求是 public 一个类可以实现一个或者多个接口 接口之间可以是继承关系 接口可以继承类（不推荐），本质上还是接口继承接口，因为在创建类的时候，会创建一个同名的接口类型 创建类时自动生成的类型中包含了除了构造函数的实例属性和实例方法，会保留访问限定符， 如果类属性是 private，将导致该类型的对象无法被初始化，生成的接口类型中不包括：\n静态类型和静态方法 构造函数 interface Alarm { alert(): void; } interface Light { lightOn(): void; lightOff(): void; } class Car implements Alarm, Light { alert() { console.log(\u0026#34;Car alert\u0026#34;); } lightOn() { console.log(\u0026#34;Car light on\u0026#34;); } lightOff() { console.log(\u0026#34;Car light off\u0026#34;); } } 接口继承类（晦涩）\nclass Point { x: number; y: number; constructor(x: number, y: number) { this.x = x; this.y = y; } } interface PointInstanceType { x: number; y: number; } // 等价于 interface Point3d extends PointInstanceType interface Point3d extends Point { z: number; } let point3d: Point3d = { x: 1, y: 2, z: 3 }; 泛型\n泛型（Generics）是指在定义函数、接口或类的时候，不预先指定具体的类型，而在使用的时候再指定类型的一种特性\nfunction createArray\u0026lt;T\u0026gt;(length: number, value: T): Array\u0026lt;T\u0026gt; { let result: T[] = []; for (let i = 0; i \u0026lt; length; i++) { result[i] = value; } return result; } createArray\u0026lt;string\u0026gt;(3, \u0026#34;x\u0026#34;); // [\u0026#39;x\u0026#39;, \u0026#39;x\u0026#39;, \u0026#39;x\u0026#39;] //多个类型参数 function swap\u0026lt;T, U\u0026gt;(tuple: [T, U]): [U, T] { return [tuple[1], tuple[0]]; } swap([7, \u0026#34;seven\u0026#34;]); // [\u0026#39;seven\u0026#39;, 7] 泛型约束，可以使用其他类型约束，也可以在类型参数之间进行约束\nfunction copyFields\u0026lt;T extends U, U\u0026gt;(target: T, source: U): T { for (let id in source) { target[id] = (\u0026lt;T\u0026gt;source)[id]; } return target; } let x = { a: 1, b: 2, c: 3, d: 4 }; copyFields(x, { b: 10, d: 20 }); 参考 Typescript 入门教程\n深入理解Typescript\n","permalink":"https://blog.niuhemoon.win/posts/tech/typescript-doc/","summary":"简介 Typescript 可以在代码编写写做类型检查，可以编写更健壮的代码。 安装 npm config set registry https://registry.npm.taobao.org sudo npm install -g typescript # 安装REPL sudo npm install -g tsun 基本概念 联合类型 表示取值是多种类型中的一种，当 TypeScript 不确定一个联合类型的变量到底是哪个类型的时候，我们只能访问此联合类型的所有类型里共有的属性或方法 let myFavoriteNumber: string | number; myFavoriteNumber = \u0026#34;seven\u0026#34;; myFavoriteNumber = 7; 接口 TypeScript 中的","title":"Typescript基础"},{"content":"CheatSheet Docker 核心架构：\n客户端 Client 服务器 Docker daemon 镜像 Image Registry 容器 Container 容器基本技术：\ncgroup 资源限额 namespace 资源隔离 Mount UTS IPC PID Network User Docker 采用 C/S 架构，客户端向服务器发送请求，服务器负责构建、运行和分发容器。客户端和服务器可以运行在同一个 host 上，客户端也可以通过 socket 或者 REST API 和远程服务器通信。docker 客户端是和服务器通信的命令行工具。服务器负责创建、运行、监控容器，构建、存储镜像。镜像是一个只读模板，通过镜像可以创建容器。容器就是镜像运行的实例。Registry 是存放镜像的仓库。\n构建镜像并启动容器 镜像管理\n# pull an image from a registry docker pull myimage:1.0 # retag a local image with new name and tag docker tag myimage:1.0 myrepo/myimage:2.0 # push an image to a registry docker push myrepo/myimage:2.0 # 查看本地镜像 docker images # list all images locally stored docker image ls # delete an image from local image store docker image rm alpine:3.4 从镜像启动容器\n# 后台从镜像启动容器，并指定Host和容器的端口映射 docker run -d -p 8000:80 \u0026lt;image\u0026gt; # 进入容器，附加到前台进程 docker attach \u0026lt;容器\u0026gt; # 离开容器 Ctrl+p Ctrl+q # 进入容器，新开一个终端 docker exec -it \u0026lt;容器\u0026gt; bash # 退出容器 exit # 启动exit的容器 docker start \u0026lt;容器\u0026gt; # 停止容器 docker stop \u0026lt;容器\u0026gt; # rum a container from alpine:3.9 image # name the running container \u0026#34;web\u0026#34; # expose port 5000 externally mapped to port 80 inside the container docker container run --name web -p 5000:80 alpine:3.9 # stop a running container through SIGTERM docker container stop web # stop a running container through SIGKILL docker container kill web # delete all running and stopped containers docker container rm -f $(docker ps -aq) # print the last 100 lines of a container\u0026#39;s logs docker container logs --tail 100 web 构建镜像\n# 查看容器 docker ps -a docker container ls -a # 构建镜像 # 1.运行容器 # 2.修改容器 # 3.将容器保存为新镜像 docker commit \u0026lt;镜像\u0026gt; # 从dockerfile构建镜像 docker build -t \u0026lt;image_name\u0026gt; -f \u0026lt;Dockerfile路径\u0026gt; # build an image from Dockerfile in the current directory and tag the image docker build -t myimage:1.0 . # 查看镜像分层 docker history \u0026lt;image\u0026gt; docker 网络 Docker 容器和主机之间网络：\nbridge host none 自定义 网络相关的命令：\nbrctl show # 显示网桥 ip r # 查看路由表 iptables-save # 查看路由 # 查看docker网桥 docker network inspect bridge # 查看docker网络 docker network ls # 以某种网络配置从镜像启动容器 docker run --network=host -it \u0026lt;镜像\u0026gt; bash 镜像的备份与恢复 docker save 导出镜像到本地文件 # Usage $ docker save [OPTIONS] IMAGE [IMAGE...] # 导出golang镜像 $ sudo docker save --output golang.tar golang:1.2 docker load 从本地文件导入文件到镜像库 # Usage $ docker load [OPTIONS] # 导入golang镜像 $ sudo docker load --input golang.tar docker export 导出容器快照到本地文件 # Usage $ docker export [OPTIONS] CONTAINER # 导出hello容器快照 $ sudo docker export --output hello.tar docker import 从容器快照文件中再导入为镜像 # Usage $ docker import [OPTIONS] URL|- [REPOSITORY[:TAG]] # 导入hello快照，并制定镜像标签为hello:1.0 $ cat hello.tar | sudo docker import - hello:1.0 容器监控 # 查看所有容器 docker container ls # 查看容器内进程 docker container top \u0026lt;容器\u0026gt; # 查看容器资源状态 # 1. 所有容器 docker container stats # 2. 特定容器 docker container stats \u0026lt;容器...\u0026gt; 工具\nSysdig Weave Scope cAdvisor Prometheus 容器日志 docker 默认将容器日志发送到 STDOUT 和 STDERR， 此外，docker 还提供了多种 logging driver，帮助从容器中提取运行日志， 默认的 logging driver 是 json-file，将容器日志保存在 json file 中， 可以在 host 目录的/var/lib/docker/containers/目录下找到日志的 json 文件\n一些系统的日志方案\nELK Graylog # 打印容器所有日志 docker logs \u0026lt;容器\u0026gt; # 持续打印日志 docker logs -f \u0026lt;容器\u0026gt; 参考 Docker 从入门到实践\nDocker 快速入门\nDocker 入门教程\n每天 5 分钟玩转 Docker\n","permalink":"https://blog.niuhemoon.win/posts/tech/docker-tutorial/","summary":"CheatSheet Docker 核心架构： 客户端 Client 服务器 Docker daemon 镜像 Image Registry 容器 Container 容器基本技术： cgroup 资源限额 namespace 资源隔离 Mount UTS IPC PID Network User Docker 采用 C/S 架构，客户端向服务器发送请求，服务器负责构建、运行和分发容器。客户端和服务器可以运行在同一个 host 上，客户端也可以通过 socket 或者 REST API 和远程服务器通信。docker 客户端是和服务器通信的命令","title":"docker基本使用"},{"content":"命令行 # 从文件中执行sql语句 sqlite\u0026gt; .read cars.sql # 打开test.db数据库文件，如果文件不存在，创建 sqlite3 test.db # 元命令 # 显示可用表 .tables sqlite\u0026gt; .mode column sqlite\u0026gt; .headers on sqlite\u0026gt; SELECT * FROM Friends; Id Name Sex ---------- ---------- ---------- 1 Jane F 2 Thomas M 3 Franklin M 4 Elisabeth F 5 Mary F 6 Lucy F 7 Jack M 本示例说明如何在 sqlite 的列模式下格式化数据。 .headers命令也已用于显示列标题。 默认情况下，标题是隐藏的。 .width命令调整列的大小。 （此 meta 命令仅与列模式下的表有关。） sqlite\u0026gt; SELECT Name, Title FROM Authors NATURAL JOIN Books; Name Title ----------- ---------- Jane Austen Emma Leo Tolstoy War and Pe Joseph Hell Catch XII Charles Dic David Copp Joseph Hell Good as Go Leo Tolstoy Anna Karen 列宽不足以正确显示所有数据。 sqlite\u0026gt; .width 15 18 sqlite\u0026gt; SELECT Name, Title FROM Authors NATURAL JOIN Books; Name Title --------------- ------------------ Jane Austen Emma Leo Tolstoy War and Peace Joseph Heller Catch XII Charles Dickens David Copperfield Joseph Heller Good as Gold Leo Tolstoy Anna Karenia SQL 在这里，我们更改列宽。 第一列为 15 个字符，第二列为 18 个字符。 .show命令列出了各种设置。 其中包括输出模式，列表模式中使用的分隔符以及标题是否打开。 .schema命令显示表的结构。 它提供了 DDL SQL 来创建表。 sqlite\u0026gt; .schema Cars CREATE TABLE Cars(Id INTEGER PRIMARY KEY, Name TEXT, Price INTEGER); SQL .schema命令显示表的结构。 它提供了 DDL SQL 来创建表。 可以使用.prompt命令更改sqlite3的提示。 sqlite\u0026gt; .prompt \u0026#34;\u0026gt; \u0026#34; \u0026#34;. \u0026#34; \u0026gt; SELECT * FROM Cars . LIMIT 2; Id Name Price ---------- ---------- ---------- 1 Audi 52642 2 Mercedes 57127 \u0026gt; SQL 有两个提示。 一个是主提示，另一个是继续提示。 默认的主提示是sqlite\u0026amp;gt;，默认的继续提示是...\u0026amp;gt;。 我们可以从 Shell 执行 SQL 命令。 $ sqlite3 test.db \u0026#34;SELECT * FROM Cars;\u0026#34; 我们将使用.dump命令转储该表。 sqlite\u0026gt; .output cars2.sql sqlite\u0026gt; .dump Cars SQL 我们还可以将输出重定向到文件。 .output命令会将输出重定向到cars2.sql文件。 sqlite\u0026gt; .tables Authors Cars Friends Reservations Books Customers Orders sqlite\u0026gt; DROP TABLE Cars; sqlite\u0026gt; .tables Authors Customers Orders Books Friends Reservations sqlite\u0026gt; .read cars.sql sqlite\u0026gt; .tables Authors Cars Friends Reservations Books Customers Orders sqlite\u0026gt; SELECT * FROM Cars WHERE Id=1; Id Name Price ---------- ---------- ---------- 1 Audi 52642 在这里，我们得到 SELECT 语句的输出。 默认情况下，输出模式为 line，分隔符为|。\n使用案例 1、输入\u0026quot; sqlite3 + 数据库名.db \u0026quot; (如： \u0026quot; sqlite3 collect.db \u0026ldquo;) 打开数据库\n2、可输入 \u0026quot; .table \u0026quot; 查看数据库中存在哪些表\n3、可输入\u0026rdquo; .schema \u0026rsquo; 查看建表语句\n4、通过 SQL 查询语句 \u0026quot; select _ from 表名 \u0026quot; （如：\u0026quot; select _ from Book \u0026ldquo;）\n参考 sqlite 教程\n","permalink":"https://blog.niuhemoon.win/posts/tech/sqlite3-tutorial/","summary":"命令行 # 从文件中执行sql语句 sqlite\u0026gt; .read cars.sql # 打开test.db数据库文件，如果文件不存在，创建 sqlite3 test.db # 元命令 # 显示可用表 .tables sqlite\u0026gt; .mode column sqlite\u0026gt; .headers on sqlite\u0026gt; SELECT * FROM Friends; Id Name Sex ---------- ---------- ---------- 1 Jane F 2 Thomas M 3 Franklin M 4 Elisabeth F 5 Mary F 6 Lucy F 7 Jack M 本示例说明如何在 sqlite 的列模式下格式化数据。 .headers命令也已用于显示列标题。 默认情况下","title":"sqlite3入门"},{"content":"快捷键 awk 是 linux 上用于文本处理的脚本语言，你可以实现：\n定义变量 使用字符串和算术运算符 使用控制流程和循环 生成格式化的输出 用法：awk [POSIX 或 GNU 风格选项] [--] \u0026#39;程序\u0026#39; 文件 ... POSIX 选项：\tGNU 长选项：(标准) -f 脚本文件\t--file=脚本文件 -F fs\t--field-separator=fs -v var=val\t--assign=var=val 使用变量 $0 整行 $1 第一列字段 $2 第二列字段 $n 第 n 列字段 空格或者制表符是默认的列分隔符 可以通过-F 指定分隔符\nawk -F: \u0026#39;{print $1}\u0026#39; /etc/passwd cat /etc/passwd | awk -F: \u0026#39;{print $1}\u0026#39; 使用脚本文件 将 awk 脚本保存在 testfile 文件中\n{print $1 \u0026#34; home at \u0026#34; $6} 然后执行文件\nawk -F: -f testfile /etc/passwd 预处理和后处理 保存 testfile 如下\nBEGIN { print \u0026#34;Users and thier corresponding home\u0026#34; print \u0026#34; UserName \\t HomePath\u0026#34; print \u0026#34;___________ \\t __________\u0026#34; FS=\u0026#34;:\u0026#34; } { print $1 \u0026#34; \\t \u0026#34; $6 } END { print \u0026#34;The end\u0026#34; } 执行脚本\nawk -f testfile /etc/passwd 内置变量 一些内置变量如下\nFS 指定 field 段分隔符 OFS [Output Filed Separator]输出分隔符 ORS [Output Record Separator] 输出行分隔符 FIELDWIDTHS 按段长度分割 RS [Record Separator]记录分隔符，默认是换行符 指定输出分隔符\nawk \u0026#39;BEGIN{FS=\u0026#34;:\u0026#34;; OFS=\u0026#34;-\u0026#34;} {print $1,$6,$7}\u0026#39; /etc/passwd 使用长度分割\n素材如下，保存为 testrecord：\n1235.96521 927-8.3652 36257.8157 awk \u0026#39;BEGIN{FIELDWIDTHS=\u0026#34;3 4 3\u0026#34;}{print $1,$2,$3}\u0026#39; testrecord 输出如下：\n123 5.96 521 927 -8.3 652 362 57.8 157 使用 Record Separator 素材如下，保存为 testrecord：\nPerson Name 123 High Street (222) 466-1234 Another person 487 High Street (523) 643-8754 awk \u0026#39;BEGIN{FS=\u0026#34;\\n\u0026#34;; RS=\u0026#34;\u0026#34;} {print $1,$3}\u0026#39; testrecord 输出如下：\nPerson Name (222) 466-1234 Another person (523) 643-8754 参考 30 Examples For Awk Command In Text Processing\n","permalink":"https://blog.niuhemoon.win/posts/tech/awk-usage/","summary":"快捷键 awk 是 linux 上用于文本处理的脚本语言，你可以实现： 定义变量 使用字符串和算术运算符 使用控制流程和循环 生成格式化的输出 用法：awk [POSIX 或 GNU 风格选项] [--] \u0026#39;程序\u0026#39; 文件 ... POSIX 选项： GNU 长选项：(标准) -f 脚本文件 --file=脚本文件 -F fs --field-separator=fs -v var=val --assign=var=val 使用变量 $0 整行 $1 第一列字段 $2 第二列字","title":"awk基本使用"},{"content":"# 创建目录并进入 function mkdircd () { mkdir -p \u0026#34;$@\u0026#34; \u0026amp;\u0026amp; eval cd \u0026#34;\\\u0026#34;\\$$#\\\u0026#34;\u0026#34;; } 查找文件 # 找到大于100M的文件 find / -type f -size +100M # 找到文件名中含有mail的文件/文件夹 find /etc -name \u0026#34;*mail*\u0026#34; # 找到修改时间在60天之前的文件 find . -mtime +60 # 找到修改时间在2天内的文件 find . -mtime -2 # 批量显示TS后缀且大于100M文件的详情 find . -type f -name \u0026#39;*.TS\u0026#39; -size +100M -exec ls -l {} \\; # 批量删除TS后缀且大于100M的文件 find . -type f -name \u0026#39;*.TS\u0026#39; -size +100M -exec rm -f {} \\; # 查找修改时间60天前的文件并打包 find /home/jsmith -type f -mtime +60 | xargs tar -cvf /tmp/`date \u0026#39;+%d%m%Y\u0026#39;_archive.tar` 输出重定向 # 标准输出重定向，只显示error信息 ./shell-script.sh \u0026gt; /dev/null # 标准错误信息重定向 ./shell-script.sh 2\u0026gt; /dev/null # 标准错误和输出都重定向 ./shell-script.sh \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 # 将所有大写转化为小写 tr A-Z a-z \u0026lt; department.txt # 将所有小写转化为大写 tr a-z A-Z \u0026lt; employee.txt xargs 基本使用 # 删除log文件 find ~ -name \u0026#39;*.log\u0026#39; -print0 | xargs -0 rm -f find /etc -name \u0026#34;*.conf\u0026#34; | xargs ls -l cat url-list.txt | xargs wget -c find / -name *.jpg -type f -print | xargs tar -cvzf images.tar.gz ls *.jpg | xargs -n1 -i cp {} /external-harddrive/directory 文件中截取列 # 以:分割，第一列 cut -d: -f 1 /etc/passwd # 以:分割，第1和第3列 cut -d: -f 1,3 /etc/passwd # 截取每行前边1-8个字符 cut -c 1-8 /etc/passwd 后台运行 nohup ./backup.sh \u0026amp; screen -S backup # 以特定间隔时间执行命令 watch df -h sed 基础 # thegeekstuff.txt # Instruction Guides 1. Linux Sysadmin, Linux Scripting etc. 2. Databases - Oracle, mySQL etc. 3. Security (Firewall, Network, Online Security etc) 4. Storage in Linux 5. Productivity (Too many technologies to explore, not much time available) # Additional FAQS 6. Windows- Sysadmin, reboot etc. # 将第一个Linux替换为Linux-Unix sed \u0026#39;s/Linux/Linux-Unix/\u0026#39; thegeekstuff.txt # 将所有Linux替换为Linux-Unix sed \u0026#39;s/Linux/Linux-Unix/g\u0026#39; thegeekstuff.txt # 将第2个出现的Linux替换为Linux-Unix sed \u0026#39;s/Linux/Linux-Unix/2\u0026#39; thegeekstuff.txt # 输出修改行并写入指定的output文件 sed -n \u0026#39;s/Linux/Linux-Unix/gpw output\u0026#39; thegeekstuff.txt # 行正则匹配到-，则从-到行尾的字符被替换为空 sed \u0026#39;/\\-/s/\\-.*//g\u0026#39; thegeekstuff.txt # 删除每行的后3个字符 sed \u0026#39;s/...$//\u0026#39; thegeekstuff.txt # 直接修改源文件，去除#开头的注释 sed -e \u0026#39;s/#.*//\u0026#39; thegeekstuff.txt # 直接修改源文件，去除#开头的注释并去除空行 sed -e \u0026#39;s/#.*//;/^$/d\u0026#39; thegeekstuff.txt # 去除html的箭头标签 sed -e \u0026#39;s/\u0026lt;[^\u0026gt;]*\u0026gt;//g\u0026#39; # 同时显示多个文件的日志 tail -f /var/log/syslog -f /var/log/auth.log # 修改命令行提示符号 export PS1=\u0026#34;\\u@\\h \\w\u0026gt; \u0026#34; # 修改系统时间 date {mmddhhmiyyyy.ss} # Jan 31st 2009, 10:19 p.m, 53 seconds date 013122192009.53 date +%Y%m%d -s \u0026#34;20090131\u0026#34; date +%T -s \u0026#34;22:19:53\u0026#34; date -s \u0026#34;01/31/2009 22:19:53\u0026#34; # 显示时间 date +\u0026#34;%d-%m-%Y\u0026#34; 01-01-2009 date +\u0026#34;%d/%m/%Y\u0026#34; # 01/01/2009 date +\u0026#34;%A,%B %d %Y\u0026#34; # Thursday,January 01 2009 压缩和解压 zip var-log-files.zip /var/log/* zip -r var-log-dir.zip /var/log/ unzip var-log.zip unzip -v var-log.zip unzip -l var-log.zip unzip -t var-log.zip zip -P mysecurepwd var-log-protected.zip /var/log/* unzip var-log-protected.zip tar [options] [tar-archive-name] [other-file-names] # 压缩文件 tar cvf /tmp/my_home_directory.tar /home/jsmith # 显示压缩文件目录 tar tvf /tmp/my_home_directory.tar # 提取压缩文件 tar xvf /tmp/my_home_directory.tar # 指定提取目录 tar xvfz /tmp/my_home_directory.tar.gz -C /home/ramesh # gzip压缩文件(*.tar.gz) tar cvfz /tmp/my_home_directory.tar.gz /home/jsmith tar xvfz /tmp/my_home_directory.tar.gz tar tvfz /tmp/my_home_directory.tar.gz # bzip压缩文件(*.tar.bz2) tar cvfj /tmp/my_home_directory.tar.bz2 /home/jsmith tar xvfj /tmp/my_home_directory.tar.bz2 tar tvfj /tmp/my_home_directory.tar.bz2 命令行历史 # CTRL+r 查找匹配历史 # CTRL+p 上一条命令 history -c # 清除历史 # 忽略重复命令 export HISTCONTROL=ignoredups # 忽略以空格开头的命令 export HISTCONTROL=ignorespace # 不记录历史 export HISTSIZE=0 系统管理 # 创建swap分区 dd if=/dev/zero of=/home/swap-fs bs=1M count=512 ls -l /home/swap-fs mkswap /home/swap-fs swapon /home/swap-fs # edit in /etc/fstab /home/swap-fs swap swap defaults 0 0 # 生成ssh公钥 ssh-keygen ssh-copy-id -i ~/.ssh/id_rsa.pub remote-host # 定时任务管理 crontab -e {minute} {hour} {day-of-month} {month} {day-of-week} {full-path-to-shell-script} # run at 00:01am 1 0 * * * /root/bin/backup.sh # run at weekday 11:59pm 59 11 * * 1,2,3,4,5 /root/bin/backup.sh 59 11 * * 1-5 /root/bin/backup.sh # run every 5 minute */5 * * * * /root/bin/check-status.sh # run at 13:10pm on lst of every month 10 13 1 * * /root/bin/full-backup.sh # run at 11:00pm every weekday 0 23 * * 1-5 /root/bin/incremental-backup.sh # 同步文件 rsync options source destination # sync two directory in a local computer # -z enable compression # -v verbose # -r recursive # -a archive mode:will preserve symbolic link/permission/timestamp/owner/group rsync -zvr /var/opt/installation/inventory/ /root/temp # sync one file rsync -v /var/lib/rpm/Pubkeys /root/temp/ # sync to remote machine rsync -avz /root/temp/ thegeekstuff@192.168.200.10:/home/thegeekstuff/temp/ # sync from remote to local rsync -avz thegeekstuff@192.168.200.10:/var/lib/rpm /root/temp # netcat命令nc # 从server1拷贝文件到server2 # 1. 在server2(102.168.200.27)上监听 nc -l 2222 \u0026gt; 1234.txt # 2. 在server1上开启传输 nc -w 1 102.168.200.27 2222 \u0026lt; abc.txt # 网络拷贝硬盘 # 1. server2(102.168.200.27)监听 nc -l -p 2222 | dd of=/dev/sda # 2. server1执行传输 dd if=/dev/sda | nc 102.168.200.27 2222 # nc端口扫描 # 扫描20-30端口 nc -v -w 1 192.168.200.29 -z 20-30 系统性能监控 ps axl ps aux ps axuf ps U niuhe netstat -tap netstat --route # 路由表 sar lsof ","permalink":"https://blog.niuhemoon.win/posts/tech/linux-101-hackers/","summary":"# 创建目录并进入 function mkdircd () { mkdir -p \u0026#34;$@\u0026#34; \u0026amp;\u0026amp; eval cd \u0026#34;\\\u0026#34;\\$$#\\\u0026#34;\u0026#34;; } 查找文件 # 找到大于100M的文件 find / -type f -size +100M # 找到文件名中含有mail的文件/文件夹 find /etc -name \u0026#34;*mail*\u0026#34; # 找到修改时间在60天之前的文件 find . -mtime +60 # 找到修改时间在2天内的文件 find . -mtime -2 # 批量显示TS后缀且大于100M文件的详情 find . -type f -name \u0026#39;*.TS\u0026#39; -size +100M -exec ls -l {} \\; # 批","title":"linux 101 Hackers 笔记"},{"content":"快捷键 CTRL+B 打开/收起侧边栏目\nCTRL+` 打开内置终端\nCTRL+, 打开设置\nCTRL+p 快速搜索并打开文件\nCTRL+TAB 在已经打开的标签页中跳转\nCTRL+\\ 将标签页移动到右侧分割栏\nCTRL+w 关闭标签页\nCTRL+f 查找内容\nCTRL+h 查找并替换\nCTRL+SHIFT+f 全局搜索\nCTRL+SHIFT+p 命令面板\nCTRL+/ 注释/解除注释\nCTRL+HOME/END 跳转到文件首/尾\nCTRL+c/v 复制或剪切当前光标行/在当前光标行或下一行粘贴\nCTRL+SHIFT+箭头上下箭头 多个光标用于列编辑\nALT+CLICK 获取多个编辑的光标\nCTRL+d 选中单词\nCTRL+SHIFT+l 选中所有该选中的内容\nF2 重命名变量\nCTRL+CLICK 代码跳转\nCTRL+k z 进入/推出 zen 模式\n官方常用快捷键 General Ctrl+Shift+P, F1 Show Command Palette Ctrl+P Quick Open, Go to File\u0026hellip; Ctrl+Shift+N New window/instance Ctrl+W Close window/instance Ctrl+, User Settings Ctrl+K Ctrl+S Keyboard Shortcuts Basic editing Ctrl+X Cut line (empty selection) Ctrl+C Copy line (empty selection) Alt+ ↓ / ↑ Move line down/up Ctrl+Shift+K Delete line Ctrl+Enter / Insert line below/ above Ctrl+Shift+Enter Ctrl+Shift+\\ Jump to matching bracket Ctrl+] / Ctrl+[ Indent/Outdent line Home / End Go to beginning/end of line Ctrl+ Home / End Go to beginning/end of file Ctrl+ ↑ / ↓ Scroll line up/down Alt+ PgUp / PgDn Scroll page up/down Ctrl+Shift+ [ / ] Fold/unfold region Ctrl+K Ctrl+ [ / ] Fold/unfold all subregions Ctrl+K Ctrl+0 / Fold/Unfold all regions Ctrl+K Ctrl+J Ctrl+K Ctrl+C Add line comment Ctrl+K Ctrl+U Remove line comment Ctrl+/ Toggle line comment Ctrl+Shift+A Toggle block comment Alt+Z Toggle word wrap Rich languages editing Ctrl+Space Trigger suggestion Ctrl+Shift+Space Trigger parameter hints Ctrl+Shift+I Format document Ctrl+K Ctrl+F Format selection F12 Go to Definition Ctrl+Shift+F10 Peek Definition Ctrl+K F12 Open Definition to the side Ctrl+. Quick Fix Shift+F12 Show References F2 Rename Symbol Ctrl+K Ctrl+X Trim trailing whitespace Ctrl+K M Change file language Multi-cursor and selection Alt+Click Insert cursor* Shift+Alt+ ↑ / ↓ Insert cursor above/below Ctrl+U Undo last cursor operation Shift+Alt+I Insert cursor at end of each line selected Ctrl+L Select current line Ctrl+Shift+L Select all occurrences of current selection Ctrl+F2 Select all occurrences of current word Shift+Alt + → Expand selection Shift+Alt + ← Shrink selection Shift+Alt + drag mouse Column (box) selection Display F11 Toggle full screen Shift+Alt+0 Toggle editor layout (horizontal/vertical) Ctrl+ = / - Zoom in/out Ctrl+B Toggle Sidebar visibility Ctrl+Shift+E Show Explorer / Toggle focus Ctrl+Shift+F Show Search Ctrl+Shift+G Show Source Control Ctrl+Shift+D Show Debug Ctrl+Shift+X Show Extensions Ctrl+Shift+H Replace in files Ctrl+Shift+J Toggle Search details Ctrl+Shift+C Open new command prompt/terminal Ctrl+K Ctrl+H Show Output panel Ctrl+Shift+V Open Markdown preview Ctrl+K V Open Markdown preview to the side Ctrl+K Z Zen Mode (Esc Esc to exit) Search and replace Ctrl+F Find Ctrl+H Replace F3 / Shift+F3 Find next/previous Alt+Enter Select all occurrences of Find match Ctrl+D Add selection to next Find match Ctrl+K Ctrl+D Move last selection to next Find match Navigation Ctrl+T Show all Symbols Ctrl+G Go to Line\u0026hellip; Ctrl+P Go to File\u0026hellip; Ctrl+Shift+O Go to Symbol\u0026hellip; Ctrl+Shift+M Show Problems panel F8 Go to next error or warning Shift+F8 Go to previous error or warning Ctrl+Shift+Tab Navigate editor group history Ctrl+Alt+- Go back Ctrl+Shift+- Go forward Ctrl+M Toggle Tab moves focus Editor management Ctrl+W Close editor Ctrl+K F Close folder Ctrl+\\ Split editor Ctrl+ 1 / 2 / 3 Focus into 1st, 2nd, 3rd editor group Ctrl+K Ctrl + ← Focus into previous editor group Ctrl+K Ctrl + → Focus into next editor group Ctrl+Shift+PgUp Move editor left Ctrl+Shift+PgDn Move editor right Ctrl+K ← Move active editor group left/up Ctrl+K → Move active editor group right/down File management Ctrl+N New File Ctrl+O Open File\u0026hellip; Ctrl+S Save Ctrl+Shift+S Save As\u0026hellip; Ctrl+W Close Ctrl+K Ctrl+W Close All Ctrl+Shift+T Reopen closed editor Ctrl+K Enter Keep preview mode editor open Ctrl+Tab Open next Ctrl+Shift+Tab Open previous Ctrl+K P Copy path of active file Ctrl+K R Reveal active file in Explorer Ctrl+K O Show active file in new window/instance Debug F9 Toggle breakpoint F5 Start / Continue F11 / Shift+F11 Step into/out F10 Step over Shift+F5 Stop Ctrl+K Ctrl+I Show hover Integrated terminal Ctrl+` Show integrated terminal Ctrl+Shift+` Create new terminal Ctrl+Shift+C Copy selection Ctrl+Shift+V Paste into active terminal Ctrl+Shift+ ↑ / ↓ Scroll up/down Shift+ PgUp / PgDn Scroll page up/down Shift+ Home / End Scroll to top/bottom Keyboard shortcuts for Linux The Alt+Click gesture may not work on some Linux distributions. You can change the modifier key for the Insert cursor command to Ctrl+Click with the \u0026rsquo;editor.multiCursorModifier\u0026rsquo; setting\n导入/导出扩展 //导出扩展名 code --list-extensions \u0026gt;\u0026gt; vs_code_extensions_list.txt cat vs_code_extensions_list.txt | xargs -n 1 code --install-extension //删除所有扩展 code --list-extensions | xargs -n 1 code --uninstall-extension 2gua.rainbow-brackets Angular.ng-template cyrilletuzi.angular-schematics davidbabel.vscode-simpler-icons dbaeumer.vscode-eslint doggy8088.angular-extension-pack donjayamanne.githistory eamodio.gitlens EditorConfig.EditorConfig EFanZh.graphviz-preview esbenp.prettier-vscode formulahendry.auto-rename-tag golang.go Gruntfuggly.todo-tree humao.rest-client infinity1207.angular2-switcher jebbs.plantuml johnpapa.Angular2 krizzdewizz.refactorix MariusAlchimavicius.json-to-ts Mikael.Angular-BeastCode mikeburgh.xml-format ms-azuretools.vscode-docker ms-mssql.mssql ms-python.python ms-toolsai.jupyter ms-vscode-remote.remote-containers ms-vscode-remote.remote-ssh ms-vscode-remote.remote-ssh-edit ms-vscode.cpptools ms-vscode.typescript-javascript-grammar ms-vscode.vscode-typescript-tslint-plugin msjsdiag.debugger-for-chrome nhoizey.gremlins obenjiro.arrr oderwat.indent-rainbow PKief.material-icon-theme quicktype.quicktype shd101wyy.markdown-preview-enhanced steoates.autoimport stringham.move-ts tht13.html-preview-vscode twxs.cmake wayou.vscode-todo-highlight xabikos.JavaScriptSnippets ","permalink":"https://blog.niuhemoon.win/posts/tech/vscode-shortcut/","summary":"快捷键 CTRL+B 打开/收起侧边栏目 CTRL+` 打开内置终端 CTRL+, 打开设置 CTRL+p 快速搜索并打开文件 CTRL+TAB 在已经打开的标签页中跳转 CTRL+\\ 将标签页移动到右侧分割栏 CTRL+w 关闭标签页 CTRL+f 查找内容 CTRL+h 查找并替换 CTRL+SHIFT+f 全局搜索 CTRL+SHIFT+p 命令面板 CTRL+/ 注释/解除注释 CTRL+HOME/END 跳转到文件首/尾 CTRL+c/v 复制或剪切当前光标行/在当前光标行或下一行粘贴 CTRL+SHIFT+","title":"vscode 快捷键和插件记录"},{"content":"创建虚拟机 1、界面或命令行通过RESTful API向keystone获取认证信息。\n2、keystone通过用户请求认证信息，并生成auth-token返回给对应的认证请求。\n3、界面或命令行通过RESTful API向nova-api发送一个boot instance的请求（携带auth-token）。\n4、nova-api接受请求后向keystone发送认证请求，查看token是否为有效用户和token。\n5、keystone验证token是否有效，如有效则返回有效的认证和对应的角色（注：有些操作需要有角色权限才能操作）。\n6、通过认证后nova-api和数据库通讯。\n7、初始化新建虚拟机的数据库记录。\n8、nova-api通过rpc.call向nova-scheduler请求是否有创建虚拟机的资源(Host ID)。\n9、nova-scheduler进程侦听消息队列，获取nova-api的请求。\n10、nova-scheduler通过查询nova数据库中计算资源的情况，并通过调度算法计算符合虚拟机创建需要的主机。\n11、对于有符合虚拟机创建的主机，nova-scheduler更新数据库中虚拟机对应的物理主机信息。\n12、nova-scheduler通过rpc.cast向nova-compute发送对应的创建虚拟机请求的消息。\n13、nova-compute会从对应的消息队列中获取创建虚拟机请求的消息。\n14、nova-compute通过rpc.call向nova-conductor请求获取虚拟机消息。（Flavor）\n15、nova-conductor从消息队队列中拿到nova-compute请求消息。\n16、nova-conductor根据消息查询虚拟机对应的信息。\n17、nova-conductor从数据库中获得虚拟机对应信息。\n18、nova-conductor把虚拟机信息通过消息的方式发送到消息队列中。\n19、nova-compute从对应的消息队列中获取虚拟机信息消息。\n20、nova-compute通过keystone的RESTfull API拿到认证的token，并通过HTTP请求glance-api获取创建虚拟机所需要镜像。\n21、glance-api向keystone认证token是否有效，并返回验证结果。\n22、token验证通过，nova-compute获得虚拟机镜像信息(URL)。\n23、nova-compute通过keystone的RESTfull API拿到认证k的token，并通过HTTP请求neutron-server获取创建虚拟机所需要的网络信息。\n24、neutron-server向keystone认证token是否有效，并返回验证结果。\n25、token验证通过，nova-compute获得虚拟机网络信息。\n26、nova-compute通过keystone的RESTfull API拿到认证的token，并通过HTTP请求cinder-api获取创建虚拟机所需要的持久化存储信息。\n27、cinder-api向keystone认证token是否有效，并返回验证结果。\n28、token验证通过，nova-compute获得虚拟机持久化存储信息。\n29、nova-compute根据instance的信息调用配置的虚拟化驱动来创建虚拟机。\n以Nova为例，nova/compute目录并不是一定在nova-compute节点上运行，而主要是和compute相关(虚拟机操作相关）的功能实现，同样的，scheduler目录代码并不全在scheduler服务节点运行，但主要是和调度相关的代码。不过目录结构遵循一定的规律。\n通常一个OpenStack项目的代码目录都会包含api.py、rpcapi.py、manager.py，这三个是最重要的模块。\napi.py： 通常是供其它组件调用的封装库。换句话说，该模块通常并不会由本模块调用。比如compute目录的api.py，通常由nova-api服务的controller调用。可以简单认为是供其他服务调用的sdk。 rpcapi.py：这个是RPC请求的封装，或者说是RPC封装的client端，该模块封装了RPC请求调用。 manager.py： 这个才是真正服务的功能实现，也是RPC的server端，即处理RPC请求的入口，实现的方法通常和rpcapi实现的方法一一对应。 关机流程\nAPI节点 nova-api接收用户请求 -\u0026gt; nova-api调用compute/api.py -\u0026gt; compute/api调用compute/rpcapi.py -\u0026gt; rpcapi.py向目标计算节点发起stop_instance()RPC请求 计算节点 收到stop_instance()请求 -\u0026gt; 调用compute/manager.py的callback方法stop_instance() -\u0026gt; 调用libvirt关机虚拟机\n前面提到OpenStack项目的目录结构是按照功能划分的，而不是服务组件，因此并不是所有的目录都能有对应的组件。仍以Nova为例:\nnova/cmd：这是服务的启动脚本，即所有服务的main函数。看服务怎么初始化，就从这里开始。 nova/db: 封装数据库访问，目前支持的driver为sqlalchemy。 nova/conf：Nova所有配置项声明都放在这个目录。 nova/locale: 本地化处理。 nova/image: 封装Glance接口。 nova/network: 封装Neutron接口。 nova/volume: 封装Cinder接口。 nova/virt: 这是支持的所有虚拟化驱动实现，即compute driver实现，主流的如libvirt、hyperv、ironic、vmwareapi等。 nova/objects: 对象模型，封装了所有Nova对象的CURD操作，相对以前直接调用db的model更安全，并且支持版本控制。 nova/policies： API policy集合。 nova/tests: 测试代码，如单元测试、功能测试。 nova/hacking: Nova代码规范定义的一些规则。 nova \u0026ndash;debug boot \u0026ndash;image 81e58b1a-4732-4255-b4f8-c844430485d2 \u0026ndash;flavor 1 yikun\ncontroller的index方法对应list操作、show方法对应get操作、create对应创建操作、delete对应删除操作、update对应更新操作等。\nopenstack-nova-compute.service 两个职责，其一，是守护进程，负责基于各种虚拟化技术Hypervisior实现创建和终止虚拟机；其二，整合了计算资源CPU，存储，网络三类资源部署管理虚拟机，实现计算能力的交付。\nCell V2的设计思想是，由API、Super Conductor去访问上层的全局数据库（nova_api数据库），而底下的cell中的组件，只需要关心cell中的逻辑即可\n首先，api中进行第一次Quota检测，主要方法就是收集地下各个cell数据库中的资源信息，然后和api数据库中的quota上限进行对比。例如，一个用户可以创建10个虚拟机，在cell1中有2个，cell2中有7个，再创建一个虚拟机时，会搜集cell1和cell2中的虚拟机个数之和（9个），然后加上变化（新增一个），与总配额进行比较。 二次检测（cell v2在super conductor里做）。由于在并发场景下，可能出现同时检测发现满足，之后进行创建，就会造成配额的超分，针对这个问题，社区目前给出的方案是，在创建虚拟机记录之后，再进行recheck，如果发现超额了，会将超额分配的虚拟机标记为ERROR，不再继续往下走了。 在Cell v2场景，虚拟机的创建记录已经需要写入的子cell中，因此，conductor需要做的事，包括一下几个步骤：\n进行调度，选出host。 根据host，通过host_mappings找到对应的cell 在对应的cell db中创建虚拟机记录，并且记录instances_mappings信息 通过cell_mappings来查找对应的cell的mq，然后投递到对应的cell中的compute 完成这些操作时，需要牵扯到3个关键的数据结构，我们来简单的看一下：\nhost_mappings：记录了host和cell的映射信息 instances_mappings：记录了虚拟机和cell的映射信息 cell_mappings：记录了cell和cell对应的mq的映射信息 与Cell v1不太相同，在目前的设计中，认为scheduler能看到的应该是底下能够提供资源的具体的所有的Resource Provider（对于计算资源来说，就是所有的计算节点），而不是整个cell，也就是说所有cell中的资源scheduler都可以看到，而子cell就负责创建就好了。因此，在super conductor中，需要做一些transfer的事情，这样也就不必在像cell v1那样，在子cell里还得搞个scheduler去做调度。\n通过Placement获取可用的备选资源，参考Placement Allocation Requests的实现。 在Ocata版本时，Resource Providers - Scheduler Filters in DB这个BP就已经在调度前加了一步，获取备选节点。从BP的标题就可以看出，设计者想通过Placement服务提供的新的一套机制，来做过滤。原因是之前的调度需要在scheduler维护每一个compute节点的hoststate信息，然后调度的时候，再一个个去查，这太低效了，尤其是在计算节点数目比较多的时候。因此，增加了一个“预过滤”的流程，通过向Placement查询，Placement服务直接通过SQL去查一把，把满足条件（比如CPU充足、RAM充足等）先获取到。 而原来获取备选节点的时候，只支持获取单一的Resource Provider，这个BP增强了获取备选资源的能力，用于后续支持更复杂的请求，比如共享资源、嵌套资源的Provider查询。后面，Placement还会陆续支持更多的请求，比如对一些非存量不可计数的资源的支持。这样留给后面Filter\u0026amp;Weight的压力就小一些了，再往后，会不会完全取代Filter呢？我想，现有的各种过滤都可以通过Placement支持后，完全有可能的。 Scheduler通过Placement来claim资源。参考Scheduler claiming resources to the Placement API的实现。 在最早的时候，claim资源是由compute来做的，现在相当于提前到scheduler去搞了。有什么好处呢？我们先看看原来的问题： 调度时刻和真正的去compute节点去claim资源的时刻之间是由一段时间的，在资源不是那么充足的环境，就会造成在scheduler调度的时候，资源还没刷新，所以调度时候成功了，但是真正下来的时候，才发现compute实际已经没有资源了，然后又“跨越半个地球”去做重调度，无形地增加了系统的负载。 而且增加了创建的时长（哦，哪怕创建失败呢？），你想想，用户创了那么久的虚拟机，最后你告诉我调度失败了，用户不太能忍。 所以这个BP就把Claim资源放在调度处了，我上一个调度请求处理完，马上就告诉placement，这资源老子用了，其他人不要动了。OK，世界终于清净了，能拿到资源的拿到了，拿不到资源的马上也知道自己拿不到了，大大增强了调度的用户体验。 2.4 Placement 恩，在调度的时候，已经介绍过这个服务了，在虚拟机创建的流程中，比较常用的接口就是获取备选资源和claim资源。 Placement目标很宏伟，大致的作用就是：资源我来管，要资源问我要，用了资源告诉我。后面准备用一篇文章整体介绍一下Placement。（yep，这个Flag我立下了，会写的）\nservice的详细信息主要包括如下几项： binary, host, zone, status, state 其中： binary，可以理解为service的名称，类似于nova-compute； host是service所在的主机名称； zone是service所属的AZ，其实就是service所在的主机所属的aggregate，只是aggregate的概念不对外呈现，所以用户看到的是AZ。其实，在Nova内部，AZ是AG的metadata而已。\nzone的确定，涉及到两个配置项，对于非计算节点，zone的名称依赖于配置项internal_service_availability_zone（默认是internal）； 对于计算节点，如果不属于任何AG，或者所属的AG没有AZ的metadata信息，默认的zone依赖于配置项default_availability_zone（默认是nova）。 status是服务disable属性的体现，该属性可以直接通过API修改; state是服务真实的状态，是通过servicegroup api获取。每个服务在启动时会加入servicegroup，以db后端为例，会在服务中启动定时器，更新service表中的report_count的值，同时也会刷新更新时间，后续会根据这个更新时间确定服务的死活；\n当然，查询service信息也支持过滤条件，比如： 1、查询某个host相关的service； 2、按binary名称查询service；\n其实Nova中没有host这个独立的资源（数据库对象），但是Nova却有针对host的API操作，其实，在内部实现中，就是通过前面的service信息，间接组装返回host信息。\n租户：配额\n与此同时，虚拟机state或task_state发生变化时，也会向外部发送通知。 前提是配置项notify_on_state_change要配置为vm_state或vm_and_task_state。 Nova中的虚拟机每个操作（启动、停止、暂停、恢复等等），都会在db中保存相关的操作记录，给用户提供查询。利用这个功能，用户对自己的虚拟机整个生命周期的过程和状态都会了如指掌，便于用户的管理。参见这里。示例如下：\n在内部实现中，nova-api层会记录action开始的记录，在nova-compute层，则会添加event开始和结束的信息，action和event根据request id（一次消息请求的标识）关联。\n先说通知，虚拟机操作异常时，一般都会发送error通知，通知中包含异常的函数名称、异常时函数的参数以及异常信息。 再说db，虚拟机操作异常时，无论是在conductor, scheduler还是compute层，除了会发送通知外，还会记录异常信息到数据库（instance_faults表），当查询虚拟机信息时，会返回虚拟机的异常信息。 一个hypervisor，是创建虚拟机能够调度到的最小单元。\napi.py提供对外访问的接口，可以从这开始入手跟踪各个功能实现。 rpcapi.py封装RPC请求调用，大多数是异步调用。 manager.py各种RPC调用的实现，基本和rpcapi.py中调用的名称一一对应。 此外还有一点，Openstack的目录结构是根据功能划分的，比如Nova中compute目录不一定都是在nova-compute节点上运行，而是所有和虚拟机创建相关的功能都在这里。\n从配置文件可以明显的看出，nova-api对应的文件是nova/cmd/api.py的main()函数：\nvm_state\npower_state\ntask_state\n_record_action_start notify_about_instance_action elevated\n@startuml title: 创建虚拟机 participant \u0026#34;API\u0026#34; as api note left of api nova/api/openstack/compute/servers.py end note participant \u0026#34;Scheduler\u0026#34; as sch database \u0026#34;Database\u0026#34; as db #Green participant \u0026#34;Condutor(super)\u0026#34; as pconductor participant \u0026#34;Placement\u0026#34; as placement box \u0026#34;internal service\u0026#34; participant \u0026#34;Compute\u0026#34; as compute participant \u0026#34;Libvirt\u0026#34; as virt end box participant \u0026#34;Conductor(cell)\u0026#34; as ccondutor participant \u0026#34;Neutron\u0026#34; as neutron participant \u0026#34;Cinder\u0026#34; as cinder participant \u0026#34;Glance\u0026#34; as glance autonumber \u0026#34;\u0026lt;b\u0026gt; [00]\u0026#34; [-\u0026gt; api++ : 创建虚拟机 api -\u0026gt; api : validate schema api -\u0026gt; api : get context api -\u0026gt; api : get server_dict api -\u0026gt; api : gen create_kwargs api -\u0026gt; api : policy check api -\u0026gt; api : provision instance api -\u0026gt; glance : 获取镜像信息 api -\u0026gt; api : policy校验 api -\u0026gt; api : 配额校验 api -\u0026gt; api : 添加Group hnote left #FFAAAA vm_state: Building task_state: Scheduling end note api -\u0026gt; db : 创建instance db -\u0026gt; api : create success [\u0026lt;- api : return 202 deactivate api api -\u0026gt; pconductor ++: schedule \u0026amp; build pconductor -\u0026gt; sch : select_destination sch -\u0026gt; placement : get allocation candidates placement -\u0026gt; sch : alloc_reqs.provider_summarys sch -\u0026gt; sch : Filter \u0026amp; weighter sch -\u0026gt; placement : claim Resources placement -\u0026gt; sch : hello sch -\u0026gt; pconductor : return host pconductor -\u0026gt; pconductor : in target cell DB中创建instance pconductor -\u0026gt; pconductor : 配额校验 recheck pconductor -\u0026gt; pconductor : 刷新instance cell 信息 pconductor -\u0026gt; pconductor : 删除build request() pconductor -\u0026gt; compute : 在指定cell中创建虚拟机 hnote left #FFAAAA vm_state: Building task_state: None end note compute -\u0026gt; neutron : 创建网络 hnote left #FFAAAA vm_state: Building task_state: Networking end note compute -\u0026gt; cinder : 构建块设备 hnote left #FFAAAA vm_state: Building task_state: Block Device Mapping end note compute -\u0026gt; compute : spawn() hnote left #FFAAAA vm_state: Building task_state: Spawning end note compute -\u0026gt; glance : 下载镜像 compute -\u0026gt; compute : 生成xml compute -\u0026gt; compute : 刷新虚拟机状态 hnote left #FFAAAA vm_state: Building task_state: None end note @enduml @startuml title: Lock虚拟机 participant \u0026#34;API\u0026#34; as api database \u0026#34;Database\u0026#34; as db #Green autonumber \u0026#34;\u0026lt;b\u0026gt; [00]\u0026#34; [-\u0026gt; api : lock api -\u0026gt; api : get_context api -\u0026gt; api : authorize action [lock] policy api -\u0026gt; db : get instance by id db -\u0026gt; api : done api -\u0026gt; api : check policy api -\u0026gt; db : instance.locked = True\\n locked_by=owner or admin\\n record locked reason db -\u0026gt; api : done [\u0026lt;- api : response @enduml @startuml title: Pause虚拟机 participant \u0026#34;API\u0026#34; as api database \u0026#34;Database\u0026#34; as db #Green box \u0026#34;internal service\u0026#34; participant \u0026#34;Compute\u0026#34; as compute participant \u0026#34;Libvirt\u0026#34; as virt end box autonumber \u0026#34;\u0026lt;b\u0026gt; [00]\u0026#34; [-\u0026gt; api++ : pause instance api -\u0026gt; api : authorize context api -\u0026gt; db++ : get instance by uuid return done api -\u0026gt; api : check policy api -\u0026gt; api : check instance lock api -\u0026gt; api : check instance cell api -\u0026gt; api : ensure instance state is ACTIVE api -\u0026gt; db++ : task_state = PAUSING return done api -\u0026gt; api : record pause action api -\u0026gt; compute++ : pause_instance compute -\u0026gt; compute : notify : pause.start compute -\u0026gt; virt++ : pause virt -\u0026gt; virt : get domain virt -\u0026gt; virt : domain.suspend() return done compute -\u0026gt; db++ : vm_state = PAUSE\\n task_state = None return done compute -\u0026gt; compute : notify: pause.end [\u0026lt;- api : response @enduml @startuml title: Rename虚拟机 participant \u0026#34;API\u0026#34; as api database \u0026#34;Database\u0026#34; as db #Green autonumber \u0026#34;\u0026lt;b\u0026gt; [00]\u0026#34; [-\u0026gt; api : update name activate api api -\u0026gt; api : validate schema api -\u0026gt; api : get context api -\u0026gt; api : authorize [update] policy api -\u0026gt; api : get update_dict[\u0026#34;display_name\u0026#34;] api -\u0026gt; db++ : get server by id return done api -\u0026gt; db : update(update_dict) db -\u0026gt; db : save [\u0026lt;- api : responee @enduml @startuml title: Suspend虚拟机 participant \u0026#34;API\u0026#34; as api database \u0026#34;Database\u0026#34; as db #Green box \u0026#34;internal service\u0026#34; participant \u0026#34;Compute\u0026#34; as compute participant \u0026#34;Libvirt\u0026#34; as virt end box autonumber \u0026#34;\u0026lt;b\u0026gt; [00]\u0026#34; [-\u0026gt; api++ : suspend instance api -\u0026gt; api : authorize context api -\u0026gt; db++ : get instance by uuid return done api -\u0026gt; api : check policy api -\u0026gt; api : check instance lock api -\u0026gt; api : check instance cell api -\u0026gt; api : ensure instance state is ACTIVE api -\u0026gt; db++ : task_state = SUSPANDING return done api -\u0026gt; api : record action : suspand api -\u0026gt; compute++ : suspand_instance compute -\u0026gt; compute : notify : suspand.start compute -\u0026gt; virt++ : suspand virt -\u0026gt; virt : get instance guest virt -\u0026gt; virt : detach pci device virt -\u0026gt; virt : detach sriow ports virt -\u0026gt; virt : guest.save_memory_state() return done compute -\u0026gt; db++ : vm_state = SUSPENDED\\n task_state = None return done compute -\u0026gt; compute : notify: suspend.end [\u0026lt;- api : response @enduml @startuml hide empty description [*] --\u0026gt; State1 State1 --\u0026gt; [*] vm_state:powering\\n task_state:good\\n nihao State1 : this is another string State1 -\u0026gt; State2 State2 --\u0026gt; [*] @enduml @startuml title: Unlock虚拟机 participant \u0026#34;API\u0026#34; as api database \u0026#34;Database\u0026#34; as db #Green autonumber \u0026#34;\u0026lt;b\u0026gt; [00]\u0026#34; [-\u0026gt; api : lock api -\u0026gt; api : get_context api -\u0026gt; api : authorize action [unlock] policy api -\u0026gt; db : get instance by id db -\u0026gt; api : done api -\u0026gt; api : check policy api -\u0026gt; db : query instance.locked db -\u0026gt; api : done api -\u0026gt; db : instance.locked = False\\n locked_by=None\\n clear locked reason db -\u0026gt; api : done [\u0026lt;- api : response @enduml @startuml title: Unpause虚拟机 participant \u0026#34;API\u0026#34; as api database \u0026#34;Database\u0026#34; as db #Green box \u0026#34;internal service\u0026#34; participant \u0026#34;Compute\u0026#34; as compute participant \u0026#34;Libvirt\u0026#34; as virt end box autonumber \u0026#34;\u0026lt;b\u0026gt; [00]\u0026#34; [-\u0026gt; api++ : unpause instance api -\u0026gt; api : authorize context api -\u0026gt; db++ : get instance by uuid return done api -\u0026gt; api : check policy api -\u0026gt; api : check instance lock api -\u0026gt; api : check instance cell api -\u0026gt; api : ensure instance state is PAUSED api -\u0026gt; db++ : task_state = UNPAUSING return done api -\u0026gt; api : record action : unpause api -\u0026gt; compute++ : unpause_instance compute -\u0026gt; compute : notify : unpause.start compute -\u0026gt; virt++ : unpause virt -\u0026gt; virt : get domain virt -\u0026gt; virt : domain.resume() return done compute -\u0026gt; db++ : vm_state = ACTIVE\\n task_state = None return done compute -\u0026gt; compute : notify: unpause.end [\u0026lt;- api : response @enduml 参考 Openstack虚拟机启动方式\nOpenstack源码学习笔记\nOpenstack词汇表\nOpenstack从硬盘启动实例\nNova虚拟机创建流程分析\nNova创建虚拟机流程分析\n如何阅读openstack源码\n虚拟机创建的50个步骤和100个知识点\nOpenstack源码学习之热迁移\n","permalink":"https://blog.niuhemoon.win/posts/tech/create-instance/","summary":"创建虚拟机 1、界面或命令行通过RESTful API向keystone获取认证信息。 2、keystone通过用户请求认证信息，并生成auth-token返回给对应的认证请求。 3、界面或命令行通过RESTful API向nova-api发送一个boot instance的请求（携带aut","title":"openstack创建虚拟机"},{"content":" 不同命令的功能有重复和交集\nOpenstack篇 Openstack每个组件都有其命令，openstack社区为了方便使用，将所有组件的命令进行了统一，以openstack开头\n# 查看所有openstack服务 openstack service list # 查看openstack服务状态 openstack-service status # 重启本节点所有openstack服务 openstack-service restart # openstack服务URL列表查询 # endpoint表示一个服务在哪可被访问的URL和端口号列表 openstack endpoint list # 查询domain，domain是一个keystone验证实体 openstack domain list # 查看nova服务列表 openstack compute service list # 查看网络服务列表 openstack network agent list # ======================================================= # 项目（租户）列表查询 openstack project list # 查看租户详情 openstack project show \u0026lt;project_id/name\u0026gt; # 创建租户 openstack project create --description \u0026#39;Admin Project\u0026#39; \u0026lt;租户名\u0026gt; # 删除租户 openstack project delete \u0026lt;租户id/name\u0026gt; # 禁用启用租户 openstack project set \u0026lt;租户id/name\u0026gt; --disable/enable # 更新租户名称 openstack project set \u0026lt;租户id/name\u0026gt; --name \u0026lt;new name\u0026gt; # ======================================================== # 查看某一个项目下所有用户user openstack user list --project=\u0026lt;project_id/name\u0026gt; # 查看所有用户 openstack user list # 查看用户详情 openstack user show \u0026lt;user_name/id\u0026gt; # 创建用户 openstack user create --domain \u0026lt;域名\u0026gt; --project \u0026lt;项目/租户名\u0026gt; --password \u0026lt;密码\u0026gt; \u0026lt;用户名\u0026gt; # 删除用户 openstack user delete \u0026lt;用户名\u0026gt; # 禁用启用某一个用户 openstack user set \u0026lt;user_name/id\u0026gt; --disable/enable # 更新用户名称 openstack user set \u0026lt;user_name/id\u0026gt; --name \u0026lt;new name\u0026gt; # 查询某一用户与项目、角色的关系 openstack role assignment list --user=用户名 # ======================================================= # 角色查询 # 一个角色包括一组权利和特权，角色访问控制提供预定义的用户可操作列表，如开启或停止虚机，重置密码等。在身份验证服务和计算服务中均被支持。 openstack role list # 角色详情查询 opensatck role show \u0026lt;role_name/id\u0026gt; # 创建角色 openstack role create \u0026lt;role_name\u0026gt; # 分配角色，将项目和用户加入到角色中 openstack role add --user \u0026lt;用户名\u0026gt; --project \u0026lt;项目名\u0026gt; \u0026lt;角色名\u0026gt; # 删除角色 openstack role remove --user \u0026lt;用户名\u0026gt; --project \u0026lt;项目名\u0026gt; \u0026lt;角色名\u0026gt; # ====================================================== # 列出所有的镜像 openstack image list # 查看某一个镜像信息 openstack image show \u0026lt;image_id\u0026gt; # 设置镜像标签 openstack image set --tag \u0026lt;标签名\u0026gt; \u0026lt;image_name/id\u0026gt; # 创建镜像 # 格式化类型包括raw、qcow2、vmdk等 openstack image create \u0026lt;镜像名\u0026gt; --file \u0026lt;镜像文件名\u0026gt; --disk-format \u0026lt;格式化类型\u0026gt; --container-format bare --public # openstack image create “test1” --file cirros-0.5.1-x86_64-disk.img --disk-format qcow2 --container-format bare --public # 查看安全组信息 openstack group list # 查看flavor类型 openstack flavor list # 查询网络信息 openstack network list # 查看端口信息（虚拟网络） openstack port list # 创建虚拟机 openstack server create --image \u0026lt;image_id/name\u0026gt; --flavor \u0026lt;flavor_id/name\u0026gt; --nic net-id=\u0026lt;net_id\u0026gt; \u0026lt;instance_name\u0026gt; # 创建虚拟机帮助 openstack server create --help # ================================================== # 查看openstack环境主机列表 openstack host list # 查看某个host主机资源情况 openstack host show \u0026lt;host_name\u0026gt; # 查看虚拟机列表 openstack server list # 查看虚拟机详情 openstack server show \u0026lt;instance_id\u0026gt; # 虚拟机暂停 openstack server pause \u0026lt;instance_id\u0026gt; # 虚拟机从暂停中恢复 openstack server unpause \u0026lt;instance_id\u0026gt; # 虚拟机重启 openstack server reboot \u0026lt;instance_id\u0026gt; # 虚拟机删除 openstack server delete \u0026lt;instance_id\u0026gt; Nova篇 # 查看openstack版本 nova-manage version # 查看命令帮助信息 nova help \u0026lt;command\u0026gt; # 返回nova服务所在的host信息 # 在电子通信领域，host和node的区别在于，host是向外提供某种服务，而node只需要是连接到网络的设备 # 运行有nova服务的主机被认为是host nova host-list # 查看host具体资源信息 nova host-describe \u0026lt;host_name\u0026gt; # 查看nova服务和状态 nova service-list # ======================================= # 查看计算节点 nova hypervisor-list # 查看计算节点详情 nova hypervisor-show \u0026lt;hypervisor ID\u0026gt; # 查看计算节点上的虚拟机 nova hypervisor-servers \u0026lt;hypervisor ID\u0026gt; # ====================================== # 列出所有flavor(模板) nova flavor-list # 创建flavor，模板ID建议为auto nova flavor-create --is-public true \u0026lt;模板名称\u0026gt; \u0026lt;模板ID\u0026gt; \u0026lt;内存(MB)\u0026gt; \u0026lt;磁盘(GB)\u0026gt; \u0026lt;VCPUS\u0026gt; # 显示flavro详情 nova flavor-show \u0026lt;模板ID\u0026gt; # 删除flavor nova flavor-delet \u0026lt;模板ID\u0026gt; # ======================================= # 查看虚拟机列表 nova list nova list --all-te # 查看虚拟机详情 nova show \u0026lt;instance_id\u0026gt; # 查看虚拟机控制台日志 nova console-log \u0026lt;instance_id\u0026gt; # 查看密钥对列表 nova keypair-list # 查看镜像列表 nova image-list # 查看浮动ip列表 nova floating-ip-list # 查看安全组列表 nova secgroup-list # ===================================== # 查看浮动ip列表 nova-manage floating list # 数据库同步 nova-manage db sync nova-manage api_db sync nova-manage placement sync # 查看数据库版本 nova-manage db version # nova组件更新检查 nova-status upgrade check # ==================================== nova suspend \u0026lt;instance_id\u0026gt; nova resume \u0026lt;instance_id\u0026gt; nova start \u0026lt;instance_id\u0026gt; nova stop \u0026lt;instance_id\u0026gt; nova delete \u0026lt;instance_id\u0026gt; nova reboot \u0026lt;instance_id\u0026gt; # 硬重启 nova reboot --hard \u0026lt;instance_id\u0026gt; # 进入救援模式 nova rescue \u0026lt;instance_id\u0026gt; # 使用指定镜像进入救援模式 nova rescue --image \u0026lt;image_id\u0026gt; \u0026lt;instance_id\u0026gt; # 重启虚拟机，由救援模式进入正常模式 nova unrescue \u0026lt;instance_id\u0026gt; # 重置虚拟机状态 nova reset-state \u0026lt;instance_id\u0026gt; # 指定节点热迁移 nova live-migration \u0026lt;instance_id\u0026gt; \u0026lt;compute_node_id\u0026gt; # 调整虚拟机资源 nova resize \u0026lt;instance_id\u0026gt; \u0026lt;flavor_id\u0026gt; --poll # 确认调整虚拟机资源 nova resize-confirm \u0026lt;instance_id\u0026gt; # 资源调整失败回滚 nova resize-revert \u0026lt;instance_id\u0026gt; # 通过快照创建一个镜像 nova image-create \u0026lt;instance_id\u0026gt; \u0026lt;image_name\u0026gt; # =================================== # 从镜像创建虚拟机 nova boot --image cirros --flavor 1 --nic net-name=net1 vm1 # 从卷(块设备)创建虚拟机 # 1. 从镜像生成volumn cinder create --image-id \u0026lt;image_id\u0026gt; --name \u0026lt;volume_name\u0026gt; \u0026lt;size_in_gb\u0026gt; # 2. 从volumn创建虚拟机 nova boot --flavor \u0026lt;flavor_id\u0026gt; source=volumn,,id=卷ID,dest=volume,shutdown=preserve,bootindex=0 虚拟机名称 # ================================== # 挂载云硬盘 nova volume-attach \u0026lt;instance_id\u0026gt; \u0026lt;volume_name\u0026gt; /dev/sdb # 卸载云硬盘 nova volume-detach \u0026lt;instance_id\u0026gt; \u0026lt;volume_name\u0026gt; Neutron篇 # 列出当前租户网络 neutron net-list # 列出所有租户网络 neutron net-list --all-te # 查看网络详情 neutron net-show \u0026lt;net_id\u0026gt; # 删除一个网络 neutron net-delete \u0026lt;net_id\u0026gt; # 查看所有agent neutron agent-list # 查看所有租户拥有的port # port是虚拟网口，是路由器和虚拟机挂接网络的着附点 neutron port-list # 查看port详情 neutron port-show \u0026lt;port_id\u0026gt; # 查看安全组 neutron security-group-rule-list Glance篇 # 列出全部镜像 glance image-list # 查看image具体信息 glance show \u0026lt;image ID\u0026gt; # 上传镜像 glance image-create --visibility public --container-format docker/bare --disk-format raw/qcow2 --name xxx --file /root/xxx --progress glance image-create --name \u0026#34;CentOS7.0\u0026#34; --disk-format qcow2 --container-format bare --progress \u0026lt;/opt/images/centos_7-x86_64_xiandian.qcow2 Cinder篇 # 显示存储列表 cinder list # 显示存储卷类型列表 cinder type-list # 创建存储卷 cinder create --display-name VOLNAME SIZE（SIZE的单位为GB） Ceilmeter篇 # 查看监控资源 ceilometer meter-list #查看告警列表 ceilometer alarm-list # 删除一个告警 ceilometer alarm-delete -a ALARM_ID # 获取某一个告警信息 ceilometer alarm-state-get ALARM_ID 服务状态 systemctl list-units | grep openstack systemctl status httpd.service # 查看Apache的http服务日志 cd /etc/httpd/logs tail -f \u0026lt;日志文件\u0026gt; 参考 Openstack常用命令\nOpenstack官方常用命令手册\nOpenstack命令行操作虚拟机\nNova命令行官方参考\nOpenstack用户指南\n","permalink":"https://blog.niuhemoon.win/posts/tech/openstack-command-tutorial/","summary":"不同命令的功能有重复和交集 Openstack篇 Openstack每个组件都有其命令，openstack社区为了方便使用，将所有组件的命令进行了统一，以openstack开头 # 查看所有openstack服务 openstack service list # 查看openstack服务状态 openstack-service status # 重启本节点所有openstack","title":"Openstack命令行基础"},{"content":"环境搭建和准备 # 查看cpu是否支持硬件虚拟化 grep -E -c \u0026#34;vmx|svm\u0026#34; /proc/cpuinfo sudo apt install -y qemu qemu-kvm libvirt-daemon bridge-utils virt-manager virtinst # if centos # yum install -y kvm virt-manager libvirt libvirt-python python-virtinst virt-install qemu-kvm lsmod | grep -i kvm sudo systemctl status libvirtd.service # 如果服务未启动 sudo systemctl enable libvirtd --now # 配置网桥使得libvirt可以从外部访问 cat /etc/netplan/00-installer-config.yaml # 可选，GUI管理工具 sudo apt-get install virt-manager python-spice-client-gtk 下载调试镜像： 从官方地址下载cirros镜像，用来调试虚拟机，用户名和密码如下\nuser:cirros pass:cubswin:) # 不同版本密码不同 通常将cirros镜像放置到/var/lib/libvirt/boot路径下\n可以查看镜像信息\nqemu-img info cirros-0.5.0-x86_64-disk.img 至此，vrish学习的基本环境就搭建完成\nLibvirt基本概念 virsh命令大概分组\nDomain Management（域管理） Domain Monitoring（域监控） Host and Hypervisor（主机及虚拟化） Interface（网卡接口） Network Filter（网络防火墙） Networking（网络） Node Device（节点设备驱动）存在 Secret Snapshot（快照） Storage Pool（存储池或存储策略） Storage Volume（存储卷） Virsh itself（virsh shell自身相关） 定义过的或者能够被libvirt感知到的虚机的配置文件都在/etc/libvirt目录下\n虚拟机文件和其它的相关文件都保存在 /var/lib/libvirt/ 下\n镜像的默认路径是 /var/lib/libvirt/boot/。\n上图时一个libvirt虚拟机的生命周期图，虚拟机分为两种：\n持久性的 短暂性的 持久性虚拟机会一直存在，直到被删除；\n短暂性的虚拟机只有在虚拟机被关机或重启前存在\n虚拟机常用命令 virsh和qemu的命令非常多，下面罗列一些常用的命令\nvirsh help\t# 查看帮助信息 virsh version\t# 查看qemu版本 virsh help \u0026lt;特定命令\u0026gt;\t# 查看特定命令帮助信息 virsh \u0026lt;特定命令\u0026gt; --help\t# 查看特定命令帮助信息 virsh nodeinfo\t# 查看宿主机信息 virsh uri # 查看当前主机hyperviso的连接路径； virsh connect \u0026lt;hypervisor uri\u0026gt;\t# 连接到特定hypervisor,默认qemu:///system virsh sysinfo\t# 查看hypervisro信息 virsh start \u0026lt;虚拟机名称\u0026gt; # 启动一个之前已经定义define过的虚拟机（domain) virsh shutdown \u0026lt;虚拟机名称\u0026gt;\t# 关闭虚拟机,类似虚拟机内执行关机 virsh reboot \u0026lt;虚拟机名称\u0026gt;\t# 重启虚拟机 virsh destroy \u0026lt;虚拟机名称\u0026gt;\t# 强制关闭虚拟机，类似于断电 virsh suspend \u0026lt;虚拟机名称\u0026gt; # 挂起虚拟机，将当前状态保存在内存中 virsh resume \u0026lt;虚拟机名称\u0026gt;\t# 恢复虚拟机挂起状态，从内存中恢复虚拟机状态 virsh save \u0026lt;虚拟机名称\u0026gt; \u0026lt;img镜像文件名\u0026gt;\t# 暂停虚拟机，将虚拟机状态保存在磁盘镜像文件中 virsh restore \u0026lt;img镜像文件名\u0026gt;\t#重新载入暂停的虚拟机 virsh autostart \u0026lt;虚拟机名称\u0026gt;\t# 虚拟机随着物理机启动自动启动 virsh autostart \u0026lt;虚拟机名称\u0026gt; --disable\t# 禁止开机启动 virsh dominfo \u0026lt;虚拟机名称\u0026gt;\t# 查看虚拟机domain信息 virsh domblklist \u0026lt;虚拟机名称\u0026gt;\t# 列出虚拟机所有块存储设备 virsh console \u0026lt;虚拟机名称\u0026gt;\t# 控制台连接虚拟机 virsh dumpxml \u0026lt;虚拟机名称\u0026gt;\t# 查看虚拟机xml文件 virsh edit \u0026lt;虚拟机名称\u0026gt;\t# 编辑虚拟机xml文件 virsh managedsave \u0026lt;虚拟机名称\u0026gt;\t# 保存状态save并关闭虚拟机，下次启动会恢复到之前保存的状态 virsh start \u0026lt;虚拟机名称\u0026gt;\t# 启动并恢复managedsave保存的状态 virsh reset \u0026lt;虚拟机名称\u0026gt;\t# 对虚拟机执行强制重启，类似重置电源按钮 virsh create \u0026lt;虚拟机xml文件\u0026gt; # 从xml文件中创建domain，创建完成后会自动启动； # 一个xml对应一个domain虚拟机 virsh define \u0026lt;虚拟机xml文件\u0026gt;\t# 从xml文件定义define新的domain，不会自动启动 virsh undefine \u0026lt;虚拟机名称\u0026gt;\t# 对于运行中的持久性虚拟机，将状态转换为暂时的，关机后virsh无法感知其存在 # 对于非活动的虚拟机，undefine后virsh将无法感知其存在 # undefine后磁盘依然存在，只是删除虚拟机的配置文件/etc/libvirt/qemu virsh undefine \u0026lt;虚拟机名称\u0026gt; --remove-all-storage\t# 删除虚拟机并删除所有磁盘文件 virsh snapshot-create-as \u0026lt;虚拟机名称\u0026gt; --name \u0026lt;快照名称\u0026gt; # 从命令行创建快照 virsh snapshot-create \u0026lt;虚拟机名称\u0026gt;\t# 从xml文件创建快照 virsh snapshot-list \u0026lt;虚拟机名称\u0026gt;\t# 查看虚拟机快照列表 virsh snapshot-parent \u0026lt;虚拟机名称\u0026gt; --current\t# 查看当前快照的上一级快照 virsh snapshot-edit \u0026lt;虚拟机名称\u0026gt; --snapshotname \u0026lt;快照名\u0026gt;\t# 编辑快照 virsh snapshot-revert \u0026lt;虚拟机名称\u0026gt; --snapshotname \u0026lt;快照名\u0026gt;\t# 恢复快照 virsh snapshot-delete \u0026lt;虚拟机名称\u0026gt; --snapshotname \u0026lt;快照名\u0026gt;\t# 删除快照 virsh list\t# 查看活动虚拟机状态 virsh list --all\t# 查看所有虚拟机状态 virsh setvcpus \u0026lt;虚拟机名称\u0026gt; 4 --maximum --config # 设置最大vcpu数（只能用--config，下次运行生效） virsh setvcpus \u0026lt;虚拟机名称\u0026gt; 4 --config # 下次启动使用vcpu数 virsh vcpuinfo \u0026lt;虚拟机名称\u0026gt; # 查看vcpu信息 virsh vcpupin \u0026lt;虚拟机名称\u0026gt;\t# 查询域 vcpu亲和性,即vcpu和物理cpu之间关系 virsh maxvcpus\t# 显示本机vcpu最大值 virsh setmaxmem \u0026lt;虚拟机名称\u0026gt; [--size] 2G --current # 设置最大内存限制值 virsh setmem \u0026lt;虚拟机名称\u0026gt; [--size] 2G --current # 设置内存分配 virsh domblklist cirros\t# 查看虚拟机的存储块设备 创建磁盘文件 #qcow2是文件类型，test1-add1.qcow2是磁盘文件，5G是大小 qemu-img create -f qcow2 /var/lib/libvirt/images/test1-add1.qcow2 5G qemu-img info \u0026lt;虚拟机镜像\u0026gt;\t# 查看镜像信息 virt-install \u0026lt;命令行\u0026gt; # 通过命令行指定来创建虚拟机 virsh attach-disk \u0026lt;虚拟机名称\u0026gt; virsh attach-device \u0026lt;虚拟机名称\u0026gt; /etc/libvirt/qemu/test2-add.xml --persistent\t# 从XML文件附加设备 virsh detach-device \u0026lt;虚拟机名称\u0026gt; /etc/libvirt/qemu/test2-add.xml --persistent\t# 卸载设备 虚拟机操作实践 实验1：修改虚拟机vcpu 修改虚拟机的最大vcpu数量，可以修改maximum config 和current config的值\n# 查看vcpu配置 ➜ ~ virsh vcpucount cirros maximum config 2\t# 指定下次重启虚拟机后可用的最大vcpu数量 maximum live 2\t# 指定运行/暂停状态下虚拟机可用的最大vcpu数量,重启后和maximum config一致 current config 2\t# 下次重启时虚拟机使用的vcpu数量 current live 2\t# 正在运行的虚拟机vcpu实际数量 通过修改xml文件修改vcpu数量\nvirsh edit cirros # \u0026lt;vcpu placement=\u0026#39;static\u0026#39;\u0026gt;2\u0026lt;/vcpu\u0026gt; # 修改为 \u0026lt;vcpu placement=\u0026#39;static\u0026#39;\u0026gt;3\u0026lt;/vcpu\u0026gt; 关闭并重新启动虚拟机\nvirsh shutdown cirros virsh list --all # 确认已经是shut off状态 virsh start cirros 再次查看vcpu数量，发现已经改变\n➜ ~ virsh vcpucount cirros maximum config 3 maximum live 3 current config 3 current live 3 同样方式，修改xml文件，恢复为2个vcpu，执行virsh reboot 后并没释放vcpu\n➜ ~ virsh vcpucount cirros maximum config 2 maximum live 3 current config 2 current live 3 必须执行shutdown或者destory，然后重新start才能改变运行时的vcpu\n➜ ~ virsh vcpucount cirros maximum config 2 maximum live 2 current config 2 current live 2 还可以通过命令行修改vcpu的各个配置值\n➜ ~ virsh setvcpus cirros 3 --maximum --config ➜ ~ virsh vcpucount cirros maximum config 3 maximum live 2 current config 2 current live 2 ➜ ~ virsh setvcpus cirros 3 --config # 修改current config ➜ ~ virsh vcpucount cirros maximum config 3 maximum live 2 current config 3 current live 2 # 重启虚拟机使其生效 ➜ ~ virsh shutdown cirros ➜ ~ virsh start cirros ➜ ~ virsh vcpucount cirros maximum config 3 maximum live 3 current config 3 current live 3 在宿主机上无法设置vcpu的current live小于current config，只能在虚拟机内部执行chcpu指令来修改，使得vcpu离线\n也可以将最大可用vcpu设置较大，方便后续在虚拟机运行时可以动态调整vcpu的数量\n➜ ~ virsh setvcpus cirros 5 --maximum --config ➜ ~ virsh setvcpus cirros 2 --config ➜ ~ virsh shutdown cirros ➜ ~ virsh start cirros ➜ ~ virsh vcpucount cirros maximum config 5 maximum live 5 current config 2 current live 2 ➜ ~ virsh setvcpus cirros 3\t# 动态调整vcpu数量，调整范围[current config，比maximum config] ➜ ~ virsh vcpucount cirros maximum config 5 maximum live 5 current config 2 current live 3 ➜ ~ virsh setvcpus cirros 2\t➜ ~ virsh vcpucount cirros maximum config 5 maximum live 5 current config 2 current live 2 ➜ ~ virsh vcpuinfo cirros\t# 查看vcpu运行状态 实验2：修改虚拟机的内存 通过修改xml配置文件并重新启动虚拟机\n# 修改项，修改完成后 \u0026lt;memory unit=\u0026#39;KiB\u0026#39;\u0026gt;462144\u0026lt;/memory\u0026gt;\t# 启动后最大允许可用内存 \u0026lt;currentMemory unit=\u0026#39;KiB\u0026#39;\u0026gt;262144\u0026lt;/currentMemory\u0026gt;# 启动时使用的内存大小 在最大可用内存范围内，可以在虚拟机运行时调整内存使用\n虚拟机最大内存只能在虚拟机关闭状态更改，重启后生效\n使用virsh setmaxmem命令，和直接修改xml文件等效\n➜ ~ virsh dominfo cirros | grep memory # 虚拟机启动时显示的used memory不准确 Max memory: 462144 KiB Used memory: 262144 KiB ➜ ~ virsh setmem cirros 300000 ➜ ~ virsh dominfo cirros | grep memory Max memory: 462144 KiB Used memory: 300000 KiB ➜ ~ virsh shutdown cirros ➜ ~ virsh setmaxmem cirros 700000 实验3：调整虚拟机的磁盘 虚拟机支持在虚拟机开机时动态挂载新的磁盘\n➜ ~ qemu-img create -f qcow2 -o size=20M,preallocation=metadata /var/lib/libvirt/boot/second.qcow2 Formatting \u0026#39;/var/lib/libvirt/boot/second.qcow2\u0026#39;, fmt=qcow2 size=20971520 cluster_size=65536 preallocation=metadata lazy_refcounts=off refcount_bits=16 ➜ ~ qemu-img info /var/lib/libvirt/boot/second.qcow2 image: /var/lib/libvirt/boot/second.qcow2 file format: qcow2 virtual size: 20 MiB (20971520 bytes) disk size: 260 KiB cluster_size: 65536 Format specific information: compat: 1.1 lazy refcounts: false refcount bits: 16 corrupt: false virsh attach-disk cirros /images/cirros/second.qcow2 vda --targetbus virtio # 卸载磁盘（前提时没有分区或者挂载 virsh detach-disk 26 vda 也可以手动修改xml文件，然后重启虚拟机\n# 首先创建一个qcow2磁盘或者raw磁盘 # dd命令创建一个非稀疏的磁盘 dd if=/dev/zero of=/vm-images/vm1-add.img bs=1M count=1024 # 在xml文件中加入一个新的xml段 \u0026lt;disk type=\u0026#39;file\u0026#39; device=\u0026#39;disk\u0026#39;\u0026gt; \u0026lt;driver name=\u0026#39;qemu\u0026#39; type=\u0026#39;raw\u0026#39; cache=\u0026#39;none\u0026#39; io=\u0026#39;threads\u0026#39;/\u0026gt; \u0026lt;source file=\u0026#39;/vm-images/vm1-add.img\u0026#39;/\u0026gt; \u0026lt;target dev=\u0026#39;vdb\u0026#39; bus=\u0026#39;virtio\u0026#39;/\u0026gt; \u0026lt;address type=\u0026#39;pci\u0026#39; domain=\u0026#39;0x0000\u0026#39; bus=\u0026#39;0x00\u0026#39; slot=\u0026#39;0x06\u0026#39; function=\u0026#39;0x0\u0026#39;/\u0026gt; \u0026lt;/disk\u0026gt; 实验4：创建虚拟机 virt-install --name=cirros --ram=256 --vcpus=1 --disk path=/var/lib/libvirt/boot/cirros-0.5.0-x86_64-disk.img,format=qcow2 --import --network network:default --vnc --vncport=5920 # 也可以通过xml文件来创建 实验5：复制虚拟机 可以通过命令行复制一个虚拟机，也可以通过拷贝并修改配置文件和存储卷文件进行复制\n# 通过virt-clone命令复制 # virt-clone -o \u0026lt;虚拟机名称\u0026gt; -n \u0026lt;新虚拟机名称\u0026gt; -f /var/lib/libvirt/images/test4.qcow2 # qcow2磁盘不需要预先创建，创建完成后需要进虚拟机手动改变ip地址和用户名 ➜ ~ virt-clone -o cirros -n cirros1 -f /var/lib/libvirt/boot/cirros1.qcow2 # 自动生产不一样的mac地址和uuid ➜ ~ diff cirros.xml cirros1.xml \u0026lt; \u0026lt;name\u0026gt;cirros\u0026lt;/name\u0026gt; \u0026lt; \u0026lt;uuid\u0026gt;874fe199-47ea-45d1-a25d-98d0535dddb3\u0026lt;/uuid\u0026gt; --- \u0026gt; \u0026lt;name\u0026gt;cirros1\u0026lt;/name\u0026gt; \u0026gt; \u0026lt;uuid\u0026gt;fba2f776-432d-4870-a7ec-bbf73fa1b086\u0026lt;/uuid\u0026gt; 39c39 \u0026lt; \u0026lt;source file=\u0026#39;/var/lib/libvirt/boot/cirros-0.5.0-x86_64-disk.img\u0026#39;/\u0026gt; --- \u0026gt; \u0026lt;source file=\u0026#39;/var/lib/libvirt/boot/cirros1.qcow2\u0026#39;/\u0026gt; 63c63 \u0026lt; \u0026lt;mac address=\u0026#39;52:54:00:fa:5c:d3\u0026#39;/\u0026gt; --- \u0026gt; \u0026lt;mac address=\u0026#39;52:54:00:29:8d:06\u0026#39;/\u0026gt; 81c81 \u0026lt; \u0026lt;graphics type=\u0026#39;vnc\u0026#39; port=\u0026#39;5921\u0026#39; autoport=\u0026#39;no\u0026#39;\u0026gt; --- \u0026gt; \u0026lt;graphics type=\u0026#39;vnc\u0026#39; port=\u0026#39;-1\u0026#39; autoport=\u0026#39;yes\u0026#39;\u0026gt; #virt-clone -f指定的文件不要事先创建，如果有多个磁盘文件就用多个-f选项 如 virt-clone -o \u0026lt;虚拟机名称\u0026gt; -n \u0026lt;新虚拟机名称\u0026gt; -f /home/lib/libvirt/images/test4.qcow2 -f /mnt/images/test4-add1.qcow2 可以手动复制xml文件和磁盘镜像来复制\n➜ qemu cp cirros.xml cirros3.xml # 修改xml文件中的domain name和mac地址等信息 ➜ qemu vim cirros3.xml ➜ qemu cd /var/lib/libvirt/boot # 在define新的克隆虚拟机之前准备好所需的磁盘 ➜ boot cp cirros-0.5.0-x86_64-disk.img cirros3.img ➜ boot qemu-img info cirros3.img image: cirros3.img file format: qcow2 virtual size: 112 MiB (117440512 bytes) disk size: 198 MiB cluster_size: 65536 Snapshot list: ID TAG VM SIZE DATE VM CLOCK 1 1603292226 101 MiB 2020-10-21 22:57:06 94:39:31.435 Format specific information: compat: 1.1 lazy refcounts: false refcount bits: 16 corrupt: false ➜ ~ virsh define /etc/libvirt/qemu/cirros3.xml Domain cirros3 defined from /etc/libvirt/qemu/cirros3.xml ➜ ~ virsh list --all Id Name State -------------------------- 2 cirros running - cirros3 shut off ➜ ~ virsh start cirros3 Domain cirros3 started # 此时，一个新的克隆虚拟机就创建完成 # 需要手动修改IP和hostname 实验6：误删虚拟机恢复 # 误删除虚拟机 ➜ ~ virsh undefine cirros1 Domain cirros1 has been undefined ➜ ~ virsh dominfo cirros1 Id: 1 Name: cirros1 UUID: fba2f776-432d-4870-a7ec-bbf73fa1b086 OS Type: hvm State: running CPU(s): 2 CPU time: 41.7s Max memory: 700416 KiB Used memory: 262144 KiB Persistent: no\t# 已经变为非持久化的，无法重新启动 Autostart: disable Managed save: no Security model: apparmor Security DOI: 0 Security label: libvirt-fba2f776-432d-4870-a7ec-bbf73fa1b086 (enforcing) ➜ ~ virsh shutdown cirros1 # 关机后再也启动不了了 ➜ ~ ls /etc/libvirt/qemu # cirros1的xml文件已经不存在 ➜ ~ ls /var/lib/libvirt/boot # cirros1的qcow2镜像仍然存在 # 需要在关闭虚拟机之前，重新定义define一个配置文件即可恢复 virsh dumpxml centos-C \u0026gt; /etc/libvirt/qemu/centos-C.xml virsh define /etc/libvirt/qemu/centos-C.xml # 如果是在关闭着的服务器上执行的virsh undefine centos-C 删除命令，则会把对应的配置文件清空，虚拟机再也启动不了，重新定义也不行 对于 参考 在 Ubuntu 的 KVM 中安装 Windows 系统\nvirsh使用总结\nkvm管理和基础命令\nkvm命令总结和虚机器备份迁移\nHow to Install KVM on Ubuntu 20.04 LTS Server (Focal Fossa)\nLibvirt虚拟机生命周期\nModify cpu number ","permalink":"https://blog.niuhemoon.win/posts/tech/virsh-tutorial/","summary":"环境搭建和准备 # 查看cpu是否支持硬件虚拟化 grep -E -c \u0026#34;vmx|svm\u0026#34; /proc/cpuinfo sudo apt install -y qemu qemu-kvm libvirt-daemon bridge-utils virt-manager virtinst # if centos # yum install -y kvm virt-manager libvirt libvirt-python python-virtinst virt-install qemu-kvm lsmod | grep -i kvm sudo systemctl status libvirtd.service # 如果服务未启动 sudo systemctl enable libvirtd --now # 配置网桥使得libvirt可以从外部访问 cat /etc/netplan/00-installer-config.yaml # 可选，GUI管理工具 sudo apt-get install virt-manager python-spice-client-gtk 下载调试镜像： 从官方地址下载cirros镜像，用来调试虚","title":"Virsh命令和虚拟机"},{"content":" Tcpdump是一个linux命令行的抓包工具，可以抓取TCP/IP和其他数据包，如UDP,ARP,ICMP，可以使用过滤器过滤出想要的包。\n抓取特定接口上的包 当使用tcpdump不加任何参数，将分析所有接口上的数据包。\nsudo tcpdump 可以使用-i选项指定特定接口\n可以使用-c选项限制数据包的个数\nsudo tcpdump -i wlp2s0 -c 10 抓取特定主机的数据包 可以使用-host选项指定和特定主机相关的数据包\nsudo tcpdump -i ens160 -c 5 -ttttnnvvS host 14.249.62.219 通过指定端口抓包 可以指定接口的端口进行抓包，也可以抓取特定接口以外的数据包\n# 抓取22端口数据包 sudo tcpdump -i ens160 -c 5 -nn port 22 # 抓取22端口以外数据包 sudo tcpdump -i ens160 -nn not port 22 # 指定端口号的范围 sudo tcpdump -i ens160 -c 3 -nns 0 portrange 20-23 抓取特定代理的数据包 sudo tcpdump -i ens160 -c 5 -nn tcp 保存抓包日志 使用-s选项来指定每个数据包保存的长度，默认保存68个字节，剩余字节被忽略，指定0表示完整保存。\nsudo tcpdump -i ens160 -c 5 -nn tcp -w packets-record.pcap -s 0 读取tcpdump记录文件 更常用的是使用wireshark软件分析\nsudo tcpdump -r packets-record.pcap 过滤特定源头的数据包 使用src选项指定来自特定源IP的数据包\n使用dst选项指定特定目的IP的数据包\nsudo tcpdump src 100.9.8.40 sudo tcpdump dst 14.249.62.219 抓取特定网段的数据包 使用-net选项指定incoming/outgoing特定网段的数据包\nsudu tcpdump net 192.169.0.0/24 指定数据包格式 # 16进制格式 sudo tcpdump -X -i eth0 # Ascii码格式 sudo tcpdump -A -i eth0 抓取IPV6包 sudo tcpdump -nn ip6 proto 6 过滤Http的User Agent 从http请求头中过滤出user agent和host信息\nsudo tcpdump -nn -A -s1500 -l | egrep -i \u0026#39;User-Agent:|Host:\u0026#39; 过滤cookie信息\nsudo tcpdump -nn -A -s0 -l | egrep -i \u0026#39;Set-Cookie|Host:|Cookie:\u0026#39; 列出可选的接口 sudo tcpdump -D 循环写入抓包文件 对于长时间的抓包，为了防止单个文件过大，每30分钟（1800秒）写入一个新文件，文件大小限制为100M，文件个数的24个\nsudo tcpdump -i ens160 -w /tmp/network-%H-%M.pcap -W 24 -G 1800 -C 100 Tcpdump选项 -i \u0026lt;interface\u0026gt;: 监听特定接口 -n: Don’t resolve hostnames. You can use -nn to don’t resolve hostnames or port names. -t: Print human-readable timestamp on each dump line, -tttt: Give maximally human-readable timestamp output. -X: 以ascii和十六进制两种格式显示数据包内容 -v, -vv, -vvv: 增加获取数据包的数量 -c N: 只获取N个数据包然后停止 -s: Define the snaplength (size) of the capture in bytes. Use -s0 to get everything, unless you are intentionally capturing less. -S: 打印绝对序列号 -q: 显示较少的协议信息 -w \u0026lt;file name\u0026gt;: 将原始数据包写入文件 逻辑运算符号 Tcpdump支持更精确的过滤，使用and/or/not这种逻辑运算。\n抓取来自 10.20.0.0/16 网段，并且目的地址是10.30.0.0/16 网段的数据包，使用便于阅读的时间戳， 不求解主机名和端口号，反向输出并使用绝对序号。\nCapture traffic coming from 10.20.0.0/16 and going to the network 10.30.0.0/16 with showing human-readable timestamps (tt), with no resolution of hostnames or port numbers (nn), verbose output (vv) and using absolute sequence numbers (S):\n$ sudo -ttnnvvS tcpdump src net 10.20.0.0/16 and dst net 10.30.0.0/16 Display traffic from source 192.168.0.10 which is not UDP protocol:\n$ sudo tcpdump src 192.168.0.10 and src net and not udp To capture arp or ping traffic for a specific host and save the output to a file named packetfile.txt:\n$ sudo tcpdump -nnti eth0 arp or icmp and host 192.168.0.1 -w packetfile.txt Tcpdump 输出格式 截取一行输出，分析其输出的格式\n10:31:13.440803 IP Ubuntu.ssh \u0026gt; 117.6.129.86.50736: Flags [P.], seq 188:400, ack 1, win 501, options [nop,nop,TS val 468736347 ecr 335665367], length 212 其中:\n10:31:13.401128 - 本地数据包被抓取的时间\nIP - 表示数据包是IPV4协议的\nUbuntu.ssh - 标识源IP地址或者主机名 ，.ssh 表示端口，这里时22端口\n117.6.129.86.50376 - 表示数据包的目的IP地址 ，使用.分割端口号\n标志位：\n[P.] - This is TCP flags field.\n[.] - ACK (Acknowledgment).\n[S] - SYN (Start Connection).\n[P] - PSH (Push Data).\n[F] - FIN (Finish Connection).\n[R] - RST (Reset Connection).\n[S.] - SYN-ACK (SynAcK Packet).\nseq 188:400 - 序列号表示该数据包包含序列是188-400字节的数据\nwin 501 - 窗口大小，表示接受缓冲区中可用的字节\noptions [nop,nop,TS val 468736347 ecr 335665367] - These are TCP options such as the MSS (Maximum Segment Size) or Window Scale. You can refer more about TCP protocol options.\nlength 212 - 表示数据包中payload数据的字节\n参考 Tcpdump基本使用\n","permalink":"https://blog.niuhemoon.win/posts/tech/tcpdump-usage/","summary":"Tcpdump是一个linux命令行的抓包工具，可以抓取TCP/IP和其他数据包，如UDP,ARP,ICMP，可以使用过滤器过滤出想要的包。 抓取特定接口上的包 当使用tcpdump不加任何参数，将分析所有接口上的数据包。 sudo tcpdump 可以使用-i选项指定特定接口 可以使用-c选项限制数据包的个","title":"tcpdump的基本使用【译】"},{"content":"简介 Linux上对系统进行性能检测的工具非常多，本文介绍一些常用工具的使用\n性能观测工具\n▪ 首先学习的Basic Tool有如下： uptime、top(htop)、mpstat、isstat、vmstat、free、ping、nicstat、dstat。\n▪ 高级的命令如下： sar、netstat、pidstat、strace、tcpdump、blktrace、iotop、slabtop、sysctl、/proc。\n性能观测工具sar\nsar\n# install sudo apt install sysstat # usage sar -u 2 3 sar -u -f /var/log/sa/sa05 sar -P ALL 1 1 sar -r 1 3 sar -W 1 3 top\n交互模式的一些快捷操作: 全局命令: \u0026lt;回车/空格\u0026gt; ?, =, A, B, d, G, h, I, k, q, r, s, W, Z 统计区的命令: l, m, t, 1 任务区的命令： 外观: b, x, y, z 内容: c, f, H, o, S, u 大小: #, i, n 排序: \u0026lt;, \u0026gt;, F, O, R 色彩方案: \u0026lt;Ret\u0026gt;, a, B, b, H, M, q, S, T, w, z, 0 - 7 窗口命令: -, _, =, +, A, a, G, g, w Press \u0026#39;h\u0026#39; or \u0026#39;?\u0026#39; for help with Windows, Type \u0026#39;q\u0026#39; or \u0026lt;Esc\u0026gt; to continue 如果这个数除以逻辑CPU的数量，结果高于5的时候就表明系统在超负荷运转了！！！！！\n9. netstat - 显示开放的端口和连接 它是Linux管理员使用来显示各种网络信息的工具，如查看什么端口开放和什么网络连接已经建立以及何种进程运行在该连接之上。同时它也显示了不同程序间打开的Unix套接字的信息。作为大多数Linux发行版本的一部分，netstat的许多命令在netstat和它的不同输出中有详细的描述。最为常用的如下：\nnetstat | head -20 netstat -r netstat -rC netstat -i netstat -ie netstat -s netstat -g netstat -tapn vmstat是虚拟内存(virtual memory statistics)的缩写，作为一个内存监控工具，它收集和显示关于内存，进程，终端和分页和I/O阻塞的概括信息。作为一个开源程序，它可以在大部分Linux发行版本中找到，包括Solaris和FreeBSD。它用来诊断大部分的内存性能问题和其他相关问题。\n让我们看下如何了解vmstat提供的信息：\n----------------------------- procs部分的解释 r 列表示运行和等待cpu时间片的进程数，如果长期大于1，说明cpu不足，需要增加cpu。 b 列表示在等待资源的进程数，比如正在等待I``/O``、或者内存交换等。 ----------------------------- cpu部分的解释 us 列显示了用户方式下所花费 CPU 时间的百分比。us的值比较高时，说明用户进程消耗的cpu时间多，但是如果长期大于50%，需要考虑优化用户的程序。 sy 列显示了内核进程所花费的cpu时间的百分比。这里us + sy的参考值为80%，如果us+sy 大于 80%说明可能存在CPU不足。 wa 列显示了IO等待所占用的CPU时间的百分比。这里wa的参考值为30%，如果wa超过30%，说明IO等待严重，这可能是磁盘大量随机访问造成的，也可能磁盘或者 ``磁盘访问控制器的带宽瓶颈造成的(主要是块操作)。 id` `列显示了cpu处在空闲状态的时间百分比 ----------------------------- system部分的解释 in 列表示在某一时间间隔中观测到的每秒设备中断数。 cs列表示每秒产生的上下文切换次数，如当 cs 比磁盘 I/O 和网络信息包速率高得多，都应进行进一步调查。 ----------------------------- memory部分的解释 swpd 切换到内存交换区的内存数量(k表示)。如果swpd的值不为0，或者比较大，比如超过了100m，只要si、so的值长期为0，系统性能还是正常 free 当前的空闲页面列表中内存数量(k表示) buff 作为buffer cache的内存数量，一般对块设备的读写才需要缓冲。 cache: 作为page cache的内存数量，一般作为文件系统的cache，如果cache较大，说明用到cache的文件较多，如果此时IO中bi比较小，说明文件系统效率比较好。 ----------------------------- swap部分的解释 si 由内存进入内存交换区数量。 so由内存交换区进入内存数量。 ----------------------------- IO部分的解释 bi 从块设备读入数据的总量（读磁盘）（每秒kb）。 bo 块设备写入数据的总量（写磁盘）（每秒kb） Procs procs有 r列和b列。r列代表等待访问CPU的进程数量。而b列意味着睡眠进程的数量。在这些列的下面，是它们的值。从上面的截图中，我门有2个进程正在等待访问CPU，0个睡眠进程。\nMemory memory有swpd、 free、 buff 和 cache 这些列。这些信息和命令free -m相同。swpd列显示了有多少内存已经被交换到了交换文件或者磁盘。free列显示了未分配的可用内存。buff列显示了使用中的内存。cache列显示了有多少内存可以被交换到交换文件或者磁盘上如果一些应用需要他们。\nSwap swap显示了从交换系统上发送或取回了多少内存。si列告诉我们每秒有多少内存被从swap移到真实内存中（In）。so列告诉我们每秒有多少内存被从真实内存移到swap中（Out）。\nI/O io依据块的读写显示了每秒输入输出的活动。bi列告诉我们收到的块数量，bo列告诉我们发送的块数量。\nSystem system显示了每秒的系统操作数量。in列显示了系统每秒被中断的数量。cs列显示了系统为了处理所以任务而上下文切换的数量。\nCPU CPU告诉了我们CPU资源的使用情况。us列显示了处理器在非内核程序消耗的时间。sy列显示了处理器在内核相关任务上消耗的时间。id列显示了处理器的空闲时间。wa列显示了处理器在等待IO操作完成以继续处理任务上的时间。\nss是iproute2包的一部分。iproute2是用来替代一整套标准的Unix网络工具组件，它曾经用来完成网络接口配置，路由表和管理ARP表任务。ss工具用来记录套接字统计信息，它可以显示类似netstat一样的信息，同时也能显示更多TCP和状态信息。一些例子如下：\nss -tnap ss -tnap6 ss -tnap ss -s ss -tn -o state established -p lsof命令，意为“list open files”, 用于在许多类Unix系统中显示所有打开的文件及打开它们的进程。在大部分Linux发行版和其他类Linux操作系统中系统管理员用它来检查不同的进程打开了哪些文件。\n# lsof +p process_id # lsof | less # lsof –u username # lsof /etc/passwd # lsof –i TCP:ftp # lsof –i TCP:80 缓冲区与特定的块设备关联，并覆盖文件系统元数据的缓存以及跟踪运行中的页面。缓存仅包含驻留的文件数据。也就是说，缓冲区记住目录中的内容，文件权限是什么，并跟踪从特定块设备写入或读取的内存。缓存仅包含文件本身的内容。\n“缓冲区”表示有多少RAM专用于缓存磁盘块。“缓存”类似于“缓冲区”，只是这次它缓存文件读取中的页面。\n引用答案（供参考）：\n简短答案：高速缓存是页面高速缓存的大小。缓冲区是内存中块I / O缓冲区的大小。缓存的事项；缓冲区在很大程度上无关紧要。\n长答案：缓存是Linux页面缓存的大小减去交换缓存中的内存，它由SwapCached表示（因此总页面缓存大小为Cached + SwapCached）。Linux通过页面缓存执行所有文件I / O。写操作的实现是简单地将页面缓存中的相应页面标记为脏。然后，刷新程序线程会定期将所有脏页写回到磁盘。通过从页面缓存返回数据来实现读取。如果数据尚未在高速缓存中，则首先填充它。在现代Linux系统上，“缓存”可以轻松达到数GB。它只会响应内存压力而缩小。系统将清除页面缓存以及将数据交换到磁盘上，以根据需要提供更多的内存。\n缓冲区是内存中的块I / O缓冲区。他们是相对短暂的。在Linux内核版本2.4之前，Linux具有单独的页面和缓冲区高速缓存。从2.4开始，页面和缓冲区高速缓存是统一的，缓冲区是未在页面高速缓存中表示的原始磁盘块，即不是文件数据。因此，“缓冲区”度量标准的重要性最低。在大多数系统上，缓冲区通常只有几十兆字节。\n它并不像这样简单，但是可能有助于理解：\n缓冲区用于存储文件元数据（权限，位置等）。每个内存页面都在此处跟踪。\n缓存用于存储实际文件内容。\niostat\n解释说明： avg-cpu: 总体cpu使用情况统计信息，对于多核cpu，这里为所有cpu的平均值 %user: 在用户级别运行所使用的CPU的百分比. %``nice``: ``nice``操作所使用的CPU的百分比. %sys: 在系统级别(kernel)运行所使用CPU的百分比. %iowait: CPU等待硬件I``/O``时,所占用CPU百分比. %idle: CPU空闲时间的百分比. Device段:各磁盘设备的IO统计信息 tps: 每秒钟发送到的I``/O``请求数. Blk_read ``/s``: 每秒读取的block数. Blk_wrtn``/s``: 每秒写入的block数. Blk_read: 读入的block总数. Blk_wrtn: 写入的block总数. iostat -x -k -d 1\n解释说明： rrqm``/s``: 每秒对该设备的读请求被合并次数，文件系统会对读取同块(block)的请求进行合并 wrqm``/s``: 每秒对该设备的写请求被合并次数 r``/s``: 每秒完成的读次数 w``/s``: 每秒完成的写次数 rkB``/s``: 每秒读数据量(kB为单位) wkB``/s``: 每秒写数据量(kB为单位) avgrq-sz:平均每次IO操作的数据量(扇区数为单位) avgqu-sz: 平均等待处理的IO请求队列长度 await: 平均每次IO请求等待时间(包括等待时间和处理时间，毫秒为单位) svctm: 平均每次IO请求的处理时间(毫秒为单位) %util: 采用周期内用于IO操作的时间比率，即IO队列非空的时间比率 如果 %util 接近 100%，说明产生的I``/O``请求太多，I``/O``系统已经满负荷，该磁盘可能存在瓶颈。 idle小于70% IO压力就较大了,一般读取速度有较多的wait。 同时可以结合vmstat 查看查看b参数(等待资源的进程数)和wa参数(IO等待所占用的CPU时间的百分比,高过30%时IO压力高) 在Linux系统中，为了提高文件系统性能，内核利用一部分物理内存分配出缓冲区，用于缓存系统操作和数据文件，当内核收到读写的请求时，内核先去缓存区找是否有请求的数据，有就直接返回，如果没有则通过驱动程序直接操作磁盘。 缓存机制优点：减少系统调用次数，降低CPU上下文切换和磁盘访问频率。 CPU上下文切换：CPU给每个进程一定的服务时间，当时间片用完后，内核从正在运行的进程中收回处理器，同时把进程当前运行状态保存下来，然后加载下一个任务，这个过程叫做上下文切换。实质上就是被终止运行进程与待运行进程的进程切换。\nSwap用途：Swap意思是交换分区，通常我们说的虚拟内存，是从硬盘中划分出的一个分区。当物理内存不够用的时候，内核就会释放缓存区（buffers/cache）里一些长时间不用的程序，然后将这些程序临时放到Swap中，也就是说如果物理内存和缓存区内存不够用的时候，才会用到Swap。 swap清理：swapoff -a \u0026amp;\u0026amp; swapon -a 注意：这样清理有个前提条件，空闲的内存必须比已经使用的swap空间大\n1. 查看内存使用情况，发现swap虚拟内存空间竟然为0 # free -m 2. 建虚拟内存磁盘卷。做法如下： # dd if=/dev/zero of=/opt/swap bs=1024 count=2048000 # mkswap /opt/swap # swapon /opt/swap 再次查看内容，发现swap虚拟内存就有了 # free -m 3. 如果想取消文件虚拟内存，即删除swap，做法如下：（当然根据系统配置，也可以保留swap，以后继续用）。 # swapoff /opt/swap # rm /opt/swap 4. swap开机挂载 # vim /etc/fstab /opt/swap swap swap defaults 0 0 上面挂载参数分别为： 设备文件或伪文件系统 挂载点 文件系统类型 挂载选项 备份频率 开机自检次序 6. 移动虚拟内存空间 如果当前的虚存所在的磁盘空间不够，可以首先关闭虚存服务，将其移动到别的磁盘，再启用即可。 # swapoff -v /swap/swapadd # mv /swap/swapadd /mnt/swap # swapon /swap/swapadd 释放缓存区内存的方法 1）清理pagecache（页面缓存） [root@backup ~]# echo 1 \u0026gt; /proc/sys/vm/drop_caches 或者 # sysctl -w vm.drop_caches=1 2）清理dentries（目录缓存）和inodes [root@backup ~]# echo 2 \u0026gt; /proc/sys/vm/drop_caches 或者 # sysctl -w vm.drop_caches=2 3）清理pagecache、dentries和inodes [root@backup ~]# echo 3 \u0026gt; /proc/sys/vm/drop_caches 或者 # sysctl -w vm.drop_caches=3 上面三种方式都是临时释放缓存的方法，要想永久释放缓存，需要在/etc/sysctl.conf文件中配置：vm.drop_caches=1/2/3，然后sysctl -p生效即可！ 另外，可以使用sync命令来清理文件系统缓存，还会清理僵尸(zombie)对象和它们占用的内存 [root@backup ~]# sync 温馨提示： 上面操作在大多数情况下都不会对系统造成伤害，只会有助于释放不用的内存。 但是如果在执行这些操作时正在写数据，那么实际上在数据到达磁盘之前就将它从文件缓存中清除掉了，这可能会造成很不好的影响。 那么如果避免这种事情发生呢？ 因此，这里不得不提一下/proc/sys/vm/vfs_cache_pressure这个文件，告诉内核，当清理inoe/dentry缓存时应该用什么样的优先级。 [root@backup ~]# cat /proc/sys/vm/vfs_cache_pressure 100 vfs_cache_pressure=100 这个是默认值，内核会尝试重新声明dentries和inodes，并采用一种相对于页面缓存和交换缓存比较\u0026#34;合理\u0026#34;的比例。 减少vfs_cache_pressure的值，会导致内核倾向于保留dentry和inode缓存。 增加vfs_cache_pressure的值，（即超过100时），则会导致内核倾向于重新声明dentries和inodes 总之，vfs_cache_pressure的值： 小于100的值不会导致缓存的大量减少 超过100的值则会告诉内核你希望以高优先级来清理缓存。 其实无论vfs_cache_pressure的值采用什么值，内核清理缓存的速度都是比较低的。 如果将此值设置为10000，系统将会将缓存减少到一个合理的水平。 测试硬盘写入速度\n[root@redhat73 ~]# dd if=/dev/zero of=/home/linshi.a bs=1024000000 count=2 2+0 records in 2+0 records out 2048000000 bytes (2.0 GB) copied, 4.67577 s, 438 MB/s 写入一个2GB的文件，用时4.67577秒，平均438 MB/s [root@localhost home]# free -h total used free shared buff/cache available Mem: 974M 56M 818M 580K 99M 787M Swap: 2.0G 37M 2.0G # 参数解释 -m 以MB为单位输出 -g 以GB为单位输出 -h 以人类可读的单位输出，自动转换KB、MB或者GB为单位 -s N 每N秒打印一次 -c N 打印N次后退出 # 输出说明（Mem代表物理内存、Swap代表虚拟内存） total 表示系统的总内存 used 表示应用程序已经使用的内存 free 表示当前还没有被使用的内存 shared 表示共享链接库使用的内存 buff/cache 表示系统的page cache和buffer使用到的内存 available 表示应用程序还可以申请到的内存 怎么判断是否需要加内存： 1.swap使用有多少 2.available剩余是多少，而不是看free cache是Linux系统为了提高系统运行效率而将一些程序或文件写入到cache，可提高程序运行和加载速度，如果程序需要会马上释放。所以判断系统内存是否足够和是否需要增加的时候不能简单的看free Cache Pages: A cache is the part of the memory which transparently stores data so that future requests for that data can be served faster. This memory is utilized by the kernel to cache disk data and improve i/o performance. 系统当前使用到的内存是：used + buff/cache，used中包含了shared。 所以total = used + buff/cache + free = 56 + 99 + 818 = 973 available（787） \u0026lt;= free + buff/cache（818 + 99 = 917），为什么是小于呢？因为系统的一些page或cache是不能回收的。 # 查看CPU信息 # 总核数 = 物理CPU个数 X 每颗物理CPU的核数 # 总逻辑CPU数 = 物理CPU个数 X 每颗物理CPU的核数 X 超线程数 # 查看物理CPU个数 cat /proc/cpuinfo| grep \u0026#34;physical id\u0026#34;| sort| uniq| wc -l # 查看每个物理CPU中core的个数(即核数) cat /proc/cpuinfo| grep \u0026#34;cpu cores\u0026#34;| uniq # 查看逻辑CPU的个数 cat /proc/cpuinfo| grep \u0026#34;processor\u0026#34;| wc -l # 查看CPU信息（型号） cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c 性能测评工具\n性能调优工具\n参考 用十条命令在一分钟内检查Linux服务器性能\n你值得拥有 —— 25 个 Linux 性能监控工具\nTop命令详解\ngglsof命令入门\nLinux缓存机制\n30个实例详解TOP命令\nlsof命令行神器入门\n超全整理！Linux性能分析工具汇总合集\n","permalink":"https://blog.niuhemoon.win/posts/tech/linux-peformence-test/","summary":"简介 Linux上对系统进行性能检测的工具非常多，本文介绍一些常用工具的使用 性能观测工具 ▪ 首先学习的Basic Tool有如下： uptime、top(htop)、mpstat、isstat、vmstat、free、ping、nicstat、dstat。 ▪ 高级的命令如下： sar、net","title":"Linux性能测试"},{"content":"Core Dump文件 凡事皆有两面性，OS在出Core的同时，虽然会终止掉当前进程，但是也会保留下第一手的现场数据，OS仿佛是一架被按下快门的相机，而照片就是产出的Core文件。里面含有当进程被终止时内存、CPU寄存器等信息，可以供后续开发人员进行调试。\nGdb可以附着在特定进程上调试，但是为了不影响运行中的进程，可以通过生成 core file 的方式来保存进程的当前信息。\n实验环境配置 环境是Ubuntu20.04\n# 新开一个Shell的时候，ulimit选项都恢复了默认选项，需要重新设置该值 # 查看shell进程资源 ulimit -a # 查看core文件大小限制 ulimit -c # 修改core文件大小限制 ulimit -c unlimited # 查看修改是否生效 ulimit -c # 设置core_pattern # core_pattern文件中定义了当产生core dump后对core文件进行什么操作 cat /proc/sys/kernel/core_pattern # 需要修改core_pattern文件使得core文件保存在磁盘上 # 方法1 # 暂停apport服务 sudo service apport stop cat /proc/sys/kernel/core_pattern # 生成core文件后恢复apport服务 sudo service apport start # 方法2 mkdir /var/cores echo \u0026#34;/var/cores/core.%e.%p\u0026#34; \u0026gt; /proc/sys/kernel/core_pattern # 方法3 vim /etc/sysctl.conf # 在最后一行添加kernel.core_uses_pid = 1 sysctl -p # 阅读core文件头\treadelf -h core 安装Python-dbg\nsudo apt install gdb python3-dbg GDB调试Python代码 实验1：直接调试core dump文件 将如下代码保存为explode.py\nimport os def my_exploding_func(): my_local_var = \u0026#39;hi\u0026#39; number = 4 number2 = 5 number4 = number+3 os.abort() my_exploding_func() 执行代码，产生core dump文件\npython explode.py 同样的Python版本执行gdb调试\n# 读取core文件 gdb `which python` core 可以使用一些常见的命令调试\n#0 __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:50 50\t../sysdeps/unix/sysv/linux/raise.c: 没有那个文件或目录. (gdb) py-list 3 def my_exploding_func(): 4 my_local_var = \u0026#39;hi\u0026#39; 5 number = 4 6 number2 = 5 7 number4 = number+3 \u0026gt;8 os.abort() 9 10 my_exploding_func() (gdb) py-bt Traceback (most recent call first): \u0026lt;built-in method abort of module object at remote 0x7f08dfc7d360\u0026gt; File \u0026#34;explode.py\u0026#34;, line 8, in my_exploding_func os.abort() File \u0026#34;explode.py\u0026#34;, line 10, in \u0026lt;module\u0026gt; my_exploding_func() 实验2:主动生成core dump文件 如下代码保存为test.py\nimport time def do(x): time.sleep(10) def main(): for x in range(10000): do(x) if __name__ == \u0026#39;__main__\u0026#39;: main() 运行该代码后找到PID\nps -ef | grep \u0026#34;python test.py\u0026#34; 主动生成core dump文件,不影响进程继续运行\ngdb python PID generate-core-file 所有gdb命令都支持使用，同时还有安装python-dbg支持的命令\n在gdb调试命令行中输入\n# py + TAB键位弹出常用命令 py-bt py-list py-up py-bt-full py-locals python py-down py-print python-interactive # help + 命令显示帮助信息 help py-bt 参考 gdb Debugging Full Example Linux上Core Dump文件的形成和分析\nLinux上coredump实验\nGDB调试Python命令\nDebuggin with gdb\n","permalink":"https://blog.niuhemoon.win/posts/tech/gdb-debug-python/","summary":"Core Dump文件 凡事皆有两面性，OS在出Core的同时，虽然会终止掉当前进程，但是也会保留下第一手的现场数据，OS仿佛是一架被按下快门的相机，而照片就是产出的Core文件。里面含有当进程被终止时内存、CPU寄存器等信息，可以供后续开发人员进行调试。 Gdb可以附着在特定进程上调试，但","title":"GDB调试Python代码"},{"content":" 有两种常用的方法可以使得代码并行执行，多线程和多进程 。因为Cython解释器的实现不是线程安全的，具有GIL锁，同一时刻，只有一个线程可以获得解释器的锁。因此，Python利用多核心的CPU只能通过多进程，而多线程只适用于IO密集型的程序。\n多进程基础 创建并开启进程 可以使用multiprocessing.Process()来创建进程，它接受两个参数：\ntarget，一个可调用的函数，当进程开始时会执行 args，一个元组，提供目标函数的参数 使用process.start()来开始执行一个进程\n调用process.join()来告诉程序等待进程结束再执行后续代码，主进程将会被阻塞\nfrom multiprocessing import Process import os def square_numbers(): for i in range(1000): result = i * i if __name__ == \u0026#34;__main__\u0026#34;: processes = [] num_processes = os.cpu_count() # number of CPUs on the machine. Usually a good choise for the number of processes # create processes and asign a function for each process for i in range(num_processes): process = Process(target=square_numbers) processes.append(process) # start all processes for process in processes: process.start() # wait for all processes to finish # block the main programm until these processes are finished for process in processes: process.join() 进程间分享数据 因为进程的内存空间不同，需要特殊的共享内存对象来分享数据。\n数据可以保存在共享内存变量中，使用Value或者Array\nValue(type, value)创建一个ctype对象 Array(type, value)创建一个ctype类型的列表 如下程序演示年race condition资源竟态，每次执行结果都不一样，例如当两个进程读取同一个值，并对其执行+1操作，然后写会原有地址，其结果并不是预想的加2。\nfrom multiprocessing import Process, Value, Array import time def add_100(number): for _ in range(100): time.sleep(0.001) number.value += 1 def add_100_array(numbers): for _ in range(100): time.sleep(0.01) for i in range(len(numbers)): numbers[i] += 1 if __name__ == \u0026#34;__main__\u0026#34;: shared_number = Value(\u0026#39;i\u0026#39;, 0) print(\u0026#39;Value at beginning:\u0026#39;, shared_number.value) shared_array = Array(\u0026#39;d\u0026#39;, [0.0, 100.0, 200.0]) print(\u0026#39;Array at beginning:\u0026#39;, shared_array[:]) process1 = Process(target=add_100, args=(shared_number,)) process2 = Process(target=add_100, args=(shared_number,)) process3 = Process(target=add_100_array, args=(shared_array,)) process4 = Process(target=add_100_array, args=(shared_array,)) process1.start() process2.start() process3.start() process4.start() process1.join() process2.join() process3.join() process4.join() print(\u0026#39;Value at end:\u0026#39;, shared_number.value) print(\u0026#39;Array at end:\u0026#39;, shared_array[:]) print(\u0026#39;end main\u0026#39;) \u0026#34;\u0026#34;\u0026#34; Value at beginning: 0 Array at beginning: [0.0, 100.0, 200.0] Value at end: 144 Array at end: [134.0, 237.0, 339.0] end main \u0026#34;\u0026#34;\u0026#34; 可以使用锁避免资源竟态 锁（也称为互斥锁）是一种同步机制，用于在存在许多执行进程/线程的环境中强制限制对资源的访问。锁具有两种状态：锁定和解锁。 如果状态为锁定，则在再次解除锁定状态之前，不允许其他并发进程/线程进入此代码段。\n# import Lock from multiprocessing import Lock from multiprocessing import Process, Value, Array import time def add_100(number, lock): for _ in range(100): time.sleep(0.001) # lock the state lock.acquire() number.value += 1 # unlock the state lock.release() def add_100_array(numbers, lock): for _ in range(100): time.sleep(0.01) for i in range(len(numbers)): lock.acquire() numbers[i] += 1 lock.release() if __name__ == \u0026#34;__main__\u0026#34;: # create a lock lock1 = Lock() lock2 = Lock() shared_number = Value(\u0026#39;i\u0026#39;, 0) print(\u0026#39;Value at beginning:\u0026#39;, shared_number.value) shared_array = Array(\u0026#39;d\u0026#39;, [0.0, 100.0, 200.0]) print(\u0026#39;Array at beginning:\u0026#39;, shared_array[:]) # pass the lock to the target function process1 = Process(target=add_100, args=(shared_number, lock1)) process2 = Process(target=add_100, args=(shared_number, lock1)) process3 = Process(target=add_100_array, args=(shared_array, lock2)) process4 = Process(target=add_100_array, args=(shared_array, lock2)) process1.start() process2.start() process3.start() process4.start() process1.join() process2.join() process3.join() process4.join() print(\u0026#39;Value at end:\u0026#39;, shared_number.value) print(\u0026#39;Array at end:\u0026#39;, shared_array[:]) print(\u0026#39;end main\u0026#39;) \u0026#34;\u0026#34;\u0026#34; Value at beginning: 0 Array at beginning: [0.0, 100.0, 200.0] Value at end: 200 Array at end: [200.0, 300.0, 400.0] end main \u0026#34;\u0026#34;\u0026#34; 在上下文管理器中使用锁 使用上下文管理器管理锁的获取和释放更加安全\ndef add_100(number, lock): for _ in range(100): time.sleep(0.01) with lock: number.value += 1 多进程使用队列通信 使用队列的操作是进程安全的。多进程队列实现了队列的所有方法。done()和join()除外。\nq.get():移除队首第一个元素，默认情况，会阻塞直到有元素可用 q.put(item)将元素压到队尾，默认情况，阻塞直到队列有空的槽 q.empty()如果队列为空，返回True q.close()表明当前进程不会有新的数据放到队列中了 # communicate between processes with the multiprocessing Queue # Queues are thread and process safe from multiprocessing import Process, Queue import time def square(numbers, queue): for i in numbers: time.sleep(0.01) queue.put(i*i) def make_negative(numbers, queue): for i in numbers: time.sleep(0.01) queue.put(i*-1) if __name__ == \u0026#34;__main__\u0026#34;: numbers = range(1, 6) q = Queue() p1 = Process(target=square, args=(numbers,q)) p2 = Process(target=make_negative, args=(numbers,q)) p1.start() p2.start() p1.join() p2.join() # order might not be sequential while not q.empty(): print(q.get()) print(\u0026#39;end main\u0026#39;) \u0026#34;\u0026#34;\u0026#34; 1 -1 4 -2 9 -3 16 -4 25 -5 end main \u0026#34;\u0026#34;\u0026#34; 进程池 进程池对象控制一些工作进程worker，可以支持超时和回调以实现异步处理，也有一些并行的map实现。它可以自动管理多个处理器，并将数据分成小块，在多个处理器上并行处理。\n重要的函数包括：\nmap(func, iterable[, chunksize])将可迭代对象切分成小块，作为独立任务提交到进程池，并行处理。函数将会阻塞，直到返回结果。 close()阻止更多任务添加到进程池，一旦任务完成，worker进程将退出 join()等待工作进程退出，在调用join()之前需要调用close()或者terminate() apply(func, args)调用func函数，参数是args。阻塞直到返回结果，func函数只在进程池中一个worker中执行 有map_async()和apply_async()这种非阻塞的异步函数 from multiprocessing import Pool import random import time def cube(number): print(\u0026#34;Hi\u0026#34;) time.sleep(random.randint(1,2)) return number * number * number if __name__ == \u0026#34;__main__\u0026#34;: numbers = range(10) p = Pool() # by default this allocates the maximum number of available # processors for this task --\u0026gt; os.cpu_count() result = p.map(cube, numbers) # or # result = [p.apply(cube, args=(i,)) for i in numbers] p.close() p.join() print(result) 多线程基础 Python多线程相对比较鸡肋，其使用和多进程类似\n创建并开始线程 使用threading库实现\nfrom threading import Thread def square_numbers(): for i in range(1000): result = i * i if __name__ == \u0026#34;__main__\u0026#34;: threads = [] num_threads = 10 # create threads and asign a function for each thread for i in range(num_threads): thread = Thread(target=square_numbers) threads.append(thread) # start all threads for thread in threads: thread.start() # wait for all threads to finish # block the main thread until these threads are finished for thread in threads: thread.join() 线程间共享数据 线程间可以通过全局变量来共享数据，因为线程间是共享内存空间的\nfrom threading import Thread import time # all threads can access this global variable database_value = 0 def increase(): global database_value # needed to modify the global value # get a local copy (simulate data retrieving) local_copy = database_value # simulate some modifying operation local_copy += 1 time.sleep(0.1) # write the calculated new value into the global variable database_value = local_copy if __name__ == \u0026#34;__main__\u0026#34;: print(\u0026#39;Start value: \u0026#39;, database_value) t1 = Thread(target=increase) t2 = Thread(target=increase) t1.start() t2.start() t1.join() t2.join() print(\u0026#39;End value:\u0026#39;, database_value) print(\u0026#39;end main\u0026#39;) \u0026#34;\u0026#34;\u0026#34; Start value: 0 End value: 1 end main \u0026#34;\u0026#34;\u0026#34; 使用锁处理资源竟态 # import Lock from threading import Thread, Lock import time database_value = 0 def increase(lock): global database_value # lock the state lock.acquire() local_copy = database_value local_copy += 1 time.sleep(0.1) database_value = local_copy # unlock the state lock.release() if __name__ == \u0026#34;__main__\u0026#34;: # create a lock lock = Lock() print(\u0026#39;Start value: \u0026#39;, database_value) # pass the lock to the target function t1 = Thread(target=increase, args=(lock,)) # notice the comma after lock since args must be a tuple t2 = Thread(target=increase, args=(lock,)) t1.start() t2.start() t1.join() t2.join() print(\u0026#39;End value:\u0026#39;, database_value) print(\u0026#39;end main\u0026#39;) 使用上下文管理器\ndef increase(lock): global database_value with lock: local_copy = database_value local_copy += 1 time.sleep(0.1) database_value = local_copy 多线程消息队列通信 对队列的操作是线程安全的\nfrom threading import Thread, Lock, current_thread from queue import Queue def worker(q, lock): while True: value = q.get() # blocks until the item is available # do stuff... with lock: # prevent printing at the same time with this lock print(f\u0026#34;in {current_thread().name} got {value}\u0026#34;) # ... # For each get(), a subsequent call to task_done() tells the queue # that the processing on this item is complete. # If all tasks are done, q.join() can unblock q.task_done() if __name__ == \u0026#39;__main__\u0026#39;: q = Queue() num_threads = 10 lock = Lock() for i in range(num_threads): t = Thread(name=f\u0026#34;Thread{i+1}\u0026#34;, target=worker, args=(q, lock)) t.daemon = True # dies when the main thread dies t.start() # fill the queue with items for x in range(20): q.put(x) q.join() # Blocks until all items in the queue have been gotten and processed. print(\u0026#39;main done\u0026#39;) \u0026#34;\u0026#34;\u0026#34; in Thread1 got 0 in Thread2 got 1 in Thread2 got 11 in Thread2 got 12 in Thread2 got 13 in Thread2 got 14 in Thread2 got 15 in Thread2 got 16 in Thread2 got 17 in Thread2 got 18 in Thread2 got 19 in Thread8 got 5 in Thread4 got 9 in Thread1 got 10 in Thread5 got 2 in Thread6 got 3 in Thread9 got 6 in Thread7 got 4 in Thread10 got 7 in Thread3 got 8 main done \u0026#34;\u0026#34;\u0026#34; Python标准库自带的一些小工具 # 查看包安装路径 python -m site # 开启简单的http server python -m http.server # base64编码和解码 echo \u0026#34;hello\u0026#34; | python -m base64 # top level await console python -m asyncio # 查看tokenize和ast结果 python -m tokenize cgi.py pythono -m ast cgi.py # json美化输出 echo \u0026#39;{\u0026#34;foo\u0026#34;: \u0026#34;bar\u0026#34;}\u0026#39; | python -m json.tool # 显示日历 python -m calendar 参考 Python多线程和多进程 CLI tools hidden in the Python standard library | Simon Willison’s TILs\n","permalink":"https://blog.niuhemoon.win/posts/tech/python-advanced-3/","summary":"有两种常用的方法可以使得代码并行执行，多线程和多进程 。因为Cython解释器的实现不是线程安全的，具有GIL锁，同一时刻，只有一个线程可以获得解释器的锁。因此，Python利用多核心的CPU只能通过多进程，而多线程只适用于IO密集型的程序。 多进程基础 创建并开启进程 可以使用mult","title":"Python进阶下"},{"content":"生成器Generator def countdown(num): print(\u0026#39;Starting\u0026#39;) while num \u0026gt; 0: yield num num -= 1 # this will not print \u0026#39;Starting\u0026#39; cd = countdown(3) # this will print \u0026#39;Starting\u0026#39; and the first value print(next(cd)) # will print the next values print(next(cd)) print(next(cd)) # this will raise a StopIteration print(next(cd)) 迭代器使用方式\n# you can iterate over a generator object with a for in loop cd = countdown(3) for x in cd: print(x) # you can use it for functions that take iterables as input cd = countdown(3) sum_cd = sum(cd) print(sum_cd) cd = countdown(3) sorted_cd = sorted(cd) print(sorted_cd) 生成器表达式\n# generator expression mygenerator = (i for i in range(1000) if i % 2 == 0) print(sys.getsizeof(mygenerator), \u0026#34;bytes\u0026#34;) # list comprehension mylist = [i for i in range(1000) if i % 2 == 0] print(sys.getsizeof(mylist), \u0026#34;bytes\u0026#34;) # 120bytes # 4272 bytes 生成器概念 类可以实现生成器作为一个可迭代对象，它需要实现__iter__方法和__next__方法，使得类对象可迭代。此外，还需要注意记录迭代次数，以及最后raise一个StopIteration异常。\nclass firstn: def __init__(self, n): self.n = n self.num = 0 def __iter__(self): return self def __next__(self): if self.num \u0026lt; self.n: cur = self.num self.num += 1 return cur else: raise StopIteration() firstn_object = firstn(1000000) print(sum(firstn_object)) 装饰器Decorators 装饰器的典型使用场景有：\n计算函数执行时间 用于调试，打印出函数的参数和调用信息 作为函数的参数校验 以插件的形式注册函数 降低代码执行速度来测试网络，如使用sleep函数 缓存代码执行结果Memoization 附加信息或者更新状态 装饰器就是一个语法糖，如下装饰器就类似target = somedecorator(target)\n装饰器一个特性就是将被装饰函数替换为其他函数\n@somedecorator def target(): print(\u0026#34;running target\u0026#34;) 简单装饰器模板\nimport functools def my_decorator(func): @functools.wraps(func)\t# 保持被装饰函数的属性 def wrapper(*args, **kwargs): # Do something before result = func(*args, **kwargs) # Do something after return result return wrapper 带参数的装饰器函数，可以认为是两层函数，在简单装饰器外部套一个函数来扩展装饰器的行为。\n即两层闭包，一个持有外部环境变量的函数就是闭包。\ndef repeat(num_times): def decorator_repeat(func): @functools.wraps(func) def wrapper(*args, **kwargs): for _ in range(num_times): result = func(*args, **kwargs) return result return wrapper return decorator_repeat @repeat(num_times=3) def greet(name): print(f\u0026#34;Hello {name}\u0026#34;) greet(\u0026#39;Alex\u0026#39;) # 输出 \u0026#34;\u0026#34;\u0026#34; Hello Alex Hello Alex Hello Alex \u0026#34;\u0026#34;\u0026#34; 层叠装饰器 装饰器的执行顺序是decorator(func)，从外到内执行，如果有多个装饰器堆叠在一起，按照decorator2(docorator1(func))的执行顺序。\n其更清晰的执行顺序是：\nfunc = decorator1(func) func = decorator2(func) func() 多个装饰器装饰函数时，有个规律是从下到上包裹（装饰）函数，在装饰的过程中执行装饰器函数和内部闭包wrapper函数之间代码，闭包函数知识作为一个对象被返回，在装饰过程中并不执行。\n而在执行被装饰函数的过程中，从上到下执行wrapper函数内部的代码。\n如下代码多个装饰器，其装饰的顺序和wrapper执行的顺序相反。\n# 装饰过程 say_hello = start_end_decorator_2(start_end_decorator_1(say_hello)) say_hello() 多装饰器实验\nimport functools # a decorator function that prints debug information about the wrapped function def start_end_decorator_2(func): print(\u0026#39;Start decorator2\u0026#39;) @functools.wraps(func) def wrapper2(*args, **kwargs): print(\u0026#39;Exec wrapper 2\u0026#39;) result = func(*args, **kwargs) print(\u0026#39;End wrapper 2\u0026#39;) return result return wrapper2 def start_end_decorator_1(func): print(\u0026#39;Start decorator1\u0026#39;) @functools.wraps(func) def wrapper1(*args, **kwargs): print(\u0026#39;Exec wrapper 1\u0026#39;) result = func(*args, **kwargs) print(\u0026#39;End wrapper 1\u0026#39;) return result return wrapper1 @start_end_decorator_2 @start_end_decorator_1 def say_hello(name): greeting = f\u0026#39;Hello {name}\u0026#39; print(greeting) return greeting \u0026#34;\u0026#34;\u0026#34; 相当于 func = start_end_decorator_1(func) 此时func是下面这个wrapper1函数 def wrapper1(*args, **kwargs): print(\u0026#39;Exec wrapper 1\u0026#39;) result = func(*args, **kwargs) print(\u0026#39;End wrapper 1\u0026#39;) return result 再经过下一个装饰器start_end_decorator_2 func再次被替换 func = start_end_decorator_2(start_end_decorator_1(func)) \u0026#34;\u0026#34;\u0026#34; say_hello(name=\u0026#39;Alex\u0026#39;) # exec result \u0026#34;\u0026#34;\u0026#34; Start decorator1 Start decorator2 Exec wrapper 2 Exec wrapper 1 Hello Alex End wrapper 1 End wrapper 2 \u0026#34;\u0026#34;\u0026#34; 装饰器执行时间 装饰器需要区分导入时和运行时，\n装饰器一个特性就是装饰的过程在import时执行，当import代码时，装饰器立刻执行，将被装饰函数变为另一个函数。\nregistry = [] def register(func): print(\u0026#34;running register(%s)\u0026#34; % func) registry.append(func) return func @register def f1(): print(\u0026#34;running f1\u0026#34;) @register def f2(): print(\u0026#34;running f2\u0026#34;) def f3(): print(\u0026#34;running f3\u0026#34;) def main(): print(\u0026#34;running main\u0026#34;) print(\u0026#34;registry -\u0026gt;\u0026#34;, registry) f1() f2() f3() if __name__ == \u0026#34;__main__\u0026#34;: main() # import该模块的输出如下 \u0026#34;\u0026#34;\u0026#34; running register(\u0026lt;function f1 at 0x7f4105056af0\u0026gt;) running register(\u0026lt;function f2 at 0x7f4105056c10\u0026gt;) \u0026#34;\u0026#34;\u0026#34; # 执行main函数的输出如下 # 在运行时，被装饰函数才开始执行 \u0026#34;\u0026#34;\u0026#34; running register(\u0026lt;function f1 at 0x7f65b62d70d0\u0026gt;) running register(\u0026lt;function f2 at 0x7f65b62d7160\u0026gt;) running main registry -\u0026gt; [\u0026lt;function f1 at 0x7f65b62d70d0\u0026gt;, \u0026lt;function f2 at 0x7f65b62d7160\u0026gt;] running f1 running f2 running f3 \u0026#34;\u0026#34;\u0026#34; 类装饰器 可以使用类作为装饰器，因此，需要首先实现魔法方法__call__，使得对象是callable可调用的，类装饰器典型用处是保存状态，如函数被调用次数。我们使用functools.update_wrapper()而不是functools.wraps来持久化被装饰器函数信息。\nimport functools class CountCalls: # the init needs to have the func as argument and stores it def __init__(self, func): functools.update_wrapper(self, func) self.func = func self.num_calls = 0 # extend functionality, execute function, and return the result def __call__(self, *args, **kwargs): self.num_calls += 1 print(f\u0026#34;Call {self.num_calls} of {self.func.__name__!r}\u0026#34;) return self.func(*args, **kwargs) @CountCalls def say_hello(num): print(\u0026#34;Hello!\u0026#34;) say_hello(5) say_hello(5) # result \u0026#34;\u0026#34;\u0026#34; Call 1 of \u0026#39;say_hello\u0026#39; Hello! Call 2 of \u0026#39;say_hello\u0026#39; Hello! \u0026#34;\u0026#34;\u0026#34; Context Managers 上下文管理器用于资源管理，允许你方便的分配和释放资源\nPython内置的关键字with用于处理上下文管理器，上下文管理器典型的用途有：\n打开和关闭文件 打开和关闭数据库连接 获得和释放锁 from threading import Lock lock = Lock() # error-prone: lock.acquire() # do stuff # lock should always be released! lock.release() # Better: with lock: # do stuff 实现一个上下文管理器类 为了支持with关键字，需要在类中实现__enter__和__exit__方法，当Python执行到with语句，会执行__enter__方法，此时应该获取资源并返回，而当离开上下文环境时，将执行__exit__方法，此时应该释放资源。\nclass ManagedFile: def __init__(self, filename): print(\u0026#39;init\u0026#39;, filename) self.filename = filename def __enter__(self): print(\u0026#39;enter\u0026#39;) self.file = open(self.filename, \u0026#39;w\u0026#39;) return self.file def __exit__(self, exc_type, exc_value, exc_traceback): if self.file: self.file.close() print(\u0026#39;exit\u0026#39;) with ManagedFile(\u0026#39;notes.txt\u0026#39;) as f: print(\u0026#39;doing stuff...\u0026#39;) f.write(\u0026#39;some todo...\u0026#39;) 处理异常 当异常产生时，Python将异常类型、值和traceback信息传递给__exit__方法，它可以处理该异常。如果__exit__方法返回了除True之外的任何值，则由with语句引发异常。\nclass ManagedFile: def __init__(self, filename): print(\u0026#39;init\u0026#39;, filename) self.filename = filename def __enter__(self): print(\u0026#39;enter\u0026#39;) self.file = open(self.filename, \u0026#39;w\u0026#39;) return self.file def __exit__(self, exc_type, exc_value, exc_traceback): if self.file: self.file.close() print(\u0026#39;exc:\u0026#39;, exc_type, exc_value) print(\u0026#39;exit\u0026#39;) # No exception with ManagedFile(\u0026#39;notes.txt\u0026#39;) as f: print(\u0026#39;doing stuff...\u0026#39;) f.write(\u0026#39;some todo...\u0026#39;) print(\u0026#39;continuing...\u0026#39;) print() # Exception is raised, but the file can still be closed with ManagedFile(\u0026#39;notes2.txt\u0026#39;) as f: print(\u0026#39;doing stuff...\u0026#39;) f.write(\u0026#39;some todo...\u0026#39;) f.do_something() print(\u0026#39;continuing...\u0026#39;) 也可以在__exit__方法中处理异常，并返回True\nclass ManagedFile: def __init__(self, filename): print(\u0026#39;init\u0026#39;, filename) self.filename = filename def __enter__(self): print(\u0026#39;enter\u0026#39;) self.file = open(self.filename, \u0026#39;w\u0026#39;) return self.file def __exit__(self, exc_type, exc_value, exc_traceback): if self.file: self.file.close() if exc_type is not None: print(\u0026#39;Exception has been handled\u0026#39;) print(\u0026#39;exit\u0026#39;) return True with ManagedFile(\u0026#39;notes2.txt\u0026#39;) as f: print(\u0026#39;doing stuff...\u0026#39;) f.write(\u0026#39;some todo...\u0026#39;) f.do_something() print(\u0026#39;continuing...\u0026#39;) 用生成器实现一个上下文管理器 与其写一个类，也可以写一个生成器函数，并用contextlib.contextmanager来装饰它。\n为了实现这个目的，函数必须在try语句段中yield资源，而在finally语句中实现类似__exit__的功能，即释放资源。\nfrom contextlib import contextmanager @contextmanager def open_managed_file(filename): f = open(filename, \u0026#39;w\u0026#39;) try: yield f finally: f.close() with open_managed_file(\u0026#39;notes.txt\u0026#39;) as f: f.write(\u0026#39;some todo...\u0026#39;) 生成器首先获取资源，然后暂时挂起执行流程，并yeild返回资源，资源可以被调用者使用，当调用着离开with上下文，生成器接着执行后续的finally语句，释放资源。\nPython中的解引用 Python中的*号具有多种作用：\nUse *args for variable-length arguments Use **kwargs for variable-length keyword arguments Use *, followed by more function parameters to enforce keyword-only arguments def my_function(*args, **kwargs): for arg in args: print(arg) for key in kwargs: print(key, kwargs[key]) my_function(\u0026#34;Hey\u0026#34;, 3, [0, 1, 2], name=\u0026#34;Alex\u0026#34;, age=8) # Parameters after \u0026#39;*\u0026#39; or \u0026#39;*identifier\u0026#39; are keyword-only parameters and may only be passed using keyword arguments. def my_function2(name, *, age): print(name) print(age) # my_function2(\u0026#34;Michael\u0026#34;, 5) --\u0026gt; this would raise a TypeError my_function2(\u0026#34;Michael\u0026#34;, age=5) Python函数传参以及深拷贝浅拷贝 在Python中，赋值语句obj_b = obj_a不产生真正的对象拷贝，只创建一个新的变量和obj_a具有相同的引用，因此当你想要产生可变对象的真正的拷贝，并在不影响原来对象的情况下修改拷贝对象时，需要格外小心。\n可以使用copy模块产生真正的拷贝，然而，对于混合/嵌套对象，浅拷贝和深拷贝有重要的区别，\n浅拷贝\n只有一层深，对于比一层深的嵌套对象是源对象的引用，因此修改会导致源对象的更改\n深拷贝\n一份完全独立的拷贝，递归产生源对象中所有嵌套对象的拷贝\n赋值操作 会产生源对象的一个引用，修改会导致源对象的变更\nlist_a = [1, 2, 3, 4, 5] list_b = list_a list_a[0] = -10 print(list_a) print(list_b) \u0026#34;\u0026#34;\u0026#34; [-10, 2, 3, 4, 5] [-10, 2, 3, 4, 5] \u0026#34;\u0026#34;\u0026#34; 浅拷贝 浅拷贝只有一层深度，修改第一层不会影响源对象，使用copy.copy()方法或者对象特定的拷贝方法或者拷贝构造函数\nimport copy list_a = [1, 2, 3, 4, 5] list_b = copy.copy(list_a) # not affects the other list list_b[0] = -10 print(list_a) print(list_b) 但是在嵌套对象中，修改第二层或者更深层次的数据时，会影响到源对象，因为在第二层时拷贝的是引用，而不是值。\nimport copy list_a = [[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]] list_b = copy.copy(list_a) # affects the other! list_a[0][0]= -10 print(list_a) print(list_b) \u0026#34;\u0026#34;\u0026#34; [[-10, 2, 3, 4, 5], [6, 7, 8, 9, 10]] [[-10, 2, 3, 4, 5], [6, 7, 8, 9, 10]] \u0026#34;\u0026#34;\u0026#34; 对于列表，类似的浅拷贝方法还有\n# shallow copies list_b = list(list_a) list_b = list_a[:] list_b = list_a.copy() 深拷贝 深拷贝是一份完全独立的克隆，使用copy.deepcopy方法实现\nimport copy list_a = [[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]] list_b = copy.deepcopy(list_a) # not affects the other list_a[0][0]= -10 print(list_a) print(list_b) \u0026#34;\u0026#34;\u0026#34; [[-10, 2, 3, 4, 5], [6, 7, 8, 9, 10]] [[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]] \u0026#34;\u0026#34;\u0026#34; 对象的深拷贝和浅拷贝 可以使用copy模块实现对特定对象的深拷贝或者浅拷贝\n赋值只拷贝对象引用 class Person: def __init__(self, name, age): self.name = name self.age = age # Only copies the reference p1 = Person(\u0026#39;Alex\u0026#39;, 27) p2 = p1 p2.age = 28 print(p1.age) print(p2.age) \u0026#34;\u0026#34;\u0026#34; 28 28 \u0026#34;\u0026#34;\u0026#34; 浅拷贝拷贝一层 # shallow copy import copy p1 = Person(\u0026#39;Alex\u0026#39;, 27) p2 = copy.copy(p1) p2.age = 28 print(p1.age) print(p2.age) \u0026#34;\u0026#34;\u0026#34; 27 28 \u0026#34;\u0026#34;\u0026#34; 深拷贝可以完整拷贝 class Company: def __init__(self, boss, employee): self. boss = boss self.employee = employee # shallow copy will affect nested objects boss = Person(\u0026#39;Jane\u0026#39;, 55) employee = Person(\u0026#39;Joe\u0026#39;, 28) company = Company(boss, employee) company_clone = copy.copy(company) company_clone.boss.age = 56 print(company.boss.age) print(company_clone.boss.age) \u0026#34;\u0026#34;\u0026#34; 56 56 \u0026#34;\u0026#34;\u0026#34; # deep copy will not affect nested objects boss = Person(\u0026#39;Jane\u0026#39;, 55) employee = Person(\u0026#39;Joe\u0026#39;, 28) company = Company(boss, employee) company_clone = copy.deepcopy(company) company_clone.boss.age = 56 print(company.boss.age) print(company_clone.boss.age) \u0026#34;\u0026#34;\u0026#34; 55 56 \u0026#34;\u0026#34;\u0026#34; 函数传参 在C语言中传参有显式的值传递和地址传递，在Python中也有类似的机制。\nPython中数据类型存在可变和不可变的区别，即mutable和immutable。\n对于可变类型，例如列表，由于传递的是列表的引用，列表可以在一个方法中被修改\n# immutable objects -\u0026gt; no change def foo(x): x = 5 # x += 5 also no effect since x is immutable and a new variable must be created var = 10 print(\u0026#39;var before foo():\u0026#39;, var) foo(var) print(\u0026#39;var after foo():\u0026#39;, var) \u0026#34;\u0026#34;\u0026#34; var before foo(): 10 var after foo(): 10 \u0026#34;\u0026#34;\u0026#34; 可变对象 # mutable objects -\u0026gt; change def foo(a_list): a_list.append(4) my_list = [1, 2, 3] print(\u0026#39;my_list before foo():\u0026#39;, my_list) foo(my_list) print(\u0026#39;my_list after foo():\u0026#39;, my_list) \u0026#34;\u0026#34;\u0026#34; my_list before foo(): [1, 2, 3] my_list after foo(): [1, 2, 3, 4] \u0026#34;\u0026#34;\u0026#34; 重新绑定一个可变对象的引用 # Rebind a mutable reference -\u0026gt; no change def foo(a_list): # 赋值操作产生一个新的局部变量 a_list = [50, 60, 70] # a_list is now a new local variable within the function a_list.append(50) my_list = [1, 2, 3] print(\u0026#39;my_list before foo():\u0026#39;, my_list) foo(my_list) print(\u0026#39;my_list after foo():\u0026#39;, my_list) 区分+=和= # another example with rebinding references: def foo(a_list): a_list += [4, 5] # this chanches the outer variable def bar(a_list): a_list = a_list + [4, 5] # this rebinds the reference to a new local variable my_list = [1, 2, 3] print(\u0026#39;my_list before foo():\u0026#39;, my_list) foo(my_list) print(\u0026#39;my_list after foo():\u0026#39;, my_list) my_list = [1, 2, 3] print(\u0026#39;my_list before bar():\u0026#39;, my_list) bar(my_list) print(\u0026#39;my_list after bar():\u0026#39;, my_list) \u0026#34;\u0026#34;\u0026#34; my_list before foo(): [1, 2, 3] my_list after foo(): [1, 2, 3, 4, 5] my_list before bar(): [1, 2, 3] my_list after bar(): [1, 2, 3] \u0026#34;\u0026#34;\u0026#34; 参考 Python-Notebook\nPython中的*号\n闭包概念\nPython装饰器\n","permalink":"https://blog.niuhemoon.win/posts/tech/python-advanced-2/","summary":"生成器Generator def countdown(num): print(\u0026#39;Starting\u0026#39;) while num \u0026gt; 0: yield num num -= 1 # this will not print \u0026#39;Starting\u0026#39; cd = countdown(3) # this will print \u0026#39;Starting\u0026#39; and the first value print(next(cd)) # will print the next values print(next(cd)) print(next(cd)) # this will raise a StopIteration print(next(cd)) 迭代器使用方式 # you can iterate over a generator object with a for in loop cd = countdown(3) for x in cd: print(x) # you can use it for functions that take iterables as input cd = countdown(3) sum_cd = sum(cd) print(sum_cd) cd = countdown(3) sorted_cd = sorted(cd) print(sorted_cd) 生成器表达式 # generator expression mygenerator = (i for i in range(1000) if i % 2 == 0) print(sys.getsizeof(mygenerator), \u0026#34;bytes\u0026#34;) # list comprehension mylist","title":"Python进阶中"},{"content":" Ubuntn 上一直以来对中文输入法的支持都不是很完善，在升级到 20.04 版本之后，系统默认自带的是 ibus 输入法，刚刚上手使用之后不是很好用。在尝试安装 fcitx 和搜狗输入法之后，因为搜狗输入法和 Pycharm 等 IDE 冲突，会导致软件卡死，而且资源占用较高。还是切换回了 ibus 输入法。\n在简单调教之后，发现 ibus 还是很好用的。\n打开 Ibus 首选项\n勾选将每个输入记录为新词汇，然后会记录用户的输入历史，方便导出并转移\n启用模糊拼音和词典同样可以提高词汇匹配的准确程度 对于用户数据可以定期备份或者清除 当修改配置后，需要重启 ibus 服务使其生效。使用下面命令行重启 ibus\nibus-daemon -r -d -x 随着日常使用之后，ibus 会根据你的输入记录，优化其输入法。\n","permalink":"https://blog.niuhemoon.win/posts/tech/ibus-input/","summary":"Ubuntn 上一直以来对中文输入法的支持都不是很完善，在升级到 20.04 版本之后，系统默认自带的是 ibus 输入法，刚刚上手使用之后不是很好用。在尝试安装 fcitx 和搜狗输入法之后，因为搜狗输入法和 Pycharm 等 IDE 冲突，会导致软件卡死，而且资源占用较高。还是切换回了 ibus 输入法。 在简单调教之后，发现 ibus 还是很好用的。 打开 Ibus 首选项","title":"Ubuntu20_04上使用ibus中文输入法"},{"content":"基础 1.1 字符匹配 字符 说明 \\ 转义符 \\d [0-9]。表示是一位数字。 \\D [^0-9]。表示除数字外的任意字符。 \\w [0-9a-zA-Z_]。表示数字、大小写字母和下划线。 \\W [^0-9a-za-z_]。非单词字符。 \\s [\\t\\v\\n\\r\\f]。表示空白符，包括空格、水平制表符、 垂直制表符、换行符、回车符、换页符。 \\S [^\\t\\v\\n\\r\\f]。非空白符。 . [^\\n\\r\\u2028\\u2029]。通配符，表示几乎任意字符。 换行符、回车符、行分隔符和段分隔符除外。 \\uxxxx 查找以十六进制数 xxxx 规定的 Unicode 字符。 \\f 匹配一个换页符 (U+000C)。 \\n 匹配一个换行符 (U+000A)。 \\r 匹配一个回车符 (U+000D)。 \\t 匹配一个水平制表符 (U+0009)。 \\v 匹配一个垂直制表符 (U+000B)。 \\0 匹配 NULL（U+0000）字符， 不要在这后面跟其它小数，因为 \\0 是一个 八进制转义序列。 [\\b] 匹配一个退格(U+0008)。（不要和\\b 混淆了。） [abc] any of a, b, or c [^abc] not a, b, or c [a-g] character between a \u0026amp; g 1.2 位置匹配 字符 说明 \\b 是单词边界，具体就是\\w 和\\W 之间的位置，也包括\\w 和 ^ 之间的位置， 也包括\\w 和 之间的位置。 \\B 是\\b 的反面的意思，非单词边界。例如在字符串中所有位置中，扣掉\\b， 剩下的都是\\B 的。 ^abc$ 字符串开始、结束的位置 1.3 组 字符 说明 (abc) capture group，捕获组 \\n backreference to group #n，分组引用，引用第 n 个捕获组匹配的内容, 其中 n 是正整数 (?:abc) non-capturing group，非捕获组 1.4 先行断言 字符 说明 a(?=b) positive lookahead，先行断言，a 只有在 b 前面才匹配 a(?!b) negative lookahead，先行否定断言，a 只有不在 b 前面才匹配 1.5 后行断言 字符 说明 (?\u0026lt;=b)a positive lookbehind，后行断言，a 只有在 b 后面才匹配 (?\u0026lt;!b)a negative lookbehind，后行否定断言，a 只有不在 b 后面才匹配 1.6 量词和分支 字符 说明 a* 0 or more a+ 1 or more a? 0 or 1 a{5} exactly five a{2,} two or more a{1,3} between one \u0026amp; three a+? a{2,}? match as few as possible，惰性匹配，就是尽可能少的匹配 以下都是惰性匹配： {m,n}? {m,}? ?? +? *?\n1.7 分支 字符 说明 ab|cd match ab or cd，匹配\u0026rsquo;ab\u0026rsquo;或者\u0026rsquo;cd\u0026rsquo;字符子串 1.8 修饰符 字符 说明 i 执行对大小写不敏感的匹配。 g 执行全局匹配（查找所有匹配而非在找到第一个匹配后停止）。 m 执行多行匹配。 u 开启\u0026quot;Unicode 模式\u0026quot;，用来正确处理大于\\uFFFF 的 Unicode 字符。也就是说，会正确处理四个字节的 UTF-16 编码。 s 允许 . 匹配换行符。 y y 修饰符的作用与 g 修饰符类似，也是全局匹配，后一次匹配都从上一次匹配成功的下一个位置开始。不同之处在于，g 修饰符只要剩余位置中存在匹配就可，而 y 修饰符确保匹配必须从剩余的第一个位置开始，这也就是\u0026quot;粘连\u0026quot;的涵义 2. 运算符优先级 运算符 描述 \\ 转义符 (), (?:), (?=), [] 圆括号和方括号 *, +, ?, {n}, {n,}, {n,m} 限定符 ^, $, \\任何元字符、任何字符 定位点和序列（即：位置和顺序） | 替换，\u0026ldquo;或\u0026quot;操作 字符具有高于替换运算符的优先级，使得\u0026quot;m|food\u0026quot;匹配\u0026quot;m\u0026quot;或\u0026quot;food\u0026rdquo;。若要匹配\u0026quot;mood\u0026quot;或\u0026quot;food\u0026quot;，请使用括号创建子表达式，从而产生\u0026quot;(m|f)ood\u0026quot;。 使用案例 一、20 个最常用的正则表达式 #1 . 校验密码强度：密码的强度必须是包含大小写字母和数字的组合，不能使用特殊字符，长度在8-10之间 ^(?=.*\\\\d)(?=.*[a-z])(?=.*[A-Z]).{8,10}$ #2. 校验中文：字符串仅能是中文 ^[\\\\u4e00-\\\\u9fa5]{0,}$ # 3. 由数字、26个英文字母或下划线组成的字符串： ^\\\\w+$ #4. 校验E-Mail 地址：同密码一样，下面是E-mail地址合规性的正则检查语句。 [\\\\w!#$%\u0026amp;\u0026#39;*+/=?^_`{|}~-]+(?:\\\\.[\\\\w!#$%\u0026amp;\u0026#39;*+/=?^_`{|}~-]+)*@(?:[\\\\w](?:[\\\\w-]*[\\\\w])?\\\\.)+[\\\\w](?:[\\\\w #5. 校验身份证号码：下面是身份证号码的正则校验。15 或 18位。 #15位 ^[1-9]\\\\d{7}((0\\\\d)|(1[0-2]))(([0|1|2]\\\\d)|3[0-1])\\\\d{3}$ #18位 ^[1-9]\\\\d{5}[1-9]\\\\d{3}((0\\\\d)|(1[0-2]))(([0|1|2]\\\\d)|3[0-1])\\\\d{3}([0-9]|X)$ #6. 校验日期：“yyyy-mm-dd“ 格式的日期校验，已考虑平闰年。 ^(?:(?!0000)[0-9]{4}-(?:(?:0[1-9]|1[0-2])-(?:0[1-9]|1[0-9]|2[0-8])|(?:0[13-9]|1[0-2])-(?:29|30)|(?:0[13578]|1[02])-31)|(?:[0-9]{2}(?:0[48]|[2468][048]|[13579][26])|(?:0[48]|[2468][048]|[13579][26])00)-02-29)$ #7. 校验金额：金额校验，精确到2位小数。 ^[0-9]+(.[0-9]{2})?$ #8. 校验手机号：下面是国内 13、15、18开头的手机号正则表达式。 ^(13[0-9]|14[5|7]|15[0|1|2|3|5|6|7|8|9]|18[0|1|2|3|5|6|7|8|9])\\\\d{8}$ #9. 判断IE的版本：IE目前还没被完全取代，很多页面还是需要做版本兼容，下面是IE版本检查的表达式。 ^.*MSIE [5-8](?:\\\\.[0-9]+)?(?!.*Trident\\\\/[5-9]\\\\.0).*$ #10. 校验IP-v4地址：IP4 正则语句 \\\\b(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\\\b #11. 校验IP-v6地址：IP6 正则语句。 (([0-9a-fA-F]{1,4}:){7,7}[0-9a-fA-F]{1,4}|([0-9a-fA-F]{1,4}:){1,7}:|([0-9a-fA-F]{1,4}:){1,6}:[0-9a-fA-F]{1,4}|([0-9a-fA-F]{1,4}:){1,5}(:[0-9a-fA-F]{1,4}){1,2}|([0-9a-fA-F]{1,4}:){1,4}(:[0-9a-fA-F]{1,4}){1,3}|([0-9a-fA-F]{1,4}:){1,3}(:[0-9a-fA-F]{1,4}){1,4}|([0-9a-fA-F]{1,4}:){1,2}(:[0-9a-fA-F]{1,4}){1,5}|[0-9a-fA-F]{1,4}:((:[0-9a-fA-F]{1,4}){1,6})|:((:[0-9a-fA-F]{1,4}){1,7}|:)|fe80:(:[0-9a-fA-F]{0,4}){0,4}%[0-9a-zA-Z]{1,}|::(ffff(:0{1,4}){0,1}:){0,1}((25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\\\.){3,3}(25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])|([0-9a-fA-F]{1,4}:){1,4}:((25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\\\.){3,3}(25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])) #12. 检查URL的前缀：应用开发中很多时候需要区分请求是HTTPS还是HTTP，通过下面的表达式可以取出一个url的前缀然后再逻辑判断。 if (!s.match(/^[a-zA-Z]+:\\\\/\\\\//)) { s = \u0026#39;http://\u0026#39; + s; } #13. 提取URL链接：下面的这个表达式可以筛选出一段文本中的URL。 ^(f|ht){1}(tp|tps):\\\\/\\\\/([\\\\w-]+\\\\.)+[\\\\w-]+(\\\\/[\\\\w- ./?%\u0026amp;=]*)? #14. 文件路径及扩展名校验：下面的这个表达式可以筛选出一段文本中的URL。 ^(f|ht){1}(tp|tps):\\\\/\\\\/([\\\\w-]+\\\\.)+[\\\\w-]+(\\\\/[\\\\w- ./?%\u0026amp;=]*)? #14. 文件路径及扩展名校验：验证文件路径和扩展名 ^([a-zA-Z]\\\\:|\\\\\\\\)\\\\\\\\([^\\\\\\\\]+\\\\\\\\)*[^\\\\/:*?\u0026#34;\u0026lt;\u0026gt;|]+\\\\.txt(l)?$ #15. 提取Color Hex Codes：有时需要抽取网页中的颜色代码，可以使用下面的表达式。 \\\\#([a-fA-F]|[0-9]){3,6} #16. 提取网页图片：假若你想提取网页中所有图片信息，可以利用下面的表达式。 \\\\\u0026lt; *[img][^\\\\\u0026gt;]*[src] *= *[\\\\\u0026#34;\\\\\u0026#39;]{0,1}([^\\\\\u0026#34;\\\\\u0026#39;\\\\ \u0026gt;]*) #17. 提取页面超链接：提取html中的超链接。 (\u0026lt;;a\\\\s*(?!.*\\\\brel=)[^\u0026gt;;]*)(href=\u0026#34;https?://)((?!(?:(?:www\\\\.)?\u0026#39;.implode(\u0026#39;|(?:www\\\\.)?\u0026#39;, $follow_list) #18. 精炼CSS：通过下面的表达式，可以搜索相同属性值的CSS，从而达到精炼代码的目的。 ^\\\\s*[a-zA-Z\\\\-]+\\\\s*[:]{1}\\\\s[a-zA-Z0-9\\\\s.#]+[;]{1} #19. 抽取注释：如果你需要移除HMTL中的注释，可以使用如下的表达式。 \u0026lt;!--(.*?)--\u0026gt; #20. 匹配HTML标签：通过下面的表达式可以匹配出HTML中的标签。 \u0026lt;/?\\\\w+((\\\\s+\\\\w+(\\\\s*=\\\\s*(?:\u0026#34;.*?\u0026#34;|\u0026#39;.*?\u0026#39;|[\\\\^\u0026#39;\u0026#34;\u0026gt;\\\\s]+))?)+\\\\s*|\\\\s*)/?\u0026gt; 二、校验数字表达式 1 数字：^[0-9]*$ 2 n位的数字：^\\d{n}$ 3 至少n位的数字：^\\d{n,}$ 4 m-n位的数字：^\\d{m,n}$ 5 零和非零开头的数字：^(0|[1-9][0-9]*)$ 6 非零开头的最多带两位小数的数字：^([1-9][0-9]*)+(.[0-9]{1,2})?$ 7 带1-2位小数的正数或负数：^(\\-)?\\d+(\\.\\d{1,2})?$ 8 正数、负数、和小数：^(\\-|\\+)?\\d+(\\.\\d+)?$ 9 有两位小数的正实数：^[0-9]+(.[0-9]{2})?$ 10 有1~3位小数的正实数：^[0-9]+(.[0-9]{1,3})?$ 11 非零的正整数：^[1-9]\\d*$ 或 ^([1-9][0-9]*){1,3}$ 或 ^\\+?[1-9][0-9]*$ 12 非零的负整数：^\\-[1-9][]0-9\u0026#34;*$ 或 ^-[1-9]\\d*$ 13 非负整数：^\\d+$ 或 ^[1-9]\\d*|0$ 14 非正整数：^-[1-9]\\d*|0$ 或 ^((-\\d+)|(0+))$ 15 非负浮点数：^\\d+(\\.\\d+)?$ 或 ^[1-9]\\d*\\.\\d*|0\\.\\d*[1-9]\\d*|0?\\.0+|0$ 16 非正浮点数：^((-\\d+(\\.\\d+)?)|(0+(\\.0+)?))$ 或 ^(-([1-9]\\d*\\.\\d*|0\\.\\d*[1-9]\\d*))|0?\\.0+|0$ 17 正浮点数：^[1-9]\\d*\\.\\d*|0\\.\\d*[1-9]\\d*$ 或 ^(([0-9]+\\.[0-9]*[1-9][0-9]*)|([0-9]*[1-9][0-9]*\\.[0-9]+)|([0-9]*[1-9][0-9]*))$ 18 负浮点数：^-([1-9]\\d*\\.\\d*|0\\.\\d*[1-9]\\d*)$ 或 ^(-(([0-9]+\\.[0-9]*[1-9][0-9]*)|([0-9]*[1-9][0-9]*\\.[0-9]+)|([0-9]*[1-9][0-9]*)))$ 19 浮点数：^(-?\\d+)(\\.\\d+)?$ 或 ^-?([1-9]\\d*\\.\\d*|0\\.\\d*[1-9]\\d*|0?\\.0+|0)$ 三、校验字符的表达式 1 汉字：^[\\u4e00-\\u9fa5]{0,}$ 2 英文和数字：^[A-Za-z0-9]+$ 或 ^[A-Za-z0-9]{4,40}$ 3 长度为3-20的所有字符：^.{3,20}$ 4 由26个英文字母组成的字符串：^[A-Za-z]+$ 5 由26个大写英文字母组成的字符串：^[A-Z]+$ 6 由26个小写英文字母组成的字符串：^[a-z]+$ 7 由数字和26个英文字母组成的字符串：^[A-Za-z0-9]+$ 8 由数字、26个英文字母或者下划线组成的字符串：^\\w+$ 或 ^\\w{3,20}$ 9 中文、英文、数字包括下划线：^[\\u4E00-\\u9FA5A-Za-z0-9_]+$ 10 中文、英文、数字但不包括下划线等符号：^[\\u4E00-\\u9FA5A-Za-z0-9]+$ 或 ^[\\u4E00-\\u9FA5A-Za-z0-9]{2,20}$ 11 可以输入含有^%\u0026amp;\u0026#39;,;=?$\\\u0026#34;等字符：[^%\u0026amp;\u0026#39;,;=?$\\x22]+ 12 禁止输入含有~的字符：[^~\\x22]+ 四、特殊需求表达式 1 Email地址：^\\w+([-+.]\\w+)*@\\w+([-.]\\w+)*\\.\\w+([-.]\\w+)*$ 2 域名：[a-zA-Z0-9][-a-zA-Z0-9]{0,62}(/.[a-zA-Z0-9][-a-zA-Z0-9]{0,62})+/.? 3 InternetURL：[a-zA-z]+://[^\\s]* 或 ^http://([\\w-]+\\.)+[\\w-]+(/[\\w-./?%\u0026amp;=]*)?$ 4 手机号码：^(13[0-9]|14[5|7]|15[0|1|2|3|5|6|7|8|9]|18[0|1|2|3|5|6|7|8|9])\\d{8}$ 5 电话号码(\u0026#34;XXX-XXXXXXX\u0026#34;、\u0026#34;XXXX-XXXXXXXX\u0026#34;、\u0026#34;XXX-XXXXXXX\u0026#34;、\u0026#34;XXX-XXXXXXXX\u0026#34;、\u0026#34;XXXXXXX\u0026#34;和\u0026#34;XXXXXXXX)：^(\\(\\d{3,4}-)|\\d{3.4}-)?\\d{7,8}$ 6 国内电话号码(0511-4405222、021-87888822)：\\d{3}-\\d{8}|\\d{4}-\\d{7} 7 身份证号(15位、18位数字)：^\\d{15}|\\d{18}$ 8 短身份证号码(数字、字母x结尾)：^([0-9]){7,18}(x|X)?$ 或 ^\\d{8,18}|[0-9x]{8,18}|[0-9X]{8,18}?$ 9 帐号是否合法(字母开头，允许5-16字节，允许字母数字下划线)：^[a-zA-Z][a-zA-Z0-9_]{4,15}$ 10 密码(以字母开头，长度在6~18之间，只能包含字母、数字和下划线)：^[a-zA-Z]\\w{5,17}$ 11 强密码(必须包含大小写字母和数字的组合，不能使用特殊字符，长度在8-10之间)：^(?=.*\\d)(?=.*[a-z])(?=.*[A-Z]).{8,10}$ 12 日期格式：^\\d{4}-\\d{1,2}-\\d{1,2} 13 一年的12个月(01～09和1～12)：^(0?[1-9]|1[0-2])$ 14 一个月的31天(01～09和1～31)：^((0?[1-9])|((1|2)[0-9])|30|31)$ 15 钱的输入格式： 16 1.有四种钱的表示形式我们可以接受:\u0026#34;10000.00\u0026#34; 和 \u0026#34;10,000.00\u0026#34;, 和没有 \u0026#34;分\u0026#34; 的 \u0026#34;10000\u0026#34; 和 \u0026#34;10,000\u0026#34;：^[1-9][0-9]*$ 17 2.这表示任意一个不以0开头的数字,但是,这也意味着一个字符\u0026#34;0\u0026#34;不通过,所以我们采用下面的形式：^(0|[1-9][0-9]*)$ 18 3.一个0或者一个不以0开头的数字.我们还可以允许开头有一个负号：^(0|-?[1-9][0-9]*)$ 19 4.这表示一个0或者一个可能为负的开头不为0的数字.让用户以0开头好了.把负号的也去掉,因为钱总不能是负的吧.下面我们要加的是说明可能的小数部分：^[0-9]+(.[0-9]+)?$ 20 5.必须说明的是,小数点后面至少应该有1位数,所以\u0026#34;10.\u0026#34;是不通过的,但是 \u0026#34;10\u0026#34; 和 \u0026#34;10.2\u0026#34; 是通过的：^[0-9]+(.[0-9]{2})?$ 21 6.这样我们规定小数点后面必须有两位,如果你认为太苛刻了,可以这样：^[0-9]+(.[0-9]{1,2})?$ 22 7.这样就允许用户只写一位小数.下面我们该考虑数字中的逗号了,我们可以这样：^[0-9]{1,3}(,[0-9]{3})*(.[0-9]{1,2})?$ 23 8.1到3个数字,后面跟着任意个 逗号+3个数字,逗号成为可选,而不是必须：^([0-9]+|[0-9]{1,3}(,[0-9]{3})*)(.[0-9]{1,2})?$ 24 备注：这就是最终结果了,别忘了\u0026#34;+\u0026#34;可以用\u0026#34;*\u0026#34;替代如果你觉得空字符串也可以接受的话(奇怪,为什么?)最后,别忘了在用函数时去掉去掉那个反斜杠,一般的错误都在这里 25 xml文件：^([a-zA-Z]+-?)+[a-zA-Z0-9]+\\\\.[x|X][m|M][l|L]$ 26 中文字符的正则表达式：[\\u4e00-\\u9fa5] 27 双字节字符：[^\\x00-\\xff] (包括汉字在内，可以用来计算字符串的长度(一个双字节字符长度计2，ASCII字符计1)) 28 空白行的正则表达式：\\n\\s*\\r (可以用来删除空白行) 29 HTML标记的正则表达式：\u0026lt;(\\S*?)[^\u0026gt;]*\u0026gt;.*?\u0026lt;/\\1\u0026gt;|\u0026lt;.*? /\u0026gt; (网上流传的版本太糟糕，上面这个也仅仅能部分，对于复杂的嵌套标记依旧无能为力) 30 首尾空白字符的正则表达式：^\\s*|\\s*$或(^\\s*)|(\\s*$) (可以用来删除行首行尾的空白字符(包括空格、制表符、换页符等等)，非常有用的表达式) 31 腾讯QQ号：[1-9][0-9]{4,} (腾讯QQ号从10000开始) 32 中国邮政编码：[1-9]\\d{5}(?!\\d) (中国邮政编码为6位数字) 33 IP地址：\\d+\\.\\d+\\.\\d+\\.\\d+ (提取IP地址时有用) 34 IP地址：((?:(?:25[0-5]|2[0-4]\\\\d|[01]?\\\\d?\\\\d)\\\\.){3}(?:25[0-5]|2[0-4]\\\\d|[01]?\\\\d?\\\\d)) 排除特定字符串 # 不以com结尾的 ^.*?(?\u0026lt;!com)$ # 以com结尾 ^.*?(?\u0026lt;=com)$ ^.*?com$ # 不包含helloworld的字符串 ^(?!.*helloworld).*$ 匹配测试工具 在线验证工具 https://regexr.com/ https://www.debuggex.com/ https://deerchao.cn/tools/wegester/ https://regex101.com/ 正则可视化工具 https://jex.im/regulex/#!flags=\u0026amp;re=%5E(a%7Cb)*%3F%24 https://regexper.com/ 参考 正则表达式引擎执行原理\n如何匹配 3 的倍数\n实现一个正则表达式\n正则表达式 30 分钟入门\nPython 正则表达式\nPython 正则表达式指南\n利用正则表达式排除特定字符串\n一文掌握开发利器：正则表达式\nJavascript 正则 cheatsheet\nPython 正则 cheatsheet\n","permalink":"https://blog.niuhemoon.win/posts/tech/re-usage/","summary":"基础 1.1 字符匹配 字符 说明 \\ 转义符 \\d [0-9]。表示是一位数字。 \\D [^0-9]。表示除数字外的任意字符。 \\w [0-9a-zA-Z_]。表示数字、大小写字母和下划线。 \\W [^0-9a-za-z_]。非单词字符。 \\s [\\t\\v\\n\\r\\f]。表示空白符，包括空格、水平制表符、 垂直制表符、换行符","title":"正则表达式基本使用"},{"content":" 本文主要介绍PlantUML绘图环境的搭建以及时序图的绘制，主要以Linux平台为例。\n安装 若安装时序图和活动图以外的图形，需要安装graphviz\nsudo apt install graphviz VScode安装插件\nPlantUML Markdown Preview Enhanced 插件支持的文件名后缀是：.wsd, .pu, .puml, .plantuml, .iuml\n编辑文件完成后，按快捷键ALT+D预览\nCtrl+Shift+p可以选择导出图片\n需要java\nsudo apt install default-jre 浏览器插件支持PlantUML\nPlantUML Viewer插件可以打开UML文本并预览\n时序图示例 你可以用-\u0026gt;来绘制参与者之间传递的消息， 而不必显式地声明参与者。\n@startuml 用户 -\u0026gt; 认证中心: 登录操作 认证中心 -\u0026gt; 缓存: 存放(key=token+ip,value=token)token 用户 \u0026lt;- 认证中心 : 认证成功返回token 用户 -\u0026gt; 认证中心: 下次访问头部携带token认证 认证中心 \u0026lt;- 缓存: key=token+ip获取token 其他服务 \u0026lt;- 认证中心: 存在且校验成功则跳转到用户请求的其他服务 其他服务 -\u0026gt; 用户: 信息 @enduml 也可以通过关键字声明参与者\n@startuml actor Foo1 boundary Foo2 control Foo3 entity Foo4 database Foo5 collections Foo6 Foo1 -\u0026gt; Foo2 : To boundary Foo1 -\u0026gt; Foo3 : To control Foo1 -\u0026gt; Foo4 : To entity Foo1 -\u0026gt; Foo5 : To database Foo1 -\u0026gt; Foo6 : To collections @enduml @startuml title: 序列图sequence(示例) participant A participant B participant C participant D participant E note left of A: A左侧说明 note over D: 覆盖D的说明 note right of F: F右侧说明 A -\u0026gt;x B: 丢失的消息 B -\u0026gt; C: 实线箭头 C -\u0026gt;\u0026gt; D: 实线细箭头 D -\\ E: 实线半箭头 E -\\\\ F: 实线半箭头 F --/ E: 虚线半箭头 E --\u0026gt;o D: 虚线箭头加圈 D --\\o C: 虚线半箭头加圈 C \u0026lt;--\u0026gt; B: 实线双向箭头 A --\u0026gt; A: 自己到自己 @enduml 参考 PlantUML官网\n浏览器插件\n时序图示例\n","permalink":"https://blog.niuhemoon.win/posts/tech/plantuml-sequence/","summary":"\u003cblockquote\u003e\n\u003cp\u003e本文主要介绍PlantUML绘图环境的搭建以及时序图的绘制，主要以Linux平台为例。\u003c/p\u003e\n\u003c/blockquote\u003e","title":"PlantUML画时序图"},{"content":"Git版本控制 尽管 Git 的接口有些丑陋，但是它的底层设计和思想却是非常优雅的。丑陋的接口只能靠死记硬背，而优雅的底层设计则非常容易被人理解。因此，我们将通过一种自底向上的方式像您介绍 Git。我们会从数据模型开始，最后再学习它的接口。一旦您搞懂了 Git 的数据模型，再学习其接口并理解这些接口是如何操作数据模型的就非常容易了。\nGit 的数据模型 进行版本控制的方法很多。Git 拥有一个经过精心设计的模型，这使其能够支持版本控制所需的所有特性，例如维护历史记录、支持分支和促进协作。\n快照 Git 将顶级目录中的文件和文件夹作为集合，并通过一系列快照来管理其历史记录。在Git的术语里，文件被称作Blob对象（数据对象），也就是一组数据。目录则被称之为“树”，它将名字与Blob对象或树对象进行映射（使得目录中可以包含其他目录）。快照则是被追踪的最顶层的树。例如，一个树看起来可能是这样的：\n\u0026lt;root\u0026gt; (tree) | +- foo (tree) | | | + bar.txt (blob, contents = \u0026#34;hello world\u0026#34;) | +- baz.txt (blob, contents = \u0026#34;git is wonderful\u0026#34;) 这个顶层的树包含了两个元素，一个名为 “foo” 的树(它本身包含了一个blob对象 “bar.txt”)，以及一个对blob对象 “baz.txt”。\n历史记录建模：关联快照 版本控制系统和快照有什么关系呢？线性历史记录是一种最简单的模型，它包含了一组按照时间顺序线性排列的快照。不过处于种种原因，Git 并没有采用这样的模型。\n在 Git 中，历史记录是一个由快照组成的有向无环图。有向无环图，听上去似乎是什么高大上的数学名词。不过不要怕，您只需要知道这代表 Git 中的每个快照都有一系列的“父辈”，也就是其之前的一系列快照。注意，快照具有多个“父辈”而非一个，因为某个快照可能由多个父辈而来。例如，经过合并后的两条分支。\n在 Git 中，这些快照被称为“提交”。通过可视化的方式来表示这些历史提交记录时，看起来差不多是这样的：\no \u0026lt;-- o \u0026lt;-- o \u0026lt;-- o ^ \\ --- o \u0026lt;-- o 上面是一个 ASCII 码构成的简图，其中的 o 表示一次提交（快照）。\n箭头指向了当前提交的父辈（这是一种“在。。。之前”，而不是“在。。。之后”的关系）。在第三次提交之后，历史记录分岔成了两条独立的分支。这可能因为此时需要同时开发两个不同的特性，它们之间是相互独立的。开发完成后，这些分支可能会被合并并创建一个新的提交，这个新的提交会同时包含这些特性。新的提交会创建一个新的历史记录，看上去像这样（最新的合并提交用粗体标记）：\no \u0026lt;\u0026ndash; o \u0026lt;\u0026ndash; o \u0026lt;\u0026ndash; o \u0026lt;\u0026mdash;- o ^ / \\ v \u0026mdash; o \u0026lt;\u0026ndash; o\nGit 中的提交是不可改变的。但这并不代表错误不能被修改，只不过这种“修改”实际上是创建了一个全新的提交记录。而引用（参见下文）则被更新为指向这些新的提交。\n数据模型及其伪代码表示 以伪代码的形式来学习 Git 的数据模型，可能更加清晰：\n// 文件就是一组数据 type blob = array\u0026lt;byte\u0026gt; // 一个包含文件和目录的目录 type tree = map\u0026lt;string, tree | file\u0026gt; // 每个提交都包含一个父辈，元数据和顶层树 type commit = struct { parent: array\u0026lt;commit\u0026gt; author: string message: string snapshot: tree } 这是一种简洁的历史模型。\n对象和内存寻址 Git 中的对象可以是 blob、树或提交：\ntype object = blob | tree | commit Git 在储存数据时，所有的对象都会基于它们的SHA-1 hash进行寻址。\nobjects = map\u0026lt;string, object\u0026gt; def store(object): id = sha1(object) objects[id] = object def load(id): return objects[id] Blobs、树和提交都一样，它们都是对象。当它们引用其他对象时，它们并没有真正的在硬盘上保存这些对象，而是仅仅保存了它们的哈希值作为引用。\n例如，above例子中的树（可以通过git cat-file -p 698281bc680d1995c5f4caaf3359721a5a58d48d 来进行可视化），看上去是这样的：\n100644 blob 4448adbf7ecd394f42ae135bbeed9676e894af85 baz.txt 040000 tree c68d233a33c5c06e0340e4c224f0afca87c8ce87 foo 树本身会包含一些指向其他内容的指针，例如baz.txt (blob) 和 foo (树)。如果我们用git cat-file -p 4448adbf7ecd394f42ae135bbeed9676e894af85，即通过哈希值查看 baz.txte 的内容，会得到以下信息：\ngit is wonderful 引用 现在，所有的快照都可以通过它们的 SHA-1 哈希值来标记了。但这也太不方便了，谁也记不住一串 40 位的十六进制字符。\n针对这一问题，Git 的解决方法是给这些哈希值赋予人类可读的名字，也就是引用（references）。引用是指向提交的指针。与对象不同的是，它是可变的（引用可以被更新，指向新的提交）。例如，master 引用通常会指向主分支的最新一次提交。\nreferences = map\u0026lt;string, string\u0026gt; def update_reference(name, id): references[name] = id def read_reference(name): return references[name] def load_reference(name_or_id): if name_or_id in references: return load(references[name_or_id]) else: return load(name_or_id) 这样，Git 就可以使用诸如 “master” 这样人类刻度的名称来表示历史记录中某个特定的提交，而不需要在使用一长串十六进制字符了。\n有一个细节需要我们注意， 通常情况下，我们会想要知道“我们当前所在位置”，并将其标记下来。这样当我们创建新的快照的时候，我们就可以知道它的相对位置（如何设置它的“父辈”）。在 Git 中，我们当前的位置有一个特殊的索引，它就是”HEAD”。\n仓库 最后，我们可以粗略地给出 Git 仓库的定义了：对象 和 引用。\n在硬盘上，Git 仅存储对象和引用：因为其数据模型仅包含这些东西。所有的 git 命令都对应着对提交树的操作，例如增加对象，增加或删除引用。\n当您输入某个指令时，请思考一些这条命令是如何对底层的图数据结构进行操作的。另一方面，如果您希望修改提交树，例如“丢弃未提交的修改和将 ‘master’ 引用指向提交5d83f9e 时，有什么命令可以完成该操作（针对这个具体问题，您可以使用git checkout master; git reset --hard 5d83f9e）\n暂存区 Git 中还包括一个和数据模型完全不相关的概念，但它确是创建提交的接口的一部分。\n就上面介绍的快照系统来说，您也许会期望它的实现里包括一个 “创建快照” 的命令，该命令能够基于当前工作目录的当前状态创建一个全新的快照。有些版本控制系统确实是这样工作的，但 Git 不是。我们希望简洁的快照，而且每次从当前状态创建快照可能效果并不理想。例如，考虑如下场景，您开发了两个独立的特性，然后您希望创建两个独立的提交，其中第一个提交仅包含第一个特性，而第二个提交仅包含第二个特性。或者，假设您在调试代码时添加了很多打印语句，然后您仅仅希望提交和修复 bug 相关的代码而丢弃所有的打印语句。\nGit 处理这些场景的方法是使用一种叫做 “暂存区（staging area）”的机制，它允许您指定下次快照中要包括那些改动。\nGit 的命令行接口 为了避免重复信息，我们将不会详细解释以下命令行。强烈推荐您阅读Pro Git 中文版或可以观看本讲座的视频来学习。\n基础 git help \u0026lt;command\u0026gt;: 获取 git 命令的帮助信息 git init: 创建一个新的 git 仓库，其数据会存放在一个名为 .git 的目录下 git status: 显示当前的仓库状态 git add \u0026lt;filename\u0026gt;: 添加文件到暂存区 git commit: 创建一个新的提交 如何编写 良好的提交信息! git log: 显示历史日志 git log --all --graph --decorate: 可视化历史记录（有向无环图） git diff \u0026lt;filename\u0026gt;: 显示与上一次提交之间的差异 git diff \u0026lt;revision\u0026gt; \u0026lt;filename\u0026gt;: 显示某个文件两个版本之间的差异 git checkout \u0026lt;revision\u0026gt;: 更新 HEAD 和目前的分支 分支和合并 git branch: 显示分支 git branch \u0026lt;name\u0026gt;: 创建分支 git checkout -b \u0026lt;name\u0026gt;: 创建分支并切换到该分支 相当于 git branch \u0026lt;name\u0026gt;; git checkout \u0026lt;name\u0026gt; git merge \u0026lt;revision\u0026gt;: 合并到当前分支 git mergetool: 使用工具来处理合并冲突 git rebase: 将一系列补丁变基（rebase）为新的基线 远端操作 git remote: 列出远端 git remote add \u0026lt;name\u0026gt; \u0026lt;url\u0026gt;: 添加一个远端 git push \u0026lt;remote\u0026gt; \u0026lt;local branch\u0026gt;:\u0026lt;remote branch\u0026gt;: 将对象传送至远端并更新远端引用 git branch --set-upstream-to=\u0026lt;remote\u0026gt;/\u0026lt;remote branch\u0026gt;: 创建本地和远端分支的关联关系 git fetch: 从远端获取对象/索引 git pull: 相当于 git fetch; git merge git clone: 从远端下载仓库 撤销 git commit --amend: 编辑提交的内容或信息 git reset HEAD \u0026lt;file\u0026gt;: 恢复暂存的文件 git checkout -- \u0026lt;file\u0026gt;: 丢弃修改 Git 高级操作 git config: Git 是一个 高度可定制的 工具 列出所有config文件里的设置项\ngit config --system -l # 系统级的配置 /etc文件加下 git config --global -l # 用户级的配置 /home/用户名/的文件下 git config --local -l # 仓库/项目级 ./.git/目录下 git config user.name \u0026#39;你的姓名\u0026#39; git config user.email \u0026#39;你的email\u0026#39; 删除配置文件中的配置项\ngit config --unset user.name 查看配置文件中的配置项\ngit config --get user.name 定义指令别名\n# git config alias.指令别名 \u0026#39;标准指令\u0026#39; git config alias.con \u0026#39;config -l\u0026#39; git con git config --unset alias.con 创建.gitignore文件 vi .gitignore .gitignore文件规定了git系统该忽略哪些文件\n/images *.txt !requirements.txt 如以上配置规定了git忽略\n/images目录下的所有文件 除了requirements.txt外的所有txt文件 git clone --shallow: 克隆仓库，但是不包括版本历史信息 git add -p: 交互式暂存 git rebase -i: 交互式变基 git blame: 查看最后修改某行的人 git stash: 暂时移除工作目录下的修改内容 git bisect: 通过二分查找搜索历史记录 .gitignore: 指定 故意不追踪的文件 杂项 图形用户界面: Git 的 图形用户界面客户端 有很多，但是我们自己并不使用这些图形用户界面的客户端，我们选择使用命令行接口 Shell 集成: 将 Git 状态集成到您的 shell 中会非常方便。(zsh,bash)。Oh My Zsh这样的框架中一般以及集成了这一功能 编辑器集成: 和上面一条类似，将 Git 集成到编辑器中好处多多。fugitive.vim 是 Vim 中集成 GIt 的常用插件 工作流:我们已经讲解了数据模型与一些基础命令，但还没讨论到进行大型项目时的一些惯例 ( 有很多 不同的 处理方法) GitHub: Git 并不等同于 GitHub。 在 GitHub 中您需要使用一个被称作拉取请求（pull request）的方法来像其他项目贡献代码 Other Git 提供商: GitHub 并不是唯一的。还有像GitLab 和 BitBucket这样的平台。 资源 Pro Git ，强烈推荐！学习前五章的内容可以教会您流畅使用 Git 的绝大多数技巧，因为您已经理解了 Git 的数据模型。后面的章节提供了很多有趣的高级主题。（Pro Git 中文版）； Oh Shit, Git!?! ，简短的介绍了如何从 Git 错误中恢复； Git for Computer Scientists ，简短的介绍了 Git 的数据模型，与本文相比包含较少量的伪代码以及大量的精美图片； Git from the Bottom Up详细的介绍了 Git 的实现细节，而不仅仅局限于数据模型。好奇的同学可以看看； How to explain git in simple words； Learn Git Branching 通过基于浏览器的游戏来学习 Git ； 参考 课程列表\n","permalink":"https://blog.niuhemoon.win/posts/tech/coding-tools-git/","summary":"Git版本控制 尽管 Git 的接口有些丑陋，但是它的底层设计和思想却是非常优雅的。丑陋的接口只能靠死记硬背，而优雅的底层设计则非常容易被人理解。因此，我们将通过一种自底向上的方式像您介绍 Git。我们会从数据模型开始，最后再学习它的接口。一旦您搞懂了 Git 的数据模型，再学习其接口并理解这些接口","title":"编程工具之Git"},{"content":"Vim编辑器 编辑模式 Vim的设计以大多数时间都花在阅读、浏览和进行少量编辑改动为基础，因此它具有多种操作模式：\n正常模式：在文件中四处移动光标进行修改 插入模式：插入文本 替换模式：替换文本 可视化（一般，行，块）模式：选中文本块 命令模式：用于执行命令 在不同的操作模式下， 键盘敲击的含义也不同。比如，x 在插入模式会插入字母x，但是在正常模式 会删除当前光标所在下的字母，在可视模式下则会删除选中文块。\n在默认设置下，Vim会在左下角显示当前的模式。 Vim启动时的默认模式是正常模式。通常你会把大部分 时间花在正常模式和插入模式。\n你可以按下 \u0026lt;ESC\u0026gt; （逃脱键） 从任何其他模式返回正常模式。 在正常模式，键入 i 进入插入 模式， R 进入替换模式， v 进入可视（一般）模式， V 进入可视（行）模式， \u0026lt;C-v\u0026gt; （Ctrl-V, 有时也写作 ^V）进入可视（块）模式， : 进入命令模式。\n因为你会在使用 Vim 时大量使用 \u0026lt;ESC\u0026gt; 键，可以考虑把大小写锁定键重定义成逃脱键 （MacOS 教程 ）。\n基本操作 插入文本 在正常模式， 键入 i 进入插入模式。 现在 Vim 跟很多其他的编辑器一样， 直到你键入\u0026lt;ESC\u0026gt; 返回正常模式。 你只需要掌握这一点和上面介绍的所有基知识就可以使用 Vim 来编辑文件了 （虽然如果你一直停留在插入模式内不一定高效）。\n缓存， 标签页， 窗口 Vim 会维护一系列打开的文件，称为 “缓存”。 一个 Vim 会话包含一系列标签页，每个标签页包含 一系列窗口 （分隔面板）。每个窗口显示一个缓存。 跟网页浏览器等其他你熟悉的程序不一样的是， 缓存和窗口不是一一对应的关系； 窗口只是视角。 一个缓存可以在 多个 窗口打开，甚至在同一 个标签页内的多个窗口打开。这个功能其实很好用， 比如在查看同一个文件的不同部分的时候。\nVim 默认打开一个标签页，这个标签也包含一个窗口。\n命令行 在正常模式下键入 : 进入命令行模式。 在键入 : 后，你的光标会立即跳到屏幕下方的命令行。 这个模式有很多功能， 包括打开， 保存， 关闭文件， 以及 退出 Vim。\n:q 退出 （关闭窗口） :w 保存 （写） :wq 保存然后退出 :e {文件名} 打开要编辑的文件 :ls 显示打开的缓存 :help {标题} 打开帮助文档 :help :w 打开 :w 命令的帮助文档 :help w 打开 w 移动的帮助文档 Vim 的接口其实是一种编程语言 Vim 最重要的设计思想是 Vim 的界面本身是一个程序语言。 键入操作 （以及他们的助记名） 本身是命令， 这些命令可以组合使用。 这使得移动和编辑更加高效，特别是一旦形成肌肉记忆。\n移动 多数时候你会在正常模式下，使用移动命令在缓存中导航。在 Vim 里面移动也被成为 “名词”， 因为它们指向文字块。\n基本移动: hjkl （左， 下， 上， 右） 词： w （下一个词）， b （词初）， e （词尾） 行： 0 （行初）， ^ （第一个非空格字符）， $ （行尾） 屏幕： H （屏幕首行）， M （屏幕中间）， L （屏幕底部） 翻页： Ctrl-u （上翻）， Ctrl-d （下翻） 文件： gg （文件头）， G （文件尾） 行数： :{行数}\u0026lt;CR\u0026gt; 或者 {行数}G ({行数}为行数) 杂项： % （找到配对，比如括号或者 /* */ 之类的注释对） 查找： f{字符}， t{字符}， F{字符}， T{字符} 查找/到 向前/向后 在本行的{字符} , / ; 用于导航匹配 搜索: /{正则表达式}, n / N 用于导航匹配 选择 可视化模式:\n可视化 可视化行 可视化块 可以用移动命令来选中。\n编辑 所有你需要用鼠标做的事， 你现在都可以用键盘：采用编辑命令和移动命令的组合来完成。 这就是 Vim 的界面开始看起来像一个程序语言的时候。Vim 的编辑命令也被称为 “动词”， 因为动词可以施动于名词。\ni 进入插入模式 但是对于操纵/编辑文本，不单想用退格键完成 O / o 在之上/之下插入行 d{移动命令} 删除 {移动命令} 例如， dw 删除词, d$ 删除到行尾, d0 删除到行头。 c{移动命令} 改变 {移动命令} 例如， cw 改变词 比如 d{移动命令} 再 i x 删除字符 （等同于 dl） s 替换字符 （等同于 xi） 可视化模式 + 操作 选中文字, d 删除 或者 c 改变 u 撤销, \u0026lt;C-r\u0026gt; 重做 y 复制 / “yank” （其他一些命令比如 d 也会复制） p 粘贴 更多值得学习的: 比如 ~ 改变字符的大小写 计数 你可以用一个计数来结合“名词” 和 “动词”， 这会执行指定操作若干次。\n3w 向前移动三个词 5j 向下移动5行 7dw 删除7个词 修饰语 你可以用修饰语改变 “名词” 的意义。修饰语有 i， 表示 “内部” 或者 “在内“， 和 a， 表示 ”周围“。\nci( 改变当前括号内的内容 ci[ 改变当前方括号内的内容 da' 删除一个单引号字符窗， 包括周围的单引号 自定义 Vim Vim 由一个位于 ~/.vimrc 的文本配置文件 （包含 Vim 脚本命令）。 你可能会启用很多基本 设置。\n我们提供一个文档详细的基本设置， 你可以用它当作你的初始设置。 我们推荐使用这个设置因为 它修复了一些 Vim 默认设置奇怪行为。 在 这儿 下载我们的设置， 然后将它保存成 ~/.vimrc.\nVim 能够被重度自定义， 花时间探索自定义选项是值得的。 你可以参考其他人的在 GitHub 上共享的设置文件， 比如， 你的授课人的 Vim 设置 (Anish, Jon (uses neovim), Jose)。 有很多好的博客文章也聊到了这个话题。 尽量不要复制粘贴别人的整个设置文件， 而是阅读和理解它， 然后采用对你有用的部分。\n扩展 Vim Vim 有很多扩展插件。 跟很多互联网上已经过时的建议相反， 你 不 需要在 Vim 使用一个插件 管理器（从 Vim 8.0 开始）。 你可以使用内置的插件管理系统。 只需要创建一个 ~/.vim/pack/vendor/start/ 的文件夹， 然后把插件放到这里 （比如通过 git clone）。\n以下是一些我们最爱的插件：\nctrlp.vim: 模糊文件查找 ack.vim: 代码搜索 nerdtree: 文件浏览器 vim-easymotion: 魔术操作 我们尽量避免在这里提供一份冗长的插件列表。 你可以查看讲师们的开源的配置文件 (Anish, Jon, Jose) 来看看我们使用的其他插件。 浏览 Vim Awesome 来了解一些很棒的插件。 这个话题也有很多博客文章： 搜索 “best Vim plugins”。\nVim 进阶 这里我们提供了一些展示这个编辑器能力的例子。我们无法把所有的这样的事情都教给你， 但是你 可以在使用中学习。 一个好的对策是: 当你在使用你的编辑器的时候感觉 “一定有更好的方法来做这个”， 那么很可能真的有： 上网搜寻一下。\n搜索和替换 :s （替换） 命令 （文档）。\n%s/foo/bar/g 在整个文件中将 foo 全局替换成 bar %s/\\[.*\\](\\(.*\\))/\\1/g 将有命名的 Markdown 链接替换成简单 URLs 多窗口 用 :sp / :vsp 来分割窗口 同一个缓存可以在多个窗口中显示。 宏 q{字符} 来开始在寄存器 {字符} 中录制宏 q 停止录制 @{字符} 重放宏 宏的执行遇错误会停止 {计数}@{字符} 执行一个宏 {计数} 次 宏可以递归 首先用 q{字符}q 清除宏 录制该宏， 用 @{字符} 来递归调用该宏 （在录制完成之前不会有任何操作） 例子： 将 xml 转成 json (file) 一个有 “name” / “email” 键对象的数组 用一个 Python 程序？ 用 sed / 正则表达式 g/people/d %s/\u0026lt;person\u0026gt;/{/g %s/\u0026lt;name\u0026gt;\\(.*\\)\u0026lt;\\/name\u0026gt;/\u0026quot;name\u0026quot;: \u0026quot;\\1\u0026quot;,/g … Vim 命令 / 宏 Gdd, ggdd 删除第一行和最后一行 格式化最后一个元素的宏 （寄存器 e） 跳转到有 \u0026lt;name\u0026gt; 的行 qe^r\u0026quot;f\u0026gt;s\u0026quot;: \u0026quot;\u0026lt;ESC\u0026gt;f\u0026lt;C\u0026quot;\u0026lt;ESC\u0026gt;q 格式化一个人的宏 跳转到有 \u0026lt;person\u0026gt; 的行 qpS{\u0026lt;ESC\u0026gt;j@eA,\u0026lt;ESC\u0026gt;j@ejS},\u0026lt;ESC\u0026gt;q 格式化一个人然后转到另外一个人的宏 跳转到有 \u0026lt;person\u0026gt; 的行 qq@pjq 执行宏到文件尾 999@q 手动移除最后的 , 然后加上 [ 和 ] 分隔符 扩展资料 vimtutor 是一个 Vim 安装时自带的教程 Vim Adventures 是一个学习使用 Vim 的游戏 Vim Tips Wiki Vim Advent Calendar 有很多 Vim 小技巧 Vim Golf 是用 Vim 的用户界面作为程序语言的 code golf Vi/Vim Stack Exchange Vim Screencasts Practical Vim （书） 参考 课程列表\n","permalink":"https://blog.niuhemoon.win/posts/tech/coding-tools-vim/","summary":"Vim编辑器 编辑模式 Vim的设计以大多数时间都花在阅读、浏览和进行少量编辑改动为基础，因此它具有多种操作模式： 正常模式：在文件中四处移动光标进行修改 插入模式：插入文本 替换模式：替换文本 可视化（一般，行，块）模式：选中文本块 命令模式：用于执行命令 在不同的操作模式下， 键盘敲击的含义也","title":"编程工具之Vim"},{"content":"最后一节课，我们回答学生提出的问题:\n学习操作系统相关内容的推荐，比如进程，虚拟内存，中断，内存管理等 你会优先学习的工具有那些？ 使用 Python VS Bash脚本 VS 其他语言? source script.sh 和 ./script.sh 有什么区别? 各种软件包和工具存储在哪里？引用过程是怎样的? /bin 或 /lib 是什么？ 我应该用 apt-get install 还是 pip install 去下载软件包呢? 用于提高代码性能，简单好用的性能分析工具有哪些? 你使用那些浏览器插件? 有哪些有用的数据整理工具？ Docker和虚拟机有什么区别? 不同操作系统的优缺点是什么，我们如何选择（比如选择最适用于我们需求的Linux发行版)? 使用 Vim 编辑器 VS Emacs 编辑器? 机器学习应用的提示或技巧? 还有更多的 Vim 小窍门吗？ 2FA是什么，为什么我需要使用它? 对于不同的 Web 浏览器有什么评价? 参考 学习操作系统相关内容的推荐，比如进程，虚拟内存，中断，内存管理等 首先，不清楚你是不是真的需要了解这些更底层的话题。 当你开始编写更加底层的代码，比如实现或修改内核的时候，这些内容是很重要的。除了其他课程中简要介绍过的进程和信号量之外，大部分话题都不相关。\n学习资源：\nMIT\u0026rsquo;s 6.828 class - 研究生阶段的操作系统课程（课程资料是公开的）。 现代操作系统 第四版（Modern Operating Systems 4th ed） - 作者是Andrew S. Tanenbaum 这本书对上述很多概念都有很好的描述。 FreeBSD的设计与实现（The Design and Implementation of the FreeBSD Operating System） - 关于FreeBSD OS 不错的资源(注意，FreeBSD OS 不是 Linux)。 其他的指南例如 用 Rust 写操作系统 这里用不同的语言逐步实现了内核，主要用于教学的目的。 你会优先学习的工具有那些？ 值得优先学习的内容：\n多去使用键盘，少使用鼠标。这一目标可以通过多加利用快捷键，更换界面等来实现。 学好编辑器。作为程序员你大部分时间都是在编辑文件，因此值得学好这些技能。 学习怎样去自动化或简化工作流程中的重复任务。因为这会节省大量的时间。 学习像 Git 之类的版本控制工具并且知道如何与 GitHub 结合，以便在现代的软件项目中协同工作。 使用 Python VS Bash脚本 VS 其他语言? 通常来说，Bash 脚本对于简短的一次性脚本有效，比如当你想要运行一系列的命令的时候。但是Bash 脚本有一些比较奇怪的地方，这使得大型程序或脚本难以用 Bash 实现：\nBash 可以获取简单的用例，但是很难获得全部可能的输入。例如，脚本参数中的空格会导致Bash 脚本出错。 Bash 对于代码重用并不友好。因此，重用你先前已经写好的代码很困难。通常 Bash 中没有软件库的概念。 Bash 依赖于一些像 $? 或 $@ 的特殊字符指代特殊的值。其他的语言却会显式地引用，比如 exitCode 或 sys.args。 因此，对于大型或者更加复杂的脚本我们推荐使用更加成熟的脚本语言例如 Python 和 Ruby。 你可以找到很多用这些语言编写的，用来解决常见问题的在线库。 如果你发现某种语言实现了你所需要的特定功能库，最好的方式就是直接去使用那种语言。\nsource script.sh 和 ./script.sh 有什么区别? 这两种情况 script.sh 都会在bash会话中被读取和执行，不同点在于那个会话执行这个命令。 对于 source 命令来说，命令是在当前的bash会话种执行的，因此当 source 执行完毕，对当前环境的任何更改（例如更改目录或是定义函数）都会留存在当前会话中。 单独运行 ./script.sh 时，当前的bash会话将启动新的bash会话（实例），并在新实例中运行命令 script.sh。 因此，如果 script.sh 更改目录，新的bash会话（实例）会更改目录，但是一旦退出并将控制权返回给父bash会话，父会话仍然留在先前的位置（不会有目录的更改）。 同样，如果 script.sh 定义了要在终端中访问的函数，需要用 source 命令在当前bash会话中定义这个函数。否则，如果你运行 ./script.sh，只有新的bash会话（进程）才能执行定义的函数，而当前的shell不能。\n各种软件包和工具存储在哪里？引用过程是怎样的? /bin 或 /lib 是什么？ 根据你在命令行中运行的程序，这些包和工具会全部在 PATH 环境变量所列出的目录中查找到， 你可以使用 which 命令(或是 type 命令)来检查你的shell在哪里发现了特定的程序。 一般来说，特定种类的文件存储有一定的规范，文件系统，层次结构标准（Filesystem, Hierarchy Standard）可以查到我们讨论内容的详细列表。\n/bin - 基本命令二进制文件 /sbin - 基本的系统二进制文件，通常是root运行的 /dev - 设备文件，通常是硬件设备接口文件 /etc - 主机特定的系统配置文件 /home - 系统用户的家目录 /lib - 系统软件通用库 /opt - 可选的应用软件 /sys - 包含系统的信息和配置(第一堂课介绍的) /tmp - 临时文件( /var/tmp ) 通常在重启之间删除 /usr/ - 只读的用户数据 /usr/bin - 非必须的命令二进制文件 /usr/sbin - 非必须的系统二进制文件，通常是由root运行的 /usr/local/bin - 用户编译程序的二进制文件 /var -变量文件 像日志或缓存 我应该用 apt-get install 还是 pip install 去下载软件包呢? 这个问题没有普遍的答案。这与使用系统程序包管理器还是特定语言的程序包管理器来安装软件这一更笼统的问题相关。需要考虑的几件事：\n常见的软件包都可以通过这两种方法获得，但是小众的软件包或较新的软件包可能不在系统程序包管理器中。在这种情况下，使用特定语言的程序包管理器是更好的选择。 同样，特定语言的程序包管理器相比系统程序包管理器有更多的最新版本的程序包。 当使用系统软件包管理器时，将在系统范围内安装库。如果出于开发目的需要不同版本的库，则系统软件包管理器可能不能满足你的需要。对于这种情况，大多数编程语言都提供了隔离或虚拟环境，因此你可以用特定语言的程序包管理器安装不同版本的库而不会发生冲突。对于 Python，可以使用 virtualenv，对于 Ruby，使用 RVM 。 根据操作系统和硬件架构，其中一些软件包可能会附带二进制文件或者软件包需要被编译。例如，在树莓派（Raspberry Pi）之类的ARM架构计算机中，在软件附带二进制文件和软件包需要被编译的情况下，使用系统包管理器比特定语言包管理器更好。这在很大程度上取决于你的特定设置。 你应该仅使用一种解决方案，而不同时使用两种方法，因为这可能会导致难以解决的冲突。我们的建议是尽可能使用特定语言的程序包管理器，并使用隔离的环境（例如 Python 的 virtualenv）以避免影响全局环境。 用于提高代码性能，简单好用的性能分析工具有哪些? 性能分析方面相当有用和简单工具是print timing。你只需手动计算代码不同部分之间花费的时间。通过重复执行此操作，你可以有效地对代码进行二分法搜索，并找到花费时间最长的代码段。\n对于更高级的工具， Valgrind 的 Callgrind可让你运行程序并计算所有的时间花费以及所有调用堆栈（即哪个函数调用了另一个函数）。然后，它会生成带注释的代码版本，其中包含每行花费的时间。但是，它会使程序运行速度降低一个数量级，并且不支持线程。其他的， perf 工具和其他特定语言的采样性能分析器可以非常快速地输出有用的数据。Flamegraphs 是对采样分析器结果的可视化工具。你还可以使用针对特定编程语言或任务的工具。例如，对于 Web 开发而言，Chrome 和 Firefox 内置的开发工具具有出色的性能分析器。\n有时，代码中最慢的部分是系统等待磁盘读取或网络数据包之类的事件。在这些情况下，需要检查根据硬件性能估算的理论速度是否不偏离实际数值，也有专门的工具来分析系统调用中的等待时间，包括用于用户程序内核跟踪的eBPF 。如果需要低级的性能分析， bpftrace 值得一试。\n你使用那些浏览器插件? 我们钟爱的插件主要与安全性与可用性有关：\nuBlock Origin - 是一个用途广泛（wide-spectrum）的拦截器，它不仅可以拦截广告，还可以拦截第三方的页面，也可以拦截内部脚本和其他种类资源的加载。如果你打算花更多的时间去配置，前往中等模式（medium mode）或者 强力模式（hard mode）。在你调整好设置之前一些网站会停止工作，但是这些配置会显著提高你的网络安全水平。另外， 简易模式（easy mode）作为默认模式已经相当不错了，可以拦截大部分的广告和跟踪，你也可以自定义规则来拦截网站对象。 Stylus - 是Stylish的分支（不要使用Stylish，它会窃取浏览记录)），这个插件可让你将自定义CSS样式加载到网站。使用Stylus，你可以轻松地自定义和修改网站的外观。可以删除侧边框，更改背景颜色，更改文字大小或字体样式。这可以使你经常访问的网站更具可读性。此外，Stylus可以找到其他用户编写并发布在userstyles.org中的样式。大多数常用的网站都有一个或几个深色主题样式。 全页屏幕捕获 - 内置于 Firefox 和 Chrome 扩展程序中。这些插件提供完整的网站截图，通常比打印要好用。 多账户容器 - 该插件使你可以将Cookie分为“容器”，从而允许你以不同的身份浏览web网页并且/或确保网站无法在它们之间共享信息。 密码集成管理器 - 大多数密码管理器都有浏览器插件，这些插件帮你将登录凭据输入网站的过程不仅方便，而且更加安全。与简单复制粘贴用户名和密码相比，这些插件将首先检查网站域是否与列出的条目相匹配，以防止冒充网站的网络钓鱼窃取登录凭据。 有哪些有用的数据整理工具？ 在数据整理那一节课程中，我们没有时间讨论一些数据整理工具，包括分别用于JSON和HTML数据的专用解析器， jq 和 pup。Perl语言是另一个更高级的可以用于数据整理管道的工具。另一个技巧是使用 column -t 命令，可以将空格文本（不一定对齐）转换为对齐的文本。\n一般来说，vim和Python是两个不常规的数据整理工具。对于某些复杂的多行转换，vim宏是非常有用的工具。你可以记录一系列操作，并根据需要重复执行多次，例如，在编辑的讲义(去年 视频)中，有一个示例是使用vim宏将XML格式的文件转换为JSON。\n对于通常以CSV格式显示的表格数据， Python pandas库是一个很棒的工具。不仅因为它能让复杂操作的定义（如分组依据，联接或过滤器）变得非常容易，而且还便于根据不同属性绘制数据。它还支持导出多种表格格式，包括 XLS，HTML 或 LaTeX。另外，R语言(一种有争议的不好的语言）具有很多功能，可以计算数据的统计数字，这在管道的最后一步中非常有用。 ggplot2是R中很棒的绘图库。\nDocker和虚拟机有什么区别? Docker 基于容器这个更为概括的概念。关于容器和虚拟机之间最大的不同是，虚拟机会执行整个的 OS 栈，包括内核（即使这个内核和主机内核相同）。与虚拟机不同，容器避免运行其他内核实例，而是与主机分享内核。在Linux环境中，有LXC机制来实现，并且这能使一系列分离的主机像是在使用自己的硬件启动程序，而实际上是共享主机的硬件和内核。因此容器的开销小于完整的虚拟机。\n另一方面，容器的隔离性较弱而且只有在主机运行相同的内核时才能正常工作。例如，如果你在macOS 上运行 Docker，Docker 需要启动 Linux虚拟机去获取初始的 Linux内核，这样的开销仍然很大。最后，Docker 是容器的特定实现，它是为软件部署而定制的。基于这些，它有一些奇怪之处：例如，默认情况下，Docker 容器在重启之间不会有以任何形式的存储。\n不同操作系统的优缺点是什么，我们如何选择（比如选择最适用于我们需求的Linux发行版)? 关于Linux发行版，尽管有相当多的版本，但大部分发行版在大多数使用情况下的表现是相同的。 可以使用任何发行版去学习 Linux 与 UNIX 的特性和其内部工作原理。 发行版之间的根本区别是发行版如何处理软件包更新。 某些版本，例如 Arch Linux 采用滚动更新策略，用了最前沿的软件包（bleeding-edge），但软件可能并不稳定。另外一些发行版（如Debian，CentOS 或 Ubuntu LTS）其更新策略要保守得多，因此更新的内容会更稳定，但会牺牲一些新功能。我们建议你使用 Debian 或 Ubuntu 来获得简单稳定的台式机和服务器体验。\nMac OS 是介于 Windows 和 Linux 之间的一个操作系统，它有很漂亮的界面。但是，Mac OS 是基于BSD 而不是 Linux，因此系统的某些部分和命令是不同的。 另一种值得体验的是 FreeBSD。虽然某些程序不能在 FreeBSD 上运行，但与 Linux 相比，BSD 生态系统的碎片化程度要低得多，并且说明文档更加友好。 除了开发Windows应用程序或需要使用某些Windows系统更好支持的功能（例如对游戏的驱动程序支持）外，我们不建议使用 Windows。\n对于双系统，我们认为最有效的是 macOS 的 bootcamp，长期来看，任何其他组合都可能会出现问题，尤其是当你结合了其他功能比如磁盘加密。\n使用 Vim 编辑器 VS Emacs 编辑器? 我们三个都使用 vim 作为我们的主要编辑器。但是 Emacs 也是一个不错的选择，你可以两者都尝试，看看那个更适合你。Emacs 不使用 vim 的模式编辑，但是这些功能可以通过 Emacs 插件像Evil 或 Doom Emacs来实现。 Emacs的优点是可以用Lisp语言进行扩展（Lisp比vim默认的脚本语言vimscript要更好用）。\n机器学习应用的提示或技巧? 课程的一些经验可以直接用于机器学习程序。 就像许多科学学科一样，在机器学习中，你需要进行一系列实验，并检查哪些数据有效，哪些无效。 你可以使用 Shell 轻松快速地搜索这些实验结果，并且以合理的方式汇总。这意味着需要在限定时间内或使用特定数据集的情况下，检查所有实验结果。通过使用JSON文件记录实验的所有相关参数，使用我们在本课程中介绍的工具，这件事情可以变得极其简单。 最后，如果你不使用集群提交你的 GPU 作业，那你应该研究如何使该过程自动化，因为这是一项非常耗时的任务，会消耗你的精力。\n还有更多的 Vim 小窍门吗？ 更多的窍门：\n插件 - 花时间去探索插件。有很多不错的插件修复了vim的缺陷或者增加了能够与现有vim工作流结合的新功能。关于这部分内容，资源是VimAwesome 和其他程序员的dotfiles。 标记 - 在vim里你可以使用 m\u0026lt;X\u0026gt; 为字母 X 做标记，之后你可以通过 '\u0026lt;X\u0026gt; 回到标记位置。这可以让你快速定位到文件内或文件间的特定位置。 导航 - Ctrl+O 和 Ctrl+I 命令可以使你在最近访问位置前后移动。 撤销树 - vim 有不错的更改跟踪机制，不同于其他的编辑器，vim存储变更树，因此即使你撤销后做了一些修改，你仍然可以通过撤销树的导航回到初始状态。一些插件比如 gundo.vim 和 undotree 通过图形化来展示撤销树。 时间撤销 - :earlier 和 :later 命令使得你可以用时间而非某一时刻的更改来定位文件。 持续撤销 - 是一个默认未被开启的vim的内置功能，它在vim启动之间保存撤销历史，需要配置在 .vimrc 目录下的undofile 和 undodir，vim会保存每个文件的修改历史。 热键（Leader Key） - 热键是一个用于用户自定义配置命令的特殊按键。这种模式通常是按下后释放这个按键（通常是空格键）并与其他的按键组合去实现一个特殊的命令。插件也会用这些按键增加它们的功能，例如，插件UndoTree使用 \u0026lt;Leader\u0026gt; U 去打开撤销树。 高级文本对象 - 文本对象比如搜索也可以用vim命令构成。例如，d/\u0026lt;pattern\u0026gt; 会删除下一处匹配 pattern 的字符串，cgn 可以用于更改上次搜索的关键字。 2FA是什么，为什么我需要使用它? 双因子验证（Two Factor Authentication 2FA）在密码之上为帐户增加了一层额外的保护。为了登录，你不仅需要知道密码，还必须以某种方式“证明”可以访问某些硬件设备。最简单的情形是可以通过接收手机的 SMS 来实现（尽管 SMS 2FA 存在 已知问题）。我们推荐使用YubiKey之类的U2F方案。\n对于不同的 Web 浏览器有什么评价? 2020的浏览器现状是，大部分的浏览器都与 Chrome 类似，因为它们都使用同样的引擎(Blink)。 Microsoft Edge 同样基于 Blink，至于 Safari 基于 WebKit(与Blink类似的引擎)，这些浏览器仅仅是更糟糕的 Chorme 版本。不管是在性能还是可用性上，Chorme 都是一款很不错的浏览器。如果你想要替代品，我们推荐 Firefox。Firefox 与 Chorme 的在各方面不相上下，并且在隐私方面更加出色。 有一款目前还没有完成的叫 Flow 的浏览器，它实现了全新的渲染引擎，有望比现有引擎速度更快。\n参考 课程列表\n","permalink":"https://blog.niuhemoon.win/posts/tech/coding-tools-qa/","summary":"最后一节课，我们回答学生提出的问题: 学习操作系统相关内容的推荐，比如进程，虚拟内存，中断，内存管理等 你会优先学习的工具有那些？ 使用 Python VS Bash脚本 VS 其他语言? source script.sh 和 ./script.sh 有什么区别? 各种软件包和工具存储在哪里？引用过程是怎样的? /bin 或 /lib 是什么？ 我应该用 apt-get install 还是 pip install 去下载软件包呢? 用于提","title":"编程工具之问答"},{"content":"安全和密码 熵(Entropy) 度量了不确定性并可以用来决定密码的强度。\n熵的单位是 比特。对于一个均匀分布的随机离散变量，熵等于log_2(所有可能的个数，即n)。 扔一次硬币的熵是1比特。掷一次（六面）骰子的熵大约为2.58比特。\n对称加密与非对称加密\n当你运行ssh-keygen命令，它会生成一个非对称密钥对：公钥和私钥(public_key, private_key)。 生成过程中使用的随机数由系统提供的熵决定。这些熵可以来源于硬件事件(hardware events)等。 公钥最终会被分发，它可以直接明文存储。 但是为了防止泄露，私钥必须加密存储。ssh-keygen命令会提示用户输入一个密码，并将它输入密钥生成函数 产生一个密钥。最终，ssh-keygen使用对称加密算法和这个密钥加密私钥。\n在实际运用中，当服务器已知用户的公钥（存储在.ssh/authorized_keys文件中，一般在用户HOME目录下），尝试连接的客户端可以使用非对称签名来证明用户的身份——这便是挑战应答方式。 简单来说，服务器选择一个随机数字发送给客户端。客户端使用用户私钥对这个数字信息签名后返回服务器。 服务器随后使用.ssh/authorized_keys文件中存储的用户公钥来验证返回的信息是否由所对应的私钥所签名。这种验证方式可以有效证明试图登录的用户持有所需的私钥。\n大杂烩 修改键位映射 作为一名程序员，键盘是你的主要输入工具。它像计算机里的其他部件一样是可配置的，而且值得你在这上面花时间。\n一个很常见的配置是修改键位映射。通常这个功能由在计算机上运行的软件实现。当某一个按键被按下，软件截获键盘发出的按键事件（keypress event）并使用另外一个事件取代。比如：\n将 Caps Lock 映射为 Ctrl 或者 Escape：Caps Lock 使用了键盘上一个非常方便的位置而它的功能却很少被用到，所以我们（讲师）非常推荐这个修改； 将 PrtSc 映射为播放/暂停：大部分操作系统支持播放/暂停键； 交换 Ctrl 和 Meta 键（Windows 的徽标键或者 Mac 的 Command 键）。 你也可以将键位映射为任意常用的指令。软件监听到特定的按键组合后会运行设定的脚本。\n打开一个新的终端或者浏览器窗口； 输出特定的字符串，比如：一个超长邮件地址或者 MIT ID； 使计算机或者显示器进入睡眠模式。 甚至更复杂的修改也可以通过软件实现：\n映射按键顺序，比如：按 Shift 键五下切换大小写锁定； 区别映射单点和长按，比如：单点 Caps Lock 映射为 Escape，而长按 Caps Lock 映射为 Ctrl； 对不同的键盘或软件保存专用的映射配置。 下面是一些修改键位映射的软件：\nmacOS - karabiner-elements, skhd 或者 BetterTouchTool Linux - xmodmap 或者 Autokey Windows - 控制面板，AutoHotkey 或者 SharpKeys QMK - 如果你的键盘支持定制固件，QMK 可以直接在键盘的硬件上修改键位映射。保留在键盘里的映射免除了在别的机器上的重复配置。 守护进程 即便守护进程（daemon）这个词看上去有些陌生，你应该已经大约明白它的概念。大部分计算机都有一系列在后台保持运行，不需要用户手动运行或者交互的进程。这些进程就是守护进程。以守护进程运行的程序名一般以 d 结尾，比如 SSH 服务端 sshd，用来监听传入的 SSH 连接请求并对用户进行鉴权。\nLinux 中的 systemd（the system daemon）是最常用的配置和运行守护进程的方法。运行 systemctl status 命令可以看到正在运行的所有守护进程。这里面有很多可能你没有见过，但是掌管了系统的核心部分的进程：管理网络、DNS解析、显示系统的图形界面等等。用户使用 systemctl 命令和 systemd 交互来enable（启用）、disable（禁用）、start（启动）、stop（停止）、restart（重启）、或者status（检查）配置好的守护进程及系统服务。\nsystemd 提供了一个很方便的界面用于配置和启用新的守护进程或系统服务。下面的配置文件使用了守护进程来运行一个简单的 Python 程序。文件的内容非常直接所以我们不对它详细阐述。systemd 配置文件的详细指南可参见 freedesktop.org。\n## /etc/systemd/system/myapp.service [Unit] ## 配置文件描述 Description=My Custom App ## 在网络服务启动后启动该进程 After=network.target [Service] ## 运行该进程的用户 User=foo ## 运行该进程的用户组 Group=foo ## 运行该进程的根目录 WorkingDirectory=/home/foo/projects/mydaemon ## 开始该进程的命令 ExecStart=/usr/bin/local/python3.7 app.py ## 在出现错误时重启该进程 Restart=on-failure [Install] ## 相当于Windows的开机启动。即使GUI没有启动，该进程也会加载并运行 WantedBy=multi-user.target ## 如果该进程仅需要在GUI活动时运行，这里应写作： # WantedBy=graphical.target # graphical.target在multi-user.target的基础上运行和GUI相关的服务 如果你只是想定期运行一些程序，可以直接使用 cron。它是一个系统内置的，用来执行定期任务的守护进程。\nFUSE 现在的软件系统一般由很多模块化的组件构建而成。你使用的操作系统可以通过一系列共同的方式使用不同的文件系统上的相似功能。比如当你使用 touch 命令创建文件的时候，touch 使用系统调用（system call）向内核发出请求。内核再根据文件系统，调用特有的方法来创建文件。这里的问题是，UNIX 文件系统在传统上是以内核模块的形式实现，导致只有内核可以进行文件系统相关的调用。\nFUSE（用户空间文件系统）允许运行在用户空间上的程序实现文件系统调用，并将这些调用与内核接口联系起来。在实践中，这意味着用户可以在文件系统调用中实现任意功能。\nFUSE 可以用于实现如：一个将所有文件系统操作都使用 SSH 转发到远程主机，由远程主机处理后返回结果到本地计算机的虚拟文件系统。这个文件系统里的文件虽然存储在远程主机，对于本地计算机上的软件而言和存储在本地别无二致。sshfs就是一个实现了这种功能的 FUSE 文件系统。\n一些有趣的 FUSE 文件系统包括：\nsshfs：使用 SSH 连接在本地打开远程主机上的文件 rclone：将 Dropbox、Google Drive、Amazon S3、或者 Google Cloud Storage 一类的云存储服务挂载为本地文件系统 gocryptfs：覆盖在加密文件上的文件系统。文件以加密形式保存在磁盘里，但该文件系统挂载后用户可以直接从挂载点访问文件的明文 kbfs：分布式端到端加密文件系统。在这个文件系统里有私密（private），共享（shared），以及公开（public）三种类型的文件夹 borgbackup：方便用户浏览删除重复数据后的压缩加密备份 备份 任何没有备份的数据都可能在一个瞬间永远消失。复制数据很简单，但是可靠地备份数据很难。下面列举了一些关于备份的基础知识，以及一些常见做法容易掉进的陷阱。\n首先，复制存储在同一个磁盘上的数据不是备份，因为这个磁盘是一个单点故障（single point of failure）。这个磁盘一旦出现问题，所有的数据都可能丢失。放在家里的外置磁盘因为火灾、抢劫等原因可能会和源数据一起丢失，所以是一个弱备份。推荐的做法是将数据备份到不同的地点存储。\n同步方案也不是备份。即使方便如 Dropbox 或者 Google Drive，当数据在本地被抹除或者损坏，同步方案可能会把这些“更改”同步到云端。同理，像 RAID 这样的磁盘镜像方案也不是备份。它不能防止文件被意外删除、损坏、或者被勒索软件加密。\n有效备份方案的几个核心特性是：版本控制，删除重复数据，以及安全性。对备份的数据实施版本控制保证了用户可以从任何记录过的历史版本中恢复数据。在备份中检测并删除重复数据，使其仅备份增量变化可以减少存储开销。在安全性方面，作为用户，你应该考虑别人需要有什么信息或者工具才可以访问或者完全删除你的数据及备份。最后一点，不要盲目信任备份方案。用户应该经常检查备份是否可以用来恢复数据。\n备份不限制于备份在本地计算机上的文件。云端应用的重大发展使得我们很多的数据只存储在云端。当我们无法登录这些应用，在云端存储的网络邮件，社交网络上的照片，流媒体音乐播放列表，以及在线文档等等都会随之丢失。用户应该有这些数据的离线备份，而且已经有项目可以帮助下载并存储它们。\n如果想要了解更多具体内容，请参考本课程2019年关于备份的课堂笔记。\nAPI（应用程序接口） 关于如何使用计算机有效率地完成 本地 任务，我们这堂课已经介绍了很多方法。这些方法在互联网上其实也适用。大多数线上服务提供的 API（应用程序接口）让你可以通过编程方式来访问这些服务的数据。比如，美国国家气象局就提供了一个可以从 shell 中获取天气预报的 API。\n这些 API 大多具有类似的格式。它们的结构化 URL 通常使用 api.service.com 作为根路径，用户可以访问不同的子路径来访问需要调用的操作，以及添加查询参数使 API 返回符合查询参数条件的结果。\n以美国天气数据为例，为了获得某个地点的天气数据，你可以发送一个 GET 请求（比如使用curl）到https://api.weather.gov/points/42.3604,-71.094。返回中会包括一系列用于获取特定信息（比如小时预报、气象观察站信息等）的 URL。通常这些返回都是JSON格式，你可以使用jq等工具来选取需要的部分。\n有些需要认证的 API 通常要求用户在请求中加入某种私密令牌（secret token）来完成认证。请阅读你想访问的 API 所提供的文档来确定它请求的认证方式，但是其实大多数 API 都会使用 OAuth。OAuth 通过向用户提供一系列仅可用于该 API 特定功能的私密令牌进行校验。因为使用了有效 OAuth 令牌的请求在 API 看来就是用户本人发出的请求，所以请一定保管好这些私密令牌。否则其他人就可以冒用你的身份进行任何你可以在这个 API 上进行的操作。\nIFTTT 这个网站可以将很多 API 整合在一起，让某 API 发生的特定事件触发在其他 API 上执行的任务。IFTTT 的全称If This Then That 足以说明它的用法，比如在检测到用户的新推文后，自动发布在其他平台。但是你可以对它支持的 API 进行任意整合，所以试着来设置一下任何你需要的功能吧！\n常见命令行标志参数及模式 命令行工具的用法千差万别，阅读 man 页面可以帮助你理解每种工具的用法。即便如此，下面我们将介绍一下命令行工具一些常见的共同功能。\n大部分工具支持 --help 或者类似的标志参数（flag）来显示它们的简略用法。\n会造成不可撤回操作的工具一般会提供“空运行”（dry run）标志参数，这样用户可以确认工具真实运行时会进行的操作。这些工具通常也会有“交互式”（interactive）标志参数，在执行每个不可撤回的操作前提示用户确认。\n--version 或者 -V 标志参数可以让工具显示它的版本信息（对于提交软件问题报告非常重要）。\n基本所有的工具支持使用 --verbose 或者 -v 标志参数来输出详细的运行信息。多次使用这个标志参数，比如 -vvv，可以让工具输出更详细的信息（经常用于调试）。同样，很多工具支持 --quiet 标志参数来抑制除错误提示之外的其他输出。\n大多数工具中，使用 - 代替输入或者输出文件名意味着工具将从标准输入（standard input）获取所需内容，或者向标准输出（standard output）输出结果。\n会造成破坏性结果的工具一般默认进行非递归的操作，但是支持使用“递归”（recursive）标志函数（通常是 -r）。\n有的时候你可能需要向工具传入一个 看上去 像标志参数的普通参数，比如：\n使用 rm 删除一个叫 -r 的文件； 在通过一个程序运行另一个程序的时候（ssh machine foo），向内层的程序（foo）传递一个标志参数。 这时候你可以使用特殊参数 -- 让某个程序 停止处理 -- 后面出现的标志参数以及选项（以 - 开头的内容）：\nrm -- -r 会让 rm 将 -r 当作文件名； ssh machine --for-ssh -- foo --for-foo 的 -- 会让 ssh 知道 --for-foo 不是 ssh 的标志参数。 窗口管理器 大部分人适应了 Windows、macOS、以及 Ubuntu 默认的“拖拽”式窗口管理器。这些窗口管理器的窗口一般就堆在屏幕上，你可以拖拽改变窗口的位置、缩放窗口、以及让窗口堆叠在一起。这种堆叠式（floating/stacking）管理器只是窗口管理器中的一种。特别在 Linux 中，有很多种其他的管理器。\n平铺式（tiling）管理器就是一个常见的替代。顾名思义，平铺式管理器会把不同的窗口像贴瓷砖一样平铺在一起而不和其他窗口重叠。这和 tmux 管理终端窗口的方式类似。平铺式管理器按照写好的布局显示打开的窗口。如果只打开一个窗口，它会填满整个屏幕。新开一个窗口的时候，原来的窗口会缩小到比如三分之二或者三分之一的大小来腾出空间。打开更多的窗口会让已有的窗口进一步调整。\n就像 tmux 那样，平铺式管理器可以让你在完全不使用鼠标的情况下使用键盘切换、缩放、以及移动窗口。它们值得一试！\nVPN VPN 现在非常火，但我们不清楚这是不是因为一些好的理由。你应该了解 VPN 能提供的功能和它的限制。使用了 VPN 的你对于互联网而言，最好的情况下也就是换了一个网络供应商（ISP）。所有你发出的流量看上去来源于 VPN 供应商的网络而不是你的“真实”地址，而你实际接入的网络只能看到加密的流量。\n虽然这听上去非常诱人，但是你应该知道使用 VPN 只是把原本对网络供应商的信任放在了 VPN 供应商那里——网络供应商 能看到的，VPN 供应商 也都能看到。如果相比网络供应商你更信任 VPN 供应商，那当然很好。反之，则连接VPN的价值不明确。机场的不加密公共热点确实不可以信任，但是在家庭网络环境里，这个差异就没有那么明显。\n你也应该了解现在大部分包含用户敏感信息的流量已经被 HTTPS 或者 TLS 加密。这种情况下你所处的网络环境是否“安全”不太重要：供应商只能看到你和哪些服务器在交谈，却不能看到你们交谈的内容。\n这一切的大前提都是“最好的情况”。曾经发生过 VPN 提供商错误使用弱加密或者直接禁用加密的先例。另外，有些恶意的或者带有投机心态的供应商会记录和你有关的所有流量，并很可能会将这些信息卖给第三方。找错一家 VPN 经常比一开始就不用 VPN 更危险。\nMIT 向有访问校内资源需求的成员开放自己运营的 VPN。如果你也想自己配置一个 VPN，可以了解一下 WireGuard 以及 Algo。\nMarkdown 你在职业生涯中大概率会编写各种各样的文档。在很多情况下这些文档需要使用标记来增加可读性，比如：插入粗体或者斜体内容，增加页眉、超链接、以及代码片段。\n在不使用 Word 或者 LaTeX 等复杂工具的情况下，你可以考虑使用 Markdown 这个轻量化的标记语言（markup language）。你可能已经见过 Markdown 或者它的一个变种。很多环境都支持并使用 Markdown 的一些子功能。\nMarkdown 致力于将人们编写纯文本时的一些习惯标准化。比如：\n用*包围的文字表示强调（斜体），或者用**表示特别强调（粗体）；\n以#开头的行是标题，#的数量表示标题的级别，比如：##二级标题；\n以-开头代表一个无序列表的元素。一个数字加.（比如1.）代表一个有序列表元素；\n反引号 `（backtick）包围的文字会以代码字体显示。如果要显示一段代码，可以在每一行前加四个空格缩进，或者使用三个反引号包围整个代码片段：\n就像这样 如果要添加超链接，将 需要显示 的文字用方括号包围，并在后面紧接着用圆括号包围链接：[显示文字](指向的链接)。\nMarkdown 不仅容易上手，而且应用非常广泛。实际上本课程的课堂笔记和其他资料都是使用 Markdown 编写的。点击这个链接可以看到本页面的原始 Markdown 内容。\nHammerspoon (macOS 桌面自动化) Hammerspoon 是面向 macOS 的一个桌面自动化框架。它允许用户编写和操作系统功能挂钩的 Lua 脚本，从而与键盘、鼠标、窗口、文件系统等交互。\n下面是 Hammerspoon 的一些示例应用：\n绑定移动窗口到的特定位置的快捷键 创建可以自动将窗口整理成特定布局的菜单栏按钮 在你到实验室以后，通过检测所连接的 WiFi 网络自动静音扬声器 在你不小心拿了朋友的充电器时弹出警告 从用户的角度，Hammerspoon 可以运行任意 Lua 代码，绑定菜单栏按钮、按键、或者事件。Hammerspoon 提供了一个全面的用于和系统交互的库，因此它能没有限制地实现任何功能。你可以从头编写自己的 Hammerspoon 配置，也可以结合别人公布的配置来满足自己的需求。\n资源 Getting Started with Hammerspoon：Hammerspoon 官方教程 Sample configurations：Hammerspoon 官方示例配置 Anish\u0026rsquo;s Hammerspoon config：Anish 的 Hammerspoon 配置 开机引导以及 Live USB 在你的计算机启动时，BIOS 或者 UEFI 会在加载操作系统之前对硬件系统进行初始化，这被称为引导（booting）。你可以通过按下计算机提示的键位组合来配置引导，比如 Press F9 to configure BIOS. Press F12 to enter boot menu。在 BIOS 菜单中你可以对硬件相关的设置进行更改，也可以在引导菜单中选择从硬盘以外的其他设备加载操作系统——比如 Live USB。\nLive USB 是包含了完整操作系统的闪存盘。Live USB 的用途非常广泛，包括：\n作为安装操作系统的启动盘； 在不将操作系统安装到硬盘的情况下，直接运行 Live USB 上的操作系统； 对硬盘上的相同操作系统进行修复； 恢复硬盘上的数据。 Live USB 通过在闪存盘上 写入 操作系统的镜像制作，而写入不是单纯的往闪存盘上复制 .iso 文件。你可以使用 UNetbootin 、Rufus 等 Live USB 写入工具制作。\nDocker, Vagrant, VMs, Cloud, OpenStack 虚拟机（Virtual Machine）以及如容器化（containerization）等工具可以帮助你模拟一个包括操作系统的完整计算机系统。虚拟机可以用于创建独立的测试或者开发环境，以及用作安全测试的沙盒。\nVagrant 是一个构建和配置虚拟开发环境的工具。它支持用户在配置文件中写入比如操作系统、系统服务、需要安装的软件包等描述，然后使用 vagrant up 命令在各种环境（VirtualBox，KVM，Hyper-V等）中启动一个虚拟机。Docker 是一个使用容器化概念的类似工具。\n租用云端虚拟机可以享受以下资源的即时访问：\n便宜、常开、且有公共IP地址的虚拟机用来托管网站等服务 有大量 CPU、磁盘、内存、以及 GPU 资源的虚拟机 超出用户可以使用的物理主机数量的虚拟机 相比物理主机的固定开支，虚拟机的开支一般按运行的时间计算。所以如果用户只需要在短时间内使用大量算力，租用1000台虚拟机运行几分钟明显更加划算。 受欢迎的 VPS 服务商有 Amazon AWS，Google Cloud，以及 DigitalOcean。\nMIT CSAIL 的成员可以使用 CSAIL OpenStack instance 申请免费的虚拟机用于研究。\n交互式记事本编程 交互式记事本可以帮助开发者进行与运行结果交互等探索性的编程。现在最受欢迎的交互式记事本环境大概是 Jupyter。它的名字来源于所支持的三种核心语言：Julia、Python、R。Wolfram Mathematica 是另外一个常用于科学计算的优秀环境。\nGitHub GitHub 是最受欢迎的开源软件开发平台之一。我们课程中提到的很多工具，从 vim 到 Hammerspoon，都托管在 Github 上。向你每天使用的开源工具作出贡献其实很简单，下面是两种贡献者们经常使用的方法：\n创建一个议题（issue）。 议题可以用来反映软件运行的问题或者请求新的功能。创建议题并不需要创建者阅读或者编写代码，所以它是一个轻量化的贡献方式。高质量的问题报告对于开发者十分重要。在现有的议题发表评论也可以对项目的开发作出贡献。 使用拉取请求（pull request）提交代码更改。由于涉及到阅读和编写代码，提交拉取请求总的来说比创建议题更加深入。拉取请求是请求别人把你自己的代码拉取（且合并）到他们的仓库里。很多开源项目仅允许认证的管理者管理项目代码，所以一般需要复刻（fork）这些项目的上游仓库（upstream repository），在你的 Github 账号下创建一个内容完全相同但是由你控制的复刻仓库。这样你就可以在这个复刻仓库自由创建新的分支并推送修复问题或者实现新功能的代码。完成修改以后再回到开源项目的 Github 页面创建一个拉取请求。 提交请求后，项目管理者会和你交流拉取请求里的代码并给出反馈。如果没有问题，你的代码会和上游仓库中的代码合并。很多大的开源项目会提供贡献指南，容易上手的议题，甚至专门的指导项目来帮助参与者熟悉这些项目。\n参考 课程列表\n","permalink":"https://blog.niuhemoon.win/posts/tech/coding-tools-foobar/","summary":"安全和密码 熵(Entropy) 度量了不确定性并可以用来决定密码的强度。 熵的单位是 比特。对于一个均匀分布的随机离散变量，熵等于log_2(所有可能的个数，即n)。 扔一次硬币的熵是1比特。掷一次（六面）骰子的熵大约为2.58比特。 对称加密与非对称加密 当你运行ssh-keygen命令，","title":"编程工具之杂烩"},{"content":"代码不能完全按照您的想法运行，它只能完全按照您的写法运行，这是编程界的一条金科玉律。\n让您的写法符合您的想法是非常困难的。在这节课中，我们会传授给您一些非常有用技术，帮您处理代码中的 bug 和程序性能问题。\n调试代码 打印调试法与日志 \u0026ldquo;最有效的 debug 工具就是细致的分析，配合恰当位置的打印语句\u0026rdquo; — Brian Kernighan, Unix 新手入门。\n调试代码的第一种方法往往是在您发现问题的地方添加一些打印语句，然后不断重复此过程直到您获取了足够的信息并找到问题的根本原因。\n另外一个方法是使用日志，而不是临时添加打印语句。日志较普通的打印语句有如下的一些优势：\n您可以将日志写入文件、socket 或者甚至是发送到远端服务器而不仅仅是标准输出； 日志可以支持严重等级（例如 INFO, DEBUG, WARN, ERROR等)，这使您可以根据需要过滤日志； 对于新发现的问题，很可能您的日志中已经包含了可以帮助您定位问题的足够的信息。 这里 是一个包含日志的例程序：\n$ python logger.py # Raw output as with just prints $ python logger.py log # Log formatted output $ python logger.py log ERROR # Print only ERROR levels and above $ python logger.py color # Color formatted output 有很多技巧可以使日志的可读性变得更好，我最喜欢的一个是技巧是对其进行着色。到目前为止，您应该已经知道，以彩色文本显示终端信息时可读性更好。但是应该如何设置呢？\nls 和 grep 这样的程序会使用 ANSI escape codes，它是一系列的特殊字符，可以使您的 shell 改变输出结果的颜色。例如，执行 echo -e \u0026quot;\\e[38;2;255;0;0mThis is red\\e[0m\u0026quot; 会打印红色的字符串：This is red 。下面这个脚本向您展示了如何在终端中打印多种颜色。\n#!/usr/bin/env bash for R in $(seq 0 20 255); do for G in $(seq 0 20 255); do for B in $(seq 0 20 255); do printf \u0026#34;\\e[38;2;${R};${G};${B}m█\\e[0m\u0026#34;; done done done 第三方日志系统 如果您正在构建大型软件系统，您很可能会使用到一些依赖，有些依赖会作为程序单独运行。如 Web 服务器、数据库或消息代理都是此类常见的第三方依赖。\n和这些系统交互的时候，阅读它们的日志是非常必要的，因为仅靠客户端侧的错误信息可能并不足以定位问题。\n幸运的是，大多数的程序都会将日志保存在您的系统中的某个地方。对于 UNIX 系统来说，程序的日志通常存放在 /var/log。例如， NGINX web 服务器就将其日志存放于/var/log/nginx。\n目前，系统开始使用 system log，您所有的日志都会保存在这里。大多数的（但不是全部）Linux 系统都会使用 systemd，这是一个系统守护进程，它会控制您系统中的很多东西，例如哪些服务应该启动并运行。systemd 会将日志以某种特殊格式存放于/var/log/journal，您可以使用 journalctl 命令显示这些消息。\n类似地，在 macOS 系统中是 /var/log/system.log，但是有更多的工具会使用系统日志，它的内容可以使用 log show 显示。\n对于大多数的 UNIX 系统，您也可以使用dmesg 命令来读取内核的日志。\n如果您希望将日志加入到系统日志中，您可以使用 logger 这个 shell 程序。下面这个例子显示了如何使用 logger并且如何找到能够将其存入系统日志的条目。\n不仅如此，大多数的编程语言都支持向系统日志中写日志。\nlogger \u0026#34;Hello Logs\u0026#34; # On macOS log show --last 1m | grep Hello # On Linux journalctl --since \u0026#34;1m ago\u0026#34; | grep Hello 正如我们在数据整理那节课上看到的那样，日志的内容可以非常的多，我们需要对其进行处理和过滤才能得到我们想要的信息。\n如果您发现您需要对 journalctl 和 log show 的结果进行大量的过滤，那么此时可以考虑使用它们自带的选项对其结果先过滤一遍再输出。还有一些像 lnav 这样的工具，它为日志文件提供了更好的展现和浏览方式。\n调试器 当通过打印已经不能满足您的调试需求时，您应该使用调试器。\n调试器是一种可以允许我们和正在执行的程序进行交互的程序，它可以做到：\n当到达某一行时将程序暂停； 一次一条指令地逐步执行程序； 程序崩溃后查看变量的值； 满足特定条件是暂停程序； 其他高级功能。 很多编程语言都有自己的调试器。Python 的调试器是pdb.\n下面对pdb 支持对命令进行简单对介绍：\nl(ist) - 显示当前行附近的11行或继续执行之前的显示； s(tep) - 执行当前行，并在第一个可能的地方停止 n(ext) - 继续执行直到当前函数的下一条语句或者 return 语句； b(reak) - 设置断点（基于传入对参数）； p(rint) - 在当前上下文对表达式求值并打印结果。还有一个命令是pp ，它使用 pprint 打印； r(eturn) - 继续执行直到当前函数返回； q(uit) - 退出调试器。 让我们使用pdb 来修复下面的 Python 代码（参考讲座视频）\ndef bubble_sort(arr): n = len(arr) for i in range(n): for j in range(n): if arr[j] \u0026gt; arr[j+1]: arr[j] = arr[j+1] arr[j+1] = arr[j] return arr print(bubble_sort([4, 2, 1, 8, 7, 6])) 注意，因为 Python 是一种解释型语言，所以我们可以通过 pdb shell 执行命令。 ipdb 是一种增强型的 pdb ，它使用IPython 作为 REPL并开启了 tab 补全、语法高亮、更好的回溯和更好的内省，同时还保留了pdb 模块相同的接口。\n对于更底层的编程语言，您可能需要了解一下 gdb ( 以及它的改进版 pwndbg) 和 lldb。\n它们都对类 C 语言的调试进行了优化，它允许您探索任意进程及其机器状态：寄存器、堆栈、程序计数器等。\n专门工具 即使您需要调试的程序是一个二进制的黑盒程序，仍然有一些工具可以帮助到您。当您的程序需要执行一些只有操作系统内核才能完成的操作时，它需要使用 系统调用。有一些命令可以帮助您追踪您的程序执行的系统调用。在 Linux 中可以使用strace ，在 macOS 和 BSD 中可以使用 dtrace。dtrace 用起来可能有些别扭，因为它使用的是它自有的 D 语言，但是我们可以使用一个叫做 dtruss 的封装使其具有和 strace (更多信息参考 这里)类似的接口\n下面的例子展现来如何使用 strace 或 dtruss 来显示ls 执行时，对stat 系统调用进行追踪对结果。若需要深入了解 strace，这篇文章 值得一读。\n# On Linux sudo strace -e lstat ls -l \u0026gt; /dev/null 4 # On macOS sudo dtruss -t lstat64_extended ls -l \u0026gt; /dev/null 有些情况下，我们需要查看网络数据包才能定位问题。像 tcpdump 和 Wireshark 这样的网络数据包分析工具可以帮助您获取网络数据包的内容并基于不同的条件进行过滤。\n对于 web 开发， Chrome/Firefox 的开发者工具非常方便，功能也很强大：\n源码 -查看任意站点的 HTML/CSS/JS 源码； 实时地修改 HTML, CSS, JS 代码 - 修改网站的内容、样式和行为用于测试（从这一点您也能看出来，网页截图是不可靠的）； Javascript shell - 在 JS REPL中执行命令； 网络 - 分析请求的时间线； 存储 - 查看 Cookies 和本地应用存储。 静态分析 有些问题是您不需要执行代码就能发现的。例如，仔细观察一段代码，您就能发现某个循环变量覆盖了某个已经存在的变量或函数名；或是有个变量在被读取之前并没有被定义。 这种情况下 静态分析 工具就可以帮我们找到问题。静态分析会将程序的源码作为输入然后基于编码规则对其进行分析并对代码的正确性进行推理。\n下面这段 Python 代码中存在几个问题。 首先，我们的循环变量foo 覆盖了之前定义的函数foo。最后一行，我们还把 bar 错写成了baz，因此当程序完成sleep (一分钟后)后，执行到这一行的时候便会崩溃。\nimport time def foo(): return 42 for foo in range(5): print(foo) bar = 1 bar *= 0.2 time.sleep(60) print(baz) 静态分析工具可以发现此类的问题。当我们使用pyflakes 分析代码的似乎，我们会得到与这两处 bug 相关的错误信息。mypy 则是另外一个工具，它可以对代码进行类型检查。这里，mypy 会经过我们bar 起初是一个 int ，然后变成了 float。这些问题都可以在不允许代码的情况下被发现。\n在 shell 工具那一节课的时候，我们介绍了 shellcheck，这是一个类似的工具，但它是应用于 shell 脚本的。\n$ pyflakes foobar.py foobar.py:6: redefinition of unused \u0026#39;foo\u0026#39; from line 3 foobar.py:11: undefined name \u0026#39;baz\u0026#39; $ mypy foobar.py foobar.py:6: error: Incompatible types in assignment (expression has type \u0026#34;int\u0026#34;, variable has type \u0026#34;Callable[[], Any]\u0026#34;) foobar.py:9: error: Incompatible types in assignment (expression has type \u0026#34;float\u0026#34;, variable has type \u0026#34;int\u0026#34;) foobar.py:11: error: Name \u0026#39;baz\u0026#39; is not defined Found 3 errors in 1 file (checked 1 source file) 大多数的编辑器和 IDE 都支持在编辑界面显示这些工具的分析结果、高亮有警告和错误的位置。 这个过程通常成为 code linting 。风格检查或安全检查的结果同样也可以进行相应的显示。\n在 vim 中，有 ale 或 syntastic 可以帮助您做同样的事情。 在 Python 中， pylint 和 pep8 是两种用于进行风格检查的工具，而 bandit 工具则用于检查安全相关的问题。\n对于其他语言的开发者来说，静态分析工具可以参考这个列表：Awesome Static Analysis (您也许会对 Writing 一节感兴趣) 。对于 linters 则可以参考这个列表： Awesome Linters。\n对于风格检查和代码格式化，还有以下一些工具可以作为补充：用于 Python 的 black、用于 Go 语言的 gofmt、用于 Rust 的 rustfmt 或是用于 JavaScript, HTML 和 CSS 的 prettier 。这些工具可以自动格式化您的代码，这样代码风格就可以与常见的风格保持一致。 尽管您可能并不想对代码进行风格控制，标准的代码风格有助于方便别人阅读您的代码，也可以方便您阅读它的代码。\n性能分析 即使您的代码能够向您期望的一样运行，但是如果它消耗了您全部的 CPU 和内存，那么它显然也不是个好程序。算法课上我们通常会介绍大O标记法，但却没交给我们如何找到程序中的热点。 鉴于 过早的优化是万恶之源，您需要学习性能分析和监控工具，它们会帮助您找到程序中最耗时、最耗资源的部分，这样您就可以有针对性的进行性能优化。\n计时 和调试代码类似，大多数情况下我们只需要打印两处代码之间的时间即可发现问题。下面这个例子中，我们使用了 Python 的 time模块。\nimport time, random n = random.randint(1, 10) * 100 ### 获取当前时间 start = time.time() ### 执行一些操作 print(\u0026#34;Sleeping for {} ms\u0026#34;.format(n)) time.sleep(n/1000) ### 比较当前时间和起始时间 print(time.time() - start) # Output # Sleeping for 500 ms ### 0.5713930130004883 不过，执行时间（wall clock time）也可能会误导您，因为您的电脑可能也在同时运行其他进程，也可能在此期间发生了等待。 对于工具来说，需要区分真实时间、用户时间和系统时间。通常来说，用户时间+系统时间代表了您的进程所消耗的实际 CPU （更详细的解释可以参照这篇文章）。\n真实时间 - 从程序开始到结束流失掉到真实时间，包括其他进程到执行时间以及阻塞消耗的时间（例如等待 I/O或网络）； User - CPU 执行用户代码所花费的时间； Sys - CPU 执行系统内核代码所花费的时间。 例如，试着执行一个用于发起 HTTP 请求的命令并在其前面添加 time 前缀。网络不好的情况下您可能会看到下面的输出结果。请求花费了 2s 才完成，但是进程仅花费了 15ms 的 CPU 用户时间和 12ms 的 CPU 内核时间。\n$ time curl https://missing.csail.mit.edu \u0026amp;\u0026gt; /dev/null` real 0m2.561s user 0m0.015s sys 0m0.012s 性能分析工具（profilers） CPU 大多数情况下，当人们提及性能分析工具的时候，通常指的是 CPU 性能分析工具。 CPU 性能分析工具有两种： 追踪分析器（tracing）及采样分析器（sampling）。 追踪分析器 会记录程序的每一次函数调用，而采样分析器则只会周期性的监测（通常为每毫秒）您的程序并记录程序堆栈。它们使用这些记录来生成统计信息，显示程序在哪些事情上花费了最多的时间。如果您希望了解更多相关信息，可以参考这篇 介绍性的文章。\n大多数的编程语言都有一些基于命令行都分析器，我们可以使用它们来分析代码。它们通常可以集成在 IDE 中，但是本节课我们会专注于这些命令行工具本身。\n在 Python 中，我们使用 cProfile 模块来分析每次函数调用所消耗都时间。 在下面的例子中，我们实现了一个基础的 grep 命令：\n#!/usr/bin/env python import sys, re def grep(pattern, file): with open(file, \u0026#39;r\u0026#39;) as f: print(file) for i, line in enumerate(f.readlines()): pattern = re.compile(pattern) match = pattern.search(line) if match is not None: print(\u0026#34;{}: {}\u0026#34;.format(i, line), end=\u0026#34;\u0026#34;) if __name__ == \u0026#39;__main__\u0026#39;: times = int(sys.argv[1]) pattern = sys.argv[2] for i in range(times): for file in sys.argv[3:]: grep(pattern, file) 我们可以使用下面的命令来对这段代码进行分析。通过它的输出我们可以直到，IO 消耗了大量的时间，编译正则表达式也比较耗费时间。因为正则表达式只需要编译一次，我们可以将其移动到 for 循环外面来改进性能。\n$ python -m cProfile -s tottime grep.py 1000 \u0026#39;^(import|\\s*def)[^,]*$\u0026#39; *.py [omitted program output] ncalls tottime percall cumtime percall filename:lineno(function) 8000 0.266 0.000 0.292 0.000 {built-in method io.open} 8000 0.153 0.000 0.894 0.000 grep.py:5(grep) 17000 0.101 0.000 0.101 0.000 {built-in method builtins.print} 8000 0.100 0.000 0.129 0.000 {method \u0026#39;readlines\u0026#39; of \u0026#39;_io._IOBase\u0026#39; objects} 93000 0.097 0.000 0.111 0.000 re.py:286(_compile) 93000 0.069 0.000 0.069 0.000 {method \u0026#39;search\u0026#39; of \u0026#39;_sre.SRE_Pattern\u0026#39; objects} 93000 0.030 0.000 0.141 0.000 re.py:231(compile) 17000 0.019 0.000 0.029 0.000 codecs.py:318(decode) 1 0.017 0.017 0.911 0.911 grep.py:3(\u0026lt;module\u0026gt;) [omitted lines] 关于 Python 的 cProfile 分析器（以及其他一些类似的一些分析器），需要注意的是它显示的是每次函数调用的时间。看上去可能快到反直觉，尤其是如果您在代码里面使用了第三方的函数库，因为内部函数调用也会被看作函数调用。\n更加符合直觉的显示分析信息的方式是包括每行代码的执行时间，这也是行分析器的工作。例如，下面这段 Python 代码会向本课程的网站发起一个请求，然后解析响应返回的页面中的全部 URL：\n#!/usr/bin/env python import requests from bs4 import BeautifulSoup ### 这个装饰器会告诉行分析器 ### 我们想要分析这个函数 @profile def get_urls(): response = requests.get(\u0026#39;https://missing.csail.mit.edu\u0026#39;) s = BeautifulSoup(response.content, \u0026#39;lxml\u0026#39;) urls = [] for url in s.find_all(\u0026#39;a\u0026#39;): urls.append(url[\u0026#39;href\u0026#39;]) if __name__ == \u0026#39;__main__\u0026#39;: get_urls() 如果我们使用 Python 的 cProfile 分析器，我们会得到超过2500行的输出结果，即使对其进行排序，我仍然搞不懂时间到底都花在哪了。如果我们使用 line_profiler，它会基于行来显示时间：\n$ kernprof -l -v a.py Wrote profile results to urls.py.lprof Timer unit: 1e-06 s Total time: 0.636188 s File: a.py Function: get_urls at line 5 Line ### Hits Time Per Hit % Time Line Contents ============================================================== 5 @profile 6 def get_urls(): 7 1 613909.0 613909.0 96.5 response = requests.get(\u0026#39;https://missing.csail.mit.edu\u0026#39;) 8 1 21559.0 21559.0 3.4 s = BeautifulSoup(response.content, \u0026#39;lxml\u0026#39;) 9 1 2.0 2.0 0.0 urls = [] 10 25 685.0 27.4 0.1 for url in s.find_all(\u0026#39;a\u0026#39;): 11 24 33.0 1.4 0.0 urls.append(url[\u0026#39;href\u0026#39;]) 内存 像 C 或者 C++ 这样的语言，内存泄漏会导致您的程序在使用完内存后不去释放它。为了应对内存类的 Bug，我们可以使用类似 Valgrind 这样的工具来检查内存泄漏问题。\n对于 Python 这类具有垃圾回收机制的语言，内存分析器也是很有用的，因为对于某个对象来说，只要有指针还指向它，那它就不会被回收。\n下面这个例子及其输出，展示了 memory-profiler 是如何工作的（注意装饰器和 line-profiler 类似）。\n@profile def my_func(): a = [1] * (10 ** 6) b = [2] * (2 * 10 ** 7) del b return a if __name__ == \u0026#39;__main__\u0026#39;: my_func() $ python -m memory_profiler example.py Line ### Mem usage Increment Line Contents ============================================== 3 @profile 4 5.97 MB 0.00 MB def my_func(): 5 13.61 MB 7.64 MB a = [1] * (10 ** 6) 6 166.20 MB 152.59 MB b = [2] * (2 * 10 ** 7) 7 13.61 MB -152.59 MB del b 8 13.61 MB 0.00 MB return a 事件分析 在我们使用strace调试代码的时候，您可能会希望忽略一些特殊的代码并希望在分析时将其当作黑盒处理。perf 命令将 CPU 的区别进行了抽象，它不会报告时间和内存的消耗，而是报告与您的程序相关的系统事件。\n例如，perf 可以报告不佳的缓存局部性（poor cache locality）、大量的页错误（page faults）或活锁（livelocks）。下面是关于常见命令的简介：\nperf list - 列出可以被 pref 追踪的事件； perf stat COMMAND ARG1 ARG2 - 收集与某个进程或指令相关的事件； perf record COMMAND ARG1 ARG2 - 记录命令执行的采样信息并将统计数据储存在perf.data中； perf report - 格式化并打印 perf.data 中的数据。 可视化 使用分析器来分析真实的程序时，由于软件的复杂性，其输出结果中将包含大量的信息。人类是一种视觉动物，非常不善于阅读大量的文字。因此很多工具都提供了可视化分析器输出结果的功能。\n对于采样分析器来说，常见的显示 CPU 分析数据的形式是 火焰图，火焰图会在 Y 轴显示函数调用关系，并在 X 轴显示其耗时的比例。火焰图同时还是可交互的，您可以深入程序的某一具体部分，并查看其栈追踪（您可以尝试点击下面的图片）。\n调用图和控制流图可以显示子程序之间的关系，它将函数作为节点并把函数调用作为边。将它们和分析器的信息（例如调用次数、耗时等）放在一起使用时，调用图会变得非常有用，它可以帮助我们分析程序的流程。 在 Python 中您可以使用 pycallgraph 来生成这些图片。\n资源监控 有时候，分析程序性能的第一步是搞清楚它所消耗的资源。程序变慢通常是因为它所需要的资源不够了。例如，没有足够的内存或者网络连接变慢的时候。\n有很多很多的工具可以被用来显示不同的系统资源，例如 CPU 占用、内存使用、网络、磁盘使用等。\n通用监控 - 最流行的工具要数 htop,了，它是 top的改进版。htop 可以显示当前运行进程的多种统计信息。htop 有很多选项和快捷键，常见的有：\u0026lt;F6\u0026gt; 进程排序、 t 显示树状结构和 h 打开或折叠线程。 还可以留意一下 glances ，它的实现类似但是用户界面更好。如果需要合并测量全部的进程， dstat 是也是一个非常好用的工具，它可以实时地计算不同子系统资源的度量数据，例如 I/O、网络、 CPU 利用率、上下文切换等等； I/O 操作 - iotop 可以显示实时 I/O 占用信息而且可以非常方便地检查某个进程是否正在执行大量的磁盘读写操作； 磁盘使用 - df 可以显示每个分区的信息，而 du 则可以显示当前目录下每个文件的磁盘使用情况（ disk usage）。-h 选项可以使命令使用对人类（human）更加友好的格式显示数据；ncdu是一个交互性更好的 du ，它可以让您在不同目录下导航、删除文件和文件夹； 内存使用 - free 可以显示系统当前空闲的内存。内存也可以使用 htop 这样的工具来显示； 打开文件 - lsof 可以列出被进程打开的文件信息。 当我们需要查看某个文件是被哪个进程打开的时候，这个命令非常有用；. 网络连接和配置 - ss le帮助我们监控网络包的收发情况以及网络接口的显示信息。ss 常见的一个使用场景是找到端口被进程占用的信息。如果要显示路由、网络设备和接口信息，您可以使用 ip 命令。注意，netstat 和 ifconfig 这两个命令已经被前面那些工具所代替了。 网络使用 - nethogs 和 iftop 是非常好的用于对网络占用进行监控的交互式命令行工具。 如果您希望测试一下这些工具，您可以使用 stress 命令来为系统人为地增加负载。\n专用工具 有时候，您只需要对黑盒程序进行基准测试，并依此对软件选择进行评估。 类似 hyperfine 这样的命令行可以帮您快速进行基准测试。例如，我们在 shell 工具和脚本那一节课中我们推荐使用 fd 来代替 find。我们这里可以用hyperfine来比较一下它们。\n例如，下面的例子中，我们可以看到fd 比 find 要快20倍。\n$ hyperfine --warmup 3 \u0026#39;fd -e jpg\u0026#39; \u0026#39;find . -iname \u0026#34;*.jpg\u0026#34;\u0026#39; Benchmark #1: fd -e jpg Time (mean ± σ): 51.4 ms ± 2.9 ms [User: 121.0 ms, System: 160.5 ms] Range (min … max): 44.2 ms … 60.1 ms 56 runs Benchmark #2: find . -iname \u0026#34;*.jpg\u0026#34; Time (mean ± σ): 1.126 s ± 0.101 s [User: 141.1 ms, System: 956.1 ms] Range (min … max): 0.975 s … 1.287 s 10 runs Summary \u0026#39;fd -e jpg\u0026#39; ran 21.89 ± 2.33 times faster than \u0026#39;find . -iname \u0026#34;*.jpg\u0026#34;\u0026#39; 和 debug 一样，浏览器也包含了很多不错的性能分析工具，可以用来分析页面加载，让我们可以搞清楚时间都消耗在什么地方（加载、渲染、脚本等等）。 更多关于 Firefox 和 Chrome的信息可以点击链接。\n课后练习 调试 使用 Linux 上的 journalctl 或 macOS 上的 log show 命令来获取最近一天中超级用户的登陆信息及其所执行的指令。如果找不到相关信息，您可以执行一些无害的命令，例如sudo ls 然后再次查看。\n学习 这份 pdb 实践教程并熟悉相关的命令。更深入的信息您可以参考这份教程。\n安装 shellcheck 并尝试对下面的脚本进行检查。这段代码有什么问题吗？请修复相关问题。在您的编辑器中安装一个linter插件，这样它就可以自动地显示相关警告信息。\n#!/bin/sh ## Example: a typical script with several problems for f in $(ls *.m3u) do grep -qi hq.*mp3 $f \\ \u0026amp;\u0026amp; echo -e \u0026#39;Playlist $f contains a HQ file in mp3 format\u0026#39; done (进阶题) 请阅读 可逆调试 并尝试创建一个可以工作的例子（使用 rr 或 RevPDB）。\n性能分析 这里 有一些排序算法的实现。请使用 cProfile 和 line_profiler 来比较插入排序和快速排序的性能。两种算法的瓶颈分别在哪里？然后使用 memory_profiler 来检查内存消耗，为什么插入排序更好一些？然后在看看原地排序版本的快排。附加题：使用 perf 来查看不同算法的循环次数及缓存命中及丢失情况。\n这里有一些用于计算斐波那契数列 Python 代码，它为计算每个数字都定义了一个函数：\n#!/usr/bin/env python def fib0(): return 0 def fib1(): return 1 s = \u0026#34;\u0026#34;\u0026#34;def fib{}(): return fib{}() + fib{}()\u0026#34;\u0026#34;\u0026#34; if __name__ == \u0026#39;__main__\u0026#39;: for n in range(2, 10): exec(s.format(n, n-1, n-2)) # from functools import lru_cache # for n in range(10): ### exec(\u0026#34;fib{} = lru_cache(1)(fib{})\u0026#34;.format(n, n)) print(eval(\u0026#34;fib9()\u0026#34;)) 将代码拷贝到文件中使其变为一个可执行的程序。安装 pycallgraph。并使用 pycallgraph graphviz -- ./fib.py 来执行代码并查看pycallgraph.png 这个文件。fib0 被调用了多少次？我们可以通过？我们可以通过记忆法来对其进行优化。将注释掉的部分放开，然后重新生成图片。这回每个fibN 函数被调用了多少次？\n我们经常会遇到的情况是某个我们希望去监听的端口已经被其他进程占用了。让我们通过进程的PID查找相应的进程。首先执行 python -m http.server 4444 启动一个最简单的 web 服务器来监听 4444 端口。在另外一个终端中，执行 lsof | grep LISTEN 打印出所有监听端口的进程及相应的端口。找到对应的 PID 然后使用 kill \u0026lt;PID\u0026gt; 停止该进程。\n限制进程资源也是一个非常有用的技术。执行 stress -c 3 并使用htop 对 CPU 消耗进行可视化。现在，执行taskset --cpu-list 0,2 stress -c 3 并可视化。stress 占用了3个 CPU 吗？为什么没有？阅读man taskset来寻找答案。附加题：使用 cgroups来实现相同的操作，尝试使用stress -m来限制内存使用\n(进阶题) curl ipinfo.io 命令或执行 HTTP 请求并获取关于您 IP 的信息。打开 Wireshark 并抓取 curl 发起的请求和收到的回复报文。（提示：可以使用http进行过滤，只显示 HTTP 报文）\n参考 课程列表\n","permalink":"https://blog.niuhemoon.win/posts/tech/coding-tools-profile/","summary":"代码不能完全按照您的想法运行，它只能完全按照您的写法运行，这是编程界的一条金科玉律。 让您的写法符合您的想法是非常困难的。在这节课中，我们会传授给您一些非常有用技术，帮您处理代码中的 bug 和程序性能问题。 调试代码 打印调试法与日志 \u0026ldquo;最有效的 debug 工具就是细致的分析，配合恰当位置的","title":"编程工具之调试和性能分析"},{"content":"Angular 开发环境 Angular 依赖于 Nodejs，需要首先安装 node 和 npm,安装完成后可以查看 node 和 npm 的版本 然后再安装 angular 依赖\nnode -v npm -v # npm换源 npm config set registry https://registry.npm.taobao.org # 查看npm的源 npm config get registry # 安装angular npm install -g @angular/cli # 更新所有套件到最新版 ng update --all --force # 创建一个angular项目 ng new testng StackBlitz是一个在线开发环境，可以快速上手开发．\n进入 testng 项目，其目录结构如下\n├── angular.json\t# angular配置文件 ├── e2e │ ├── protractor.conf.js │ ├── src │ │ ├── app.e2e-spec.ts\t# e2e 端对端测试目录 │ │ └── app.po.ts │ └── tsconfig.json ├── karma.conf.js\t# 执行自动化测试的 ├── node_modules\t# npm包安装目录，使用npm install安装 ├── package.json\t# npm 工具的配置文件，指明了当前这个应用所要用到的模块，Angular 的依赖包 ├── README.md ├── src │ ├── app │ │ ├── app.component.css\t# 项目主样式 │ │ ├── app.component.html\t# 项目的主模板 │ │ ├── app.component.spec.ts\t# 项目单元测试 │ │ ├── app.component.ts\t# 项目的主组件，定义AppModule，这个根模块会告诉Angular如何组装该应用 │ │ ├── app.module.ts\t# 项目主模块 │ │ └── app-routing.module.ts │ ├── assets\t# 静态资源如图片、翻译语言，构建时自动打包 │ ├── environments │ │ ├── environment.prod.ts\t# 生产环境配置 │ │ └── environment.ts\t# 开发环境配置，默认 │ ├── favicon.ico\t# 图标 │ ├── index.html\t# 宿主页面 │ ├── main.ts\t# 整个web应用的入口点，angular通过该项目来启动整个项目 │ ├── polyfills.ts\t# 导入一些必要的库,使得angular可以在一些老版本的浏览器中运行 │ ├── styles.css\t# 公共样式 │ └── test.ts\t# 单元测试入口点 ├── tsconfig.app.json ├── tsconfig.json\t# TypeScript 编译器的参数 ├── tsconfig.spec.json ├── typings.json\t# 为那些 TypeScript 编译器无法识别的库提供了额外的定义文件。 └── tslint.json\t# 定义 ts 文件质量检查的一些规则 参考 Angular 的 8 个核心概念\nAngular4 教程\n","permalink":"https://blog.niuhemoon.win/posts/tech/angular-devenv/","summary":"Angular 开发环境 Angular 依赖于 Nodejs，需要首先安装 node 和 npm,安装完成后可以查看 node 和 npm 的版本 然后再安装 angular 依赖 node -v npm -v # npm换源 npm config set registry https://registry.npm.taobao.org # 查看npm的源 npm config get registry # 安装angular npm install -g @angular/cli # 更新所有套件到最新版 ng update --all --force # 创建一个angular项目 ng new testng StackBlitz是一个在线开","title":"Angular开发环境搭建"},{"content":"Python种基本类型的比较：\nList is a collection which is ordered and mutable. Allows duplicate members. Tuple is a collection which is ordered and immutable. Allows duplicate members. Set is a collection which is unordered and unindexed. No duplicate members. Dictionary is a collection which is unordered, mutable and indexed. No duplicate members. Strings are immutable sequences of Unicode code points. List Python中的list是一个有序容器，容纳不同类型的数据（但推荐列表内数据类型相同)，同时其是可变类型的．\n创建列表 list_1 = [\u0026#34;banana\u0026#34;, \u0026#34;cherry\u0026#34;, \u0026#34;apple\u0026#34;] print(list_1) # Or create an empty list with the list function list_2 = list() print(list_2) # Lists allow different data types list_3 = [5, True, \u0026#34;apple\u0026#34;] print(list_3) # Lists allow duplicates list_4 = [0, 0, 1, 1] print(list_4) 内置方法 修改列表的方法尽量使用内置方法，内置方法效率较高\nmy_list = [\u0026#34;banana\u0026#34;, \u0026#34;cherry\u0026#34;, \u0026#34;apple\u0026#34;] # len() : get the number of elements in a list print(\u0026#34;Length:\u0026#34;, len(my_list)) # append() : adds an element to the end of the list my_list.append(\u0026#34;orange\u0026#34;) # insert() : adds an element at the specified position my_list.insert(1, \u0026#34;blueberry\u0026#34;) print(my_list) # pop() : removes and returns the item at the given position, default is the last item item = my_list.pop() print(\u0026#34;Popped item: \u0026#34;, item) # remove() : removes an item from the list my_list.remove(\u0026#34;cherry\u0026#34;) # Value error if not in the list print(my_list) # clear() : removes all items from the list my_list.clear() print(my_list) # reverse() : reverse the items my_list = [\u0026#34;banana\u0026#34;, \u0026#34;cherry\u0026#34;, \u0026#34;apple\u0026#34;] my_list.reverse() print(\u0026#39;Reversed: \u0026#39;, my_list) # sort() : sort items in ascending order my_list.sort() print(\u0026#39;Sorted: \u0026#39;, my_list) # use sorted() to get a new list, and leave the original unaffected. # sorted() works on any iterable type, not just lists my_list = [\u0026#34;banana\u0026#34;, \u0026#34;cherry\u0026#34;, \u0026#34;apple\u0026#34;] new_list = sorted(my_list) # create list with repeated elements list_with_zeros = [0] * 5 print(list_with_zeros) # concatenation list_concat = list_with_zeros + my_list print(list_concat) # convert string to list string_to_list = list(\u0026#39;Hello\u0026#39;) print(string_to_list) 复制列表 注意拷贝列表内容还是拷贝引用\nlist_org = [\u0026#34;banana\u0026#34;, \u0026#34;cherry\u0026#34;, \u0026#34;apple\u0026#34;] # this just copies the reference to the list, so be careful list_copy = list_org # now modifying the copy also affects the original list_copy.append(True) print(list_copy) print(list_org) # use copy(), or list(x) to actually copy the list # slicing also works: list_copy = list_org[:] list_org = [\u0026#34;banana\u0026#34;, \u0026#34;cherry\u0026#34;, \u0026#34;apple\u0026#34;] list_copy = list_org.copy() # list_copy = list(list_org) # list_copy = list_org[:] # now modifying the copy does not affect the original list_copy.append(True) print(list_copy) print(list_org) 列表切片 # a[start:stop:step], default step is 1 a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] b = a[1:3] # Note that the last index is not included print(b) b = a[2:] # until the end print(b) b = a[:3] # from beginning print(b) a[0:3] = [0] # replace sub-parts, you need an iterable here print(a) b = a[::2] # start to end with every second item print(b) a = a[::-1] # reverse the list with a negative step: print(a) b = a[:] # copy a list with slicing print(b) Tuple Tuple又称为元组，和List列表类似，主要区别在于元组是不可变类型．不可变意味着元组中的元素无法被重新赋值．使用元组而非列表的有如下原因：\nGenerally used for objects that belong together. Use tuple for heterogeneous (different) datatypes and list for homogeneous (similar) datatypes. Since tuple are immutable, iterating through tuple is slightly faster than with list. Tuples with their immutable elements can be used as key for a dictionary. This is not possible with lists. If you have data that doesn\u0026rsquo;t change, implementing it as tuple will guarantee that it remains write-protected. 创建元组 tuple_1 = (\u0026#34;Max\u0026#34;, 28, \u0026#34;New York\u0026#34;) tuple_2 = \u0026#34;Linda\u0026#34;, 25, \u0026#34;Miami\u0026#34; # Parentheses are optional # Special case: a tuple with only one element needs to have a comma at the end, # otherwise it is not recognized as tuple tuple_3 = (25,) print(tuple_1) print(tuple_2) print(tuple_3) # Or convert an iterable (list, dict, string) with the built-in tuple function tuple_4 = tuple([1,2,3]) print(tuple_4) 不可变解释 元组不提供修改元素的方法，其中的item本身无法被赋值，即其指向的对象id（类似内存地址)无法改变，但是若元组中元素item自身是可变类型的，元素本身可以改变．\nPython中可变类型的解释：\npython中对可变数据类型的定义为：当该数据类型的对应变量的值发生了改变，那么它对应的内存地址不发生改变，就称可变数据类型。包括：set（集合）、list（列表）、dict（字典）\nIn [7]: a = (1,[2,3]) In [8]: a[1] Out[8]: [2, 3] In [9]: a[1] = [1,2,3] --------------------------------------------------------------------------- TypeError Traceback (most recent call last) \u0026lt;ipython-input-9-f8fa7e0d45e2\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 1 a[1] = [1,2,3] TypeError: \u0026#39;tuple\u0026#39; object does not support item assignment In [10]: a[1].append(3) In [11]: a Out[11]: (1, [2, 3, 3]) 以此执行下面指令，可以理解一下浅拷贝和深拷贝\na = [1] # shadow copy b = a b is a　# True,has same address id(a) id(b) # deep copy,a point to another memory address a = a+[2] a id(a) # b\u0026#39;s address doesn\u0026#39;t change id(b) 内置方法 my_tuple = (\u0026#39;a\u0026#39;,\u0026#39;p\u0026#39;,\u0026#39;p\u0026#39;,\u0026#39;l\u0026#39;,\u0026#39;e\u0026#39;,) # len() : get the number of elements in a tuple print(len(my_tuple)) # count(x) : Return the number of items that is equal to x print(my_tuple.count(\u0026#39;p\u0026#39;)) # index(x) : Return index of first item that is equal to x print(my_tuple.index(\u0026#39;l\u0026#39;)) # repetition my_tuple = (\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;) * 5 print(my_tuple) # concatenation my_tuple = (1,2,3) + (4,5,6) print(my_tuple) # convert list to a tuple and vice versa my_list = [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;] list_to_tuple = tuple(my_list) print(list_to_tuple) tuple_to_list = list(list_to_tuple) print(tuple_to_list) # convert string to tuple string_to_tuple = tuple(\u0026#39;Hello\u0026#39;) print(string_to_tuple) # Result \u0026#34;\u0026#34;\u0026#34; 5 2 3 (\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;) (1, 2, 3, 4, 5, 6) (\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;) [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;] (\u0026#39;H\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;) \u0026#34;\u0026#34;\u0026#34; 元组切片 # a[start:stop:step], default step is 1 a = (1, 2, 3, 4, 5, 6, 7, 8, 9, 10) b = a[1:3] # Note that the last index is not included print(b) b = a[2:] # until the end print(b) b = a[:3] # from beginning print(b) b = a[::2] # start to end with every second item print(b) b = a[::-1] # reverse tuple # don\u0026#39;t change a ,create a new tuple and assign it to b print(b) # Result \u0026#34;\u0026#34;\u0026#34; (2, 3) (3, 4, 5, 6, 7, 8, 9, 10) (1, 2, 3) (1, 3, 5, 7, 9) (10, 9, 8, 7, 6, 5, 4, 3, 2, 1) \u0026#34;\u0026#34;\u0026#34; 元组解包 在Python中互换两个变量的值，如\na,b = b,a\n就是等号右侧将b,a自动打包为元组(b,a)，赋值给左侧后再自动解包，同样的，Python中函数返回多个值，也是隐式的将多个值打包为元组，并返回一个元组．\n# number of variables have to match number of tuple elements tuple_1 = (\u0026#34;Max\u0026#34;, 28, \u0026#34;New York\u0026#34;) name, age, city = tuple_1 print(name) print(age) print(city) # tip: unpack multiple elements to a list with * my_tuple = (0, 1, 2, 3, 4, 5) item_first, *items_between, item_last = my_tuple print(item_first) print(items_between) print(item_last) # Result \u0026#34;\u0026#34;\u0026#34; Max 28 New York 0 [1, 2, 3, 4] 5 \u0026#34;\u0026#34;\u0026#34; 元组列表对比 容纳同样的数据，元组的占用空间和迭代速度更高\n# compare the size import sys my_list = [0, 1, 2, \u0026#34;hello\u0026#34;, True] my_tuple = (0, 1, 2, \u0026#34;hello\u0026#34;, True) print(sys.getsizeof(my_list), \u0026#34;bytes\u0026#34;) print(sys.getsizeof(my_tuple), \u0026#34;bytes\u0026#34;) # compare the execution time of a list vs. tuple creation statement import timeit print(timeit.timeit(stmt=\u0026#34;[0, 1, 2, 3, 4, 5]\u0026#34;, number=1000000)) print(timeit.timeit(stmt=\u0026#34;(0, 1, 2, 3, 4, 5)\u0026#34;, number=1000000)) # Result \u0026#34;\u0026#34;\u0026#34; 104 bytes 88 bytes 0.12474981700000853 0.014836141000017733 \u0026#34;\u0026#34;\u0026#34; Dictionary 字典是无序的、可变的、可索引的一种数据类型。\n字典创建 my_dict = {\u0026#34;name\u0026#34;:\u0026#34;Max\u0026#34;, \u0026#34;age\u0026#34;:28, \u0026#34;city\u0026#34;:\u0026#34;New York\u0026#34;} print(my_dict) # or use the dict constructor, note: no quotes necessary for keys my_dict_2 = dict(name=\u0026#34;Lisa\u0026#34;, age=27, city=\u0026#34;Boston\u0026#34;) print(my_dict_2) 常用方法 删除字典元素 my_dict = {\u0026#34;name\u0026#34;:\u0026#34;Max\u0026#34;, \u0026#34;age\u0026#34;:28, \u0026#34;city\u0026#34;:\u0026#34;New York\u0026#34;} # delete a key-value pair del my_dict[\u0026#34;email\u0026#34;] # this returns the value and removes the key-value pair print(\u0026#34;popped value:\u0026#34;, my_dict.pop(\u0026#34;age\u0026#34;)) # return and removes the last inserted key-value pair # (in versions before Python 3.7 it removes an arbitrary pair) print(\u0026#34;popped item:\u0026#34;, my_dict.popitem()) print(my_dict) # clear() : remove all pairs # my_dict.clear() 检查key是否存在 my_dict = {\u0026#34;name\u0026#34;:\u0026#34;Max\u0026#34;, \u0026#34;age\u0026#34;:28, \u0026#34;city\u0026#34;:\u0026#34;New York\u0026#34;} # use if .. in .. if \u0026#34;name\u0026#34; in my_dict: print(my_dict[\u0026#34;name\u0026#34;]) # use try except try: print(my_dict[\u0026#34;firstname\u0026#34;]) except KeyError: print(\u0026#34;No key found\u0026#34;) 迭代字典元素 # loop over keys for key in my_dict: print(key, my_dict[key]) # loop over keys for key in my_dict.keys(): print(key) # loop over values for value in my_dict.values(): print(value) # loop over keys and values for key, value in my_dict.items(): print(key, value) 复制字典 dict_org = {\u0026#34;name\u0026#34;:\u0026#34;Max\u0026#34;, \u0026#34;age\u0026#34;:28, \u0026#34;city\u0026#34;:\u0026#34;New York\u0026#34;} # this just copies the reference to the dict, so be careful dict_copy = dict_org # now modifying the copy also affects the original dict_copy[\u0026#34;name\u0026#34;] = \u0026#34;Lisa\u0026#34; print(dict_copy) print(dict_org) # use copy(), or dict(x) to actually copy the dict dict_org = {\u0026#34;name\u0026#34;:\u0026#34;Max\u0026#34;, \u0026#34;age\u0026#34;:28, \u0026#34;city\u0026#34;:\u0026#34;New York\u0026#34;} dict_copy = dict_org.copy() # dict_copy = dict(dict_org) # now modifying the copy does not affect the original dict_copy[\u0026#34;name\u0026#34;] = \u0026#34;Lisa\u0026#34; print(dict_copy) print(dict_org) 合并两个字典 # Use the update() method to merge 2 dicts # existing keys are overwritten, new keys are added my_dict = {\u0026#34;name\u0026#34;:\u0026#34;Max\u0026#34;, \u0026#34;age\u0026#34;:28, \u0026#34;email\u0026#34;:\u0026#34;max@xyz.com\u0026#34;} my_dict_2 = dict(name=\u0026#34;Lisa\u0026#34;, age=27, city=\u0026#34;Boston\u0026#34;) my_dict.update(my_dict_2) print(my_dict) 可能的Key类型 任何不可变类型都可以作为字典的key，包括数字、字符串。元组如果所有元素都是不可变的，也可以作为key。\n# use numbers as key, but be careful my_dict = {3: 9, 6: 36, 9:81} # do not mistake the keys as indices of a list, e.g my_dict[0] is not possible here print(my_dict[3], my_dict[6], my_dict[9]) # use a tuple with immutable elements (e.g. number, string) my_tuple = (8, 7) my_dict = {my_tuple: 15} print(my_dict[my_tuple]) # print(my_dict[8, 7]) # a list is not possible because it is not immutable # this will raise an Error: # my_list = [8, 7] # my_dict = {my_list: 15} Sets A Set is an unordered collection data type that is unindexed, mutable, and has no duplicate elements.\n创建集合 my_set = {\u0026#34;apple\u0026#34;, \u0026#34;banana\u0026#34;, \u0026#34;cherry\u0026#34;} print(my_set) # or use the set function and create from an iterable, e.g. list, tuple, string my_set_2 = set([\u0026#34;one\u0026#34;, \u0026#34;two\u0026#34;, \u0026#34;three\u0026#34;]) my_set_2 = set((\u0026#34;one\u0026#34;, \u0026#34;two\u0026#34;, \u0026#34;three\u0026#34;)) print(my_set_2) my_set_3 = set(\u0026#34;aaabbbcccdddeeeeeffff\u0026#34;) print(my_set_3) # careful: an empty set cannot be created with {}, as this is interpreted as dict # use set() instead a = {} print(type(a)) a = set() print(type(a)) 常用方法 添加元素 my_set = set() # use the add() method to add elements my_set.add(42) my_set.add(True) my_set.add(\u0026#34;Hello\u0026#34;) # note: the order does not matter, and might differ when printed print(my_set) # nothing happens when the element is already present: my_set.add(42) print(my_set) 移除元素 # remove(x): removes x, raises a KeyError if element is not present my_set = {\u0026#34;apple\u0026#34;, \u0026#34;banana\u0026#34;, \u0026#34;cherry\u0026#34;} my_set.remove(\u0026#34;apple\u0026#34;) print(my_set) # KeyError: # my_set.remove(\u0026#34;orange\u0026#34;) # discard(x): removes x, does nothing if element is not present my_set.discard(\u0026#34;cherry\u0026#34;) my_set.discard(\u0026#34;blueberry\u0026#34;) print(my_set) # clear() : remove all elements my_set.clear() print(my_set) # pop() : return and remove a random element a = {True, 2, False, \u0026#34;hi\u0026#34;, \u0026#34;hello\u0026#34;} print(a.pop()) print(a) 交集和并集 不改变原有集合，只是会产生新的集合\nodds = {1, 3, 5, 7, 9} evens = {0, 2, 4, 6, 8} primes = {2, 3, 5, 7} # union() : combine elements from both sets, no duplication # note that this does not change the two sets u = odds.union(evens) print(u) # intersection(): take elements that are in both sets i = odds.intersection(evens) print(i) i = odds.intersection(primes) print(i) i = evens.intersection(primes) print(i) 差集 setB = {1, 2, 3, 10, 11, 12} # difference() : returns a set with all the elements from the setA that are not in setB. diff_set = setA.difference(setB) print(diff_set) # A.difference(B) is not the same as B.difference(A) diff_set = setB.difference(setA) print(diff_set) # symmetric_difference() : returns a set with all the elements that are in setA and setB but not in both diff_set = setA.symmetric_difference(setB) print(diff_set) # A.symmetric_difference(B) = B.symmetric_difference(A) diff_set = setB.symmetric_difference(setA) print(diff_set) 更新集合（交/并/差） setA = {1, 2, 3, 4, 5, 6, 7, 8, 9} setB = {1, 2, 3, 10, 11, 12} # update() : Update the set by adding elements from another set. setA.update(setB) print(setA) # intersection_update() : Update the set by keeping only the elements found in both setA = {1, 2, 3, 4, 5, 6, 7, 8, 9} setA.intersection_update(setB) print(setA) # difference_update() : Update the set by removing elements found in another set. setA = {1, 2, 3, 4, 5, 6, 7, 8, 9} setA.difference_update(setB) print(setA) # symmetric_difference_update() : Update the set by only keeping the elements found in either set, but not in both setA = {1, 2, 3, 4, 5, 6, 7, 8, 9} setA.symmetric_difference_update(setB) print(setA) # Note: all update methods also work with other iterables as argument, e.g lists, tuples # setA.update([1, 2, 3, 4, 5, 6]) 复制集合 set_org = {1, 2, 3, 4, 5} # this just copies the reference to the set, so be careful set_copy = set_org # now modifying the copy also affects the original set_copy.update([3, 4, 5, 6, 7]) print(set_copy) print(set_org) # use copy() to actually copy the set set_org = {1, 2, 3, 4, 5} set_copy = set_org.copy() # now modifying the copy does not affect the original set_copy.update([3, 4, 5, 6, 7]) print(set_copy) print(set_org) 子集/不相交 setA = {1, 2, 3, 4, 5, 6} setB = {1, 2, 3} # issubset(setX): Returns True if setX contains the set print(setA.issubset(setB)) print(setB.issubset(setA)) # True # issuperset(setX): Returns True if the set contains setX print(setA.issuperset(setB)) # True print(setB.issuperset(setA)) # isdisjoint(setX) : Return True if both sets have a null intersection, i.e. no same elements setC = {7, 8, 9} print(setA.isdisjoint(setB)) print(setA.isdisjoint(setC)) Frozenset不可变集合 Frozen set是一种不可变的集合，其在创建以后，集合中的元素id不可变\na = frozenset([0, 1, 2, 3, 4]) # The following is not allowed: # a.add(5) # a.remove(1) # a.discard(1) # a.clear() # Also no update methods are allowed: # a.update([1,2,3]) # Other set operations work odds = frozenset({1, 3, 5, 7, 9}) evens = frozenset({0, 2, 4, 6, 8}) print(odds.union(evens)) print(odds.intersection(evens)) print(odds.difference(evens)) Strings 字符串是一个字符序列，是不可变类型，对于同一个id下的字符串不可变。\n字符串创建 # use singe or double quotes my_string = \u0026#39;Hello\u0026#39; my_string = \u0026#34;Hello\u0026#34; my_string = \u0026#34;I\u0026#39; m a \u0026#39;Geek\u0026#39;\u0026#34; # escaping backslash my_string = \u0026#39;I\\\u0026#39; m a \u0026#34;Geek\u0026#34;\u0026#39; my_string = \u0026#39;I\\\u0026#39; m a \\\u0026#39;Geek\\\u0026#39;\u0026#39; print(my_string) # triple quotes for multiline strings my_string = \u0026#34;\u0026#34;\u0026#34;Hello World\u0026#34;\u0026#34;\u0026#34; print(my_string) # backslash if you want to continue in the next line my_string = \u0026#34;Hello \\ World\u0026#34; print(my_string) 访问字符或者子串 my_string = \u0026#34;Hello World\u0026#34; # get character by referring to index b = my_string[0] print(b) # Substrings with slicing b = my_string[1:3] # Note that the last index is not included print(b) b = my_string[:5] # from beginning print(b) b = my_string[6:] # until the end print(b) b = my_string[::2] # start to end with every second item print(b) b = my_string[::-1] # reverse the string with a negative step: print(b) 常用方法 my_string = \u0026#34; Hello World \u0026#34; # remove white space my_string = my_string.strip() print(my_string) # number of characters print(len(my_string)) # Upper and lower cases print(my_string.upper()) print(my_string.lower()) # startswith and endswith print(\u0026#34;hello\u0026#34;.startswith(\u0026#34;he\u0026#34;)) print(\u0026#34;hello\u0026#34;.endswith(\u0026#34;llo\u0026#34;)) # find first index of a given substring, -1 otherwise print(\u0026#34;Hello\u0026#34;.find(\u0026#34;o\u0026#34;)) # count number of characters/substrings print(\u0026#34;Hello\u0026#34;.count(\u0026#34;e\u0026#34;)) # replace a substring with another string (only if the substring is found) # Note: The original string stays the same message = \u0026#34;Hello World\u0026#34; new_message = message.replace(\u0026#34;World\u0026#34;, \u0026#34;Universe\u0026#34;) print(new_message) # split the string into a list my_string = \u0026#34;how are you doing\u0026#34; a = my_string.split() # default argument is \u0026#34; \u0026#34; print(a) my_string = \u0026#34;one,two,three\u0026#34; a = my_string.split(\u0026#34;,\u0026#34;) print(a) # join elements of a list into a string my_list = [\u0026#39;How\u0026#39;, \u0026#39;are\u0026#39;, \u0026#39;you\u0026#39;, \u0026#39;doing\u0026#39;] a = \u0026#39; \u0026#39;.join(my_list) # the given string is the separator, e.g. \u0026#39; \u0026#39; between each argument print(a) 格式化字符串 # use braces as placeholders a = \u0026#34;Hello {0} and {1}\u0026#34;.format(\u0026#34;Bob\u0026#34;, \u0026#34;Tom\u0026#34;) print(a) # the positions are optional for the default order a = \u0026#34;Hello {} and {}\u0026#34;.format(\u0026#34;Bob\u0026#34;, \u0026#34;Tom\u0026#34;) print(a) a = \u0026#34;The integer value is {}\u0026#34;.format(2) print(a) # some special format rules for numbers a = \u0026#34;The float value is {0:.3f}\u0026#34;.format(2.1234) print(a) a = \u0026#34;The float value is {0:e}\u0026#34;.format(2.1234) print(a) a = \u0026#34;The binary value is {0:b}\u0026#34;.format(2) print(a) # old style formatting by using % operator print(\u0026#34;Hello %s and %s\u0026#34; % (\u0026#34;Bob\u0026#34;, \u0026#34;Tom\u0026#34;)) # must be a tuple for multiple arguments val = 3.14159265359 print(\u0026#34;The decimal value is %d\u0026#34; % val) print(\u0026#34;The float value is %f\u0026#34; % val) print(\u0026#34;The float value is %.2f\u0026#34; % val) # since python3.6 # Use the variables directly inside the braces. name = \u0026#34;Eric\u0026#34; age = 25 a = f\u0026#34;Hello, {name}. You are {age}.\u0026#34; print(a) pi = 3.14159 a = f\u0026#34;Pi is {pi:.3f}\u0026#34; print(a) # f-Strings are evaluated at runtime, which allows expressions a = f\u0026#34;The value is {2*60}\u0026#34; print(a) More on immutability and concatenation # since a string is immutable, adding strings with +, or += always # creates a new string, and therefore is expensive for multiple operations # --\u0026gt; join method is much faster from timeit import default_timer as timer my_list = [\u0026#34;a\u0026#34;] * 1000000 # bad start = timer() a = \u0026#34;\u0026#34; for i in my_list: a += i end = timer() print(\u0026#34;concatenate string with + : %.5f\u0026#34; % (end - start)) # good start = timer() a = \u0026#34;\u0026#34;.join(my_list) end = timer() print(\u0026#34;concatenate string with join(): %.5f\u0026#34; % (end - start)) 参考 Python-Notebook\n","permalink":"https://blog.niuhemoon.win/posts/tech/python-advanced-1/","summary":"Python种基本类型的比较： List is a collection which is ordered and mutable. Allows duplicate members. Tuple is a collection which is ordered and immutable. Allows duplicate members. Set is a collection which is unordered and unindexed. No duplicate members. Dictionary is a collection which is unordered, mutable and indexed. No duplicate members. Strings are immutable sequences of Unicode code points. List Python中的list是一个有序容器，容纳不同类型的数据（但推荐列表内数据类型相同)，同时其是可变类型的． 创建列表 list_1 = [\u0026#34;banana\u0026#34;, \u0026#34;cherry\u0026#34;, \u0026#34;apple\u0026#34;] print(list_1) # Or create","title":"Python进阶上"},{"content":"Microstack 简介 Microstack 是在 ubuntu 平台上快速部署 Openstack 环境的工具，其通过 snap 构建，而 snap 安装目录是一个独立的只读文件系统，这就导致难以改动代码进行调试。 因此，Microstack 环境只适用于 Openstack 初学者学习命令行和数据库等等，调试的话可以通告 gdb 调试，而不便于通过 pdb 调试，因为无法修改源文件，并在文件还中加断点。 Microstack 是目前 Ubuntu 上最简洁的 Openstack 配置工具，可以在笔记本上部署单节点环境用于学习，也可以在多台设备上部署多节点环境。\nMicrostack 安装 需要在终端科学上网，否则 snap 镜像很慢,目前支持到 Openstack 上游的 stein 版本．\n# 配置代理 export https_proxy=http://127.0.0.1:port\u0026amp;\u0026amp; export http_proxy=http://127.0.0.1:port\u0026#34; # 安装snap包 sudo snap install --classic --beta microstack # 初始化microstack环境 sudo microstack.init --auto # 初始化完成后会自动启动Openstack进程 # 查看相关进程 systemctl list-units | grep microstack # 可以看到microstack进程的状态 # 如果全部是loaded active running，表示服务正常启动 Microstack 基本使用 Microstack 由于是 Snap 镜像，可以手动关闭和开启 其源代码在/snap/microstack/196/lib/python3.6/site-packages 但由于 snap 只读文件系统，代码无法修改 此外，其命令行 Client 都加上了 Microstack 前缀\n一些常用的命令行\n# 在.bashrc文件中配置别名 alias openstack=\u0026#34;microstack.openstack\u0026#34; source ~/.bashrc # 查看帮助 openstack --help # 数据库操作，查看nova库 sudo microstack.mysql nova # 几个数据库包括: # |cinder | # | glance | # | keystone | # | mysql | # | neutron | # | nova | # | nova_api | # | nova_cell0 # 查看配置文件和数据库地址 cd /var/snap/microstack # 配置文件，可修改配置文件重启进程 cd /var/snap/microstack/common/etc 也可以在浏览器访问 web 界面http://10.20.20.1/ 默认用户名密码是 admin 和 keystone\n总结\nMicrostack 目前不适用于开发者编辑调试代码，只适用于学习者熟悉环境，用于在自己的电脑上快速部署．\n参考 Microstack 文档\n","permalink":"https://blog.niuhemoon.win/posts/tech/microstack-usage/","summary":"Microstack 简介 Microstack 是在 ubuntu 平台上快速部署 Openstack 环境的工具，其通过 snap 构建，而 snap 安装目录是一个独立的只读文件系统，这就导致难以改动代码进行调试。 因此，Microstack 环境只适用于 Openstack 初学者学习命令行和数据库等等，调试的话可以通告 gdb 调试，而不便于通过 pdb 调试，因为无法修改源文件，并在文件还中加断点。 Microstack","title":"Microstack 基本使用"},{"content":"python3中如何对二维码QRcode进行编码解码\n通常对于二维码，我们需要进行两种操作：\n将二维码图片扫描后解析成字符串 将字符串编码生成二维码图片 这是两个逆过程，在python2中，我们可以通过zbar这个第三方库实现两个功能。可以zbar并不支持python3，而且，zbar在window平台上的安装极其繁琐，有很多坑。\n所以想通过python3处理二维码的过程中，查了很多资料。目前比较好的解决办法如下：\n用pyzbar代替zbar解析二维码 安装：\npip install pyzbar 这是可能的，因为pyzbar是一个围绕zbar库的基于ctypes的包装器，它包含在dll和Windows Python的轮子中。\n使用：\n网络上二维码图片解析 import requests import array from PIL import Image from io import BytesIO from pyzbar.pyzbar import decode # decode_result的格式是[Decoded(data=\u0026#39;****\u0026#39;,……)]，列表里包含一个nametuple def decode_qrcode(url) decode_result = decode(Image.open(BytesIO(requests.get(urls, headers=HEADERS).content)) return str(decode_result[0].data, encoding=\u0026#39;utf-8\u0026#39;) 本地二维码图片解析 from PIL import Image from pyzbar.pyzbar import decode def decode_qrcode(url) decode_result = decode(Image.open(\u0026#39;filename\u0026#39;)) return str(decode_result[0].data, encoding=\u0026#39;utf-8\u0026#39;) 更详细用法参见release页面\n下载zbar.exe文件并安装，通过系统命令行调用来解码 下载地址：zbar-0.10-setup.exe\n说明：\n使用：\nimport os os.system(r\u0026#39;C:\\zbarimg.exe -d d:\\Winapps\\Zbar\\Examples\\barcode.png\u0026#39;) Linux平台上qrtools解码 注：这个方法我没用过。\n安装：\nsudo apt-get install python-qrtools 使用：\nimport qrtools qr = qrtools.QR() qr.decode(\u0026#34;horn.png\u0026#34;) print qr.data 用PyQRCode来生成二维码 安装：\npip install pyqrcode 用法：\nimport pyqrcode url = pyqrcode.create(\u0026#39;http://uca.edu\u0026#39;) url.svg(\u0026#39;uca-url.svg\u0026#39;, scale=8) 说明： pyqrcode具有完善的帮助文档和强大的功能，可以快速生产二维码图片,细节参考：\nRelease页面 帮助文档 其他值得关注的内容 qreader是一个正在开发中的项目，使用纯Python编写的二维码解析库，不依赖zbar。感兴趣的可以去提交PR。\n免费的二维码编码解码API接口 http://api.qrserver.com/ Send a GET request of following form to our system to decode a QR code graphic (=to read a QR code from the web): http(s)://api.qrserver.com/v1/read-qr-code/?fileurl=[URL-encoded-webaddress-url-to-qrcode-image-file]\nhttp://www.7xiwang.com/Tools/Index 解码api：http://www.7xiwang.com/WebService/QRCodeDecode api参数：base64img api请求类型：HttpPost 参数数据格式（JSON）：{\u0026#34;base64img\u0026#34;:\u0026#34;\u0026#34;} api返回数据格式（JSON）： 请求成功：{\u0026#34;status\u0026#34;:\u0026#34;1\u0026#34;,\u0026#34;text\u0026#34;:\u0026#34;\u0026#34;} 请求失败：{\u0026#34;status\u0026#34;:\u0026#34;0\u0026#34;,\u0026#34;Msg\u0026#34;:\u0026#34;错误信息\u0026#34;} ","permalink":"https://blog.niuhemoon.win/posts/tech/python3-qrcode/","summary":"python3中如何对二维码QRcode进行编码解码 通常对于二维码，我们需要进行两种操作： 将二维码图片扫描后解析成字符串 将字符串编码生成二维码图片 这是两个逆过程，在python2中，我们可以通过zbar这个第三方库实现两个功能。可以zbar并不支持python3，而且，zbar在","title":"Python3操作二维码图片"},{"content":" Linux内核支持用户进程和内核进程两种进程。内核进程指完全运行在内核空间的进程，这种进程主要处理内核事务；用户进程一般运行在用户态，需要使用内核资源时，通过系统调用进入内核态，系统调用结束后，重新返回用户态。\n###创建进程\n可通过fork函数创建子进程，理论上，父子进程拥有各自独立的用户空间。但Linux为了提高效率，采用COW(copy on write)算法。\nfork函数原型如下:\n/* 成功：子进程pid返回给父进程，0返回给子进程 失败：\t-1返回给父进程，设置errno */ pid_t fork(); 案例：创建子进程\n#include \u0026lt;unistd.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; int glob = 10; int main() { int local; pid_t pid; local = 8; //向子进程的pid传值0 if((pid = fork()) == 0) {\t//子进程 sleep(1); printf(\u0026#34;i am in child process,%d\\n\u0026#34;,getpid()); }zijc else {\t//父进程 printf(\u0026#34;i am in father process,%d\\n\u0026#34;,getpid()); glob++; local--; sleep(5); } printf(\u0026#34;pid = %d,glob = %d,localar = %d\\n\u0026#34;,getpid(),glob,local); return 0; } /*输出为： i am in father process,13023 i am in child process,13024 pid = 13024,glob = 10,localar = 8 pid = 13023,glob = 11,localar = 7 */ 当用fork()函数创建子进程时，Linux内核为子进程分配一个进程控制块task_struct。子进程的进程控制块用来存放子进程拥有的资源、管理信息和进程状态等。\n此时，在父子进程没有对数据进行读写操作之前，父子进程共享用户地址空间。当父进程执行glob++，Linux内核采用COW算法，首先为子进程创建相应的数据区，接着内核将父进程地址空间中的数据区相关页复制到子进程地址空间中数据区的相关页,此时，父子进程各自拥有独立的全局变量glob。当执行local\u0026ndash;语句，内核以同样方法在子进程用户地址空间的栈区的相应页建立复制。而代码区是只读的，所以父子进程共享代码区，直接建立映射，不进行复制。\n###程序启动和结束 初始化程序\n在加载可执行文件后，首先运行的是称为start-up的代码，此部分代码在程序链接为可执行程序时，由链接器加入，作用是从内核读取进程运行的环境信息，如环境变量、命令行参数等。\nstart-up完成初始化工作后，调用main函数，执行完进程后，通过exit函数结束进程。\n结束进程\n每个进程都有父进程，当子进程运行结束后，子进程进程僵尸状态，并向父进程发送SIGCHLD信号，通知子进程已经终止。在该状态下子进程几乎释放了所有内存资源，不能被重新调度，仅在进程列表中保留一个位置，只保留进程如何终止的一些状态信息，以供回收者使用。父进程可以通过调用wait或waitpid函数获取子进程的退出码，以便判断子进程结束的原因。由父进程释放子进程余下的所有资源。\n但当父进程在子进程之前终止，子进程的父进程将更改为init进程，由init进程负责子进程的善后处理工作。\n//终止进程,status返回值 void exit(int status); /* 登记终止处理函数，ANSI C规定，一个进程可以登记最多32个终止处理函数，这些函数由exit自动调用。exit以先进后出的方式调用atexit登记的函数，同一函数登记多次，也被调用多次。 根据ANSI C，exit首先调用终止处理函数，然后按需调用fclose，关闭所有打开的文件流，保证基于缓冲区的文件I/O操作完整性。 这样，在进程结束前，将未写入文件的缓冲区数据，通过exit函数进行保存。 func终止处理函数 成功返回0，否则非0 */ int atexit(void(*func)(void)); //直接结束进程，不进行任何其他处理 void _exit(int status); 案例：直接退出进程\n#include \u0026lt;unistd.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; int main() { printf(\u0026#34;output begin\\n\u0026#34;); printf(\u0026#34;content in buffer\u0026#34;); printf(\u0026#34;drop the buffer\u0026#34;); _exit(0); } 案例：登记终止处理函数\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; static void my_exit1(void) { printf(\u0026#34;first exit handler\\n\u0026#34;); } static void my_exit2(void) { printf(\u0026#34;second exit handler\\n\u0026#34;); } int main(void) { if(atexit(my_exit2) != 0) printf(\u0026#34;can\u0026#39;t register my_exit2\u0026#34;); if(atexit(my_exit1) != 0) printf(\u0026#34;can\u0026#39;t register my_exit1\u0026#34;); if(atexit(my_exit1) != 0) printf(\u0026#34;can\u0026#39;t register my_exit1\u0026#34;); printf(\u0026#34;main is done\\n\u0026#34;); return 0; } ###进程同步控制\n当创建一个子进程后，父子进程的执行顺序无法控制。当父子进程同事操作共享资源，不同的执行次序有可能导致不同的运行结果，从而出现数据不一致性。为解决这一问题，必须提供进程间的同步控制机制。\nwait和waitpid可用来实现父子进程同步，用来等待子进程结束。 wait函数的功能是获取子进程如何终止的信息，清除子进程的剩余资源。父进程调用wait函数，进入阻塞队列，等待某个子进程的结束。当子进程结束，会产生结束状态字status,并向父进程发送SIGCHLD信号。 父进程收到SIGCHLD信号，若希望知道子进程结束状态，调用wait,否则忽略该信号。\n/* 暂停执行，将子进程结束状态写入status中，并确认子进程已经结束 status 子进程状态 成功返回子进程PID，否则返回-1 */ pid_t wait(int *status); /* 等待指定子进程结束 pid 指定等待的子进程 \u0026lt;-1 pid所代表进程组中进程 -1 任何子进程 0 与该进程同组的进程 \u0026gt;0 进程标识符为pid的进程 status 保存子进程状态 options 等待方式 WNOHANG\t进程不阻塞 WUNTRACED 当有子进程结束时返回 */ pid_t waitpid(pid_t pid,int *status,int options); 案例：依次等待多个子进程结束，并显示结束状态\n#include \u0026lt;unistd.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;sys/wait.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; int main(void) { pid_t pid[10],wpid; int child_status,i; //创建5个子进程 for(i=0;i\u0026lt;5;i++) { if((pid[i]=fork()) == 0) exit(100+i); } //等待子进程结束，输出status for(i=0;i\u0026lt;5;i++) { wpid = waitpid(pid[i],\u0026amp;child_status,0); if(WIFEXITED(child_status)) printf(\u0026#34;Child %d exit with status %d\\n\u0026#34;, wpid,WEXITSTATUS(child_status)); else printf(\u0026#34;Child %d terminated abnormally\\n\u0026#34;,wpid); } return 0; } ","permalink":"https://blog.niuhemoon.win/posts/tech/linux-c-process-2/","summary":"Linux内核支持用户进程和内核进程两种进程。内核进程指完全运行在内核空间的进程，这种进程主要处理内核事务；用户进程一般运行在用户态，需要使用内核资源时，通过系统调用进入内核态，系统调用结束后，重新返回用户态。 ###创建进程 可通过fork函数创建子进程，理论上，父子进程拥有各自独","title":"Linux进程——进程创建和同步控制"},{"content":" 进程是程序的一次运行过程，除了进程虚拟地址空间和文件描述符等，进程控制块中还存放了进程运行的环境信息，包括用户、用户组、父进程、进程组和会话等。\n###用户和用户组\n//获得当前进程实际用户ID pid_t getuid(void); //获得当前进程有效用户ID pid_t geteuid(void); //获得当前进程实际用户组ID pid_t getgid(void); //获得当前进程有效用户组ID pid_t getegid(void); ###进程和进程组 获得父子进程ID\n//获得当前进程ID pid_t getpid(void); //获得父进程ID pid_t getppid(void); 进程组\n有时，为了完成某个工作，需多个进程参与协作，为便于管理，可以将多个进程定义为一个进程组。一个进程组包含一个以上的进程，领头进程的进程ID等于进程组ID，进程组中不包含进程时，进程组自动消失。\n会话\n会话用于标识用户登录的每一个终端，每个登录终端都有一个会话ID与其对应； 会话包括控制进程（与终端建立连接的领头进程）、一个前台进程组和任意后台进程组。一个会话只能有一个控制终端，通常是登录到其上的终端设备或伪终端设备，产生在控制终端上的输入和信号将发送给会话的前台进程组中的所有进程。\n如果调用setsid函数的进程不是进程组中的领头进程，则可建立新的会话，（可在子进程中建立新的会话），该进程成为领头会话，同时产生一个新的进程组，且该进程为新进程组的领头进程，但不拥有终端。\n/* 获得进程所属会话ID 成功返回会话ID，错误返回-1 */ pid_t getsid(pid_t pid); /* 创建一个新的会话，使进程组ID等于该会话ID 成功返回新的进程组ID，否则-1 */ pid_t setsid(void); 案例：在子进程中创建新的领头会话\n#include \u0026lt;unistd.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; int main(void) { int p,pid; printf(\u0026#34;now session id %d\\n\\n\u0026#34;,getsid(getpid())); p = fork(); if(p) //父进程退出 exit(0); pid = setsid(); printf(\u0026#34;new session id %d\\n\u0026#34;,pid); return pid; } ###守护进程\n守护进程是一种运行在后台，且不受任何终端影响的进程，因此，需要关闭标准输入、标准输出、标准错误输出的文件描述符.通常守护进程以服务进程的形式存在，例如web服务器。\n同时要使守护进程脱离用户环境，所以要将工作目录修改为系统工作目录。 创建守护进程步骤：\n创建子进程后结束父进程 在子进程中建立新的领头会话 修改工作目录和权限掩码信息 关闭文件描述符0,1,2 案例：创建一个守护进程\n#include \u0026lt;unistd.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;sys/stat.h\u0026gt; int daemon_init() { pid_t pid; char buf[80]; FILE * fout; if((pid = fork()) \u0026lt; 0) return -1; else if(pid != 0) exit(0); //结束父进程 setsid(); //创建领头会话 system(\u0026#34;cd /\u0026#34;); //改变工作目录 umask(0); //清除权限掩码 close(0); //关闭文件描述符 close(1); close(2); getcwd(buf,sizeof(buf)); fout = fopen(\u0026#34;/tmp/result.txt\u0026#34;,\u0026#34;w\u0026#34;); fprintf(fout,\u0026#34;work dirctory is %s\\n\u0026#34;,buf); fprintf(fout,\u0026#34;daemon pid is %d\\n\u0026#34;,getpid()); fprintf(fout,\u0026#34;daemon parent pid is %d\\n\u0026#34;,getppid()); fclose(fout); return 0; } int main() { FILE * fout; printf(\u0026#34;start init daemon ...\u0026#34;); daemon_init(); while(1) { fout = fopen(\u0026#34;/tmp/result.txt\u0026#34;,\u0026#34;a\u0026#34;); fputs(\u0026#34;i am still alive\\n\u0026#34;,fout); fflush(fout); sleep(1); } } ###加载可执行映像\n可执行映像是链接好的可执行的代码。\n通常，子进程创建时，继承了父进程的资源，父子进程可以并发运行，它们由同一代码流程控制，具有相似行为。有时，希望子进程拥有独立代码流程，可以通过加载可执行二进制映像文件来实现。内核通过exec系统调用在进程中建立新的运行环境。\nELF格式\nLinux系统中，采用ELF(Excutable and Linkable Format)，ELF有3中基本格式\n可执行格式 目标文件(.o文件) 共享库（.so文件) 加载可执行文件\nELF的可执行文件的加载是通过系统调用exec完成的，当进程调用exec函数加载ELF可执行文件时，exec将以新加载程序的段替换当前进程的相应的正文、数据、堆和栈段；同时保留大部分的进程属性。例如进程ID、父进程ID、进程组ID、实际用户ID、会话ID、当前目录、文件描述符等。\n但当加载可执行文件的SETUID或SETGID位被设置，进程的有效用户ID和有效用户组ID被设置为该文件的属主ID和属主用户组ID。 exec相关函数原型：\n#include \u0026lt;unistd.h\u0026gt; int execl(const char *path,const char *arg,...) int execv(.........)......... int execle(........) int execve(.........) int execlp(.........) int execvp(.........) ","permalink":"https://blog.niuhemoon.win/posts/tech/linux-c-process-3/","summary":"进程是程序的一次运行过程，除了进程虚拟地址空间和文件描述符等，进程控制块中还存放了进程运行的环境信息，包括用户、用户组、父进程、进程组和会话等。 ###用户和用户组 //获得当前进程实际用户ID pid_t getuid(void); //获得当前进程有效用户ID pid_t geteuid(void); //获得当前进程实际用户组ID pid_t getgid(void); //获得当前进程有效","title":"Linux进程——进程环境与加载可执行映像"},{"content":" 可执行程序是存储在磁盘设备上由代码和数据按某种格式组织的静态实体，而进程是可被调度的代码的动态运行。在Linux系统中，在一个进程的生命周期里，都有各自的运行环境和所需的资源，这些信息储存在各自的进程控制块中。\n进程控制块主要结构如下:\n用户标识 进程和会话标识 虚拟地址管理 文件描述符表 信号 ###进程地址空间\n在32位地址总线的计算机上，每个进程拥有4GB的虚拟地址空间。\n可执行程序被加载至进程的用户虚拟地址空阿金，即将可执行程序中的代码段和数据段的内容复制到用户地址空间。为了执行程序，内核需在用户虚拟地址空间中建立一些辅助区域，例如堆区和栈区等，从而将用户虚拟地址空间划分为若干区域，分别为代码区、未初始化数据区、初始化数据区、环境变量和命令行参数区、堆区、栈区。不同区域中存储了不同的信息，有各自不同的属性。\n代码区\n包含指令序列和只读数据，没和在创建进程加载可执行二进制映像文件时，将这部分内容映射到进程的用户地址空间形成代码区。进程运行期间，代码区内容不会改变。\n因此，一个可执行映像的多个进程可共享代码区，只需保持一个复制。\n在可执行映像文件中，代码区的内容被保存在文本段中，文本段又称代码段。\n未初始化数据区\n在可执行二进制映像文件中，未初始化数据包括没有初始化的全局变量和静态局部变量，它们在映像文件中不占用储存空间，只保留其地址和大小信息。\n若映像文件中存在未初始化数据段，内核创建进程时，在进程的用户地址空间中为其分配一块区域，用于进程运行过程中对未初始化数据的存取，成为未初始化数据区。\n初始化数据区\n初始化数据区包括已初始化的全局变量和静态局部变量。在映像文件中，初始化数据被组织在数据段中，内核将初始化数据段映射至用户地址空间形成初始化数据区。该区内容运行过程中会发生变化，一个程序的多个进程实体拥有各自的数据区。\n堆heap\n堆位于数据区和栈之间，用于应用程序的动态内存管理。Linux将动态内存的管理通过glibc实现。Linux的进程控制块中记录了虚拟内存各区域的地址信息，它们在进程初始化时由系统设置，其中包含堆的起始地址和结束地址。\n在初始状态下，brk指针指向堆的顶部。堆区大小可以通过brk和sbrk函数调整。\n栈stack\n栈用来存放进程运行过程中的局部变量、函数返回地址、参数和进程上下文环境。\n环境变量和命令行\n环境变量继承自父进程，作用范围是进程本身及其子孙进程。命令行保存执行程序时的输入参数，它们都被保存在栈区域。\n自由空间\n堆栈之间的自由空间，内核可为进程创建新的区域用于加载共享库、映射共享内存和映射文件I/O等，可以通过mmap和munmap函数申请和释放。\n###环境变量/命令行参数\n每个进程的环境变量以字符串的形式存放在数组中，数组地址存放在全局变量environ中，可通过getenv和putenv对环境变量进行存取。\n命令行参数保存在用户地址空间的栈区域。\n案例：显示当前进程所有环境变量\n#include \u0026lt;stdio.h\u0026gt; int main(int argc,char *argv[]) { int i; char **ptr; extern char **environ; for (ptr = environ;*ptr != NULL;ptr++) printf(\u0026#34;%s\\n\u0026#34;,*ptr); return 0; } 案例:使用getenv和putenv存取环境变量\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; int main(int argc,char *argv[],char *envp[]) { int i; extern char **environ; printf(\u0026#34;form argument envp\\n\u0026#34;); for(i=0;envp[i];i++) puts(envp[i]); putenv(\u0026#34;HONE=/\u0026#34;); printf(\u0026#34;\\nFrom global variable environ\\n\u0026#34;); for(i=0;environ[i];i++) puts(environ[i]); return 0; } 案例:显示所有命令行参数\n#include \u0026lt;stdio.h\u0026gt; int main(int argc,char *argv[]) { int i; for(i=0;i\u0026lt;argc;i++) printf(\u0026#34;argv[%d}:%s\\n\u0026#34;,i,argv[i]); return 0; } ###动态内存管理\n堆介于栈和全局数据区之间，这部分空间用于进程的动态内存分配，堆采用自下向上生长。相关API函数如下：\nvoid *malloc(size_t size); void free(void *ptr); /* 设置堆区域的大小 pend设置数据区域的边界 incr扩展堆区域的字节数 对brk,成功返回0，否则-1 对sbrk成功返回原来的brk,否则-1 */ int brk(void *pend); void *sbrk(int incr); 案例:使用brk和sbrk调整heap大小\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; extern int etext,edata,end; void foo(int); int main() { int ret; void *bv; printf(\u0026#34;text ends at %10p\\n\u0026#34;,\u0026amp;etext);\tprintf(\u0026#34;initailized data ends at %10p\\n\u0026#34;,\u0026amp;edata); printf(\u0026#34;uninitialized data ends at %10p\\n\u0026#34;,\u0026amp;end); bv = sbrk(0); //当前堆区边界地址 printf(\u0026#34;Current break value is %10p \\n\\n\u0026#34;,bv); ret = brk(bv+512); puts(\u0026#34;heap incresed 512bytes\u0026#34;); printf(\u0026#34;brk returned ....%d\\n\u0026#34;,ret); bv = sbrk(0); //当前堆区边界地址 printf(\u0026#34;Current break value is %10p \\n\\n\u0026#34;,bv); foo(64); foo(-1024); return 0; } void foo(int size) { void *bv; bv = sbrk(size); printf(\u0026#34;heap increased %dbytes\\n\u0026#34;,size); printf(\u0026#34;sbrk returned %10p\\n\u0026#34;,bv); bv = sbrk(0); //当前堆区边界地址 printf(\u0026#34;Current break value is %10p \\n\\n\u0026#34;,bv); } 注\n通过brk、sbrk、mmap系统调用会频繁的触发软中断，使程序陷入内核态，比较消耗资源。为了较少系统调用产生的损耗，glibc采用内存池的设计，增加一个代理层，每次内存分配，优先从内存池中寻找一个大小相近的内存块(chunk),若内存池中无法提供，再向内核申请。\n具体参考glibc内存管理\n","permalink":"https://blog.niuhemoon.win/posts/tech/linux-c-process-1/","summary":"可执行程序是存储在磁盘设备上由代码和数据按某种格式组织的静态实体，而进程是可被调度的代码的动态运行。在Linux系统中，在一个进程的生命周期里，都有各自的运行环境和所需的资源，这些信息储存在各自的进程控制块中。 进程控制块主要结构如下: 用户标识 进程和会话标识 虚拟地址管理 文件描述符表","title":"Linux进程——地址空间"},{"content":" Linux提供了应用编程接口，通过这些接口，进程可以向其他进程或进程组发送信号。root权限的进程可以向任何进程发送信号，非root权限的进程智能向属于同一个回话或同一个用户的进程发送信号。\n###发送信号\n常用的函数原型如下\n/* 向进程发送信号 pid\u0026gt;0 进程ID为pid的进程 pid=0 同一进程组的进程 pid\u0026lt;0 \u0026amp;\u0026amp; pid!=-1 进程组ID为-pid的所有进程 pid=-1 除发送给进程自身外，还发送给所有进程ID\u0026gt;1的进程 成功返回0，否则-1 */ int kill(pid_t pid,int signo); /*向进程本身发送信号，等价于kill(getpid(),sig),成功返回0，否则-1*/ int raise(int signo); /*向进程发送SIGABORT信号,默认情况下进程会退出*/ void abort(void); /* 向进程发送实时信号 pid 接收信号的进程ID，只能向一个进程发送信号 sig 指定即将发送的信号 val指定信号传递的参数 成功返回0，否则-1 */ int sigqueue(pid_t pid,int sig,const union sigval val); union sigval { int sival_int;\t//传送一个整形数 void *sival_ptr;\t//传送任何数据结构的指针 }; typedef struct { int si_signo; int si_code; union sigval si_value; int si_errno; pid_t si_pid; uid_t si_uid; void *si_addr; int si_status; int si_band; } siginfo_t; 案例:使用sigqueue发送带参数的信号\n#include \u0026lt;signal.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; void SigHandler(int signo,siginfo_t *info,void *context) { printf(\u0026#34;%s\\n\u0026#34;,(char *)info-\u0026gt;si_value.sival_ptr); } int main() { struct sigaction sigAct; sigval_t val; char *pMsg = \u0026#34;i still believe\u0026#34;; sigAct.sa_flags = SA_SIGINFO; sigAct.sa_sigaction=SigHandler; if(sigaction(SIGUSR1,\u0026amp;sigAct,NULL)==-1) { printf(\u0026#34;fail set sig_handler\u0026#34;); return 1; } val.sival_ptr=pMsg; if(sigqueue(getpid(),SIGUSR1,val)==-1) { printf(\u0026#34;fail send sigqueue\u0026#34;); return 2; } sleep(3); } ###sleep睡眠延时\n可以使用sleep函数将程序延迟一段时间后继续执行，其实现机制是：\n调用alarm函数设置延迟时间 调用pause函数挂起进程，等待系统发送SIGALARM信号，当SIGALARM信号到达进程时，进程被唤醒。 /* 设置时间闹钟 seconds表示闹钟间隔时间，原有闹钟无效 若调用alarm函数前，进程已经设置了闹钟，则返回上一个闹钟剩余时间，否则返回0 */ unsigned int alarm(unsigned int seconds); //等待信号,进程收到信号后，执行信号处理函数，pause函数返回，原进程继续执行 void pause(); 案例：实现sleep函数\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;signal.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; void alarmhandler(int signum) { printf(\u0026#34;Alarm received from kernel\\n\u0026#34;); } int mysleep(unsigned int time) { printf(\u0026#34;about to sleep for %d seconds\\n\u0026#34;,time); signal(SIGALRM,alarmhandler); alarm(time); pause(); printf(\u0026#34;continue from alarm \\n\u0026#34;); return 0; } int main(int argc,char *argv[]) { printf(\u0026#34;start run the program.\\n\u0026#34;); unsigned int time = atoi(argv[1]); mysleep(time); printf(\u0026#34;i am awake,haha\\n\u0026#34;); return 0; } 间隔计时器 alarm函数计时单位是秒，当延迟时间到来，只能触发一次。不能满足需要高精度时间、有周期性定时需求的需求。为此，引入间隔计时器，其原理是：\n当等待时间来到，内核向处于等待状态的进程发送信号，同时，再次设置时间间隔。间隔计时器属于面向进程的计时器。\n进程运行时间\n通常，LInux系统最小时钟间隔是10ms，意味着每秒产生100个时钟中断。进程以时间片的形式分享CPU，进程的执行有两种模式：用户态和内核态。当进程执行的是用户地址空间的代码，称进程运行在用户态；当进程进入系统调用或硬件中断，称进程运行在内核态。此外，进程还有休眠态，即将CPU交给其他进程。所以进程并非时刻都在运行，而是在用户态、内核态、休眠态之间切换。\n由此内核提供三种计时器：\n真实时间\t用户态+内核态+休眠态时间 虚拟时间\t用户态时间 实用时间\t用户态+内核态时间 /* 获得当前进程中指定类型间隔计时器的值 which 计时器类型 ITIMER_REAL\t真实时间，经过指定时间，内核发送SIGALRM限号 ITIMER_VIRTUAL\t用户态时间，经过指定时间，内核发送SIGVTALRM信号 ITIMER_PROF\t实用时间，经过指定时间，内核发送SIGPRT信号 value 存储获得的间隔计时器的值 */ int getitimer(int which,struct itmerval *value); struct itimerval { struct timeval it_interval;\t//下一个值 struct timeval it_value\t//当前值 }; struct timeval { long tv_sec;\t//秒 long tv_usec;\t//微秒 }; /* 设置间隔计时器 which 指定定时器类型 newval指向被设置值 oldval指向被替换设置值 成功返回0，否则-1 若oldval不为NULL，之前计时器的值将被复制到oldval */ int setitimer(int which,const struct itimerval *newval,struct itimerval *oldval); ","permalink":"https://blog.niuhemoon.win/posts/tech/linux-c-signal-2/","summary":"Linux提供了应用编程接口，通过这些接口，进程可以向其他进程或进程组发送信号。root权限的进程可以向任何进程发送信号，非root权限的进程智能向属于同一个回话或同一个用户的进程发送信号。 ###发送信号 常用的函数原型如下 /* 向进程发送信号 pid\u0026gt;0 进程ID为pid的进程 pid=0 同一进程组的进程","title":"Linux信号处理——发送信号"},{"content":"文件系统概述 Linux内核的各种真实文件系统、块设备和字符设备统一在虚拟文件系统的框架中，虚拟文件系统为应用提供了一组抽象的文件输入输出接口。\n虚拟文件系统是对各种真实文件系统的抽象，在虚拟文件系统中定义了抽象的超级块、i节点和目录，它为真实文件系统提供了一种统一的框架接口。真实文件系统通过这些接口与虚拟文件系统相连接，真实文件系统是这些抽象接口的具体实现。\n虚拟文件系统存在于内存中，在系统启动时产生，随着系统关闭而消失。\n文件操作常用的头文件 C POSIX library是C语言的POSIX系统下的标准库。包含了一些在C语言标准库之外的函数。\n#include \u0026lt;unistd.h\u0026gt; //多种必要的POSIX函数与常量 #include \u0026lt;fcntl.h\u0026gt; //文件打开、创建、加锁等操作 #include \u0026lt;sys/stat.h\u0026gt;//文件信息(stat (Unix)等) #include \u0026lt;sys/types.h\u0026gt;//不同的数据类型 #include \u0026lt;dirent.h\u0026gt; //打开与列出目录内容 //此外，还有C标注库中的 #include \u0026lt;stdio.h\u0026gt;\t//标准缓存输入输出 文件基本输入输出 文件输入输出涉及到以下函数：\nopen、creat 在\u0026lt;fcntl.h\u0026gt;中 read、write、lseek、close 在\u0026lt;unistd.h\u0026gt;中 案例:复制文件\n//cp.c #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;fcntl.h\u0026gt; #define PMODE 0644\t//权限定义为rw-r--r-- #define BUFSIZE 200 int main(int argc,char *argv[]) { int fdin,fdout,n; char buf[BUFSIZ]; if(argc !=3) { fprintf(stderr,\u0026#34;Usage:%s filein fileout\\n\u0026#34;,argv[0]); return 1; } if((fdin = open(argv[1],O_RDONLY)) == -1) { perror(argv[1]); return 2; } if((fdout = open(argv[2],O_WRONLY|O_CREAT|O_TRUNC,PMODE)) == -1) { perror(argv[2]); return 3; } while((n = read(fdin,buf,BUFSIZE)) \u0026gt; 0) write(fdout,buf,n); close(fdin); close(fdout); return 0; } ###文件属性操作 文件的属性信息存放在文件对应的i节点中，对于不同类型的物理文件系统，文件属性的组织形式不尽相同，为了获得统一的文件属性格式，Linux定义了struct stat这个数据结构，类型定义如下：\nstruct stat { dev_t st_dev; /* 文件设备编号*/ ino_t st_ino; /* i节点号 */ mode_t st_mode; /* 文件类型和存储权限 */ nlink_t st_nlink; /* 硬链接 */ uid_t st_uid; /* 用户ID */ gid_t st_gid; /* 用户组ID */ dev_t st_rdev; /* Device ID (if special file)*/ off_t st_size; /* 文件字节数bytes */ blksize_t st_blksize; /* 块大小 */ blkcnt_t st_blocks; /* 以512bytes为单位的块数 */ struct timespec st_atim; /* 文件最后一次访问时间 */ struct timespec st_mtim; /* 文件最后一次修改时间 */ struct timespec st_ctim; /* 文件属性最后一次改变时间 */ }; 以之前的cp.c为例:\n可以看到cp.c文件实际大小640bytes，在磁盘上占用了一个4096bytes的块，也就是8个512bytes的块。`\n文件属性操作常用函数：\nstat 获取文件属性信息 chmod 设置文件权限 chown 设置文件属主 utime 获取时间 案例：改变文件读写权限\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;sys/stat.h\u0026gt; int main() { mode_t fdmode = (S_IRUSR|S_IWUSR|S_IRGRP|S_IROTH); if(chmod(\u0026#34;cp.c\u0026#34;,fdmode) == -1) { printf(\u0026#34;error\\n\u0026#34;); return 1; } return 0; } 编译运行，结果如下，可见文件权限已经被修改。\nniuhe@niuhe-ubuntu:~/Linux$ gcc -o chmod chmod.c niuhe@niuhe-ubuntu:~/Linux$ ls chmod chmod.c cp cp.c niuhe@niuhe-ubuntu:~/Linux$ ll cp.c -rw-rw-r-- 1 niuhe niuhe 640 10月 6 16:55 cp.c niuhe@niuhe-ubuntu:~/Linux$ ./chmod niuhe@niuhe-ubuntu:~/Linux$ ll cp.c -rw-r--r-- 1 niuhe niuhe 640 10月 6 16:55 cp.c ###目录操作 目录是一种特殊的文件，其内容由若干目录项组成，一个目录项包括文件名和i节点号。为了便于管理，每个目录中都包含当前目录\u0026quot;.\u0026ldquo;和父目录\u0026rdquo;..\u0026quot;,当前目录项指向当前目录的i节点编号，父目录项记录了父目录对应的i节点编号。 常用库函数及头文件如下：\n#include \u0026lt;sys/stat.h\u0026gt; //在某目录中创建一个目录项，分配一个i节点和目录项相链接 //分配一个逻辑块用来存放目录内容，并在其中建立当前目录和父目录两个目录项 int mkdir(const char *pathname,mode_t mode); #include \u0026lt;unistd.h\u0026gt; //删除空目录 int rmdir(const char *pathname); //改变工作目录；在每个进程的进程控制块中保存着当前工作目录的i节点 //初始工作目录继承自父进程，进程运行过程可以改变工作目录 int chdir(const char *pathname); //获得调用者进程的当前工作目录，buf存放路径，size路径包含字节数 char *getcwd(char *buf, size_t size); #include \u0026lt;dirent.h\u0026gt; //打开目录,成功返回目录流（字符串） DIR *opendir(const char *pahtname); //读目录，成功返回下一个目录项 struct dirent *readdir(DIR *dp); //关闭目录 int closedir(DIR *dp); 目录是一种特殊的文件，存放着很多目录项，每一个目录项是一个结构体。\nstruct dirent { long d_ino;\t//i节点号 char d_name[NAME_MAX+1];\t//文件名 off_t d_off;\t//在目录文件中偏移量 unsigned short d_reclen;\t//文件名长度 } 案例：改变当前进程工作目录\n#include \u0026lt;unistd.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; int main(void) { if(chdir(\u0026#34;/tmp\u0026#34;) \u0026lt; 0) printf(\u0026#34;chdir failed\u0026#34;); printf(\u0026#34;chdir to /tmp succeeded\\n\u0026#34;); return 0; } 案例：浏览目录中所有文件名\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;dirent.h\u0026gt; int main(int argc,char *argv[]) { DIR *dirp; struct dirent *direntp; if((dirp = opendir(argv[1])) == NULL) { printf(\u0026#34;cannot open the %s directory\u0026#34;,argv[1]); return 1; } while((direntp = readdir(dirp)) != NULL) printf(\u0026#34;%s\\n\u0026#34;,direntp-\u0026gt;d_name); closedir(dirp); return 0; } ###C标准I/O库\n运用read、write等系统底层函数进行输入输出时，需要在用户态和内核态之间来回切换。若每次读取或写入数据较少，将导致频繁的I/O操作，降低了程序运行效率。 标准I/O库对底层I/O系统调用进行了封装，提供了带格式转换的I/O操作，并在用户空间增加了缓冲管理，可减少程序和输出设备之间I/O次数。函数原型定义在\u0026lt;stdio.h\u0026gt;中。 细节略\nI/O重定向 文件描述符 Linux系统中，进程拥有各自打开文件的描述符。文件描述符按生成的顺序存放在文件描述符表中，Linux内核将文件描述符表用一维数组表述，每个打开的文件占用一个单元，用来存放操作文件的必要信息，如读写操作当前位置、文件打开方式、文件操作集等。 进程在打开一个文件时，返回的是文件描述符所在数组的下标，称为文件描述符。 通常，创建子进程时，子进程从父进程继承文件描述符表，前3个描述符0,1,2分别对应标准输入、标准输出、标准错误输出，与进程的控制终端设备对应。通常已经被打开，进行读写操作时无需重新打开。 一个文件可以同时被多个进程打开，它在不同进程中对应的文件描述符以及操作状态也未必相同。\n####I/O重定向\n程序根据打开文件的描述符对文件进行读写操作，真正完成读写操作的是进程描述符表相应位置中的内容。以输出重定向为例，若将进程描述符表中1号单元的内容替换为打开的文件test，则进程在向标准输出文件输出信息时，原本数据应显示在终端显示器上，但现在这些数据将被输出至文件test。 实现I/O重定向可以通过:\nopen close open方法 系统函数调用：dupdup2 #####open close open方法\nLinux在为进程新打开文件分配描述符时，从下表0开始扫描进程文件描述符表，将打开的文件信息放在找到的第一个空闲单元，并将该下表作为打开文件的描述符。 以标准输入0为例，将标准输入关闭，使得文件描述符表第0号单元成为空闲单元，此时，进程新打开另一个文件，内核将文件描述符表0号单元分配给新打开的文件，并返回描述符0，也就实现了输入重定向。\n#####dup和dup2函数\n使用dup和dup2函数，只是复制文件描述符，使两个文件描述符指向同一个file结构体，并且file结构体引用计数是2。此时，打开文件的状态保存在同一个file结构体中。而使用open函数两次打开一个文件会存在两份file结构体，分别有各自的状态。\n#include \u0026lt;unistd.h\u0026gt; //从进程文件描述符表中寻找一个可用的最小描述符,返回此描述符 //并复制oldfd对应的File结构指针到新的最小描述符 int dup(int oldfd); //oldfd需要复制的文件描述符，newfd是复制后oldfd在文件描述符表中新的序号。 //成功返回一个新描述符，否则返回-1 //若newfd已经打开，则先关闭newfd,然后复制oldfd到newfd,使newfd也指向oldfd,此时oldfd和newfd两个描述符共享同一个文件。 int dump2(int oldfd, int newfd); 案例:输出重定向\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;fcntl.h\u0026gt; int main() { int fileID; fileID = creat(\u0026#34;ls.tst\u0026#34;,0640); if(fileID \u0026lt; 0) { fprintf(stderr,\u0026#34;error creating ls.tst\\n\u0026#34;); return 1; } dup2(fileID,1); close(fileID); execl(\u0026#34;/bin/ls\u0026#34;,\u0026#34;ls\u0026#34;,NULL); return 0; } 参考 库函数列表 C POSIX LIB ","permalink":"https://blog.niuhemoon.win/posts/tech/linux-c-io/","summary":"文件系统概述 Linux内核的各种真实文件系统、块设备和字符设备统一在虚拟文件系统的框架中，虚拟文件系统为应用提供了一组抽象的文件输入输出接口。 虚拟文件系统是对各种真实文件系统的抽象，在虚拟文件系统中定义了抽象的超级块、i节点和目录，它为真实文件系统提供了一种统一的框架接口。真实文","title":"Linux下glic库操作文件和目录"},{"content":" 信号是内核和进程之间通信的一种方式，信号是由内核产生，并发送给一个或一组进程的短消息，用不同特定的数字表示不同的信号，信号的作用是表示某种事件的发生。\n信号简介 分类\n非实时不可靠信号，值为1-31 实时的可靠信号，值为32-63 信号由内核生成，信号生成和事件的发生密切相关，可将事件发生源分为以下三类：\n信号事件发生源：\n用户，如键入CTRL+C，终端驱动程序将通知内核产生信号发送到相应的进程 内核，内核执行过程中，遇到非法指令和浮点数溢出等情况 进程，一个进程调用kill函数向另一个进程发送信号，进行进程间通信 通常，LInux为每个信号定义了缺省的处理方式，但是用户可根据需要，对信号的处理方式进行重新定义。\n信号的缺省处理方式包括\nA 结束进程 B 忽略信号 C 结束进程并写入内核文件 D 停止进程 E 信号不能被捕获 F 信号不能被忽略 G 非POSIX信号 ###自定义信号处理函数\n必须重新建立信号值和处理方式之间的对应关系，才能重新定义信号的处理方式。LInux提供signal和sigaction函数来实现信号的设置。signal和sigaction的区别在于signal不支持项信号处理函数传递数据。\n注：SIGKILL和SIGSTOP不能被重定义或忽略。\nsignal函数原型:\n// __sig为需设置的信号，__handler为新信号处理函数；失败返回SIG_ERR，否则成功。 //__handler为SIG_IGN忽略信号，SIG_DEL默认信号处理 extern __sighandler_t signal (int __sig, __sighandler_t __handler) __THROW; 案例：使用signal重定义SIGINT处理函数\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;signal.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; int main() { void f(int); int i; signal(SIGINT,f); for(i=0;i\u0026lt;5;i++) { printf(\u0026#34;hello\\n\u0026#34;); sleep(1); } return 0; } void f(int signum) { printf(\u0026#34;hello Linux\\n\u0026#34;); } sigaction函数原型\n/* signo需要处理的信号 act指向描述信号操作的结构 oact指向被替换操作的结构 成功返回0，否则返回-1 */ int sigaction(int signo,const struct sigaction *act,struct sigaction *oact); /* sa_mask指定在信号处理过程中，何种信号被阻塞。缺省情况是当前信号被阻塞，以免信号处理函数被递归调用。 */ struct sigaction { void(*sa_handler)(int);\t//信号处理函数 void(*sa_sigaction)(int,siginfo_t*,void *);\t//带参数的信号处理函数 sigset_t sa_mask;\t//信号掩码 int sa_flags;\t//设定信号处理相关行为 } 案例：sigaction定义信号SIGINT处理函数，并屏蔽其他信号\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;signal.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; int num=0; void int_handle(int signum) { printf(\u0026#34;SIGINT:%d]n\u0026#34;,signum); printf(\u0026#34;int_handle called %d times\\n\u0026#34;,++num); } int main(void) { static struct sigaction act; void int_handle(int); act.sa_handler = int_handle; sigfillset(\u0026amp;(act.sa_mask)); sigaction(SIGINT,\u0026amp;act,NULL); while(1) { printf(\u0026#34;i am sleepy..\\n\u0026#34;); sleep(1); if(num \u0026gt;=3) { return 0; } } } ###信号集、信号屏蔽与阻塞\n信号屏蔽就是临时阻塞信号被发送到某个进程，它包含一个被阻塞的信号集。当进程屏蔽某个信号时，内核将不发送该信号至屏蔽它的进程，直至该信号的屏蔽被解除。 信号集用于描述所有信号的集合。对于sigaction中的sa_mask字段，每一位对应一个信号，若某一位被设置为1，表示该位对应信号被屏蔽。\n信号集定义及其操作函数\ntypedef struct { unsigned long sig[2]; }sigset_t int sigemptyset(sigset_t *set);\t//清空信号集中所有信号 int sigfillset(sigset_t *set);\t//在set信号集中加入linux支持的所有信号 int sigaddset(sigset_t *set,int signum);\t//向信号集中加入signum信号 int sigdelset(sigset_t *set,int signum);\t//从信号集中删除signum信号 int sigismember(const sigset_t *set, int signum)\t//判断signum信号是否在信号集set中 每个进程定义一个信号掩码，该掩码对应一个信号集，该信号集中的所有信号在发送至进程后都将被阻塞。通过更改进程的信号掩码来阻塞或解除阻塞所选择的信号。以此来保护不希望由信号中断的临界代码。\n信号阻塞函数sigprocmask\n/* how 如何修改信号掩码 SIG_BLOCK 添加信号到进程屏蔽 SIG_UNBLOCK将信号从进程屏蔽中删除 SIG_SETMASK将set的值设定为新的信号掩码 set 指向设置信号列表 oldset指向之前的信号掩码列表 */ int sigprocmask(int how,const sigset_t *set,sigset_t *oldset); 案例:阻塞SIGINT信号3秒后恢复\n#include \u0026lt;signal.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; int main(void) { sigset_t set; int count = 3; sigemptyset(\u0026amp;set); sigaddset(\u0026amp;set,SIGINT); sigprocmask(SIG_BLOCK,\u0026amp;set,NULL); while(count) { printf(\u0026#34;don\u0026#39;t disturb me (%d)\\n\u0026#34;,count--); sleep(1); } sigprocmask(SIG_UNBLOCK,\u0026amp;set,NULL); printf(\u0026#34;you did not disturb me!!\\n\u0026#34;); return 0; } ","permalink":"https://blog.niuhemoon.win/posts/tech/linux-c-signal-1/","summary":"信号是内核和进程之间通信的一种方式，信号是由内核产生，并发送给一个或一组进程的短消息，用不同特定的数字表示不同的信号，信号的作用是表示某种事件的发生。 信号简介 分类 非实时不可靠信号，值为1-31 实时的可靠信号，值为32-63 信号由内核生成，信号生成和事件的发生密切相关，可将事件发生","title":"Linux信号处理——自定义信号处理函数"},{"content":"函数库介绍 函数库分为：\n静态库 共享库（动态加载库） 应用程序在链接静态库时候，将使用的静态库对象嵌入至可执行映像文件中；而在链接共享库时，仅在可执行映像文件中保留加载目标对象所需的信息，在调用时，才真正将目标对象加载至内存。 静态库特点：\n运行时无需外部库的支持，可执行文件中已经嵌入了所需的静态库目标对象，所以可执行文件可以脱离静态库独立运行。 较高的运行速度，运行时不需要加载其他目标对象 可执行文件体积较大 不容易维护，每次修改静态库，必须重新链接 共享库特点：\n可执行文件体积较小 容易维护，共享库中对象发生变化，应用程序不需要重新编译 可执行文件中不包含共享库中调用的目标对象，因此不能离开动态库独立运行 运行速度比较慢，因为程序启动时需要加载共享库 静态库 静态库创建 静态库命名规则是lib开头，.a作为文件名后缀。可以使用ar命令作为静态库管理工具。ar可以将多个.o文件打包在一起，构成一个静态库文件。\n案例:\n先准备两个C源文件如下：\n//exam1.c int add(int x, int y) { return x + y; } //exam2.c int func(int count) { int sum=0; int j; for(j=0; j\u0026lt;=count; j++) { sum=sum+j; } return sum; } 编译两个源文件,后使用ar创建静态库：\ngcc -c -Wall exam1.c gcc -c -Wall exam2.c ar -crq libdemo.a exam1.o exam2.o 这样，我们就创建了一个简单的静态库。\n####静态库使用 定义静态库应用接口\n//exam.h extern int add(int x,int y); extern int func(int count); 使用静态库\n//testexam.c #include \u0026lt;stdio.h\u0026gt; #include \u0026#34;exam.h\u0026#34; int main(void) { int val; int x,y; x=12; y=18; val=add(x,y); printf(\u0026#34;the mult of x adn y is %d\\n\u0026#34;,val); val=func(100); printf(\u0026#34;the sum is %d \\n\u0026#34;,val); return 0; } 编译运行\ngcc -o testexam testexam.c -L ./ -ldemo 解释： -L指定了静态库所在的目录， ./就是当前目录\n-ldemo指定了静态库的名称libdemo.a\n共享库 若在同一目录下存在同名的共享库和静态库，gcc会优先使用共享库，除非指定了-static.\n####创建共享库\ngcc -fPIC -c exam1.c gcc -fPIC -c exam2.c gcc -shared -o libdemo.so exam1.o exam2.o 以上三句指令将exam1.c和exam2.c编译成了libdemo.so共享库文件。 -fPIC告诉gcc创建地址独立的目标文件 -shared告诉gcc创建一个共享库文件\n共享库使用 链接着共享库的应用启动时，一个程序装载器将自动运行，该程序装载器为/lib64/ld-linux.so.X,,它的作用是查找并装载应用程序所依赖的所有共享库的中的目标对象。\n将共享库libdemo.so放在/usr/local/lib/目录下 链接共享库\ngcc -o testexam testexam.c /usr/local/lib/libdemo.so 查看程序使用共享库情况\nldd testexam ###动态链接库\n动态链接库是使用共享库的一种方式，在运行的任何时刻可以动态加载共享库。和一般共享库不同，通常应用程序启动时，不立即加载共享库，而是在需要时，动态加载共享库。 以动态链接的方式使用共享库分为三个步骤：\n打开共享库文件 接着取得要调用函数的地址，根据地址使用函数指针进行调用 关闭共享库 Linux环境提供了一组API函数可以动态链接共享库，头文件定义在/usr/include/dlfcn.h中。\n案例\n//exam.c #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;dlfcn.h\u0026gt; #include \u0026#34;exam.h\u0026#34; #include \u0026lt;stdlib.h\u0026gt; int main(void) { int val; int (*add_d)(int,int); int (*func_d)(int); void *handle; char *err; int x,y; x = 12; y = 18; handle = dlopen(\u0026#34;/usr/local/lib/libdemo.so\u0026#34;,RTLD_LAZY); if (handle == (void *)0) { fputs(dlerror(),stderr); exit(-1); } add_d = dlsym(handle,\u0026#34;add\u0026#34;); err = dlerror(); if (err != NULL) { fputs(err,stderr); exit(-1); } func_d = dlsym(handle,\u0026#34;func\u0026#34;); err = dlerror(); if(err != NULL) { fputs(err,stderr); exit(-1); } val = (*add_d)(x,y); printf(\u0026#34;the mult of x and y is %d\\n\u0026#34;,val); val = (*func_d)(100); printf(\u0026#34;the sum of 1 to 100 is %d\\n\u0026#34;,val); dlclose(handle); return 0; } 编译链接\ngcc -rdynamic exam.c -o exam -ldl 执行\nniuhe@niuhe-ubuntu:/tmp/exp$ ./exam the mult of x and y is 30 the sum of 1 to 100 is 5050 ###GUN C函数库——glibc\nglibc是GNU开发的一套标准C语言标准函数库的实现。通常linux发行版都默认安装好了，查看glic的版本号ldd --version.\n","permalink":"https://blog.niuhemoon.win/posts/tech/linux-c-lib/","summary":"函数库介绍 函数库分为： 静态库 共享库（动态加载库） 应用程序在链接静态库时候，将使用的静态库对象嵌入至可执行映像文件中；而在链接共享库时，仅在可执行映像文件中保留加载目标对象所需的信息，在调用时，才真正将目标对象加载至内存。 静态库特点： 运行时无需外部库的支持，可执行文件中已经嵌入了所","title":"Linux下创建和使用C语言函数库"},{"content":" Shell 是一种命令行解释器，目前 Linux 下最常用的是 bash 解释器。Shell 不仅可以解释用户输入的命令，还可以解释执行基于命令的 Shell 脚本语言。 Shell 脚本是由命令、Shell 变量和控制语句灯语法元素构成的文本文件。默认情况下，Shell 对脚本中的内容逐行分析，并依次在不同的进程中解释执行。通常 Shell 脚本结构如下：\n#!/bin/bash var1 = \u0026#34;hello,shell.\u0026#34; echo $var1 ###Shell 变量\nShell 变量分为四类：\n用户自定义变量 环境变量 位置变量 预定义变量 变量的几种操作:\n操作整数赋值：变量名=变量值 引用：$变量名 清除：unset 变量名 查看：set 输出为环境变量：export 用户自定义变量 位置变量:\n-位置变量和传递参数的位置有关\n$0:脚本程序名称 $1，$2\u0026hellip;..:传递给脚本的参数，$1 代表第一个参数 预定义变量:\n预定义变量 含义 $* 传递到脚本的所有参数内容 $? 命令执行后的返回状态，0 成功，其他值表示错误 $$ 当前进程的进程号 $! 后台运行的最后一个进程号 $# 传递到脚本的参数的数量 命令替换 命令替换可以使命令的输出结果赋值给变量，注意语法形式 2 中是反引号`,不是单引号'\n语法形式：\nvar = $(command) var = `command` 案例：\necho \u0026#34;today is\u0026#34; `date` 输入输出 read 命令 功能：从键盘读取输入，并赋值给变量\n语法： read [选项] 变量名列表\n选项列表：\n选项 含义 -p prompt 设置提示信息 -n num 当 read 读 n 个字符后返回 -s 键盘输入不回显 -t timeout 设置超时时间 -r 取消转义字符的转义作用 -d delim 定义新的换行符 案例：\nread -s -n 1 -p \u0026#34;Yes(Y) or not(N)?\u0026#34; answer echo $answer echo 命令 功能：显示字符串或变量的值\n语法：echo [选项] 字符串\n选项列表：\n选项 -含义 -n 不再最后自动换行 -e 启用反斜线控制字符的转换 -E 不处理转义字符，缺省选项 支持的转义符列表：\\t\\n\\r\\\\b\\a等\n引号 反引号:命令替换\n单引号:单引号中所有字符保留原有字符含义，不能包含单引号。不支持元字符、变量替换、命令替换。\n双引号:不支持元字符，支持变量替换、命令替换。\n条件表达式 条件表达式用力啊判断条件是否满足\n#!/bin/bash hour=$(date +\u0026#34;%H\u0026#34;) case $hour in 0[1-9] | 1[01]) echo \u0026#34;Good morning!!\u0026#34; ;; 1[2-7]) echo \u0026#34;Good afternoon\u0026#34; ;; *) echo \u0026#34;Good evening\u0026#34; ;; esac 测试条件表达式真假方法如下，真 0 假 1：\ntest 条件表达式\n[条件表达式]\n条件表达式中常用五类操作符：\n文件状态操作符 字符串操作符 数字操作符 逻辑操作符 文件状态操作符（略）:\n-x filename 文件可执行返回真 字符串操作符:\n操作符 含义 string string 非空为真 -n string string 长度大于 0 为真 -z sring string 长度为 0 为真 string1=string2 相等为真 string1 != string2 不等为真 数字操作符:\n操作符 含义 n1 -eq n2 n1 和 n2 相等返回 0，否则 1 n1 -ne n2 不等为真 n1 -lt n2 n1 \u0026lt; n2 为真 n1 -gt n2 n1 \u0026gt; n2 为真 n1 -le n2 n1 \u0026lt;= n2 为真 n1 -ge n2 n1 \u0026gt;= n2 为真 逻辑操作符:\n操作符 含义 e1 -a e2 e1 和 e2 两个表达式同时为真返回 0，否则 1 e1 -o e2 e1,e2 有一个为真返回 0 !e1 e1 不为真时返回 0 命令分隔符 命令分隔符可以在一行中运行多个命令\n命令分隔符 含义 cmd1;cmd2 以独立进程依次运行 cmd1 和 cmd2 (cmd1;cmd2) 在同一进程中依次运行 cmd1 和 cmd2 cmd1 \u0026amp; cmd2 cmd1 和 cmd2 同时运行，分属不同进程组 cmd1 \u0026amp;\u0026amp; cmd2 当 cmd1 为真时，才执行 cmd2 cmd1 || cmd2 当 cmd1 为假时，才执行 cmd2 cmd1 | cmd2 管道符号，cmd1 的输出作为 cmd2 的输入 cmd1 \u0026amp; cmd1 在后台运行 判断语句 案例 1：比较两个数字大小\n#!/bin/bash echo \u0026#34;Enter the first interger:\u0026#34; read first echo \u0026#34;Enter the second interger:\u0026#34; read second if (( first \u0026gt; second )); then echo \u0026#34;$first is greater than $second\u0026#34; elif (( first \u0026lt; second ));then echo \u0026#34;$first is less than $second\u0026#34; else echo \u0026#34;$first is equal to $second\u0026#34; echo \u0026#34;Done\u0026#34; fi 案例 2：获取系统时间，判断上午、下午、晚上\n#!/bin/bash hour=$(date +\u0026#34;%H\u0026#34;) case $hour in 0[1-9] | 1[01]) echo \u0026#34;Good morning!!\u0026#34; ;; 1[2-7]) echo \u0026#34;Good afternoon\u0026#34; ;; *) echo \u0026#34;Good evening\u0026#34; ;; esac 循环语句 案例 1：for 循环打印所有命令行参数\n#!/bin/bash #you can write this in short:for arg for arg in $* do echo $arg done 案例 2：while 计算 1-99 的和\n#!/bin/bash i=1 sum=0 while [ $i -lt 100 ] do sum=`expr $sum + $i` i=`expr $i + 1` done echo The sum is $sum 案例 3：显示 1-100 之间的整数\n#!/bin/bash i=1 until [ $i -gt 100 ] do echo $i i=`expr $i + 1` done break [n] 跳出 n 重循环，默认为 1\ncontinue [n]\nexit [n]\n函数 语法：\n函数名() { 命令列表 return } `\n调用方式:\n函数名 参数列表\n注意：\n调用前，必须先定义 使用 shell 位置变量接收参数传递，例如$0$1 返回值取自函数中 return 语句或函数中最后一条命令的返回状态，通过$?获得 使用 local 声明的局部变量，作用仅限于函数本身 案例：\n#!/bin/bash #name:getsum.sh get_sum() { i=$1 sum=0 while [ $i -lt $2 ] do sum=`expr $sum + $i` i=`expr $i + 1` done return $sum } get_sum $2 $3 result=$? echo $1 echo sum of $2 to $3 is $result 运行:\n./getsum.sh \u0026#34;test\u0026#34; 5 7 注意事项 变量赋值的=两侧不能有空格 条件表达式 和表达式间必须有空格 expr 算式表达式中每个运算符两侧必须有空格 ","permalink":"https://blog.niuhemoon.win/posts/tech/basic-shell/","summary":"Shell 是一种命令行解释器，目前 Linux 下最常用的是 bash 解释器。Shell 不仅可以解释用户输入的命令，还可以解释执行基于命令的 Shell 脚本语言。 Shell 脚本是由命令、Shell 变量和控制语句灯语法元素构成的文本文件。默认情况下，Shell 对脚本中的内容逐行分析，并依次在不同的进程中解释执行。通常 Shell 脚本结构","title":"Shell编程基础"},{"content":" 现在很多传感器都使用串口进行数据传送，我们再window上通常使用sscom33这类调试工具，在linux下通常使用带界面的cutecom或者命令行界面的minicom进行调试。 而使用Python写几行程API序进行自定义调试，就非常有用。并且可以快速的对传感器进行测试。本文介绍python的串口读写模块Pyserial\n安装 sudo pip install pyserial 使用 确定串口设备名 Linux下把串口设备抽象成了文件，通常放在/dev/目录下，先找出串口设备的名称。\n断开串口设备的连接，执行 ls /dev/ \u0026gt; /tmp/old.txt 将串口设备连接到计算机，执行 ls /dev/ \u0026gt; /tmp/new.txt 最后，比较old.txt和new.txt，new.txt中多出的设备名就是我们的串口设备\ndiff /tmp/old.txt /tmp/new.txt 例程 以下是我读取超声波测距传感器的例程，传感器返回串口数据,每一帧数据的帧头是0xFF,其后的2byte数据是距离值。\nimport serial import time with serial.Serial(\u0026#39;/dev/ttyUSB0\u0026#39;,9600,stopbits=serial.STOPBITS_ONE,bytesize=serial.EIGHTBITS) as ser: while 1: head = ser.read(1) if head == b\u0026#39;\\xFF\u0026#39;: distance = int.from_bytes(ser.read(2),byteorder=\u0026#39;big\u0026#39;,signed=False) print(str(distance/1000) + \u0026#39;m\u0026#39;) 常用方法 在创建串口对象后，即ser = serial.Serial('/dev/ttyUSB0')，可以操作串口读写。\nser.read(size) ser.readline(size) ser.write(str_data) ser.close() 具体使用参见官方文档API\n参考 API参考\n","permalink":"https://blog.niuhemoon.win/posts/tech/pyserial-tutorial/","summary":"现在很多传感器都使用串口进行数据传送，我们再window上通常使用sscom33这类调试工具，在linux下通常使用带界面的cutecom或者命令行界面的minicom进行调试。 而使用Python写几行程API序进行自定义调试，就非常有用。并且可以快速的对传感器进行测试。本文介绍","title":"Pyserial快速上手"},{"content":"简介 LSM303DLHC是一个三轴加速度和三轴磁场的传感器，具有倾斜补偿，工作电压在2.5-5.5V之间，工作电流10mA，数据接口是I2C接口。\n引脚定义 VIN 2.5-5.5V电压供电引脚，SCL和SDA引脚的高电平电压和VIN引脚的电压相同。\nVDD 根据VIN引脚的连接情况，VDD引脚用途不同，若VIN引脚连接了大于3.3V的电源，VDD可以想歪提供3.3V的电压和大约150mA的电流。如果VIN断开连接，可以使用2.5-3.3的电源连接VDD给LSM303DLHC模块供电。 注意：\n不能同时使用VIN和VDD给LSM303DLHC供电，只能选取一个。 不要将VDD连接到大于3.6V的电源上，会损坏LSM303DLHC模块 GND 0V，连接到电源的地。注意和I2C总线共地。\nSCL 时钟线，高电平是VIN，低电平是0V。SCL和SDA都有电平转换电路，可以使得模块可以使用VIN的逻辑电平进行通信。\nSDA 数据线，高电平是VIN，低电平是0V\nDRDY 数据可读指示，3.3V逻辑电平输出，高电平(3.3V)指示磁场数据可读，低电平表示正在向数据寄存器中写入新的数据。此输出没有电平转换。\nINT1 INT2 两个惯性中断，没有电平转换，3.3V输出。\n例程和库 Arduino例程和库参考： https://github.com/pololu/lsm303-arduino\nLinux例程： https://github.com/ControlEverythingCommunity/LSM303DLHC\n参考 产品说明书\n","permalink":"https://blog.niuhemoon.win/posts/tech/lsm303dlhc-doc/","summary":"简介 LSM303DLHC是一个三轴加速度和三轴磁场的传感器，具有倾斜补偿，工作电压在2.5-5.5V之间，工作电流10mA，数据接口是I2C接口。 引脚定义 VIN 2.5-5.5V电压供电引脚，SCL和SDA引脚的高电平电压和VIN引脚的电压相同。 VDD 根据VIN引脚的连接情况，VDD引脚用","title":"LSM303_Doc"},{"content":" 在局域网下可以通过扫描端口号，获得局域网下树莓派的ip地址，如192.168.1.118。之后通过ssh或者VNC等方法访问树莓派。那么如果我们离开局域网，怎么访问到家中的树莓派呢？ 公网IP相当于街道中的门牌号，如果远程访问到互联网中的设备，必须知道设备的公网IP。\nVPS + SSH远程代理隧道 这是这篇文章重点，需要：\n一个安装了linux的VPS，如一台腾讯云服务器。（如果没有，请看下一种方法：公网IP+路由器端口转发） 连上网络的树莓派,将树莓派ssh公钥放到VPS上 树莓派上使用autossh+crontab sudo apt install autossh cd ~/ echo \u0026#34;sleep 20; /usr/bin/autossh -M 5678 -NfR 9999:localhost:22 user@T.T.T.T -i /home/user/.ssh/id_rsa\u0026#34; \u0026gt; autossh.sh chmod +x autossh.sh crontab -e 在crontab的配置文件中加入\n@reboot /home/nh/autossh.sh \u0026gt;\u0026gt; tunnel.log 在服务器上操作 ssh -fCNL \u0026#34;*:11111:localhost:9999\u0026#34; localhost 远程登录 现在就可以使用服务器作为跳板，连接到家中的树莓派。\nssh -p 11111 user@T.T.T.T 注：user@T.T.T.T,user是树莓派用户名，T.T.T.T是服务器公网IP。-p指定向服务器1111端口发送ssh请求，1111端口转发到9999端口，也就是树莓派的22端口。\n显示效果如下：\n以下树莓派配置方法均不推荐，想折腾的自己玩，服务器配置方法都一样\n树莓派上安装sshpass(可选） sudo apt-get install sshpass 树莓派上编辑shell脚本 #!/bin/bash #saved as create_ssh_tunnel.sh createTunnel() { sshpass -p \u0026#34;Password\u0026#34; ssh -o \u0026#34;ServerAliveInterval 300\u0026#34; -o \u0026#34;ServerAliveCountMax 2\u0026#34; -fCNR 9999:localhost:22 user@T.T.T.T if [[ $? -eq 0 ]]; then echo $(date) Tunnel to jumpbox created successfully \u0026gt;\u0026gt; /root/tunnel.log else echo $(date) An error occurred creating a tunnel to jumpbox. RC was $? \u0026gt;\u0026gt; /root/tunnel.log fi } /bin/pidof ssh if [[ $? -ne 0 ]]; then echo $(date) Creating new tunnel connection \u0026gt;\u0026gt; /root/tunnel.log createTunnel fi 注：这种方式非常危险，不推荐这种方式，因为明文存储了服务器密码。\n\u0026ldquo;Password\u0026quot;指的是你的服务器密码 user@T.T.T.T是你的服务器用户名和IP地址 9999是将树莓派的22端口绑定到服务器额9999端口 推荐将树莓派上的公钥放到远程服务器上，然后使用如下脚本\n#saved as create_ssh_tunnel.sh createTunnel() { ssh -o \u0026#34;ServerAliveInterval 300\u0026#34; -o \u0026#34;ServerAliveCountMax 2\u0026#34; -NR 9999:localhost:22 user@T.T.T.T if [[ $? -eq 0 ]]; then echo $(date) Tunnel to jumpbox created successfully \u0026gt;\u0026gt; ~/tunnel.log else echo $(date) An error occurred creating a tunnel to jumpbox. RC was $? \u0026gt;\u0026gt; ~/tunnel.log fi } /bin/pidof ssh if [[ $? -ne 0 ]]; then echo $(date) Creating new tunnel connection \u0026gt;\u0026gt; ~/tunnel.log createTunnel fi 给脚本添加执行权限\nchmod +x creat_ssh_tunnel.sh 树莓派上开启cron定时任务 sudo crontab -e 在文件最后一行添加：\n*/10 * * * * ～/create_ssh_tunnel.sh \u0026gt;\u0026gt; tunnel.log 2\u0026gt;\u0026amp;1 意思是每十分钟执行一次刚刚我们编辑的shell脚本\n公网IP + 路由器端口转发 如果用网线将树莓派连接到互联网上，只要获得公网IP，皆可以通过ssh访问。但是我们在家中通常是用wifi连接树莓派，我们获得的公网iP，只是路由器的IP。由于一个路由器上连接很多设备，我们无法通过这个公网IP访问到树莓派。而且由于路由器的公网IP是会变动的。所以我们要解决的问题是：\n找到路由器的公网IP 通过公网IP找到路由器下连接的树莓派 可以在路由器设置里将树莓派分配固定的IP，并绑定固定端口。这样就可以通过访问这个端口访问到局域网下的树莓派。之后不管是将公网IP发送到邮箱还是，使用动态DNS解析服务，通过域名访问树莓派，都可以很容易实现。\n参考 SSH反向代理 外网访问树莓派方法汇总 用DDNS服务通过域名访问树莓派 使用crontab让SSH反向代理更持久 ","permalink":"https://blog.niuhemoon.win/posts/tech/raspi-remote-ssh-tunnel/","summary":"在局域网下可以通过扫描端口号，获得局域网下树莓派的ip地址，如192.168.1.118。之后通过ssh或者VNC等方法访问树莓派。那么如果我们离开局域网，怎么访问到家中的树莓派呢？ 公网IP相当于街道中的门牌号，如果远程访问到互联网中的设备，必须知道设备的公网IP。 VPS + SSH远程","title":"远程ssh连接家中的树莓派"},{"content":" 在使用超声测距模块时，需要给超声模块一个方波信号。于是可以用树莓派的的PWM功能产生一个低频的方波信号。\n产生方波信号后，如果手边没有示波器，还可以使用Arduino的ADC采样功能，做一个简单的示波器。\n树莓派产生方波 树莓派的pin12、pin33(GPIO_18、GPIO_13)是树莓派提供的PWM硬件接口，可以产生高频的PWM信号。 由于我只需要产生一个大约50Hz的方波信号。用最简单的GPIO库就可以产生可用的方波。\n#!/usr/bin/python3 # -*- coding: utf-8 -*- import RPi.GPIO as GPIO from time import sleep GPIO.setmode(GPIO.BOARD) GPIO.setup(12,GPIO.OUT) GPIO.setwarnings(False) p=GPIO.PWM(12,50) # 12是pin12,50是频率 p.start(30) # 30表示占空比30% input(\u0026#34;Press Enter key to Stop 50Hz PWM @ 30% duty cycle\u0026#34;) p.stop() GPIO.cleanup() 运行该脚本，就可以在树莓派pin12上产生方波信号。\nArduino和树莓派连线 连线图如下所示： 注意： 要将树莓派和Arduino的地线连接在一起，使它们共地。\nArduino 进行ADC采样 Arduino 有A0-A5共6个模拟输入口，每个模拟口可以进行12位的采样，可以接受0-5V的电压输入，对应着0-1023的采样输出。\n使用A0口进行采样：\nvoid setup() { Serial.begin(9600); // Starting Serial Terminal } void loop() { int value = analogRead(A0); Serial.println(value); } 打开Arduino官方的IDE的【工具】-\u0026gt; 【串口绘图器】\n设置波特率为9600,可以观察到：\n注：树莓派GPIO引脚输出电压为3.3V，而Arduino采样范围是0-5V\n还可以使用SerialPlot这个功能更丰富的串口绘图器，当做简易的示波器。 显示的波形如下： Arduino串口绘图器 Arduino串口绘图器可以绘制多个连续图形，如下程序就是在串口绘图器中画出sin和cos函数图像。\ndouble i = 0; void setup() { Serial.begin(9600); } void loop() { double temp = i*3.1415926/10.0; Serial.print(sin(temp)); Serial.print(\u0026#39;,\u0026#39;); Serial.println(cos(temp)); i+=0.1; delay(5); } 注：\n若只绘制一个图像，使用Serial.println()函数即可 若绘制多个图像，在每个串口值间使用Serial.print(\u0026rsquo;,\u0026rsquo;)进行分隔 ","permalink":"https://blog.niuhemoon.win/posts/tech/arduino-raspberry-squrewave/","summary":"在使用超声测距模块时，需要给超声模块一个方波信号。于是可以用树莓派的的PWM功能产生一个低频的方波信号。 产生方波信号后，如果手边没有示波器，还可以使用Arduino的ADC采样功能，做一个简单的示波器。 树莓派产生方波 树莓派的pin12、pin33(GPIO_18、GPIO_13)","title":"Arduino简易示波器检测树莓派产生的方波"},{"content":" Arduino可以使用PWM产生方波信号，在我的Arduino UNO R3上，支持PWM的输出口是pin 3,5,6,9,10,11这几个引脚，支持大约980Hz的PWM输出。这方面不再赘述。\n本文介绍另一种产生方波的方法，可以使用任何引脚产生方波信号。功能：\n固定频率，占空比，偏移量的方波 通过模拟口连接可调电位器，产生可变频率、占空比、偏移量的方波 // High-accuracy square wave generator // based on Arduino UNO // with runtime adjustable frequency, PWM width and offset // Output wave at pin 13 double freq; // Hz double offset; // percent (0.0 to 1.0) double width; // percent (0.0 to 1.0) // unit: microsecond unsigned long cycle_time; unsigned long raising_edge; unsigned long falling_edge; unsigned long prev_micros; // compare 2 unsigned value // true if X \u0026gt; Y while for all possible (X, Y), X - Y \u0026lt; Z #define TIME_CMP(X, Y, Z) (((X) - (Y)) \u0026lt; (Z)) inline void setHigh() { // 2 CPU cycles to balance execution time with setLow() // this is based on measurement on Arduino UNO R3, your mileage may vary PORTB = B00100000; PORTB = B00100000; } inline void setLow() { PORTB = B00000000; } void setup() { DDRB = B00100000; prev_micros = micros(); while(1) { // read everything from analog input (potentiometer) // frequency: 0.1-102.4 Hz // width: 0-100% // offset: 0-100% //freq = (double)(analogRead(1) + 1) / 10; //width = (double)(analogRead(0) + 1) / 1024; //offset = (double)analogRead(2) / 1024; // OR manual settings // max possible frequency is around 55000Hz with \u0026lt;1KHz deviation // based on measurements on Arduino UNO R3 // you may get to ~77500Hz with significantly larger deviation // note: please uncomment the next 3 expressions, then // move the following 6 expressions ahead of while loop // if you are going to use manual settings, because it is no worth // to recalculate them. freq = 50; width = 0.3; offset = 0.0; cycle_time = 1000000 / freq; raising_edge = (unsigned long)(offset * cycle_time) % cycle_time; falling_edge = (unsigned long)((offset + width) * cycle_time) % cycle_time; if (width + offset \u0026lt; 1) { // raising edge should appear earlier while (TIME_CMP(micros(), prev_micros + raising_edge, cycle_time)); setHigh(); while (TIME_CMP(micros(), prev_micros + falling_edge, cycle_time)); setLow(); } else { // falling edge should appear earlier while (TIME_CMP(micros(), prev_micros + falling_edge, cycle_time)); setLow(); while (TIME_CMP(micros(), prev_micros + raising_edge, cycle_time)); setHigh(); }pin prev_micros += cycle_time; } } 解释：\nPORTB表示的是控制pin8-pin13的寄存器，最高的两位6\u0026amp;7不使用。 可以改变B00100000来在其他pin脚上产生方波信号 同样的PROTD寄存器控制digital pin0-pin7 参考 程序来源；James Swineson github@public.swineson.me, 2017-05\nhttps://gist.github.com/Jamesits/8d164818946a65d0cafcd6203e3e5049\nhttps://blog.swineson.me/high-frequency-square-wave-generator-based-on-arduino-uno/\n操控Arduino端口寄存器\n","permalink":"https://blog.niuhemoon.win/posts/tech/arduino-squrewave/","summary":"Arduino可以使用PWM产生方波信号，在我的Arduino UNO R3上，支持PWM的输出口是pin 3,5,6,9,10,11这几个引脚，支持大约980Hz的PWM输出。这方面不再赘述。 本文介绍另一种产生方波的方法，可以使用任何引脚产生方波信号。功能： 固定频率，占空比，偏移量的方波","title":"Arduino任何引脚产生方波"},{"content":"安装LCD库 使用AdaFruit库来控制lcd库，这个库支持AdaFruit屏幕和使用HD44780的显示屏。\n通过源码安装：\ngit clone https://github.com/adafruit/Adafruit_Python_CharLCD.git cd ./Adafruit_Python_CharLCD sudo python setup.py install 将树莓派和LCD1602连接 连接的图如下所示： LCD电子钟程序 #!/usr/bin/python3 # -*- coding: utf-8 -*- import RPi.GPIO as gpio #to add the LCD library import Adafruit_CharLCD as LCD import time gpio.setmode(gpio.BCM) #声明 LCD pins（对应BCM引脚） lcd_rs = 17 lcd_en = 18 lcd_d4 = 27 lcd_d5 = 22 lcd_d6 = 23 lcd_d7 = 10 lcd_backlight = 2 lcd_columns = 16 #Lcd column lcd_rows = 2 #number of LCD rows lcd = LCD.Adafruit_CharLCD(lcd_rs, lcd_en, \\ lcd_d4, lcd_d5, lcd_d6, lcd_d7, lcd_columns, lcd_rows,\\ lcd_backlight) lcd.set_cursor(0,0) lcd.message(\u0026#39; CLOCK\u0026#39;) while True: lcd.set_cursor(0,1) localtime = time.asctime( time.localtime(time.time()) )[4:-5] print(localtime) lcd.message(localtime) time.sleep(1) 效果图 显示的效果图如下，可以显示日期和时间，每秒钟刷新屏幕一次： ","permalink":"https://blog.niuhemoon.win/posts/tech/raspberrypi-lcd-clock/","summary":"安装LCD库 使用AdaFruit库来控制lcd库，这个库支持AdaFruit屏幕和使用HD44780的显示屏。 通过源码安装： git clone https://github.com/adafruit/Adafruit_Python_CharLCD.git cd ./Adafruit_Python_CharLCD sudo python setup.py install 将树莓派和LCD1602连接 连接的图如下所示： LCD电子钟程序 #!/usr/bin/python3 # -*- coding: utf-8 -*- import RPi.GPIO as gpio #to add the LCD library import Adafruit_CharLCD as LCD import time gpio.setmode(gpio.BCM) #声明 LCD pins（对应BCM引脚","title":"树莓派连接LCD1602做一个电子钟"},{"content":" 在树莓派3B+里启用串口，并通过UART读取GPS模块的数据帧。\n树莓派3启用UART #####先更新系统\nsudo apt-get update sudo apt-get upgrade sudo raspi-config 在raspi-config中设置：\ndisable login shell over serial enable serial hardware port #####然后重启\nsudo reboot #####编辑配置文件\nsudo nano /boot/config.txt 在最后一段加上：\ndtparam=spi=on dtoverlay=pi3-disable-bt core_freq=250 enable_uart=1 force_turbo=1 然后编辑cmdline.txt\nsudo cp boot/cmdline.txt boot/cmdline_backup.txt sudo nano /boot.cmdline.txt 将cmdline.txt的内容替换为： dwc_otg.lpm_enable=0 console=tty1 root=/dev/mmcblk0p2 rootfstype=ext4 elevator=deadline fsck.repair=yes rootwait quiet splash plymouth.ignore-serial-consoles\n#####禁用树莓派Serial Getty服务\nsudo systemctl stop serial-getty@ttyS0.service sudo systemctl disable serial-getty@ttyS0.service 重启系统\nsudo reboot #####激活ttyAMAO 上一步我们禁用了ttyS0,现在我们启用ttyAMA0.\nsudo systemctl enable serial-getty@ttyAMA0.service 好了到了现在，我们已经启用了树莓派的串口，并可以通过/dev/ttyAMA0来访问串口设备。\n树莓派的串口引脚 树莓派的引脚定义图如下所示： UART的收发引脚分别为：\nTx ——GPIO14（pin8) Rx ——GPIO15（pin10) 连接串口设备并测试 我们使用GPS串口设备进行测试，GPS模块有如下5个引脚：\nVCC GND TX RX PPS GPS模块和树莓派连接方式如下：\nVCC ——pin01 GND——pin06 TX——pin10 其他引脚可以不连接。\n使用cat或者minicom调试GPS串口：\ncat /dev/ttyAMA0 #或者 minicom -D /dev/ttyAMA0 -b 9600 能够读取到类似下边的串口输出：\n$GPTXT,01,01,01,ANTENNA OK*35 $GNGGA,115810.475,,,,,0,00,25.5,,,,,,*70 $GNGLL,,,,,115810.475,V,M*6D $GPGSA,A,1,,,,,,,,,,,,,25.5,25.5,25.5*02 $BDGSA,A,1,,,,,,,,,,,,,25.5,25.5,25.5*13 $GPGSV,3,1,09,05,05,121,,10,25,314,27,12,14,138,,13,13,058,16*73 $GPGSV,3,2,09,15,49,043,32,20,53,321,18,21,52,249,,24,77,090,*77 $GPGSV,3,3,09,32,11,265,*40 参考 树莓派UART文档 Raspberry Pi GPS Module Interfacing Tutorial ","permalink":"https://blog.niuhemoon.win/posts/tech/activate-serial-uart/","summary":"在树莓派3B+里启用串口，并通过UART读取GPS模块的数据帧。 树莓派3启用UART #####先更新系统 sudo apt-get update sudo apt-get upgrade sudo raspi-config 在raspi-config中设置： disable login shell over serial enable serial hardware port #####然后重启 sudo reboot #####编辑配置文件 sudo nano /boot/config.txt 在最后一段加上： dtparam=spi=on dtoverlay=pi3-disable-bt core_freq=250 enable_uart=1 force_turbo=1 然后编辑cmdline.txt sudo","title":"树莓派3启用UART并连接GPS传感器"},{"content":"本文介绍树莓派上GPIO引脚的三种编号方式，同时介绍控制引脚的方式。并以BCM-17引脚为例，动手点亮led灯并使其闪烁。\nGPIO是通用输入输出接口。树莓派上有40个引脚，对这40个引脚主要有两种编号方式。\nBOARD 和引脚物理顺序一致 BCM wiringpi编号 wiringPi是一个用来控制GPIO的库，它对40个引脚的编号方式如下：\n详细信息参考此网址\nwiringPi 安装方式 应用于C语言和shell git clone git://git.drogon.net/wiringPi cd wiringPi git pull origin ./build # 测试安装成功 gpio -vmake gpio readall 应用于Python sudo pip install wiringpi2 测试example程序 将发光二极管的阳极连接BOARD的pin11,也就是BCM17，wiringPi 0，中间需要串一个100-500欧姆的限流电阻。阴极连接到GND引脚（BOARD 9）。 命令行输入：\ngpio write 0 1\n可观察到发光二极管被点亮\ngpio函数其他用法参见文档\n编译示例C语言程序程序，使led闪烁\ncd wiringPi/examples/ ./blink.sh # 或者 make blink ./blink 使用Python控制GPIO 树莓派原生系统内置的python已经安装了RPi.GPIO库，通过它可以方便的控制GPIO\n可以参考以下两份文档：\nGeneral-RPIO\nPython-RPIO\nPython实现LED闪烁，引脚依然是BCM-17\n#!/usr/bin/python3 # -*- coding: utf-8 -*- import RPi.GPIO as GPIO from time import sleep GPIO.setmode(GPIO.BCM) GPIO.setwarnings(False) GPIO.setup(17,GPIO.OUT) print(\u0026#34;All set in Python! Let\u0026#39;s blink the LCD on BCM-17\u0026#34;) for i in range(1,10): GPIO.output(17,GPIO.HIGH) sleep(1) GPIO.output(17,GPIO.LOW) sleep(1) GPIO.cleanup() 使用wiringpi库来控制连接BCM-17的led灯闪烁，具体使用说明参看： python-wiringpi使用教程\n#!/usr/bin/python3 # -*- coding: utf-8 -*- import wiringpi from time import sleep # 设置wiringpi编号0引脚为输出模式 wiringpi.wiringPiSetup() wiringpi.pinMode(0,1) while True: wiringpi.digitalWrite(0,True) sleep(1) wiringpi.digitalWrite(0,False) sleep(1) 以下为wiringpi的三种引脚编号：\nBCM BOARD wiringpi 三种控制模式：\ninput output pwm 设置示范如下：\n# GPIO 引脚号就是BCM编号 import wiringpi wiringpi.wiringPiSetupGpio() wiringpi.pinMode(25, 0) # sets GPIO 25 to input wiringpi.pinMode(24, 1) # sets GPIO 24 to output wiringpi.pinMode(18, 2) # sets GPIO 18 to PWM mode # wiringpi 编号 import wiringpi wiringpi.wiringPiSetup() wiringpi.pinMode(6, 0) # sets WP pin 6 to input wiringpi.pinMode(5, 1) # sets WP pin 5 to output wiringpi.pinMode(1, 2) # sets WP pin 1 to PWM mode # 物理编号 BOARD编号 import wiringpi wiringPiSetupPhys() wiringpi.pinMode(22, 0) # sets P1 pin 22 to input wiringpi.pinMode(18, 1) # sets P1 pin 18 to output wiringpi.pinMode(12, 2) # sets P1 pin 12 to PWM mode ","permalink":"https://blog.niuhemoon.win/posts/tech/wiringpi-gpio/","summary":"本文介绍树莓派上GPIO引脚的三种编号方式，同时介绍控制引脚的方式。并以BCM-17引脚为例，动手点亮led灯并使其闪烁。 GPIO是通用输入输出接口。树莓派上有40个引脚，对这40个引脚主要有两种编号方式。 BOARD 和引脚物理顺序一致 BCM wiringpi编号 wiringPi是一个用来控制G","title":"树莓派GPIO入门"},{"content":"Python3实现AES和DES对称加密算法的\n对称算法编写 本实验是使用python来实现AES和DES加密和解密过程，并对加密解密过程的正确性进行验证。\n1.实验目的 掌握分组加密的原理 掌握数据加密标准DES和高级数据加密标准AES的原理及其应用 2.实验工具 Jupyter Notebook Python3.5 3.实验环境 Ubuntu16.04LTS操作系统 Python3标准库 4.实验步骤 4.1 回顾课程，查阅资料 4.2 熟悉AES的原理 AES(Advanced Encryption Standard)高级加密标准，在密码学中又称Rijndael加密法，是美国联邦政府采用的一种区块加密标准。这个标准用来替代原先的DES，已经被多方分析且广为全世界所使用。\nAES加密过程是在一个4×4的字节矩阵上运作，这个矩阵又称为“体（state）”，其初值就是一个明文区块（矩阵中一个元素大小就是明文区块中的一个Byte）。（Rijndael加密法因支持更大的区块，其矩阵行数可视情况增加）加密时，各轮AES加密循环（除最后一轮外）均包含4个步骤：\nAddRoundKey—矩阵中的每一个字节都与该次回合密钥（round key）做XOR运算；每个子密钥由密钥生成方案产生。 SubBytes—通过一个非线性的替换函数，用查找表的方式把每个字节替换成对应的字节。 ShiftRows—将矩阵中的每个横列进行循环式移位。 MixColumns—为了充分混合矩阵中各个直行的操作。这个步骤使用线性转换来混合每内联的四个字节。最后一个加密循环中省略MixColumns步骤，而以另一个AddRoundKey取代。 4.3 编写AES加密解密的测试用例 先使用其他已有工具计算出plaintext对应的\n测试程序如下：\nimport unittest class AES: def __init__(self, master_key): self.change_key(master_key) def change_key(self, master_key): pass def encrypt(self, plaintext): pass def decrypt(self, ciphertext): pass class AES_TEST(unittest.TestCase): def setUp(self): master_key = 0x2b7e151628aed2a6abf7158809cf4f3c self.AES = AES(master_key) def test_encryption(self): plaintext = 0x3243f6a8885a308d313198a2e0370734 encrypted = self.AES.encrypt(plaintext) self.assertEqual(encrypted, 0x3925841d02dc09fbdc118597196a0b32) def test_decryption(self): ciphertext = 0x3925841d02dc09fbdc118597196a0b32 decrypted = self.AES.decrypt(ciphertext) self.assertEqual(decrypted, 0x3243f6a8885a308d313198a2e0370734) if __name__ == \u0026#39;__main__\u0026#39;: unittest.main() 4.4 编写DES算法的测试代码 4.5 编写AES和DES对称加密的python实现，并进行测试验证 代码和测试结果在报告最后\n4.6 总结实验，编写实验报告 5. 实验总结 需要善于借鉴前人经验，搜集资料 实验只支持ascii码，对其他字符编码没有解决 利用优秀的工具，如Jupyter可以提高学习效率 6. 实验完整代码 #!/usr/bin/env python #-*- coding:utf-8 -*- \u0026#34;\u0026#34;\u0026#34; ASE的python实现： 支持128位秘钥 支持秘钥和原文均为数字的AES加密解密 \u0026#34;\u0026#34;\u0026#34; Sbox = ( 0x63, 0x7C, 0x77, 0x7B, 0xF2, 0x6B, 0x6F, 0xC5, 0x30, 0x01, 0x67, 0x2B, 0xFE, 0xD7, 0xAB, 0x76, 0xCA, 0x82, 0xC9, 0x7D, 0xFA, 0x59, 0x47, 0xF0, 0xAD, 0xD4, 0xA2, 0xAF, 0x9C, 0xA4, 0x72, 0xC0, 0xB7, 0xFD, 0x93, 0x26, 0x36, 0x3F, 0xF7, 0xCC, 0x34, 0xA5, 0xE5, 0xF1, 0x71, 0xD8, 0x31, 0x15, 0x04, 0xC7, 0x23, 0xC3, 0x18, 0x96, 0x05, 0x9A, 0x07, 0x12, 0x80, 0xE2, 0xEB, 0x27, 0xB2, 0x75, 0x09, 0x83, 0x2C, 0x1A, 0x1B, 0x6E, 0x5A, 0xA0, 0x52, 0x3B, 0xD6, 0xB3, 0x29, 0xE3, 0x2F, 0x84, 0x53, 0xD1, 0x00, 0xED, 0x20, 0xFC, 0xB1, 0x5B, 0x6A, 0xCB, 0xBE, 0x39, 0x4A, 0x4C, 0x58, 0xCF, 0xD0, 0xEF, 0xAA, 0xFB, 0x43, 0x4D, 0x33, 0x85, 0x45, 0xF9, 0x02, 0x7F, 0x50, 0x3C, 0x9F, 0xA8, 0x51, 0xA3, 0x40, 0x8F, 0x92, 0x9D, 0x38, 0xF5, 0xBC, 0xB6, 0xDA, 0x21, 0x10, 0xFF, 0xF3, 0xD2, 0xCD, 0x0C, 0x13, 0xEC, 0x5F, 0x97, 0x44, 0x17, 0xC4, 0xA7, 0x7E, 0x3D, 0x64, 0x5D, 0x19, 0x73, 0x60, 0x81, 0x4F, 0xDC, 0x22, 0x2A, 0x90, 0x88, 0x46, 0xEE, 0xB8, 0x14, 0xDE, 0x5E, 0x0B, 0xDB, 0xE0, 0x32, 0x3A, 0x0A, 0x49, 0x06, 0x24, 0x5C, 0xC2, 0xD3, 0xAC, 0x62, 0x91, 0x95, 0xE4, 0x79, 0xE7, 0xC8, 0x37, 0x6D, 0x8D, 0xD5, 0x4E, 0xA9, 0x6C, 0x56, 0xF4, 0xEA, 0x65, 0x7A, 0xAE, 0x08, 0xBA, 0x78, 0x25, 0x2E, 0x1C, 0xA6, 0xB4, 0xC6, 0xE8, 0xDD, 0x74, 0x1F, 0x4B, 0xBD, 0x8B, 0x8A, 0x70, 0x3E, 0xB5, 0x66, 0x48, 0x03, 0xF6, 0x0E, 0x61, 0x35, 0x57, 0xB9, 0x86, 0xC1, 0x1D, 0x9E, 0xE1, 0xF8, 0x98, 0x11, 0x69, 0xD9, 0x8E, 0x94, 0x9B, 0x1E, 0x87, 0xE9, 0xCE, 0x55, 0x28, 0xDF, 0x8C, 0xA1, 0x89, 0x0D, 0xBF, 0xE6, 0x42, 0x68, 0x41, 0x99, 0x2D, 0x0F, 0xB0, 0x54, 0xBB, 0x16, ) InvSbox = ( 0x52, 0x09, 0x6A, 0xD5, 0x30, 0x36, 0xA5, 0x38, 0xBF, 0x40, 0xA3, 0x9E, 0x81, 0xF3, 0xD7, 0xFB, 0x7C, 0xE3, 0x39, 0x82, 0x9B, 0x2F, 0xFF, 0x87, 0x34, 0x8E, 0x43, 0x44, 0xC4, 0xDE, 0xE9, 0xCB, 0x54, 0x7B, 0x94, 0x32, 0xA6, 0xC2, 0x23, 0x3D, 0xEE, 0x4C, 0x95, 0x0B, 0x42, 0xFA, 0xC3, 0x4E, 0x08, 0x2E, 0xA1, 0x66, 0x28, 0xD9, 0x24, 0xB2, 0x76, 0x5B, 0xA2, 0x49, 0x6D, 0x8B, 0xD1, 0x25, 0x72, 0xF8, 0xF6, 0x64, 0x86, 0x68, 0x98, 0x16, 0xD4, 0xA4, 0x5C, 0xCC, 0x5D, 0x65, 0xB6, 0x92, 0x6C, 0x70, 0x48, 0x50, 0xFD, 0xED, 0xB9, 0xDA, 0x5E, 0x15, 0x46, 0x57, 0xA7, 0x8D, 0x9D, 0x84, 0x90, 0xD8, 0xAB, 0x00, 0x8C, 0xBC, 0xD3, 0x0A, 0xF7, 0xE4, 0x58, 0x05, 0xB8, 0xB3, 0x45, 0x06, 0xD0, 0x2C, 0x1E, 0x8F, 0xCA, 0x3F, 0x0F, 0x02, 0xC1, 0xAF, 0xBD, 0x03, 0x01, 0x13, 0x8A, 0x6B, 0x3A, 0x91, 0x11, 0x41, 0x4F, 0x67, 0xDC, 0xEA, 0x97, 0xF2, 0xCF, 0xCE, 0xF0, 0xB4, 0xE6, 0x73, 0x96, 0xAC, 0x74, 0x22, 0xE7, 0xAD, 0x35, 0x85, 0xE2, 0xF9, 0x37, 0xE8, 0x1C, 0x75, 0xDF, 0x6E, 0x47, 0xF1, 0x1A, 0x71, 0x1D, 0x29, 0xC5, 0x89, 0x6F, 0xB7, 0x62, 0x0E, 0xAA, 0x18, 0xBE, 0x1B, 0xFC, 0x56, 0x3E, 0x4B, 0xC6, 0xD2, 0x79, 0x20, 0x9A, 0xDB, 0xC0, 0xFE, 0x78, 0xCD, 0x5A, 0xF4, 0x1F, 0xDD, 0xA8, 0x33, 0x88, 0x07, 0xC7, 0x31, 0xB1, 0x12, 0x10, 0x59, 0x27, 0x80, 0xEC, 0x5F, 0x60, 0x51, 0x7F, 0xA9, 0x19, 0xB5, 0x4A, 0x0D, 0x2D, 0xE5, 0x7A, 0x9F, 0x93, 0xC9, 0x9C, 0xEF, 0xA0, 0xE0, 0x3B, 0x4D, 0xAE, 0x2A, 0xF5, 0xB0, 0xC8, 0xEB, 0xBB, 0x3C, 0x83, 0x53, 0x99, 0x61, 0x17, 0x2B, 0x04, 0x7E, 0xBA, 0x77, 0xD6, 0x26, 0xE1, 0x69, 0x14, 0x63, 0x55, 0x21, 0x0C, 0x7D, ) # 参考了C语言实现 http://cs.ucsb.edu/~koc/cs178/projects/JT/aes.c xtime = lambda a: (((a \u0026lt;\u0026lt; 1) ^ 0x1B) \u0026amp; 0xFF) if (a \u0026amp; 0x80) else (a \u0026lt;\u0026lt; 1) Rcon = ( 0x00, 0x01, 0x02, 0x04, 0x08, 0x10, 0x20, 0x40, 0x80, 0x1B, 0x36, 0x6C, 0xD8, 0xAB, 0x4D, 0x9A, 0x2F, 0x5E, 0xBC, 0x63, 0xC6, 0x97, 0x35, 0x6A, 0xD4, 0xB3, 0x7D, 0xFA, 0xEF, 0xC5, 0x91, 0x39, ) def text2matrix(text): matrix = [] for i in range(16): byte = (text \u0026gt;\u0026gt; (8 * (15 - i))) \u0026amp; 0xFF if i % 4 == 0: matrix.append([byte]) else: matrix[int(i / 4)].append(byte) return matrix def matrix2text(matrix): text = 0 for i in range(4): for j in range(4): text |= (matrix[i][j] \u0026lt;\u0026lt; (120 - 8 * (4 * i + j))) return text class AES: def __init__(self, master_key): self.change_key(master_key) def change_key(self, master_key): self.round_keys = text2matrix(master_key) # print self.round_keys for i in range(4, 4 * 11): self.round_keys.append([]) if i % 4 == 0: byte = self.round_keys[i - 4][0] \\ ^ Sbox[self.round_keys[i - 1][1]] \\ ^ Rcon[int(i / 4)] self.round_keys[i].append(byte) for j in range(1, 4): byte = self.round_keys[i - 4][j] \\ ^ Sbox[self.round_keys[i - 1][(j + 1) % 4]] self.round_keys[i].append(byte) else: for j in range(4): byte = self.round_keys[i - 4][j] \\ ^ self.round_keys[i - 1][j] self.round_keys[i].append(byte) # print self.round_keys def encrypt(self, plaintext): self.plain_state = text2matrix(plaintext) self.__add_round_key(self.plain_state, self.round_keys[:4]) for i in range(1, 10): self.__round_encrypt(self.plain_state, self.round_keys[4 * i : 4 * (i + 1)]) self.__sub_bytes(self.plain_state) self.__shift_rows(self.plain_state) self.__add_round_key(self.plain_state, self.round_keys[40:]) return matrix2text(self.plain_state) def decrypt(self, ciphertext): self.cipher_state = text2matrix(ciphertext) self.__add_round_key(self.cipher_state, self.round_keys[40:]) self.__inv_shift_rows(self.cipher_state) self.__inv_sub_bytes(self.cipher_state) for i in range(9, 0, -1): self.__round_decrypt(self.cipher_state, self.round_keys[4 * i : 4 * (i + 1)]) self.__add_round_key(self.cipher_state, self.round_keys[:4]) return matrix2text(self.cipher_state) def __add_round_key(self, s, k): for i in range(4): for j in range(4): s[i][j] ^= k[i][j] def __round_encrypt(self, state_matrix, key_matrix): self.__sub_bytes(state_matrix) self.__shift_rows(state_matrix) self.__mix_columns(state_matrix) self.__add_round_key(state_matrix, key_matrix) def __round_decrypt(self, state_matrix, key_matrix): self.__add_round_key(state_matrix, key_matrix) self.__inv_mix_columns(state_matrix) self.__inv_shift_rows(state_matrix) self.__inv_sub_bytes(state_matrix) def __sub_bytes(self, s): for i in range(4): for j in range(4): s[i][j] = Sbox[s[i][j]] def __inv_sub_bytes(self, s): for i in range(4): for j in range(4): s[i][j] = InvSbox[s[i][j]] def __shift_rows(self, s): s[0][1], s[1][1], s[2][1], s[3][1] = s[1][1], s[2][1], s[3][1], s[0][1] s[0][2], s[1][2], s[2][2], s[3][2] = s[2][2], s[3][2], s[0][2], s[1][2] s[0][3], s[1][3], s[2][3], s[3][3] = s[3][3], s[0][3], s[1][3], s[2][3] def __inv_shift_rows(self, s): s[0][1], s[1][1], s[2][1], s[3][1] = s[3][1], s[0][1], s[1][1], s[2][1] s[0][2], s[1][2], s[2][2], s[3][2] = s[2][2], s[3][2], s[0][2], s[1][2] s[0][3], s[1][3], s[2][3], s[3][3] = s[1][3], s[2][3], s[3][3], s[0][3] def __mix_single_column(self, a): # please see Sec 4.1.2 in The Design of Rijndael t = a[0] ^ a[1] ^ a[2] ^ a[3] u = a[0] a[0] ^= t ^ xtime(a[0] ^ a[1]) a[1] ^= t ^ xtime(a[1] ^ a[2]) a[2] ^= t ^ xtime(a[2] ^ a[3]) a[3] ^= t ^ xtime(a[3] ^ u) def __mix_columns(self, s): for i in range(4): self.__mix_single_column(s[i]) def __inv_mix_columns(self, s): # see Sec 4.1.3 in The Design of Rijndael for i in range(4): u = xtime(xtime(s[i][0] ^ s[i][2])) v = xtime(xtime(s[i][1] ^ s[i][3])) s[i][0] ^= u s[i][1] ^= v s[i][2] ^= u s[i][3] ^= v self.__mix_columns(s) master_key = 0x2b7e151628aed2a6abf7158809cf4f3c AESbar = AES(master_key) def test_encryption(): plaintext = 0x3243f6a8885a308d313198a2e0370734 encrypted = AESbar.encrypt(plaintext) print(\u0026#39;加密得密文： 0x%x\u0026#39; % encrypted) assert encrypted == 0x3925841d02dc09fbdc118597196a0b32 print(\u0026#39;加密结果正确\u0026#39;) def test_decryption(): ciphertext = 0x3925841d02dc09fbdc118597196a0b32 decrypted = AESbar.decrypt(ciphertext) print(\u0026#39;解密得原文: 0x%x\u0026#39; % decrypted) assert decrypted == 0x3243f6a8885a308d313198a2e0370734 print(\u0026#39;解密成功\u0026#39;) if __name__ == \u0026#39;__main__\u0026#39;: print(\u0026#39;开始单元测试：\u0026#39;) test_encryption() test_decryption() print(\u0026#39;单元测试成功\u0026#39;) 开始单元测试： 加密得密文： 0x3925841d02dc09fbdc118597196a0b32 加密结果正确 解密得原文: 0x3243f6a8885a308d313198a2e0370734 解密成功 单元测试成功 #!/usr/bin/env python #-*- coding:utf-8 -*- \u0026#39;\u0026#39;\u0026#39; DES加密解密的Python3实现 \u0026#39;\u0026#39;\u0026#39; #Initial permut matrix for the datas PI = [58, 50, 42, 34, 26, 18, 10, 2, 60, 52, 44, 36, 28, 20, 12, 4, 62, 54, 46, 38, 30, 22, 14, 6, 64, 56, 48, 40, 32, 24, 16, 8, 57, 49, 41, 33, 25, 17, 9, 1, 59, 51, 43, 35, 27, 19, 11, 3, 61, 53, 45, 37, 29, 21, 13, 5, 63, 55, 47, 39, 31, 23, 15, 7] #Initial permut made on the key CP_1 = [57, 49, 41, 33, 25, 17, 9, 1, 58, 50, 42, 34, 26, 18, 10, 2, 59, 51, 43, 35, 27, 19, 11, 3, 60, 52, 44, 36, 63, 55, 47, 39, 31, 23, 15, 7, 62, 54, 46, 38, 30, 22, 14, 6, 61, 53, 45, 37, 29, 21, 13, 5, 28, 20, 12, 4] #Permut applied on shifted key to get Ki+1 CP_2 = [14, 17, 11, 24, 1, 5, 3, 28, 15, 6, 21, 10, 23, 19, 12, 4, 26, 8, 16, 7, 27, 20, 13, 2, 41, 52, 31, 37, 47, 55, 30, 40, 51, 45, 33, 48, 44, 49, 39, 56, 34, 53, 46, 42, 50, 36, 29, 32] #Expand matrix to get a 48bits matrix of datas to apply the xor with Ki E = [32, 1, 2, 3, 4, 5, 4, 5, 6, 7, 8, 9, 8, 9, 10, 11, 12, 13, 12, 13, 14, 15, 16, 17, 16, 17, 18, 19, 20, 21, 20, 21, 22, 23, 24, 25, 24, 25, 26, 27, 28, 29, 28, 29, 30, 31, 32, 1] #SBOX S_BOX = [ [[14, 4, 13, 1, 2, 15, 11, 8, 3, 10, 6, 12, 5, 9, 0, 7], [0, 15, 7, 4, 14, 2, 13, 1, 10, 6, 12, 11, 9, 5, 3, 8], [4, 1, 14, 8, 13, 6, 2, 11, 15, 12, 9, 7, 3, 10, 5, 0], [15, 12, 8, 2, 4, 9, 1, 7, 5, 11, 3, 14, 10, 0, 6, 13], ], [[15, 1, 8, 14, 6, 11, 3, 4, 9, 7, 2, 13, 12, 0, 5, 10], [3, 13, 4, 7, 15, 2, 8, 14, 12, 0, 1, 10, 6, 9, 11, 5], [0, 14, 7, 11, 10, 4, 13, 1, 5, 8, 12, 6, 9, 3, 2, 15], [13, 8, 10, 1, 3, 15, 4, 2, 11, 6, 7, 12, 0, 5, 14, 9], ], [[10, 0, 9, 14, 6, 3, 15, 5, 1, 13, 12, 7, 11, 4, 2, 8], [13, 7, 0, 9, 3, 4, 6, 10, 2, 8, 5, 14, 12, 11, 15, 1], [13, 6, 4, 9, 8, 15, 3, 0, 11, 1, 2, 12, 5, 10, 14, 7], [1, 10, 13, 0, 6, 9, 8, 7, 4, 15, 14, 3, 11, 5, 2, 12], ], [[7, 13, 14, 3, 0, 6, 9, 10, 1, 2, 8, 5, 11, 12, 4, 15], [13, 8, 11, 5, 6, 15, 0, 3, 4, 7, 2, 12, 1, 10, 14, 9], [10, 6, 9, 0, 12, 11, 7, 13, 15, 1, 3, 14, 5, 2, 8, 4], [3, 15, 0, 6, 10, 1, 13, 8, 9, 4, 5, 11, 12, 7, 2, 14], ], [[2, 12, 4, 1, 7, 10, 11, 6, 8, 5, 3, 15, 13, 0, 14, 9], [14, 11, 2, 12, 4, 7, 13, 1, 5, 0, 15, 10, 3, 9, 8, 6], [4, 2, 1, 11, 10, 13, 7, 8, 15, 9, 12, 5, 6, 3, 0, 14], [11, 8, 12, 7, 1, 14, 2, 13, 6, 15, 0, 9, 10, 4, 5, 3], ], [[12, 1, 10, 15, 9, 2, 6, 8, 0, 13, 3, 4, 14, 7, 5, 11], [10, 15, 4, 2, 7, 12, 9, 5, 6, 1, 13, 14, 0, 11, 3, 8], [9, 14, 15, 5, 2, 8, 12, 3, 7, 0, 4, 10, 1, 13, 11, 6], [4, 3, 2, 12, 9, 5, 15, 10, 11, 14, 1, 7, 6, 0, 8, 13], ], [[4, 11, 2, 14, 15, 0, 8, 13, 3, 12, 9, 7, 5, 10, 6, 1], [13, 0, 11, 7, 4, 9, 1, 10, 14, 3, 5, 12, 2, 15, 8, 6], [1, 4, 11, 13, 12, 3, 7, 14, 10, 15, 6, 8, 0, 5, 9, 2], [6, 11, 13, 8, 1, 4, 10, 7, 9, 5, 0, 15, 14, 2, 3, 12], ], [[13, 2, 8, 4, 6, 15, 11, 1, 10, 9, 3, 14, 5, 0, 12, 7], [1, 15, 13, 8, 10, 3, 7, 4, 12, 5, 6, 11, 0, 14, 9, 2], [7, 11, 4, 1, 9, 12, 14, 2, 0, 6, 10, 13, 15, 3, 5, 8], [2, 1, 14, 7, 4, 10, 8, 13, 15, 12, 9, 0, 3, 5, 6, 11], ] ] #Permut made after each SBox substitution for each round P = [16, 7, 20, 21, 29, 12, 28, 17, 1, 15, 23, 26, 5, 18, 31, 10, 2, 8, 24, 14, 32, 27, 3, 9, 19, 13, 30, 6, 22, 11, 4, 25] #Final permut for datas after the 16 rounds PI_1 = [40, 8, 48, 16, 56, 24, 64, 32, 39, 7, 47, 15, 55, 23, 63, 31, 38, 6, 46, 14, 54, 22, 62, 30, 37, 5, 45, 13, 53, 21, 61, 29, 36, 4, 44, 12, 52, 20, 60, 28, 35, 3, 43, 11, 51, 19, 59, 27, 34, 2, 42, 10, 50, 18, 58, 26, 33, 1, 41, 9, 49, 17, 57, 25] #Matrix that determine the shift for each round of keys SHIFT = [1,1,2,2,2,2,2,2,1,2,2,2,2,2,2,1] def string_to_bit_array(text):#Convert a string into a list of bits array = list() for char in text: binval = binvalue(char, 8)#Get the char value on one byte array.extend([int(x) for x in list(binval)]) #Add the bits to the final list return array def bit_array_to_string(array): #Recreate the string from the bit array res = \u0026#39;\u0026#39;.join([chr(int(y,2)) for y in [\u0026#39;\u0026#39;.join([str(x) for x in bytes]) for bytes in nsplit(array,8)]]) return res def binvalue(val, bitsize): #Return the binary value as a string of the given size binval = bin(val)[2:] if isinstance(val, int) else bin(ord(val))[2:] if len(binval) \u0026gt; bitsize: raise \u0026#34;binary value larger than the expected size\u0026#34; while len(binval) \u0026lt; bitsize: binval = \u0026#34;0\u0026#34;+binval #Add as many 0 as needed to get the wanted size return binval def nsplit(s, n):#Split a list into sublists of size \u0026#34;n\u0026#34; return [s[k:k+n] for k in range(0, len(s), n)] ENCRYPT=1 DECRYPT=0 class des(): def __init__(self): self.password = None self.text = None self.keys = list() def run(self, key, text, action=ENCRYPT, padding=False): if len(key) \u0026lt; 8: raise \u0026#34;Key Should be 8 bytes long\u0026#34; elif len(key) \u0026gt; 8: key = key[:8] #If key size is above 8bytes, cut to be 8bytes long self.password = key self.text = text if padding and action==ENCRYPT: self.addPadding() elif len(self.text) % 8 != 0:#If not padding specified data size must be multiple of 8 bytes raise \u0026#34;Data size should be multiple of 8\u0026#34; self.generatekeys() #Generate all the keys text_blocks = nsplit(self.text, 8) #Split the text in blocks of 8 bytes so 64 bits result = list() for block in text_blocks:#Loop over all the blocks of data block = string_to_bit_array(block)#Convert the block in bit array block = self.permut(block,PI)#Apply the initial permutation g, d = nsplit(block, 32) #g(LEFT), d(RIGHT) tmp = None for i in range(16): #Do the 16 rounds d_e = self.expand(d, E) #Expand d to match Ki size (48bits) if action == ENCRYPT: tmp = self.xor(self.keys[i], d_e)#If encrypt use Ki else: tmp = self.xor(self.keys[15-i], d_e)#If decrypt start by the last key tmp = self.substitute(tmp) #Method that will apply the SBOXes tmp = self.permut(tmp, P) tmp = self.xor(g, tmp) g = d d = tmp result += self.permut(d+g, PI_1) #Do the last permut and append the result to result final_res = bit_array_to_string(result) if padding and action==DECRYPT: return self.removePadding(final_res) #Remove the padding if decrypt and padding is true else: return final_res #Return the final string of data ciphered/deciphered def substitute(self, d_e):#Substitute bytes using SBOX subblocks = nsplit(d_e, 6)#Split bit array into sublist of 6 bits result = list() for i in range(len(subblocks)): #For all the sublists block = subblocks[i] row = int(str(block[0])+str(block[5]),2)#Get the row with the first and last bit column = int(\u0026#39;\u0026#39;.join([str(x) for x in block[1:][:-1]]),2) #Column is the 2,3,4,5th bits val = S_BOX[i][row][column] #Take the value in the SBOX appropriated for the round (i) bin = binvalue(val, 4)#Convert the value to binary result += [int(x) for x in bin]#And append it to the resulting list return result def permut(self, block, table):#Permut the given block using the given table (so generic method) return [block[x-1] for x in table] def expand(self, block, table):#Do the exact same thing than permut but for more clarity has been renamed return [block[x-1] for x in table] def xor(self, t1, t2):#Apply a xor and return the resulting list return [x^y for x,y in zip(t1,t2)] def generatekeys(self):#Algorithm that generates all the keys self.keys = [] key = string_to_bit_array(self.password) key = self.permut(key, CP_1) #Apply the initial permut on the key g, d = nsplit(key, 28) #Split it in to (g-\u0026gt;LEFT),(d-\u0026gt;RIGHT) for i in range(16):#Apply the 16 rounds g, d = self.shift(g, d, SHIFT[i]) #Apply the shift associated with the round (not always 1) tmp = g + d #Merge them self.keys.append(self.permut(tmp, CP_2)) #Apply the permut to get the Ki def shift(self, g, d, n): #Shift a list of the given value return g[n:] + g[:n], d[n:] + d[:n] def addPadding(self):#Add padding to the datas using PKCS5 spec. pad_len = 8 - (len(self.text) % 8) self.text += pad_len * chr(pad_len) def removePadding(self, data):#Remove the padding of the plain text (it assume there is padding) pad_len = ord(data[-1]) return data[:-pad_len] def encrypt(self, key, text, padding=False): return self.run(key, text, ENCRYPT, padding) def decrypt(self, key, text, padding=False): return self.run(key, text, DECRYPT, padding) if __name__ == \u0026#39;__main__\u0026#39;: key = \u0026#34;secret_k\u0026#34; text= \u0026#34;Hello wo\u0026#34; d = des() r = d.encrypt(key,text) r2 = d.decrypt(key,r) print(\u0026#34;Ciphered: %r\u0026#34; % r) print(\u0026#34;Deciphered: \u0026#34;, r2) assert r2 == text print(\u0026#39;成功实现DES加密解密\u0026#39;) Ciphered: 'ßåýåÚ\\x9f\\\\\\x9d' Deciphered: Hello wo 成功实现DES加密解密 ","permalink":"https://blog.niuhemoon.win/posts/tech/python-symmetric-encryption/","summary":"\u003cp\u003ePython3实现AES和DES对称加密算法的\u003c/p\u003e","title":"Python对称AES和DES加密算法"},{"content":"Python3实现哈希散列算法，包含MD5和sha256。\nHash函数算法编写 本实验是使用python来编写MD5和SHA256加密函数，并对加密函数的正确性进行验证。\n验证的方式是通过和已有的标准库加密结果进行比较，如果结果相同，则加密函数正确。\n1.实验目的 熟悉MD5和SHA256加密函数的原理和应用 实现MD5和SHA256加密函数并验证 2.实验工具 Jupyter Notebook Python3.5 3.实验环境 Ubuntu16.04LTS操作系统 Python3标准库 4.实验步骤 4.1 回顾课程，查阅资料 4.2 熟悉MD5的原理 MD5(Message-Digest Algorithm)消息摘要算法是一种广泛使用的密码散列函数，可以产生出一个128位（16字节）的散列值（hash value），用于确保信息传输完整一致。\nMD5将可变长度的消息处理成128位的固定长度输出。输入消息被分解成512位块（16个32位字）块;该消息被填充以使其长度可以被512整除。\n填充的工作原理如下：\n首先将单个位1附加到消息的末尾 接着填充0，直到消息长度比512的整数倍少64 在接下来64位中填充真实消息的长度值 主MD5算法在128位状态下运行，分为四个32位字，分别表示为A，B，C和D.这些字被初始化为某些固定的常量。\n主算法然后依次使用每个512位消息块来修改状态。\n消息块的处理由四个相似的阶段组成，称为循环;每轮由基于非线性函数F，模加法和左旋等16种类似操作组成。\n图1说明了一轮内的一个操作流程。 图2表示F有四种可能的功能;每一轮使用不同的一个： 下图是非线性函数的四种可能\n4.3 查阅得MD5算法的伪代码 //Note: All variables are unsigned 32 bit and wrap modulo 2^32 when calculating var int[64] s, K var int i //s specifies the per-round shift amounts s[ 0..15] := { 7, 12, 17, 22, 7, 12, 17, 22, 7, 12, 17, 22, 7, 12, 17, 22 } s[16..31] := { 5, 9, 14, 20, 5, 9, 14, 20, 5, 9, 14, 20, 5, 9, 14, 20 } s[32..47] := { 4, 11, 16, 23, 4, 11, 16, 23, 4, 11, 16, 23, 4, 11, 16, 23 } s[48..63] := { 6, 10, 15, 21, 6, 10, 15, 21, 6, 10, 15, 21, 6, 10, 15, 21 } //Use binary integer part of the sines of integers (Radians) as constants: for i from 0 to 63 K[i] := floor(232 × abs(sin(i + 1))) end for //Initialize variables: var int a0 := 0x67452301 //A var int b0 := 0xefcdab89 //B var int c0 := 0x98badcfe //C var int d0 := 0x10325476 //D //Pre-processing: adding a single 1 bit append \u0026#34;1\u0026#34; bit to message // Notice: the input bytes are considered as bits strings, // where the first bit is the most significant bit of the byte.[48] //Pre-processing: padding with zeros append \u0026#34;0\u0026#34; bit until message length in bits ≡ 448 (mod 512) append original length in bits mod 264 to message //Process the message in successive 512-bit chunks: for each 512-bit chunk of padded message break chunk into sixteen 32-bit words M[j], 0 ≤ j ≤ 15 //Initialize hash value for this chunk: var int A := a0 var int B := b0 var int C := c0 var int D := d0 //Main loop: for i from 0 to 63 var int F, g if 0 ≤ i ≤ 15 then F := (B and C) or ((not B) and D) g := i else if 16 ≤ i ≤ 31 F := (D and B) or ((not D) and C) g := (5×i + 1) mod 16 else if 32 ≤ i ≤ 47 F := B xor C xor D g := (3×i + 5) mod 16 else if 48 ≤ i ≤ 63 F := C xor (B or (not D)) g := (7×i) mod 16 //Be wary of the below definitions of a,b,c,d F := F + A + K[i] + M[g] A := D D := C C := B B := B + leftrotate(F, s[i]) end for //Add this chunk\u0026#39;s hash to result so far: a0 := a0 + A b0 := b0 + B c0 := c0 + C d0 := d0 + D end for var char digest[16] := a0 append b0 append c0 append d0 //(Output is in little-endian) //leftrotate function definition leftrotate (x, c) return (x \u0026lt;\u0026lt; c) binary or (x \u0026gt;\u0026gt; (32-c)); 4.4 编写MD5算法测试用例 Python标准库hashlib中提供了生成MD5值的方法，是通过C语言实现的，我们用自己实现的MD5函数计算出MD5值，在和标准库的结果进行比较，如果结果相同，那么证明了MD5加密是正确的。 注意：\n只实现了对ASCII码字符串的处理 demo中存储了测试用例 my_md5()返回自己生成的MD5值 true_md5()返回Python标准库生成的MD5值 若两者结果不同，抛出AssertionError异常 测试程序如下：\nimport hashlib def my_md5(message): pass def true_md5(message): m = hashlib.md5() m.update(message) return m.hexdigest() if __name__==\u0026#39;__main__\u0026#39;: demo = [b\u0026#34;\u0026#34;, b\u0026#34;a\u0026#34;, b\u0026#34;abc\u0026#34;, b\u0026#34;message digest\u0026#34;, b\u0026#34;abcdefghijklmnopqrstuvwxyz\u0026#34;, b\u0026#34;ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789\u0026#34;, b\u0026#34;12345678901234567890123456789012345678901234567890123456789012345678901234567890\u0026#34;] for message in demo: print(my_md5(message),\u0026#39; \u0026lt;= \u0026#34;\u0026#39;,message.decode(\u0026#39;ascii\u0026#39;),\u0026#39;\u0026#34;\u0026#39;, sep=\u0026#39;\u0026#39;) assert true_md5(message)==my_md5(message) # 若和标准库中不同，会抛出异常 4.5 查阅sha256算法原理和伪代码 sha256是SHA2(Secure Hash Algorithm 2)算法的一个变体，具体内容。\n篇幅原因不再贴sha256的伪代码，维基百科中有详细说明。\n4.6 编写sha256算法的测试代码 import hashlib def my_sha256(message): pass def true_sha256(message): m = hashlib.sha256() m.update(message) return m.hexdigest() if __name__==\u0026#39;__main__\u0026#39;: demo = [b\u0026#34;\u0026#34;, b\u0026#34;a\u0026#34;, b\u0026#34;abc\u0026#34;, b\u0026#34;message digest\u0026#34;, b\u0026#34;abcdefghijklmnopqrstuvwxyz\u0026#34;, b\u0026#34;ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789\u0026#34;, b\u0026#34;12345678901234567890123456789012345678901234567890123456789012345678901234567890\u0026#34;] for message in demo: print(my_sha256(message),\u0026#39; \u0026lt;= \u0026#34;\u0026#39;,message.decode(\u0026#39;ascii\u0026#39;),\u0026#39;\u0026#34;\u0026#39;, sep=\u0026#39;\u0026#39;) assert true_sha256(message)==my_sha256(message) # 若和标准库中不同，会抛出异常 4.7 编写MD5和SHA256的python实现，并进行测试验证 代码和测试结果在报告最后\n4.8 总结实验，编写实验报告 5. 实验总结 需要善于借鉴前人经验，搜集资料 实验只支持ascii码，对其他字符编码没有解决 利用优秀的工具，如Jupyter可以提高学习效率 6. 实验完整代码 5.1 MD5实现 # MD5 实现及其验证 import math import hashlib rotate_amounts = [7, 12, 17, 22, 7, 12, 17, 22, 7, 12, 17, 22, 7, 12, 17, 22, 5, 9, 14, 20, 5, 9, 14, 20, 5, 9, 14, 20, 5, 9, 14, 20, 4, 11, 16, 23, 4, 11, 16, 23, 4, 11, 16, 23, 4, 11, 16, 23, 6, 10, 15, 21, 6, 10, 15, 21, 6, 10, 15, 21, 6, 10, 15, 21] constants = [int(abs(math.sin(i+1)) * 2**32) \u0026amp; 0xFFFFFFFF for i in range(64)] # A B C D init_values = [0x67452301, 0xefcdab89, 0x98badcfe, 0x10325476] # 非线性函数 functions = 16*[lambda b, c, d: (b \u0026amp; c) | (~b \u0026amp; d)] + \\ 16*[lambda b, c, d: (d \u0026amp; b) | (~d \u0026amp; c)] + \\ 16*[lambda b, c, d: b ^ c ^ d] + \\ 16*[lambda b, c, d: c ^ (b | ~d)] index_functions = 16*[lambda i: i] + \\ 16*[lambda i: (5*i + 1)%16] + \\ 16*[lambda i: (3*i + 5)%16] + \\ 16*[lambda i: (7*i)%16] # 对x左移amount位 def left_rotate(x, amount): x \u0026amp;= 0xFFFFFFFF return ((x\u0026lt;\u0026lt;amount) | (x\u0026gt;\u0026gt;(32-amount))) \u0026amp; 0xFFFFFFFF def md5(message): message = bytearray(message) #copy our input into a mutable buffer orig_len_in_bits = (8 * len(message)) \u0026amp; 0xffffffffffffffff message.append(0x80) while len(message)%64 != 56: message.append(0) message += orig_len_in_bits.to_bytes(8, byteorder=\u0026#39;little\u0026#39;) hash_pieces = init_values[:] for chunk_ofst in range(0, len(message), 64): a, b, c, d = hash_pieces chunk = message[chunk_ofst:chunk_ofst+64] for i in range(64): f = functions[i](b, c, d) g = index_functions[i](i) to_rotate = a + f + constants[i] + int.from_bytes(chunk[4*g:4*g+4], byteorder=\u0026#39;little\u0026#39;) new_b = (b + left_rotate(to_rotate, rotate_amounts[i])) \u0026amp; 0xFFFFFFFF a, b, c, d = d, new_b, b, c for i, val in enumerate([a, b, c, d]): hash_pieces[i] += val hash_pieces[i] \u0026amp;= 0xFFFFFFFF return sum(x\u0026lt;\u0026lt;(32*i) for i, x in enumerate(hash_pieces)) def md5_to_hex(digest): raw = digest.to_bytes(16, byteorder=\u0026#39;little\u0026#39;) return \u0026#39;{:032x}\u0026#39;.format(int.from_bytes(raw, byteorder=\u0026#39;big\u0026#39;)) def true_md5(message): m = hashlib.md5() m.update(message) return m.hexdigest() def my_md5(message): return md5_to_hex(md5(message)) if __name__==\u0026#39;__main__\u0026#39;: demo = [b\u0026#34;\u0026#34;, b\u0026#34;a\u0026#34;, b\u0026#34;abc\u0026#34;, b\u0026#34;message digest\u0026#34;, b\u0026#34;abcdefghijklmnopqrstuvwxyz\u0026#34;, b\u0026#34;ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789\u0026#34;, b\u0026#34;12345678901234567890123456789012345678901234567890123456789012345678901234567890\u0026#34;] for message in demo: print(my_md5(message),\u0026#39; \u0026lt;= \u0026#34;\u0026#39;,message.decode(\u0026#39;ascii\u0026#39;),\u0026#39;\u0026#34;\u0026#39;, sep=\u0026#39;\u0026#39;) assert true_md5(message)==my_md5(message) # 若和标准库中不同，会抛出异常 print(\u0026#39;\\nMD5测试全部通过\u0026#39;) # d41d8cd98f00b204e9800998ecf8427e \u0026lt;= \u0026#34;\u0026#34; # 0cc175b9c0f1b6a831c399e269772661 \u0026lt;= \u0026#34;a\u0026#34; # 900150983cd24fb0d6963f7d28e17f72 \u0026lt;= \u0026#34;abc\u0026#34; # f96b697d7cb7938d525a2f31aaf161d0 \u0026lt;= \u0026#34;message digest\u0026#34; # c3fcd3d76192e4007dfb496cca67e13b \u0026lt;= \u0026#34;abcdefghijklmnopqrstuvwxyz\u0026#34; # d174ab98d277d9f5a5611c2c9f419d9f \u0026lt;= \u0026#34;ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789\u0026#34; # 57edf4a22be3c955ac49da2e2107b67a \u0026lt;= \u0026#34;12345678901234567890123456789012345678901234567890123456789012345678901234567890\u0026#34; # # MD5测试全部通过 5.2 Sha256实现 import copy import struct import binascii import hashlib F32 = 0xFFFFFFFF _k = [0x428a2f98, 0x71374491, 0xb5c0fbcf, 0xe9b5dba5, 0x3956c25b, 0x59f111f1, 0x923f82a4, 0xab1c5ed5, 0xd807aa98, 0x12835b01, 0x243185be, 0x550c7dc3, 0x72be5d74, 0x80deb1fe, 0x9bdc06a7, 0xc19bf174, 0xe49b69c1, 0xefbe4786, 0x0fc19dc6, 0x240ca1cc, 0x2de92c6f, 0x4a7484aa, 0x5cb0a9dc, 0x76f988da, 0x983e5152, 0xa831c66d, 0xb00327c8, 0xbf597fc7, 0xc6e00bf3, 0xd5a79147, 0x06ca6351, 0x14292967, 0x27b70a85, 0x2e1b2138, 0x4d2c6dfc, 0x53380d13, 0x650a7354, 0x766a0abb, 0x81c2c92e, 0x92722c85, 0xa2bfe8a1, 0xa81a664b, 0xc24b8b70, 0xc76c51a3, 0xd192e819, 0xd6990624, 0xf40e3585, 0x106aa070, 0x19a4c116, 0x1e376c08, 0x2748774c, 0x34b0bcb5, 0x391c0cb3, 0x4ed8aa4a, 0x5b9cca4f, 0x682e6ff3, 0x748f82ee, 0x78a5636f, 0x84c87814, 0x8cc70208, 0x90befffa, 0xa4506ceb, 0xbef9a3f7, 0xc67178f2] _h = [0x6a09e667, 0xbb67ae85, 0x3c6ef372, 0xa54ff53a, 0x510e527f, 0x9b05688c, 0x1f83d9ab, 0x5be0cd19] def _pad(msglen): mdi = msglen \u0026amp; 0x3F length = struct.pack(\u0026#39;!Q\u0026#39;, msglen \u0026lt;\u0026lt; 3) if mdi \u0026lt; 56: padlen = 55 - mdi else: padlen = 119 - mdi return b\u0026#39;\\x80\u0026#39; + (b\u0026#39;\\x00\u0026#39; * padlen) + length def _rotr(x, y): return ((x \u0026gt;\u0026gt; y) | (x \u0026lt;\u0026lt; (32 - y))) \u0026amp; F32 def _maj(x, y, z): return (x \u0026amp; y) ^ (x \u0026amp; z) ^ (y \u0026amp; z) def _ch(x, y, z): return (x \u0026amp; y) ^ ((~x) \u0026amp; z) class SHA256: _output_size = 8 blocksize = 1 block_size = 64 digest_size = 32 def __init__(self, m=None): self._counter = 0 self._cache = b\u0026#39;\u0026#39; self._k = copy.deepcopy(_k) self._h = copy.deepcopy(_h) self.update(m) def _compress(self, c): w = [0] * 64 w[0:16] = struct.unpack(\u0026#39;!16L\u0026#39;, c) for i in range(16, 64): s0 = _rotr(w[i-15], 7) ^ _rotr(w[i-15], 18) ^ (w[i-15] \u0026gt;\u0026gt; 3) s1 = _rotr(w[i-2], 17) ^ _rotr(w[i-2], 19) ^ (w[i-2] \u0026gt;\u0026gt; 10) w[i] = (w[i-16] + s0 + w[i-7] + s1) \u0026amp; F32 a, b, c, d, e, f, g, h = self._h for i in range(64): s0 = _rotr(a, 2) ^ _rotr(a, 13) ^ _rotr(a, 22) t2 = s0 + _maj(a, b, c) s1 = _rotr(e, 6) ^ _rotr(e, 11) ^ _rotr(e, 25) t1 = h + s1 + _ch(e, f, g) + self._k[i] + w[i] h = g g = f f = e e = (d + t1) \u0026amp; F32 d = c c = b b = a a = (t1 + t2) \u0026amp; F32 for i, (x, y) in enumerate(zip(self._h, [a, b, c, d, e, f, g, h])): self._h[i] = (x + y) \u0026amp; F32 def update(self, m): if not m: return self._counter += len(m) m = self._cache + m for i in range(0, len(m) // 64): self._compress(m[64 * i:64 * (i + 1)]) self._cache = m[-(len(m) % 64):] def digest(self): r = copy.deepcopy(self) r.update(_pad(self._counter)) data = [struct.pack(\u0026#39;!L\u0026#39;, i) for i in r._h[:self._output_size]] return b\u0026#39;\u0026#39;.join(data) def hexdigest(self): return binascii.hexlify(self.digest()).decode(\u0026#39;ascii\u0026#39;) def true_sha256(message): m = hashlib.sha256() m.update(message) return m.hexdigest() def my_sha256(message): return SHA256(message).hexdigest() if __name__==\u0026#39;__main__\u0026#39;: demo = [b\u0026#34;\u0026#34;, b\u0026#34;a\u0026#34;, b\u0026#34;abc\u0026#34;, b\u0026#34;message digest\u0026#34;, b\u0026#34;abcdefghijklmnopqrstuvwxyz\u0026#34;, b\u0026#34;ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789\u0026#34;, b\u0026#34;12345678901234567890123456789012345678901234567890123456789012345678901234567890\u0026#34;] for message in demo: print(my_sha256(message),\u0026#39; \u0026lt;= \u0026#34;\u0026#39;,message.decode(\u0026#39;ascii\u0026#39;),\u0026#39;\u0026#34;\u0026#39;, sep=\u0026#39;\u0026#39;) assert true_sha256(message)==my_sha256(message) # 若和标准库中不同，会抛出异常 print(\u0026#39;\\nSHA256测试全部通过\u0026#39;) # e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855 \u0026lt;= \u0026#34;\u0026#34; # ca978112ca1bbdcafac231b39a23dc4da786eff8147c4e72b9807785afee48bb \u0026lt;= \u0026#34;a\u0026#34; # ba7816bf8f01cfea414140de5dae2223b00361a396177a9cb410ff61f20015ad \u0026lt;= \u0026#34;abc\u0026#34; # f7846f55cf23e14eebeab5b4e1550cad5b509e3348fbc4efa3a1413d393cb650 \u0026lt;= \u0026#34;message digest\u0026#34; # 71c480df93d6ae2f1efad1447c66c9525e316218cf51fc8d9ed832f2daf18b73 \u0026lt;= \u0026#34;abcdefghijklmnopqrstuvwxyz\u0026#34; # db4bfcbd4da0cd85a60c3c37d3fbd8805c77f15fc6b1fdfe614ee0a7c8fdb4c0 \u0026lt;= \u0026#34;ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789\u0026#34; # f371bc4a311f2b009eef952dd83ca80e2b60026c8e935592d0f9c308453c813e \u0026lt;= \u0026#34;12345678901234567890123456789012345678901234567890123456789012345678901234567890\u0026#34; # # SHA256测试全部通过 ","permalink":"https://blog.niuhemoon.win/posts/tech/python-hash-experiment/","summary":"Python3实现哈希散列算法，包含MD5和sha256。 Hash函数算法编写 本实验是使用python来编写MD5和SHA256加密函数，并对加密函数的正确性进行验证。 验证的方式是通过和已有的标准库加密结果进行比较，如果结果相同，则加密函数正确。 1.实验目的 熟悉MD5和SHA25","title":"Python实现MD5和Sha256"},{"content":"程序效果图如下：\n程序的效果就是可以在Terminal浏览一下每天知乎日报的标题和url，然后根据兴趣选择是否继续阅读。\n程序十分简单，只十几行代码。使用python3，需要安装requests包。\n源代码如下，也可以从我的github下载。\n#!/usr/bin/python3 #-*- coding: utf-8 -*- import requests import json headers = {\u0026#39;User-Agent\u0026#39; : \u0026#39;Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)\u0026#39;} def get_daily(): page = requests.get(\u0026#39;http://news-at.zhihu.com/api/3/news/latest\u0026#39;, headers=headers).text response = json.loads(page) date = response[\u0026#39;date\u0026#39;] stories = response[\u0026#39;stories\u0026#39;] date = \u0026#39;\\n%s年%s月%s日\u0026#39; % (date[:4], date[4:6], date[6:]) print(\u0026#39;%s 共%d条日报\\n\u0026#39; % (date, len(stories))) for index,story in enumerate(stories, 1): print(\u0026#39;{0:\u0026lt;2d}：{1:s}\\n url：http://news-at.zhihu.com/story/{2:d}\u0026#39;.format(index, story[\u0026#39;title\u0026#39;], story[\u0026#39;id\u0026#39;])) if __name__ == \u0026#39;__main__\u0026#39;: get_daily() 保存为zhdaily.py文件，然后将文件放到/usr/local/bin/目录下，并给zhdaily.py增加执行权限：\n$ sudo mv zhdaily.py /usr/local/bin/ $ cd /usr/local/bin/ $ sudo chmod +x zhdaily.py 这样，当你下次进入终端，可以直接执行：\n$ zhdaily.py 就可以获得图示的效果。\n碎碎念：\n曾经知乎是一个优秀的社区，也确实让我发现了更大的世界。可是，从某个时刻开始，充斥我时间线的内容都是被知乎官方筛选的。整个社区充斥着喧嚣、广告、营销和带节奏。\n我不喜欢：\n被煽动的愤怒\n被策划的欢乐\n最后，我又回到了RSS的怀抱，可以控制我接收到相对有价值的信息。\n不过，浏览知乎日报能了解下当下热点，增加聊天谈资。省的聊天时都不知道《创造101》是啥？赫赫:)\n我基本每天只浏览日报的标题，这个小程序正好满足需求。\n","permalink":"https://blog.niuhemoon.win/posts/tech/zhihu-daily-crawler/","summary":"程序效果图如下： 程序的效果就是可以在Terminal浏览一下每天知乎日报的标题和url，然后根据兴趣选择是否继续阅读。 程序十分简单，只十几行代码。使用python3，需要安装requests包。 源代码如下，也可以从我的github下载。 #!/usr/bin/python3 #-*- coding: utf-8 -*- import requests import json headers = {\u0026#39;User-Agent\u0026#39; : \u0026#39;Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)\u0026#39;} def get_daily(): page","title":"在Linux终端里浏览知乎日报"},{"content":"上一节中，IntegralOccupancyMap()函数用来确定单词位置，其中调用了query_integral_image()方法。而query_integral_image是用来Cython。下边介绍Cython。\nCython介绍 Cython 的本质可以总结如下：Cython 是包含C 数据类型的Python。\nCython可以将Python代码编译成动态链接库，在某些情况下，可以极大提高Python程序的运行效率。\n可以看到源码中包括了query_integral_image.pyx和query_integral_image.c两个文件。其中.c文件是Cython自动生产的对应query_integral_image.pyxC语言程序。\nCython的工作流程大致如下：\n我们只需要关心.pyx文件中的代码\n为了测试使用Cython是否真的可以提高程序效率，我们做如下测试，\n系统：Ubuntu 16\n环境：python3.5\n依赖：\nCython==0.28.2\nnumpy==1.14.2\nPillow==5.1.0\n测试代码均可从github下载\n测试过程 目录tree\n. ├── python_query_integral_image.py ├── query_integral_image.pyx ├── setup.py ├── test.py └── venv setup.py用来编译动态链接库,内容如下：\nfrom distutils.core import setup from Cython.Build import cythonize setup( ext_modules = cythonize(\u0026#34;query_integral_image.pyx\u0026#34;) ) 执行：\npython setup.py build_ext --inplace 然后目录变为了：\n. ├── build ├── python_query_integral_image.py ├── query_integral_image.c ├── query_integral_image.cpython-35m-i386-linux-gnu.so ├── query_integral_image.pyx ├── setup.py ├── test.py └── venv 下边我们就可以在python程序中import编译后的.so文件了, 测试程序test.py如下：\n# 导入python编写的程序，为了和.so区别，改名为python_query_integral_image from python_query_integral_image import query_integral_image as q1 # 导入经过Cython处理的.so链接库 from query_integral_image import query_integral_image as q2 from random import Random import numpy as np import timeit DX = 3000 DY = 3000 # 相当于一个3000*3000=900万像素的图片 integral = np.zeros((DX, DY), dtype=np.uint32) random_state = Random() start_time = timeit.default_timer() q1(integral, 50, 50, random_state) end_time = timeit.default_timer() q1_dur = (end_time - start_time)/60. start_time = timeit.default_timer() q2(integral, 50, 50, random_state) end_time = timeit.default_timer() q2_dur = (end_time - start_time)/60. print(\u0026#39;C程序耗时\u0026#39;, q2_dur, \u0026#39;Python耗时\u0026#39;, q1_dur) print(\u0026#39;相差%f倍\u0026#39; % (float(q1_dur) / float(q2_dur))) 运行测试程序，结果令人吃惊：\nC程序耗时 0.0007940871333024309 Python耗时 0.6854850105833369 相差863.236516倍\n经过Cython简单的处理，同样的代码，运行效率提高了800多倍。刺不刺激？\n可见在作矩阵计算或者循环次数较多时，Cython具有较大作用。\n参考 Cython三分钟入门 Cython官方文档中文版 ","permalink":"https://blog.niuhemoon.win/posts/tech/wordcloud-src-note2/","summary":"上一节中，IntegralOccupancyMap()函数用来确定单词位置，其中调用了query_integral_image()方法。而query_integral_image是用来Cython。下边介绍Cython。 Cython介绍 Cython 的本质可以总结如下：Cython 是包含C 数","title":"wordcloud源码阅读2——Cython"},{"content":"wordcloud是python用来生成词云的第三方库，github地址是word_cloud\n下载源码：\ngit clone https://github.com/amueller/word_cloud 然后，直接看最老的版本，有精力的话看完最老版本可以再看最新的版本。\ngit tag git checkout 1.2.1 现在我们的目录结构如下： 可以看到，核心代码都在wordcloud目录下：\nwordcloud项目主要用了以下第三方库：\nNumpy PIL Cython 储备知识：\nHSL和HSV色彩空间 图片RGB 框架大体流程： 可以看出wordcloud类的两个核心方法process_text()和generate_from_frequencies()。\n如果输入是处理后的{string:frequency}字典类型，直接生成词云的layout，若输入是未处理过的字符串，就要先进行process_text()统计词频等工作。\n继续看generate_from_frequencies()的流程：\n就得到了词云的layout，后边可以通过recolor()重新上色。\nmask是一个n维数组（图片），在我的理解是photoshop里的蒙板，也叫遮色片。在wordcloud里mask决定了生成词云的位置。白色#FFFFFF称为mask out，即不在白色区域绘制词云。\nlayout is list of tuple ,格式(string, int, (int, int), int, color)) 定义了每个单词的（字符串，字体大小，（x位置，y位置), 方向， 颜色）\n参考：\nwordcloud的API可以参考wordcloud文档中文翻译 具体使用也可以参考网易云音乐歌手词云\n","permalink":"https://blog.niuhemoon.win/posts/tech/wordcloud-src-note1/","summary":"wordcloud是python用来生成词云的第三方库，github地址是word_cloud 下载源码： git clone https://github.com/amueller/word_cloud 然后，直接看最老的版本，有精力的话看完最老版本可以再看最新的版本。 git tag git checkout 1.2.1 现在我们的目录结构如下： 可以看到，核心代码都在wordcloud目录下： wordcloud项目","title":"wordcloud源码阅读1——初探"},{"content":"Python模块wordcloud参考文档的中文翻译 官网链接：wordcloud api reference\nGithub链接：wordcloud\n所有函数均封装在WordCloud类里:\nWordCloud([\u0026hellip;]) 生成并绘制WordCloud对象 ImageColorGenerator(image) 词云颜色生成器（基于图片颜色） random_color_func([]) 词云颜色随机生成 wordcloud.WordCloud class wordcloud.WordCloud(font_path=None, width=400, height=200, margin=2, ranks_only=None, prefer_horizontal=0.9, mask=None, scale=1, color_func=None, max_words=200, min_font_size=4, stopwords=None, random_state=None, background_color=\u0026#39;black\u0026#39;, max_font_size=None, font_step=1, mode=\u0026#39;RGB\u0026#39;, relative_scaling=0.5, regexp=None, collocations=True, colormap=None, normalize_plurals=True) 参数： font_path : string\n需要使用的字体路径(支持OTF和TTF)。Linux系统上默认指向DroidSansMono路径。若使用其他操作系统或者没有DroidSansMono字体，需要指定字体路径。\nwidth : int (default=400)\n画布的宽度。\nheight : int (default=200)\n画布的高度。\nprefer_horizontal : float (default=0.90)\n尝试水平摆放字体和垂直摆放字体的比例，若prefer_horizontal \u0026lt; 1，当摆放不合适的时候，算法将尝试旋转单词。目前没有内置的方法来获取垂直单词\nmask : nd-array or None (default=None)\n如果不是None，则在哪里绘制单词时给出二进制掩码。 如果mask不是None，那么宽度和高度将被忽略，并且将使用mask的形状。 所有白色（#FF或#FFFFFF）条目将被视为“被遮盖”，而其他条目将被自由绘制。 [这在最新版本中发生了变化！]\nscale : float (default=1)\n计算值和绘图之间的缩放比例。 对于大型文字云图像，使用缩放而不是较大的画布尺寸要快得多，但可能导致较粗糙的文字。\nmin_font_size : int (default=4)\n使用最小的字体大小。 当没有更多的空间时停止绘制。\nfont_step : int (default=1)\n字体的步长。 font_step \u0026gt; 1时可能会加快计算速度，但会导致糟糕的字体适应性布局。\nmax_words : number (default=200)\n单词的最大数目。\nstopwords : set of strings or None\n敏感词集合，这些词将被忽略。 如果没有，则将使用内置的STOPWORDS列表。\nbackground_color : color value (default=”black”)\n词云图像的背景颜色。\nmax_font_size : int or None (default=None)\n使用的最大字体大小。 默认使用图像的高度。\nmode : string (default=”RGB”)\n当模式为“RGBA”且background_color为None时，会生成透明背景。\nrelative_scaling : float (default=.5)\n字体大小的相对单词频率的重要性。 relative_scaling = 0时，只考虑单词等级。 使用relative_scaling = 1时，频繁两倍的单词将具有两倍的大小。 如果你想考虑单词的频率，而不仅仅是他们的等级，那么0.5左右的relative_scaling通常看起来不错。\ncolor_func : callable, default=None\n可用参数关键字font_size, position, orientation, font_path, random_state调用，它为每个单词返回一个PIL颜色。 覆盖\u0026quot;colormap\u0026quot;。 请参阅colormap来指定matplotlib的颜色映射。\nregexp : string or None (optional)\n正则表达式将输入文本拆分为待处理文本中的标记。 如果指定None，则使用r“\\ w [\\ w\u0026rsquo;] +”。\ncollocations : bool, default=True\n是否包含两个词的搭配（bigrams），默认为True。\ncolormap : string or matplotlib colormap, default=”viridis”\nMatplotlib色彩映射表为每个单词随机绘制颜色。 如果指定了“color_func”，则忽略。\nnormalize_plurals : bool, default=True\n是否从文字中删除“尾随”。 如果为真，当出现以\u0026rsquo;s\u0026rsquo;结尾的单词，则以\u0026rsquo;s\u0026rsquo;结尾的单词将被删除，并将其计数添加到没有\u0026rsquo;s\u0026rsquo;结尾的版本\n以\u0026rsquo;ss\u0026rsquo;结尾的单词被忽略。\n注意： 较大的画布使代码明显变慢。 如果您需要较大的单词云，请尝试较小的画布大小，然后设置缩放参数。\n根据最大字体大小和缩放启发式算法，相比单词实际出现的频率，算法可能会给单词的等级更多的权重。\n属性： words_\n(dict of string to float) 单词标识对应其出现次数，如{\u0026lsquo;hello\u0026rsquo;:90, \u0026lsquo;good\u0026rsquo;:30}\n.. versionchanged: 2.0 words_: 现在是一个字典\nlayout_\n(list of tuples (string, int, (int, int), int, color)))\n编码拟合词云，为每个单词编码字符串，字体大小，位置，方向和颜色。\n方法： # 初始化实例 __init__(font_path=None, width=400, height=200, margin=2, ranks_only=None, prefer_horizontal=0.9, mask=None, scale=1, color_func=None, max_words=200, min_font_size=4, stopwords=None, random_state=None, background_color=\u0026#39;black\u0026#39;, max_font_size=None, font_step=1, mode=\u0026#39;RGB\u0026#39;, relative_scaling=0.5, regexp=None, collocations=True, colormap=None, normalize_plurals=True) fit_words(frequencies)\n根据单词和频率创建一个word_cloud。\n别名为generate_from_frequencies。\n参数:\nfrequencies : array of tuples\n元组包含单词及其频率。Note:最新版已经改为字典了。\n返回值：\nself\ngenerate(text)\n从文本生成wordcloud。\n别名generate_from_text。\n实际调用process_text和generate_from_frequencies。\n返回值：\nself\ngenerate_from_frequencies(frequencies, max_font_size=None)\n根据单词和频率创建词云。\n参数:\nfrequencies : dict from string to float\n字典包含单词及其频率。\nmax_font_size : int\n使用此最大字体大，而不是self.max_font_size\n返回值：\nself\ngenerate_from_text(text)\n从文本生成wordcloud。\n实际调用process_text和generate_from_frequencies。\n..versionchanged:: 1.2.2 process_text（)的返回值不再是generate_from_frequencies（）的参数。\n返回值：\nself\nprocess_text(text)\n将长文本拆分为单词，去除停用词(敏感词)。\n参数:\ntext : string\n待处理的文本\n返回值：\nwords : dict (string, int)\n带有关联频率的词语标记。\n..versionchanged:: 1.2.2 将返回类型从元组列表更改为字典。\nNotes\n有更好的方法做词频分析，在此不做赘述。\nrecolor(random_state=None, color_func=None, colormap=None)\n重新着色现有布局。\n应用新的着色要比生成整个词云快得多。\n参数:\nrandom_state : RandomState, int, or None, default=None\n如果不是None，则使用固定的随机状态。 如果给出了一个int，它将用作random.Random状态的种子。\ncolor_func : function or None, default=None\n根据(word count, font size, position and orientation)字数，字体大小，位置和方向生成新颜色的函数。 如果为None，则使用self.color_func。\ncolormap : string or matplotlib colormap, default=None\n使用此颜色映射表来生成新的颜色。 如果指定了color_func，则忽略。 如果没有，则使用self.color_func（或self.color_map）。\n返回值：\nself\nto_array()\n转换为numpy数组。\n返回值：\nimage : n维数组 (width, height, 3)\n词云图像作为numpy矩阵。\nto_file(filename)\n导出为图像文件。\n参数:\nfilename : string\n要写入的位置。\n返回值：\nself\nwordcloud.ImageColorGenerator class wordcloud.ImageColorGenerator(image) 基于彩色图像的颜色生成器.\n根据RGB图像生成颜色。 一个单词将使用彩色图像中包围矩形的平均颜色进行着色。 构造完成后，该对象充当可调用对象，可以将其作为color_func传递给WordCloud类的构造函数或recolor这个重新着色方法。\n参数:\nimage : n维矩阵, shape (height, width, 3)\n用于生成文字颜色的图像。 Alpha通道被忽略。对于wordcloud实例，这应该和画布大小相同。\n方法\n__call__(word, font_size, font_path, ...)\n使用特定图像为给定单词生成颜色。\n__init__(image)\nwordcloud.random_color_func wordcloud.random_color_func(word=None, font_size=None, position=None, orientation=None, font_path=None, random_state=None) 随机色调颜色生成.\n默认着色方法。 这只是随机选择一个值为80％和光照50％的色调。\n参数:\nword, font_size, position, orientation: ignored.\nrandom_state : random.Random object or None, (default=None)\n如果给定了一个随机对象，则用它来生成随机数。\n","permalink":"https://blog.niuhemoon.win/posts/tech/word-cloud-doc-cn/","summary":"Python模块wordcloud参考文档的中文翻译 官网链接：wordcloud api reference Github链接：wordcloud 所有函数均封装在WordCloud类里: WordCloud([\u0026hellip;]) 生成并绘制WordCloud对象 ImageColorGenerator(image) 词云颜色生成器（基于图片颜色） random_color_func([]) 词云颜色随机生成 wordcloud.WordCloud class wordcloud.WordCloud(font_path=None, width=400, height=200, margin=2, ranks_only=None, prefer_horizontal=0.9, mask=None, scale=1, color_func=None, max_words=200, min_font_size=4, stopwords=None, random_state=None, background_color=\u0026#39;black\u0026#39;, max_font_size=None, font_step=1,","title":"WordCloud文档中文翻译"},{"content":"This is my first blog\n美国诗人纳·斯待尔在87岁那年写过一首诗：《我会采更多的雏菊》，摘取片段于下。\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e 如果我能够从头活过， 我会试着犯更多的错。 我会放松一点，我会灵活一点。 我会比这一趟过得傻。 很少有什么事情能让我当真。 我会疯狂一些，我会少讲点卫生。 我会冒更多的险。我会更经常的旅行。 我会爬更多的山，游更多的河，看更多的日落。 我会多吃冰激凌，少吃豆子。 我会惹更多的麻烦，可是不在想象中担忧。 噢，我有过难忘的时刻。 如果我能够重来一次，我会要更多这样的时刻。 我会更经常的逃学。 我不会考那么高的分数，除非是一不小心。 我会多骑些旋转木马， 我会采更多的雏菊。 人不仅要拥有此生此世，还要那诗意的世界\n","permalink":"https://blog.niuhemoon.win/posts/read/my-first-blog/","summary":"This is my first blog 美国诗人纳·斯待尔在87岁那年写过一首诗：《我会采更多的雏菊》，摘取片段于下。 \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e 如果我能够从头活过， 我会试着犯更多的错。 我会放松一点，我会灵活一点。 我会比这一趟过得傻。 很少有什么事情能让我当真。 我会疯狂一些，我会少讲点卫生。 我会冒更多的险。我会更经常的旅行。 我会爬更","title":"一首小诗"},{"content":" 一名软件工程师\n","permalink":"https://blog.niuhemoon.win/about/","summary":"一名软件工程师","title":"🙋🏻‍♂️关于我"},{"content":"matlba基础和简单用法\n命令行基础 x = [1,2,3] x = 1:3 A = [1,2;3,4;5,6] B = ones(3,4) C = eye(4) #单位矩阵 D = zeros(3,4)\t#零矩阵 det(C)\t#行列式 x = inv(A)*b\u0026#39;\t#解线性方程,等价于A^-1*b\u0026#39; e = eig(A) #返回一个列向量，其中包含方阵 A 的特征值。 [V,D] = eig(A) #返回特征值的对角矩阵 D 和矩阵 V，其列是对应的右特征向量，使得 A*V = V*D norm(x) #计算向量范数和矩阵范数 gallery()\t#生成测试矩阵 dot(x,y)\t#向量点乘 cross(x,y)\t#向量叉乘 A.*B\t#矩阵点乘（对应元素相同，要求两个矩阵大小相同） A*B\t#矩阵叉乘（要求A的列数等于B的行数） whos\t#查看当前工作空间 class()\t#数据类型 函数 function y = sinh( x ) %UNTITLED Summary of this function goes here % Detailed explanation goes here y = (exp(x) - exp(-x))/2; end 循环 \u0026gt;\u0026gt; for i = [1 3 5 7 9] disp(i); end \u0026gt;\u0026gt; E = randn(1000,1); \u0026gt;\u0026gt; SSE = 0; \u0026gt;\u0026gt; for i = 1:1000 SSE = SSE + E(i)*E(i); end \u0026gt;\u0026gt; SSE SSE = 983.5102 \u0026gt;\u0026gt; SSE/1000 ans = 0.9835 #对一段程序计时 tic E = randn(1000,1); SSE = 0; for i = 1:1000 SSE = SSE + E(i)*E(i); end MSE = SSE/1000; toc #Elapsed time is 0.016618 seconds. # 和点乘比较速度,明显点乘较快 tic E = randn(1000,1); MSE = E.*E /1000; toc # Elapsed time is 0.010042 seconds. 选择 x=1:10; y=zeros(1,10); for i =1:10 if mod(x(i),2) == 0 y(i) = 1; else y(i) = 0; end end #另一种写法，Sum输出为18 X = 1:10;\tSum = 0; for x = X\t#x会遍历X中的元素 if mod(x,3) == 0 Sum =Sum +x; end end x = 1:10 found = 0; i = 0; while ~found i = i +1 if x(i) == 8 disp(\u0026#39;I found it\u0026#39;); found = 1; end end # or for i =1:10 fprintf(\u0026#39;i = %d\\n\u0026#39;,i); if x(i) ==8 disp(\u0026#39;i found it\u0026#39;); break; end end 数据结构 结构体 \u0026gt;\u0026gt; my_struct.name=\u0026#39;niuhe\u0026#39; my_struct = name: \u0026#39;niuhe\u0026#39; \u0026gt;\u0026gt; class(my_struct) ans = struct \u0026gt;\u0026gt; my_struct.age = 25 my_struct = name: \u0026#39;niuhe\u0026#39; age: 25 \u0026gt;\u0026gt; class(my_struct.name) ans = char \u0026gt;\u0026gt; isfield(my_struct,\u0026#39;name\u0026#39;) ans = 1 \u0026gt;\u0026gt; isfield(my_struct,\u0026#39;gender\u0026#39;) ans = 0 \u0026gt;\u0026gt; rmfield(my_struct,\u0026#39;age\u0026#39;) ans = name: \u0026#39;niuhe\u0026#39; \u0026gt;\u0026gt; setfield(my_struct,\u0026#39;gender\u0026#39;,\u0026#39;f\u0026#39;) ans = name: \u0026#39;niuhe\u0026#39; age: 25 gender: \u0026#39;f\u0026#39; \u0026gt;\u0026gt; S = struct(\u0026#39;name\u0026#39;,\u0026#39;bob\u0026#39;,\u0026#39;age\u0026#39;,32,\u0026#39;email\u0026#39;,\u0026#39;123@gmail.com\u0026#39;) S = name: \u0026#39;bob\u0026#39; age: 32 email: \u0026#39;123@gmail.com\u0026#39; 哈希表 \u0026gt;\u0026gt; my_cell{1} = \u0026#39;hello world.\u0026#39; my_cell = \u0026#39;hello world.\u0026#39; \u0026gt;\u0026gt; my_cell{\u0026#39;A\u0026#39;} = [1,2,3,4,5] my_cell = Columns 1 through 9 \u0026#39;hello world.\u0026#39; [] [] [] [] [] [] [] [] Columns 10 through 20 [] [] [] [] [] [] [] [] [] [] [] Columns 21 through 31 [] [] [] [] [] [] [] [] [] [] [] Columns 32 through 42 [] [] [] [] [] [] [] [] [] [] [] Columns 43 through 53 [] [] [] [] [] [] [] [] [] [] [] Columns 54 through 64 [] [] [] [] [] [] [] [] [] [] [] Column 65 [1x5 double] \u0026gt;\u0026gt; my_cell{1} ans = hello world. \u0026gt;\u0026gt; my_cell{\u0026#39;A\u0026#39;} ans = 1 2 3 4 5 plot画图 \u0026gt;\u0026gt; x = [0 0.1 0.2 0.3 ]; \u0026gt;\u0026gt; y = 1:4; \u0026gt;\u0026gt; plot(x,y); \u0026gt;\u0026gt; x = linspace(0,100,200); \u0026gt;\u0026gt; y =sin(x); \u0026gt;\u0026gt; plot(x,y); \u0026gt;\u0026gt; x = linspace(0,2*pi,100); \u0026gt;\u0026gt; y1 = sin(x); \u0026gt;\u0026gt; y2 = cos(x); \u0026gt;\u0026gt; plot(x,y1,x,y2); \u0026gt;\u0026gt; plot(x,y1,\u0026#39;-\u0026#39;,x,y2,\u0026#39;.\u0026#39;); \u0026gt;\u0026gt; plot(x,y1,\u0026#39;--\u0026#39;,x,y2,\u0026#39;.\u0026#39;); \u0026gt;\u0026gt; bar(x)\t#画柱状图 \u0026gt;\u0026gt; x = randn(1000,1)\t#生成1000*1的随机数矩阵，服从均值是0，方差是1的正太分布\t\u0026gt;\u0026gt; hist(x) \u0026gt;\u0026gt; hist(x,50) \u0026gt;\u0026gt; x = 1:5; \u0026gt;\u0026gt; pie(x); \u0026gt;\u0026gt; x = linspace(0,2*pi,1000); \u0026gt;\u0026gt; y = 10*sin(x) + randn(1,1000); \u0026gt;\u0026gt; plot(x,y); \u0026gt;\u0026gt; scatter(x,y); \u0026gt;\u0026gt; x = randn(1000,1) * 2; \u0026gt;\u0026gt; y = 5*sin(x) + rand(1000,1); \u0026gt;\u0026gt; plot(x,y); \u0026gt;\u0026gt; scatter(x,y); subplot()\t#子图 surf() contour() title() xlabel() ylabel() 读写文件 CSV文件\ncsvread() csvwrite() # 保存现有工作空间 save() load() ","permalink":"https://blog.niuhemoon.win/posts/tech/matlab-basic/","summary":"\u003cp\u003ematlba基础和简单用法\u003c/p\u003e","title":"Matlab 基础"},{"content":"Ankidroid和插件 ankidroid下载网址https://apps.ankiweb.net/\n目前还是推荐下载anki2.0旧版，2.1版插件支持的不全。\n必装插件列表：\nAwesome TTS:301952613 Review Heatmap:1771074083 Night Mode:1496166067 词库分享\nanki-deck 从网络抓取单词/例句文本 示例从轻松背单词网站抓取，网站上涵盖了从小学到GRE以及各个专业的单词和例句。内容非常丰富，希望大家多支持这个良心网站。本抓取方法仅作示例，侵删。 网站爬取需要两个参数：\nbook_id group_id 具体爬取代码参见我的github,anki_spider\n食用方法 将爬取下来的文本，保存为文本文件\n编辑单词书的字段，可自定义样式进行美化\n打开anki,选择文件 ——\u0026gt; 导入，文件类型为以tab分割的文件类型，并允许使用HTML，匹配对应字段\n导入结果示意图\n用awesomeTTS批量添加单词和句子发音\n导出制作好的ankidroid单词包，并分享\n完毕\n","permalink":"https://blog.niuhemoon.win/posts/tech/anki-python/","summary":"Ankidroid和插件 ankidroid下载网址https://apps.ankiweb.net/ 目前还是推荐下载anki2.0旧版，2.1版插件支持的不全。 必装插件列表： Awesome TTS:301952613 Review Heatmap:1771074083 Night Mode:1496166067 词库分享 anki-deck 从网络抓取单词/例句文本 示例从轻松背单词网站抓取，网站上涵盖了从小学到GRE以及各个","title":"Python 生成ankidroid单词表/语音包"},{"content":"STM32F103学习笔记 GPIO初始化和读写操作 STM32的GPIO引脚有多种模式使用，在使用前需要进行配置。\nLED灯实验 #include \u0026#34;stm32f10x.h\u0026#34; #define LED GPIO_Pin_All\tvoid delay(u32 i) { while(i--); } //LED的GPIO初始化程序 void LED_Init() { GPIO_InitTypeDef GPIO_InitStructure; //GPIO时钟初始化 SystemInit(); RCC_APB2PeriphClockCmd(RCC_APB2Periph_GPIOC,ENABLE); //配置GPIO模式和端口 GPIO_InitStructure.GPIO_Pin = LED; GPIO_InitStructure.GPIO_Mode = GPIO_Mode_Out_PP; //推挽模式 GPIO_InitStructure.GPIO_Speed = GPIO_Speed_50MHz; GPIO_Init(GPIOC,\u0026amp;GPIO_InitStructure);\t//初始化GPIO } void led_display() { GPIO_SetBits(GPIOC,LED); delay(6000000); GPIO_ResetBits(GPIOC,LED); delay(6000000); } int main() { LED_Init(); while(1) { led_display(); } } 蜂鸣器实验 使用无源蜂鸣器\n#include \u0026#34;stm32f10x.h\u0026#34; #define BZ GPIO_Pin_5 //PB5\t定义端口PB5 void delay(u32 i) { while(i--); } void BEEP_Init() { GPIO_InitTypeDef GPIO_InitStructure; //GPIO时钟初始化 SystemInit(); RCC_APB2PeriphClockCmd(RCC_APB2Periph_GPIOB,ENABLE); //配置GPIO模式和端口 GPIO_InitStructure.GPIO_Pin = BZ; GPIO_InitStructure.GPIO_Mode = GPIO_Mode_Out_PP; //推挽模式 GPIO_InitStructure.GPIO_Speed = GPIO_Speed_50MHz; GPIO_Init(GPIOB,\u0026amp;GPIO_InitStructure);\t//初始化GPIO } void sound(u32 i) // i= 5000救护车，i=1000电动车 { while(i--) { GPIO_SetBits(GPIOB,BZ); delay(i); GPIO_ResetBits(GPIOB,BZ); delay(i); } } int main() {\tBEEP_Init();\t//端口初始化 while(1) { sound(5000);\t}\t} SysTick实验 系统滴答计时器比延时函数更加精确，可移植性更高。systick定时器是24位的定时器，当定时器计数到0时，将自动从RELOAD寄存器中重装定时器初值，如果开启了中断，此时还会产生异常中断信号。\n定时器必须要一个时钟来驱动，systick定时器的时钟来源时系统时钟，不过它的时钟可以选择成直接取自系统时钟，也可以将系统时钟8分频后再赋给systick定时器。\nsystick定时器的操作步骤：\n设置systick定时器的时钟源 设置systick定时器的重装初始值（若使用中断，就将中断使能打开） 清零systick定时器计数器的值 打开systick定时器 #include \u0026#34;stm32f10x.h\u0026#34; #define LED GPIO_Pin_All void delay_us(u32 i) { u32 temp; SysTick-\u0026gt;LOAD = 9*i;\t//设置重装数值 72MHz时 SysTick-\u0026gt;CTRL = 0x01;\t//使能，减到零时无动作，采用外部时钟源(8分频系统时钟) SysTick-\u0026gt;VAL = 0;\t//清零计数器 do { temp = SysTick-\u0026gt;CTRL;\t//读取当前计数值 } while((temp \u0026amp; 0x01) \u0026amp;\u0026amp; (!(temp \u0026amp;(1\u0026lt;\u0026lt;16))));\t//与运算取出指定位的数值 //实际上就是CTRL寄存器的第1位为0或第16位为1时，退出循环 SysTick-\u0026gt;CTRL=0;\t//关闭计数器 SysTick-\u0026gt;VAL = 0;\t//清空计数器 } void delay_ms(u32 i) { u32 temp; SysTick-\u0026gt;LOAD=9000*i;\t//设置重装数值, 72MHZ时 SysTick-\u0026gt;CTRL=0x01;\t//使能，减到零是无动作，采用外部时钟源 SysTick-\u0026gt;VAL=0;\t//清零计数器 do { temp=SysTick-\u0026gt;CTRL;\t//读取当前倒计数值 } while((temp\u0026amp;0x01)\u0026amp;\u0026amp;(!(temp\u0026amp;(1\u0026lt;\u0026lt;16))));\t//等待时间到达 SysTick-\u0026gt;CTRL=0;\t//关闭计数器 SysTick-\u0026gt;VAL=0;\t//清空计数器 } //LED的GPIO初始化程序 void LED_Init() { GPIO_InitTypeDef GPIO_InitStructure; //GPIO时钟初始化 SystemInit(); RCC_APB2PeriphClockCmd(RCC_APB2Periph_GPIOC,ENABLE); //配置GPIO模式和端口 GPIO_InitStructure.GPIO_Pin = LED; GPIO_InitStructure.GPIO_Mode = GPIO_Mode_Out_PP; //推挽模式 GPIO_InitStructure.GPIO_Speed = GPIO_Speed_50MHz; GPIO_Init(GPIOC,\u0026amp;GPIO_InitStructure);\t//初始化GPIO } int main() { u8 i; LED_Init(); while(1) { for(i=0;i\u0026lt;8;i++) { GPIO_Write(GPIOC,(u16)~(0x01\u0026lt;\u0026lt;i)); delay_ms(1000);\t//精确延时1秒 } } } 系统时钟实验 STM32一共可以有4个时钟源\nHSI（High Speed Inner)内部自带的高速时钟，单片机启动后默认使用的时钟源 HSE (High Speed External)外部高速时钟，大多数时8M晶振 LSE（Low Speed External）外部低速时钟，给单片机内部RTC提供时钟 LSI，内部低速时钟，主要给单片机内部RTC和看门狗提供时钟 STM32的系统时钟源，有3个时钟来源：\n直接来自内部高速时钟HSI 直接来自外部的高速时钟HSE 将HSI或HSE进行处理，倍频之后的PLL时钟（Phase-Locked Loops锁相环） STM32设置RCC（复位和时钟控制）时钟的步骤\n以设置外部高速时钟作为PLL输入，然后以PLL作为时钟源的例子：\n复位RCC时钟 打开HSE外部高速时钟 监测HSE外部高速时钟是否开启成功 设置FLASH读写 设置AHB总线的分频，还有APB1和APB2的分频，注：AHB和APB2最大频率72MHz，APB1做大频率36MHz 设置HSE外部高速时钟作为PLLs时钟的时钟输入 设置PLL时钟的倍频倍数 打开PLL时钟的使能 等待PLL时钟的开启成功 将系统时钟源设置为PLL时 等待时钟源切换成功 //在上一个实验基础上 //自定义系统时钟 void RCC_HSE_Configuration() { RCC_DeInit();\t//重置RCC外设寄存器 RCC_HSEConfig(RCC_HSE_ON);\t//设置外部高速晶振（HSE） if(RCC_WaitForHSEStartUp() == SUCCESS)\t//等待HSE起振 { RCC_HCLKConfig(RCC_SYSCLK_Div1);\t//设置AHB时钟 RCC_PCLK1Config(RCC_HCLK_Div2);\t//设置低速AHB时钟（PCLK1） RCC_PCLK2Config(RCC_HCLK_Div1);\t//设置高速AHB时钟（PCLK2） RCC_PLLConfig(RCC_PLLSource_HSE_Div2,RCC_PLLMul_9);//设置PLL时钟源及倍频系数，实际系统时钟36MHz RCC_PLLCmd(ENABLE);\t//PLL使能 while(RCC_GetFlagStatus(RCC_FLAG_PLLRDY)==RESET);//检查指定的RCC标志位设置与否，PLL就绪 RCC_SYSCLKConfig(RCC_SYSCLKSource_PLLCLK);\t//设置系统时钟（SYSCLK） while(RCC_GetSYSCLKSource()网易 != 0x08);\t//返回用作系统时钟的时钟源，0x08，PLL作为系统时钟 } } int main() { LED_Init(); RCC_HSE_Configuration();\t//自定义系统时钟，修改倍频或分频参数 while(1) { GPIO_SetBits(GPIOC,LED); delay_ms(500);\t//精确延时0.5s，实际延时1s GPIO_ResetBits(GPIOC,LED); delay_ms(500); } } 按键实验 注意按键5ms-10ms左右的延时消抖，注意按键的上拉还是下拉\n#include \u0026#34;stm32f10x.h\u0026#34; #define K_UP GPIO_Pin_0 //PA0 #define K_DOWN GPIO_Pin_3 //PE3 #define K_LEFT GPIO_Pin_2 //PE2 #define K_RIGHT GPIO_Pin_4 //PE4 #define k_up GPIO_ReadInputDataBit(GPIOA,K_UP)\t//获取按键的状态 #define k_down GPIO_ReadInputDataBit(GPIOE,K_DOWN) #define k_left GPIO_ReadInputDataBit(GPIOE,K_LEFT) #define k_right GPIO_ReadInputDataBit(GPIOE,K_RIGHT) #define LED GPIO_Pin_All\tvoid delay_ms(u32 i) { u32 temp; SysTick-\u0026gt;LOAD=9000*i;\t//设置重装数值, 72MHZ时 SysTick-\u0026gt;CTRL=0x01;\t//使能，减到零是无动作，采用外部时钟源 SysTick-\u0026gt;VAL=0;\t//清零计数器 do { temp=SysTick-\u0026gt;CTRL;\t//读取当前倒计数值 } while((temp\u0026amp;0x01)\u0026amp;\u0026amp;(!(temp\u0026amp;(1\u0026lt;\u0026lt;16))));\t//等待时间到达 SysTick-\u0026gt;CTRL=0;\t//关闭计数器 SysTick-\u0026gt;VAL=0;\t//清空计数器 } //LED的GPIO初始化程序 void LED_Init() { GPIO_InitTypeDef GPIO_InitStructure; //GPIO时钟初始化 SystemInit(); RCC_APB2PeriphClockCmd(RCC_APB2Periph_GPIOC,ENABLE); //配置GPIO模式和端口 GPIO_InitStructure.GPIO_Pin = LED; GPIO_InitStructure.GPIO_Mode = GPIO_Mode_Out_PP; //推挽模式 GPIO_InitStructure.GPIO_Speed = GPIO_Speed_50MHz; GPIO_Init(GPIOC,\u0026amp;GPIO_InitStructure);\t//初始化GPIO } void key_init(void) { GPIO_InitTypeDef GPIO_InitStructure; SystemInit(); //开启GPIO时钟 RCC_APB2PeriphClockCmd(RCC_APB2Periph_GPIOA|RCC_APB2Periph_GPIOE,ENABLE); /* 配置GPIO的模式和IO口 */ GPIO_InitStructure.GPIO_Pin=K_UP;\t//选择你要设置的IO口 GPIO_InitStructure.GPIO_Mode=GPIO_Mode_IPD;//下拉输入 GPIO_InitStructure.GPIO_Speed=GPIO_Speed_50MHz;\t//设置传输速率 GPIO_Init(GPIOA,\u0026amp;GPIO_InitStructure);\t/* 初始化GPIO */ GPIO_InitStructure.GPIO_Pin=K_DOWN|K_LEFT|K_RIGHT; GPIO_InitStructure.GPIO_Mode=GPIO_Mode_IPU;\t//上拉输入 GPIO_InitStructure.GPIO_Speed=GPIO_Speed_50MHz; GPIO_Init(GPIOE,\u0026amp;GPIO_InitStructure); GPIO_ResetBits(GPIOA,K_UP);\t//对K_UP初始化输出0 } void key_pros()\t//按键处理函数 { if(k_up==1)\t//判断按键k_up是否按下 { delay_ms(10); //消抖处理 if(k_up==1)\t//再次判断按键k_up是否按下 { GPIO_Write(GPIOC,(u16)0xfe);\t} while(k_up); //等待按键松开 } if(k_down==0) { delay_ms(10); if(k_down==0) { GPIO_Write(GPIOC,(u16)(0xfd));\t} while(!k_down); } if(k_left==0) { delay_ms(10); if(k_left==0) { GPIO_Write(GPIOC,(u16)(0xfb));\t} while(!k_left); } if(k_right==0) { delay_ms(10); if(k_right==0) { GPIO_Write(GPIOC,(u16)(0xf7));\t} while(!k_right); }\t} int main() {\tLED_Init();\t//LED初始化 key_init();\t//按键端口初始化函数 GPIO_Write(GPIOC,(u16)(0xff)); while(1) {\tkey_pros();\t//按键处理函数\t}\t} 数码管实验 #include \u0026#34;stm32f10x.h\u0026#34; #define smg_duan (GPIO_Pin_0|GPIO_Pin_1|GPIO_Pin_2|GPIO_Pin_3|GPIO_Pin_4|GPIO_Pin_5|GPIO_Pin_6|GPIO_Pin_7)//PC0~PC7 u8 smgduan[16]={0x3F, 0x06, 0x5B, 0x4F, 0x66, 0x6D, 0x7D, 0x07, 0x7F, 0x6F, 0x77, 0x7C, 0x39, 0x5E, 0x79, 0x71};//0~F 数码管段选数据 void delay_ms(u32 i) { u32 temp; SysTick-\u0026gt;LOAD=9000*i;\t//设置重装数值, 72MHZ时 SysTick-\u0026gt;CTRL=0x01;\t//使能，减到零是无动作，采用外部时钟源 SysTick-\u0026gt;VAL=0;\t//清零计数器 do { temp=SysTick-\u0026gt;CTRL;\t//读取当前倒计数值 } while((temp\u0026amp;0x01)\u0026amp;\u0026amp;(!(temp\u0026amp;(1\u0026lt;\u0026lt;16))));\t//等待时间到达 SysTick-\u0026gt;CTRL=0;\t//关闭计数器 SysTick-\u0026gt;VAL=0;\t//清空计数器 } void smg_init() { GPIO_InitTypeDef GPIO_InitStructure; //声明一个结构体变量，用来初始化GPIO /* 开启GPIO时钟 */ RCC_APB2PeriphClockCmd(RCC_APB2Periph_GPIOC,ENABLE); /* 配置GPIO的模式和IO口 */ GPIO_InitStructure.GPIO_Pin=smg_duan;\t//选择你要设置的IO口 GPIO_InitStructure.GPIO_Mode=GPIO_Mode_Out_PP; GPIO_InitStructure.GPIO_Speed=GPIO_Speed_50MHz; GPIO_Init(GPIOC,\u0026amp;GPIO_InitStructure);\t/* 初始化GPIO */ } void static_smg_display()\t//静态数码管显示 {\tu8 i; for(i=0;i\u0026lt;16;i++) { GPIO_Write(GPIOC,(u16)(~smgduan[i])); delay_ms(1000);\t}\t} int main() {\tsmg_init();\t//数码管端口初始化函数 while(1) { static_smg_display();\t//静态数码管显示 }\t} 中断和定时器 外部中断实验 注意将端口引脚映射到外部中断线路上，注意配置中断优先级\n#include \u0026#34;stm32f10x.h\u0026#34; #include \u0026#34;stm32f10x_exti.h\u0026#34; #include \u0026#34;misc.h\u0026#34; #define k_left GPIO_Pin_2 #define LED GPIO_Pin_All void delay_ms(u32 i) { u32 temp; SysTick-\u0026gt;LOAD=9000*i;\t//设置重装数值, 72MHZ时 SysTick-\u0026gt;CTRL=0x01;\t//使能，减到零是无动作，采用外部时钟源 SysTick-\u0026gt;VAL=0;\t//清零计数器 do { temp=SysTick-\u0026gt;CTRL;\t//读取当前倒计数值 } while((temp\u0026amp;0x01)\u0026amp;\u0026amp;(!(temp\u0026amp;(1\u0026lt;\u0026lt;16))));\t//等待时间到达 SysTick-\u0026gt;CTRL=0;\t//关闭计数器 SysTick-\u0026gt;VAL=0;\t//清空计数器 } //LED的GPIO初始化程序 void led_init(void) { GPIO_InitTypeDef GPIO_InitStructure; //GPIO时钟初始化 SystemInit(); RCC_APB2PeriphClockCmd(RCC_APB2Periph_GPIOC,ENABLE); //配置GPIO模式和端口 GPIO_InitStructure.GPIO_Pin = LED; GPIO_InitStructure.GPIO_Mode = GPIO_Mode_Out_PP; //推挽模式 GPIO_InitStructure.GPIO_Speed = GPIO_Speed_50MHz; GPIO_Init(GPIOC,\u0026amp;GPIO_InitStructure);\t//初始化GPIO GPIO_SetBits(GPIOC,LED); } void exti_init(void)\t//外部中断初始化 { GPIO_InitTypeDef GPIO_InitStructure; EXTI_InitTypeDef EXTI_InitStructure; NVIC_InitTypeDef NVIC_InitStructure; SystemInit(); //开启GPIO时钟，用到了时引脚复用功能，开启复用时钟 RCC_APB2PeriphClockCmd(RCC_APB2Periph_AFIO,ENABLE); RCC_APB2PeriphClockCmd(RCC_APB2Periph_GPIOE,ENABLE); GPIO_InitStructure.GPIO_Pin = k_left; GPIO_InitStructure.GPIO_Mode = GPIO_Mode_IPU;\t//上拉输入 GPIO_InitStructure.GPIO_Speed = GPIO_Speed_50MHz; GPIO_Init(GPIOE,\u0026amp;GPIO_InitStructure); //选择外部中断线路对应的GPIO管脚，此处是PE2 GPIO_EXTILineConfig(GPIO_PortSourceGPIOE,GPIO_PinSource2); //设置外部中断的模式 EXTI_InitStructure.EXTI_Line = EXTI_Line2; EXTI_InitStructure.EXTI_Mode = EXTI_Mode_Interrupt; EXTI_InitStructure.EXTI_Trigger = EXTI_Trigger_Falling;\t//下降沿触发 EXTI_InitStructure.EXTI_LineCmd = ENABLE; EXTI_Init(\u0026amp;EXTI_InitStructure); //设置NVIC参数（nested vector interrupt config) NVIC_PriorityGroupConfig(NVIC_PriorityGroup_1); NVIC_InitStructure.NVIC_IRQChannel = EXTI2_IRQn;\t//打开EXTI2的全局中断 NVIC_InitStructure.NVIC_IRQChannelPreemptionPriority = 0;\t//抢占优先级为0 NVIC_InitStructure.NVIC_IRQChannelSubPriority = 0;\t//响应优先级为0 NVIC_InitStructure.NVIC_IRQChannelCmd = ENABLE;\t//使能 NVIC_Init(\u0026amp;NVIC_InitStructure); } void EXTI2_IRQHandler(void)\t//外部中断2中断处理函数 { if(EXTI_GetITStatus(EXTI_Line2)==SET) { EXTI_ClearITPendingBit(EXTI_Line2);//清除EXTI2线路挂起位 delay_ms(10);//消抖 if(GPIO_ReadInputDataBit(GPIOE,k_left)==Bit_RESET)\t//k_left按下 { delay_ms(10);//消抖 if(GPIO_ReadOutputDataBit(GPIOC,GPIO_Pin_0)==Bit_RESET) { //LED熄灭 GPIO_SetBits(GPIOC,GPIO_Pin_0);\t} else { //LED发光 GPIO_ResetBits(GPIOC,GPIO_Pin_0); } } while(GPIO_ReadInputDataBit(GPIOE,GPIO_Pin_2)==0);\t//等待按键松开 }\t} int main() { led_init(); exti_init(); while(1); } 定时器实验 STM32中一共有11个定时器：\n2个高级控制定时器\t（TIM1,TIM8) 4个通用定时器 （TIM2-TIM5） 2个基本定时器 (TIM6,TIm7) 2个看门狗定时器 1个系统滴答定时器 注:TIM2-TIM7的时钟由APB1产生，TIM1和TIM8是由APB2产生时钟\n//实现1s流水灯实验 #include \u0026#34;stm32f10x.h\u0026#34; #include \u0026#34;stm32f10x_tim.h\u0026#34; #include \u0026#34;stm32f10x_exti.h\u0026#34; #include \u0026#34;misc.h\u0026#34; #define LED GPIO_Pin_All //LED的GPIO初始化程序 void led_init(void) { GPIO_InitTypeDef GPIO_InitStructure; //GPIO时钟初始化 SystemInit(); RCC_APB2PeriphClockCmd(RCC_APB2Periph_GPIOC,ENABLE); //配置GPIO模式和端口 GPIO_InitStructure.GPIO_Pin = LED; GPIO_InitStructure.GPIO_Mode = GPIO_Mode_Out_PP; //推挽模式 GPIO_InitStructure.GPIO_Speed = GPIO_Speed_50MHz; GPIO_Init(GPIOC,\u0026amp;GPIO_InitStructure);\t//初始化GPIO } void time_init() { TIM_TimeBaseInitTypeDef TIM_TimeBaseInitStructure;\t//定时器初始化结构体 NVIC_InitTypeDef NVIC_InitStructure; //开启定时器3时钟 RCC_APB1PeriphClockCmd(RCC_APB1Periph_TIM3,ENABLE); TIM_ClearITPendingBit(TIM3,TIM_IT_Update);\t//清除TIMx的中断待处理位：TIM中断源 TIM_TimeBaseInitStructure.TIM_Period = 2000;//设置自动重装寄存器周期的值 TIM_TimeBaseInitStructure.TIM_Prescaler = 36000-1;//设置作为TIMx时钟频率的预分频值，2Khz计数频率 TIM_TimeBaseInitStructure.TIM_ClockDivision = 0;//设置时钟分割：TDTS = Tck_tim TIM_TimeBaseInitStructure.TIM_CounterMode = TIM_CounterMode_Up;//向上计数模式 TIM_TimeBaseInit(TIM3,\u0026amp;TIM_TimeBaseInitStructure); TIM_Cmd(TIM3,ENABLE);\t//使能或失能TIMx外设 //设置中断参数，并打开中断 TIM_ITConfig(TIM3,TIM_IT_Update,ENABLE);\t//使能或失能指定的TIM中断 //设置NVIC参数 NVIC_PriorityGroupConfig(NVIC_PriorityGroup_1); NVIC_InitStructure.NVIC_IRQChannel = TIM3_IRQn;\t//打开EXTI2的全局中断 NVIC_InitStructure.NVIC_IRQChannelPreemptionPriority = 0;\t//抢占优先级为0 NVIC_InitStructure.NVIC_IRQChannelSubPriority = 1;\t//响应优先级为1 NVIC_InitStructure.NVIC_IRQChannelCmd = ENABLE;\t//使能 NVIC_Init(\u0026amp;NVIC_InitStructure); } void TIM3_IRQHandler()\t//定时器3的中断处理函数 { static u8 i = 0; TIM_ClearITPendingBit(TIM3,TIM_IT_Update); GPIO_Write(GPIOC,(u16)~(0x01\u0026lt;\u0026lt;i++)); if(i==8)i=0; } int main() { time_init(); led_init(); while(1); } ","permalink":"https://blog.niuhemoon.win/posts/tech/stm32-example-1/","summary":"STM32F103学习笔记 GPIO初始化和读写操作 STM32的GPIO引脚有多种模式使用，在使用前需要进行配置。 LED灯实验 #include \u0026#34;stm32f10x.h\u0026#34; #define LED GPIO_Pin_All void delay(u32 i) { while(i--); } //LED的GPIO初始化程序 void LED_Init() { GPIO_InitTypeDef GPIO_InitStructure; //GPIO时钟初始化 SystemInit(); RCC_APB2PeriphClockCmd(RCC_APB2Periph_GPIOC,ENABLE); //配置GPIO模式和端口 GPIO_InitStructure.GPIO_Pin = LED; GPIO_InitStructure.GPIO_Mode = GPIO_Mode_Out_PP; //推挽模式 GPIO_InitStructure.GPIO_Speed = GPIO_Speed_50MHz; GPIO_Init(GPIOC,\u0026amp;GPIO_InitStructure); //初始化","title":"STM32笔记"},{"content":" 目前ROS只支持Linux版，如果不方便装Linux主机，可以通过以太网桥接的方式获得雷达的数据帧；对一些串口传感器，也可以通过Serial to USB，然后在virtualbox里选择对应的USB设备进行调试。\n本方法适用于通过以太网/WLAN来传送数据包的雷达。\n使用Virtualbox里建立虚拟机，将雷达连接到主机电脑， 设置以太网的IP地址为雷达的目标地址 虚拟机桥接到对应的以太网 在虚拟机中测试收发数据帧 tcpdump ","permalink":"https://blog.niuhemoon.win/posts/tech/virual-lidar/","summary":"目前ROS只支持Linux版，如果不方便装Linux主机，可以通过以太网桥接的方式获得雷达的数据帧；对一些串口传感器，也可以通过Serial to USB，然后在virtualbox里选择对应的USB设备进行调试。 本方法适用于通过以太网/WLAN来传送数据包的雷达。 使用Virtualb","title":"Virtualbox虚拟机连接雷达和UART串口"}]